{
  "0": {
    "question": "How does the performance of the Filtering algorithm compare to the performance of MLE with noise?",
    "ground_truth": "The Filtering algorithm performs better than MLE with noise in both the random tree and random graph settings.",
    "selected_figures_names": [
      "1606.07384v2-Figure1-1.png",
      "1606.07384v2-Figure2-1.png"
    ],
    "all_figures_names": [
      "1606.07384v2-Figure1-1.png",
      "1606.07384v2-Figure2-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 1,
    "rationale": "Image 1 directly shows the performance comparison of the Filtering algorithm and MLE with noise against the fraction of corrupted samples. Text 1 discusses the performance of the filtering algorithm in the presence of corrupted samples, which is relevant to the question.",
    "answer": "The Filtering algorithm performs better than MLE with noise as the fraction of corrupted samples increases. The error for Filtering remains lower compared to MLE with noise, indicating better robustness to noise.",
    "text_chunks": [
      "The experiments show that our filtering algorithm outperforms \\texttt{MLE and \\texttt{RANSAC, and that the error of our algorithm degrades gracefully as \\eps increases. It is worth noting that even the ALARM network does not satisfy our balancedness assumption on the parameters, our algorithm still performs well on it and recovers the conditional probability table of ALARM in the presence of corrupted samples. \\ifthenelse{\\boolean{hasAppendix{ \\paragraph{Details of the RANSAC Algorithm.",
      "We are interested in whether our filtering algorithm can learn a Bayes net that is ``close'' to ALARM when samples are corrupted; and how many corrupted samples can our algorithm tolerate. For \\eps = [0.05, 0.1, \\ldots, 0.4], we draw N = 10^6 samples, where a (1-\\eps)-fraction of the samples come from ALARM, and the other \\eps-fraction comes from a noise distribution. In Figure~, we compare the performance of (1) our filtering algorithm, (2) the empirical conditional means with noise, and (3) a RANSAC-based algorithm.",
      "We use the error of the empirical conditional mean without noise (i.e., MLE estimator with only good samples) as the gold standard, since this is the best one could hope for even if all the corrupted samples are identified. We tried various graph structures for the Bayes net P and noise distributions, and similar patterns arise for all of them. In the top figure, the dependency graph of P is a randomly generated tree, and the noise distribution is a binary product distribution; In the bottom figure, the dependency graph of P is a random graph, and the noise distribution is the tree Bayes net used as the ground truth in the first experiment.",
      "Our experiments show that our filtering algorithm works very well in this setting, even when the assumptions under which we can prove theoretical guarantees are not satisfied. This complements our theoretical results and illustrates that our algorithm is not limited by these assumptions and can apply to more general settings in practice. In Figure~, we compare the performance of (1) our filtering algorithm, (2) the empirical conditional probability table with noise, and (3) a RANSAC-based algorithm (see the end of Section~ for a detailed description).",
      "However, we will prove that our filter rejects more samples from E than S, so |E|+|L| must get smaller. We will prove Theorem~ by iteratively running the following efficient filtering procedure: \\end{proposition If this algorithm produces a subset S'', then we iterate using S'' in place of S'. We will present the algorithm establishing Proposition in the following section.",
      "\\ifthenelse{\\boolean{hasAppendix{ The main difference between the proof of correctness of our algorithm and those given in lies in analyzing the mean, covariance and tail bounds of F(X,q), and showing that its mean and covariance are well-behaved when q is close to p (see Lemma~ in Appendix~).{ Main Technical Lemma and Proof of Theorem~ First, we need to show that a large enough set of samples with no noise satisfy properties we expect from a representative set of samples. We need that the mean, covariance, and tail bounds of F(X,p) behave like we would expect them to.",
      "Our evaluation allowed us to verify the accuracy and the sample complexity rates of our theoretical results. In all cases, the experiments validate the usefulness of our algorithm, which significantly outperforms previous approaches, almost exactly matching the best rate without noise. \\paragraph{Related Work.",
      "We assume that S is \\eps-good. In the beginning, we have |E|+|L| \\leq 2 \\eps |S|. As we add filters in each iteration, E gets smaller and L gets larger."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 1,\n    \"Rationale\": \"Image 1 directly shows the performance comparison of the Filtering algorithm and MLE with noise against the fraction of corrupted samples. Text 1 discusses the performance of the filtering algorithm in the presence of corrupted samples, which is relevant to the question.\",\n    \"Answer\": \"The Filtering algorithm performs better than MLE with noise as the fraction of corrupted samples increases. The error for Filtering remains lower compared to MLE with noise, indicating better robustness to noise.\"\n}"
  },
  "1": {
    "question": "Which method performs the best when there is a high fraction of corrupted samples?",
    "ground_truth": "RANSAC",
    "selected_figures_names": [
      "1606.07384v2-Figure2-1.png"
    ],
    "all_figures_names": [
      "1606.07384v2-Figure1-1.png",
      "1606.07384v2-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 provides a visual comparison of different methods' performance against the fraction of corrupted samples, which directly answers the question. Text 3 discusses the performance of the filtering algorithm and other methods, providing context for the results shown in the image.",
    "answer": "The filtering method performs the best when there is a high fraction of corrupted samples.",
    "text_chunks": [
      "\\ifthenelse{\\boolean{hasAppendix{ This happens with high probability. The details are given in Lemma~ in Appendix~. { We call a set of samples that satisfies these properties \\eps-good for P. Our algorithm takes as input an \\eps-corrupted multiset S' of N = \\wt \\Omega(m \\log (1/\\tau) / \\eps^2) samples. We write S'=(S \\setminus L) \\cup E, where S is the set of samples before corruption, L contains the good samples that have been removed or (in later iterations) incorrectly rejected by filters, and E represents the remaining corrupted samples.",
      "If we produce a filter, we then iterate on those samples that pass the filter. To produce a filter, we compute a matrix M which is roughly the empirical covariance matrix of F(X). We show that if the corruptions are sufficient to notably disrupt the sample mean of F(X), there must be many erroneous samples that are all far from the mean in roughly the same direction, and we can detect this direction by looking at the largest eigenvector of M. If we project all samples onto this direction, concentration bounds of F(X) will imply that almost all samples far from the mean are erroneous, and thus filtering them out will provide a cleaner set of samples.",
      "We know that we have enough samples that the empirical conditional probability table with no noise \\wt p is a good approximation to p. Therefore, we will be in good shape so long as the corruption of our samples does not introduce a large error in the conditional probability table. Thinking more concretely about this error, we may split it into two parts: L, the subtractive error, and E the additive error.",
      "We are interested in whether our filtering algorithm can learn a Bayes net that is ``close'' to ALARM when samples are corrupted; and how many corrupted samples can our algorithm tolerate. For \\eps = [0.05, 0.1, \\ldots, 0.4], we draw N = 10^6 samples, where a (1-\\eps)-fraction of the samples come from ALARM, and the other \\eps-fraction comes from a noise distribution. In Figure~, we compare the performance of (1) our filtering algorithm, (2) the empirical conditional means with noise, and (3) a RANSAC-based algorithm.",
      "Intuitively, given a set of good samples (from the true model), an adversary is allowed to inspect the samples before corrupting them, both by adding corrupted points and deleting good samples. In contrast, in Huber's model, the adversary is oblivious to the samples and is only allowed to add bad points. We would like to design robust learning algorithms for Question~ whose sample complexity, N, is close to the information-theoretic minimum, and whose computational complexity is polynomial in N.",
      "In the {\\em non-robust setting, this learning problem is straightforward: the ``empirical estimator'' (which coincides with the maximum likelihood estimator) is known to be sample and computationally efficient~. In sharp contrast, even this most basic regime is surprisingly challenging in the robust setting. For example, the very special case of robustly learning a Bernoulli product distribution (corresponding to an empty network with no edges) was analyzed only recently in~. To formally state our results, we first give a detailed description of the corruption model we study.",
      "\\begin{definition[\\eps-Corrupted Samples] Given 0 < \\eps < 1/2 and a distribution family \\mathcal{P, the algorithm specifies some number of samples N, and N samples X_1, X_2, \\ldots, X_N are drawn from some (unknown) ground-truth P \\in \\mathcal{P. The adversary is allowed to inspect P and the samples, and replaces \\eps N of them with arbitrary points.",
      "Recall that our algorithm takes as input an \\eps-corrupted multiset S' of N = \\wt \\Omega(m \\log(1/\\tau) /\\eps^2) samples from a d-dimensional ground-truth Bayesian network P. We write S'=(S \\setminus L) \\cup E, where S is the set of samples before corruption, L contains the good samples that have been removed or (in later iterations) incorrectly rejected by filters, and E represents the remaining corrupted samples. We mapped each sample X to a vector F(X,q) which contains information about the empirical conditional means q."
    ],
    "raw_gpt": "{\n  \"Image\": 0,\n  \"Text\": 3,\n  \"Rationale\": \"Image 0 provides a visual comparison of different methods' performance against the fraction of corrupted samples, which directly answers the question. Text 3 discusses the performance of the filtering algorithm and other methods, providing context for the results shown in the image.\",\n  \"Answer\": \"The filtering method performs the best when there is a high fraction of corrupted samples.\"\n}"
  }
}