{
  "0": {
    "question": "What are the main differences between the educational philosophies of the Bonaparte and Voltaire schools?",
    "ground_truth": "The Bonaparte school focuses on outdoor physical activities, maneuvers, and strategies, with a specialization in horse riding, lances, and swords. They aim to develop students into good leaders. The Voltaire school, on the other hand, encourages independent thinking and focuses on indoor activities. They aim to instill good moral values and develop students into philosophical thinkers.",
    "selected_figures_names": [
      "1611.04684v1-Table1-1.png"
    ],
    "all_figures_names": [
      "1611.04684v1-Table3-1.png",
      "1611.04684v1-Table2-1.png",
      "1611.04684v1-Table1-1.png",
      "1611.04684v1-Table4-1.png",
      "1611.04684v1-Table5-1.png",
      "1611.04684v1-Table6-1.png",
      "1611.04684v1-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides a direct comparison between the Bonaparte and Voltaire schools, highlighting their educational philosophies. Text 0 discusses the challenge of extracting useful features from complex sentences, which relates to understanding the nuanced differences in the schools' philosophies.",
    "answer": "The Bonaparte school focuses on outdoor physical activities, leadership, and strategies, while the Voltaire school emphasizes independent thinking, indoor activities, and moral values.",
    "text_chunks": [
      "These models only rely on the text within a pair to perform matching, whereas we find that sentences in a pair could have very complicated semantic and syntactic structures, and it is difficult for the-state-of-the-art neural models to extract useful features from such sentences to bridge the semantic gaps in the text pair. Table gives an example from community QA to illustrate the challenge. The answer is very long and contains a lot of information that well compare the two schools but semantically far from the question (e.g., ``horse riding'' and ``lances swords'').",
      "From Table , we can see that advanced neural networks, such as CNTN, MV-LSTM, LSTM_a and KEHNN, outperform JAIST's model, indicating that hand-crafted features are less powerful than deep learning methods. Models that match text pairs by interaction representations like Arc2 and MatchPyramid are not better than models that perform matching with sentence embeddings like Arc1. This is because the training data is small and we fixed the word embedding in learning.",
      "We denote the two models as LSTM and LSTM_a. MV-LSTM: the model generates an interaction vector by combining hidden states of two sentences given by a shared BiLSTM. Then the interaction vector is fed to an MLP to compute the matching score.",
      "The larger an entry of k_w is, the more information from the corresponding entry of e_w will be kept in \\widetilde{e_w. In contrast, the smaller an entry of k_w is, the more information from the corresponding entry of \\mathbf{k_x will flow into \\widetilde{e_w. Since k_w is determined by both e_w and \\mathbf{k_x and learned from training data, it will keep the useful parts in the representations of w and the prior knowledge and at the same time filter out noise from them.",
      "We considered the following models as baselines: Multi-layer perceptron (MLP): each sentence is represented as a vector by averaging its word vectors. The two vectors were fed to a two-layer feedforward neural network to calculate a matching score. MLP shared the embedding tables with our model.",
      "In order to construct \\mathbf{k_{x,i and \\mathbf{k_{y,i, we separately assigned a topic to a message and a response by the inference algorithm of Twitter LDA. Then we transformed the topic to a vector by averaging the embeddings of top 20 words under the topic. Word embedding tables were initialized using the public word vectors available at \\url{http://nlp.stanford.edu/projects/glove (trained on Twitter) and updated in learning.",
      "MatchPyramid: the model proposed by Pang et al. who match two sentences using an approach of image recognition. The model is a special case of our model with only channel one. LSTMs: sentence vectors are generated by the last hidden state of LSTM , or the attentive pooling result of all hidden states .",
      "This is because we ignored context for short message-response pairs, while long pairs are usually independent with context and have complete semantics. Furthermore, we also report the contributions of different channels of our model in Table . We can see that channel two is the most powerful one on the conversation data, while channel three is the best one on the QA data."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides a direct comparison between the Bonaparte and Voltaire schools, highlighting their educational philosophies. Text 0 discusses the challenge of extracting useful features from complex sentences, which relates to understanding the nuanced differences in the schools' philosophies.\",\n    \"Answer\": \"The Bonaparte school focuses on outdoor physical activities, leadership, and strategies, while the Voltaire school emphasizes independent thinking, indoor activities, and moral values.\"\n}"
  },
  "1": {
    "question": "Which model performs the best for response selection, and how can we tell?",
    "ground_truth": "The KEHNN model performs the best for response selection. This is evident because it achieves the highest scores across all metrics (R$2$@1, R${10}$@1, R${10}$@2, and R${10}$@5) compared to all other models in the table.",
    "selected_figures_names": [
      "1611.04684v1-Table4-1.png"
    ],
    "all_figures_names": [
      "1611.04684v1-Table3-1.png",
      "1611.04684v1-Table2-1.png",
      "1611.04684v1-Table1-1.png",
      "1611.04684v1-Table4-1.png",
      "1611.04684v1-Table5-1.png",
      "1611.04684v1-Table6-1.png",
      "1611.04684v1-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 5,
    "rationale": "Image 0 provides a table with evaluation results for different models on response selection, showing performance metrics. Text 5 explains why LSTM-based models generally perform better and highlights KEHNN's superior performance due to its use of sequential information and prior knowledge.",
    "answer": "The KEHNN model performs the best for response selection. This is evident from the table in Image 0, where KEHNN has the highest scores across all metrics. Text 5 supports this by explaining that KEHNN leverages both sequential information and prior knowledge, leading to statistically significant improvements.",
    "text_chunks": [
      "As a consequence, our knowledge gate can learn a better representation for matching with the prior knowledge. Experiments We tested our model on two matching tasks: answer selection for question answering and response selection for conversation. Baseline",
      "It is worthy to note that the gap between different methods is not big. This is because answers labeled as \"potential\" only cover 10\\% of the data and are hard to predict. Response Selection Response selection is important for building retrieval-based chatbots .",
      "In the data set, as the training data becomes large and we updated word embedding in learning, Arc2 and MatchPyraimd are much better than Arc1. LSTM based models perform better than CNN based models, which is consistent with the results in the QA task. Discussions We first investigate the performance of KEHNN in terms of text length, as shown in Table .",
      "Answer Selection The goal of answer selection is to recognize high quality answers in answer candidates of a question. We used a public data set of answer selection in SemEval 2015 , % \\url{http://alt.qcri.org/semeval2015/task3/, which collects question-answer pairs from Qatar Living Forum and requires to classify the answers into 3 categories (i.e. C=3 in our model) including good, potential and bad.",
      "The training set contains 1 million message-response pairs with a ratio 1 : 1 between positive and negative responses, and both the validation set and the test set have 0.5 million message-response pairs with a ratio 1 : 9 between positive and negative responses. We followed Lowe et al. and employed recall at position k in n candidates as evaluation metrics and denoted the metrics as R_n @ k. R_n @ k indicates if the correct response is in the top k results from n candidates.",
      "LSTM based models in general performs better than CNN based models, because they can capture sequential information in sentences. KEHNN leverages both the sequential information and the prior knowledge from categories in matching by a CNN with multiple channels. Therefore, it outperforms all other methods, and the improvement is statistically significant (t-test with p-value \\leq 0.05).",
      "DeepMatch_{topic: the matching model proposed in which only used topic information to perform matching. CNNs: the Arc1 model and the Arc2 model proposed by Hu et al. \\shortcite{hu2014convolutional. CNTN: the convolution neural tensor network proposed for community question answering.",
      "Tanh is used as h in Equation (), (), (). \\subsubsection{Results Table reports the evaluation results on response selection. Our method outperforms baseline models on all metrics, and the improvement is statistically significant (t-test with p-value \\leq 0.01)."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 5,\n    \"Rationale\": \"Image 0 provides a table with evaluation results for different models on response selection, showing performance metrics. Text 5 explains why LSTM-based models generally perform better and highlights KEHNN's superior performance due to its use of sequential information and prior knowledge.\",\n    \"Answer\": \"The KEHNN model performs the best for response selection. This is evident from the table in Image 0, where KEHNN has the highest scores across all metrics. Text 5 supports this by explaining that KEHNN leverages both sequential information and prior knowledge, leading to statistically significant improvements.\"\n}"
  },
  "2": {
    "question": "Which model performs best on the Ubuntu dataset for text lengths between 60 and 90 words?",
    "ground_truth": "KEHNN",
    "selected_figures_names": [
      "1611.04684v1-Table5-1.png"
    ],
    "all_figures_names": [
      "1611.04684v1-Table3-1.png",
      "1611.04684v1-Table2-1.png",
      "1611.04684v1-Table1-1.png",
      "1611.04684v1-Table4-1.png",
      "1611.04684v1-Table5-1.png",
      "1611.04684v1-Table6-1.png",
      "1611.04684v1-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides accuracy data for different models on the Ubuntu dataset, specifically for text lengths between 60 and 90 words. Text 0 discusses the performance of KEHNN and MV-LSTM on long texts, which is relevant to the question.",
    "answer": "KEHNN performs best on the Ubuntu dataset for text lengths between 60 and 90 words.",
    "text_chunks": [
      "From the results, we can see that on relatively short text (i.e., length in [0,30) ), KEHNN performs comparably well with MV-LSTM, while on long text, KEHNN significantly improves the matching accuracy. The results verified our claim that matching with multiple channels and prior knowledge can enhance accuracy on long text. Note that on the Ubuntu data, all models perform worse on short text than them on long text.",
      "In the data set, as the training data becomes large and we updated word embedding in learning, Arc2 and MatchPyraimd are much better than Arc1. LSTM based models perform better than CNN based models, which is consistent with the results in the QA task. Discussions We first investigate the performance of KEHNN in terms of text length, as shown in Table .",
      "For example, in a public QA data set, 54.8\\% question answer pairs are longer than 60 words (question length plus answer length). More seriously, the-state-of-the-art model can only achieves 74.2\\% matching accuracy on pairs longer than 60 words compared to its performance 78.8\\% on pairs shorter than 30 words. These evidence indicates us that improving matching performance on pairs with long text is important but challenging, because the semantic gap is even bigger in such pairs.",
      "We implemented all baselines and KEHNN by an open-source deep learning framework Theano . For all baselines and our model, we set the dimension of word embedding (i.e.,d) as 100 and the maximum text length (i.e., I and J) as 200. In LSTMs, MV-LSTM, and BiGRU in our model, we set the dimension of hidden states as 100 (i.e., m).",
      "DeepMatch_{topic: the matching model proposed in which only used topic information to perform matching. CNNs: the Arc1 model and the Arc2 model proposed by Hu et al. \\shortcite{hu2014convolutional. CNTN: the convolution neural tensor network proposed for community question answering.",
      "For all baselines and our model, the word embedding and the topic model (in DeepMatch_{topic) were trained on a Qatar living raw text provided by SemEval-2015 . We fixed the word embedding during the training process, and set h in Equation (), (), () as ReLU. \\subsubsection{Results JAIST, the champion of the task in SemEval15, used 12 features and an SVM classifier and achieved an accuracy of 0.725.",
      "The goal of the task is to select a proper response for a message from a candidate pool to realize human-machine conversation. We used a public English conversation data set, the Ubuntu Corpus , to conduct the experiment. The corpus consists of a large number of human-human dialogue about Ubuntu technique.",
      "Typical models in this group include DSSM , NTN , CDSSM , Arc1 , CNTN , and LSTMs . These methods, however, lose useful information in sentence representation, and leads to the emergence of methods in the second group. The second group matches text pairs by an interaction representation of sentences which allows them to meet at the first step."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides accuracy data for different models on the Ubuntu dataset, specifically for text lengths between 60 and 90 words. Text 0 discusses the performance of KEHNN and MV-LSTM on long texts, which is relevant to the question.\",\n    \"Answer\": \"KEHNN performs best on the Ubuntu dataset for text lengths between 60 and 90 words.\"\n}"
  },
  "3": {
    "question": "What is the role of the knowledge gates in the KEHNN architecture?",
    "ground_truth": "The knowledge gates are responsible for selecting relevant information from the knowledge base and incorporating it into the model's hidden state.",
    "selected_figures_names": [
      "1611.04684v1-Figure1-1.png"
    ],
    "all_figures_names": [
      "1611.04684v1-Table3-1.png",
      "1611.04684v1-Table2-1.png",
      "1611.04684v1-Table1-1.png",
      "1611.04684v1-Table4-1.png",
      "1611.04684v1-Table5-1.png",
      "1611.04684v1-Table6-1.png",
      "1611.04684v1-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides a visual representation of the KEHNN architecture, including the knowledge gates, which are central to understanding their role. Text 0 explains the function of the knowledge gates in filtering and enhancing relevant information.",
    "answer": "The knowledge gates in the KEHNN architecture control the flow of information by determining how much information from the words is retained and how much from the prior knowledge is incorporated. This helps in filtering out noise and strengthening useful information.",
    "text_chunks": [
      "% Given a text pair, KEHNN exploits a knowledge gate to fuse the semantic information carried by the prior knowledge into the representation of words and generates a knowledge enhanced representation for each word. The knowledge gate is a non-linear unit and controls how much information from the word is kept in the new representation and how much information from the prior knowledge flows to the representation. By this means, noise from the irrelevant words is filtered out, and useful information from the relevant words is strengthened.",
      "S_x and S_y in KEHNN shared word embeddings, knowledge embeddings, parameters of BiGRUs, and parameters of the knowledge gates. All tuning was conducted on validation sets. The activation functions in baselines are the same as those in our model.",
      "Matching with Multiple Channels \\caption{Architecture of KEHNN \\end{figure* With the knowledge enhanced representations, we propose a knowledge enhanced hybrid neural network (KEHNN) which conducts matching with multiple channels. Figure gives the architecture of our model.",
      "It strengthens the effect of its semantically similar words like ``kids'',``child'' and ``activity'' in QA matching, and at the same time reduce the influence of ``horse riding'' and ``lances swords'' to matching. With the tag as a bridge, the semantic relation between the question and the answer can be identified, which is difficult to achieve only by themselves. We propose a knowledge enhanced hybrid neural network (KEHNN) to leverage the prior knowledge in matching.",
      "KEHNN inherits the advantage of 2D CNN that matching two objects by letting them meet at the beginning. Moreover, it constructs interaction matrices by considering multiple matching features. Therefore semantic relations between the two objects can be sufficiently modeled and leveraged in building the matching function.",
      "In the combination, the knowledge gate element-wisely controls how much information from word w is preserved, and how much information from prior knowledge \\mathbf{k_x flows in. The advantage of the element-wise operation is that it offers a way to precisely control the contributions of prior knowledge and words in matching. Entries of k_w lie in [0,1].",
      "Evaluation results show that KEHNN can significantly outperform the-state-of-the-art matching methods, and particularly improve the matching accuracy on long text. Our contributions in this paper are three-folds: 1) proposal of leveraging prior knowledge to improve matching on long text; 2) proposal of a knowledge enhanced hybrid neural network which incorporates prior knowledge into matching in a general way and conducts matching on multiple levels; 3) empirical verification of the effectiveness of the proposed method on two public data sets. Related Work Early work on semantic matching is based on bag of words and employs statistical techniques like LDA and translation models to overcome the semantic gaps.",
      "To learn g(\\cdot,\\cdot), we need to answer two questions: 1) how to use prior knowledge in matching; 2) how to perform matching with both text pairs and prior knowledge. In the following sections, we first describe our method on incorporating prior knowledge into matching, then we show details of our model. Knowledge Gate Inspired by the powerful gate mechanism which controls information in and out when processing sequential data with recurrent neural networks (RNN), we propose using knowledge gates to incorporate prior knowledge into matching."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides a visual representation of the KEHNN architecture, including the knowledge gates, which are central to understanding their role. Text 0 explains the function of the knowledge gates in filtering and enhancing relevant information.\",\n    \"Answer\": \"The knowledge gates in the KEHNN architecture control the flow of information by determining how much information from the words is retained and how much from the prior knowledge is incorporated. This helps in filtering out noise and strengthening useful information.\"\n}"
  },
  "4": {
    "question": "How does the average number of answers per question differ between the training and development sets? What might be a possible explanation for this difference?",
    "ground_truth": "The training set has a higher average number of answers per question (6.36) compared to the development set (5.48). This suggests that questions in the training set tend to have more potential answers associated with them than those in the development set.",
    "selected_figures_names": [
      "1611.04684v1-Table2-1.png"
    ],
    "all_figures_names": [
      "1611.04684v1-Table3-1.png",
      "1611.04684v1-Table2-1.png",
      "1611.04684v1-Table1-1.png",
      "1611.04684v1-Table4-1.png",
      "1611.04684v1-Table5-1.png",
      "1611.04684v1-Table6-1.png",
      "1611.04684v1-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 provides the statistics needed to compare the average number of answers per question between the training and development sets. Text 1 explains the evaluation metrics used in the dataset, which is relevant for understanding the data structure.",
    "answer": "The average number of answers per question is 6.36 in the training set and 5.48 in the development set. A possible explanation for this difference could be the different ratios of positive to negative responses in the datasets, as mentioned in Text 1.",
    "text_chunks": [
      "For example, in a public QA data set, 54.8\\% question answer pairs are longer than 60 words (question length plus answer length). More seriously, the-state-of-the-art model can only achieves 74.2\\% matching accuracy on pairs longer than 60 words compared to its performance 78.8\\% on pairs shorter than 30 words. These evidence indicates us that improving matching performance on pairs with long text is important but challenging, because the semantic gap is even bigger in such pairs.",
      "The training set contains 1 million message-response pairs with a ratio 1 : 1 between positive and negative responses, and both the validation set and the test set have 0.5 million message-response pairs with a ratio 1 : 9 between positive and negative responses. We followed Lowe et al. and employed recall at position k in n candidates as evaluation metrics and denoted the metrics as R_n @ k. R_n @ k indicates if the correct response is in the top k results from n candidates.",
      "It is worthy to note that the gap between different methods is not big. This is because answers labeled as \"potential\" only cover 10\\% of the data and are hard to predict. Response Selection Response selection is important for building retrieval-based chatbots .",
      "We set the initial learning rate and the batch size as 0.01 and 50 respectively. Prior Knowledge Acquisition Prior knowledge plays a key role to the success of our model. As described above, in learning, we expect prior knowledge to represent global context of input.",
      "This is because the prior knowledge in the conversation data is automatically generated rather than obtained from meta-data like that in the QA data. The automatically generated prior knowledge contains noise which hurts the performance of channel three. The full model outperforms all single channels consistently, demonstrating that matching with multiple channels could leverage the three types of features and sufficiently model the semantic relations in text pairs.",
      "In the data set, as the training data becomes large and we updated word embedding in learning, Arc2 and MatchPyraimd are much better than Arc1. LSTM based models perform better than CNN based models, which is consistent with the results in the QA task. Discussions We first investigate the performance of KEHNN in terms of text length, as shown in Table .",
      "As a consequence, our knowledge gate can learn a better representation for matching with the prior knowledge. Experiments We tested our model on two matching tasks: answer selection for question answering and response selection for conversation. Baseline",
      "These models only rely on the text within a pair to perform matching, whereas we find that sentences in a pair could have very complicated semantic and syntactic structures, and it is difficult for the-state-of-the-art neural models to extract useful features from such sentences to bridge the semantic gaps in the text pair. Table gives an example from community QA to illustrate the challenge. The answer is very long and contains a lot of information that well compare the two schools but semantically far from the question (e.g., ``horse riding'' and ``lances swords'')."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 provides the statistics needed to compare the average number of answers per question between the training and development sets. Text 1 explains the evaluation metrics used in the dataset, which is relevant for understanding the data structure.\",\n    \"Answer\": \"The average number of answers per question is 6.36 in the training set and 5.48 in the development set. A possible explanation for this difference could be the different ratios of positive to negative responses in the datasets, as mentioned in Text 1.\"\n}"
  }
}