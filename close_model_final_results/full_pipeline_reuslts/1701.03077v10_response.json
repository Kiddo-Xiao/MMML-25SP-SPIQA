{
  "0": {
    "question": "How does the shape of the IRLS weight function change as the shape parameter \u03b1 increases?",
    "ground_truth": "The IRLS weight function becomes more peaked and concentrated around zero as the shape parameter \u03b1 increases.",
    "selected_figures_names": [
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure6-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 1,
    "rationale": "Image 1 shows the general loss function and its gradient for different values of the shape parameter \u03b1, which is directly related to the IRLS weight function. Text 1 discusses the IRLS weights and their relation to the shape parameter, explaining how reducing \u03b1 attenuates the effect of outliers.",
    "answer": "As the shape parameter \u03b1 increases, the IRLS weight function becomes less robust to outliers, resembling a non-normalized form of Student's t-distribution. This means the influence of larger residuals increases, making the function more sensitive to outliers.",
    "text_chunks": [
      "At the origin the IRLS weight of our loss is \\frac{1{c^2: For all values of \\power, when \\abs{x is small with respect to c the loss is well-approximated by a quadratic bowl: Because the second derivative of the loss is maximized at x = 0, this quadratic approximation tells us that the second derivative is bounded from above: When \\power is negative the loss approaches a constant as |x| approaches infinity, letting us bound the loss: The loss's \\Psi-function increases monotonically with respect to \\power when \\power < 2 for all values of z in [0, 1]: The roots of the second derivative of \\lossfun{x, \\power, c are: This tells us at what value of x the loss begins to redescend. This point has a magnitude of c when \\power = -\\infty, and that magnitude increases as \\power increases. The root is undefined when \\power \\geq 1, as our loss is redescending iff \\power < 1.",
      "\\over 2 \\left( \\sfrac{x{c \\right)^2 z + \\Psi(z, \\power) \\right) \\end{align \\Psi(z, \\power) is not defined when \\power \\geq 2 because for those values the loss is no longer robust, and so is not well described as a process that rejects outliers. We can also derive our loss's weight function to be used during iteratively reweighted least squares : \\end{equation Curiously, these IRLS weights resemble a non-normalized form of Student's t-distribution. These weights are not used in any of our experiments, but they are an intuitive way to demonstrate how reducing \\power attenuates the effect of outliers.",
      "Loss Function The simplest form of our loss function is: Here \\power \\in \\mathbb{R is a shape parameter that controls the robustness of the loss and c > 0 is a scale parameter that controls the size of the loss's quadratic bowl near x=0. Though our loss is undefined when \\power = 2, it approaches L2 loss (squared error) in the limit: When \\power=1 our loss is a smoothed form of L1 loss: This is often referred to as Charbonnier loss , pseudo-Huber loss (as it resembles Huber loss ), or L1-L2 loss (as it behaves like L2 loss near the origin and like L1 loss elsewhere). Our loss's ability to express L2 and smoothed L1 losses is shared by the ``generalized Charbonnier'' loss , which has been used in flow and depth estimation tasks that require robustness and is commonly defined as: Our loss has significantly more expressive power than the generalized Charbonnier loss, which we can see by setting our shape parameter \\power to nonpositive values.",
      "To further demonstrate the value of independently modeling the robustness of each individual coefficient, we ran a more thorough experiment in which we densely sampled values for \\power in [0, 2] that are used for all coefficients. In Figure~ we visualize the validation set ELBO for each fixed value of \\power compared to an independently-adapted model. As we can see, though quality can be improved by selecting a value for \\power in between 0 and 2, no single global setting of the shape parameter matches the performance achieved by allowing each coefficient's shape parameter to automatically adapt itself to the training data.",
      "Naturally, its inverse maps from YUV to RGB. Because our model can adapt the shape and scale parameters of our general distribution to each output coefficient, after training we can inspect the shapes and scales that have emerged during training, and from them gain insight into how optimization has modeled our training data. In Figures~ and we visualize the shape and scale parameters for our ``Pixels + RGB'' and ``Wavelets + YUV'' VAEs respectively.",
      "(Cauchy) as a function of training iteration, which outperforms all ``fixed'' models. In the ``adaptive \\power \\in (0, 2)'' model we assign each wavelet coefficient its own shape parameter as a free variable and we allow those variables to be optimized alongside our network weights during training as was done in Section~, but with \\amin=0 and \\amax=2. This ``adaptive'' strategy outperforms the ``annealing'' and all ``fixed'' strategies, thereby demonstrating the value of allowing the model to adaptively determine the robustness of its loss during training.",
      "Because the reconstruction loss in is the sum of the means of the losses imposed at each scale in a D-level pyramid of side predictions, we use a D level normalized wavelet decomposition (wherein images in [0,1] result in wavelet coefficients in [0,1]) and then scale each coefficient's loss by 2^d, where d is the coefficient\u2019s level. In Figure~ we visualize the final shape parameters for each output coefficient that were converged upon during training. These results provide some insight into why our adaptive model produces improved results compared to the ablations of our model in which we use a single fixed or annealed value for \\power for all output coefficients.",
      "If \\power=2, the derivative's magnitude stays linearly proportional to the residual's magnitude --- a larger residual has a correspondingly larger effect. If \\power=1 the derivative's magnitude saturates to a constant \\sfrac{1{c as \\abs{x grows larger than c, so as a residual increases its effect never decreases but never exceeds a fixed amount. If \\power < 1 the derivative's magnitude begins to decrease as \\abs{x grows larger than c (in the language of M-estimation , the derivative, aka ``influence'', is ``redescending'')"
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 1,\n    \"Rationale\": \"Image 1 shows the general loss function and its gradient for different values of the shape parameter \u03b1, which is directly related to the IRLS weight function. Text 1 discusses the IRLS weights and their relation to the shape parameter, explaining how reducing \u03b1 attenuates the effect of outliers.\",\n    \"Answer\": \"As the shape parameter \u03b1 increases, the IRLS weight function becomes less robust to outliers, resembling a non-normalized form of Student's t-distribution. This means the influence of larger residuals increases, making the function more sensitive to outliers.\"\n}"
  },
  "1": {
    "question": "How do the reconstructed faces in the \"Mean Reconstruction\" differ from those in the \"Sampled Reconstruction\"?",
    "ground_truth": "The reconstructed faces in the \"Mean Reconstruction\" are smoother and less detailed than those in the \"Sampled Reconstruction\". This is because the mean reconstruction is based on the average of all the possible reconstructions, while the sampled reconstruction is based on a single sample from the distribution.",
    "selected_figures_names": [
      "1701.03077v10-Figure12-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 provides a visual comparison between 'Mean Reconstruction' and 'Sampled Reconstruction', which is directly relevant to the question. Text 1 explains that the reconstructions are means of the output distributions, not samples, which helps clarify the difference.",
    "answer": "The 'Mean Reconstruction' images are smoother and more consistent, representing the average of the output distributions. In contrast, the 'Sampled Reconstruction' images appear noisier and more varied, as they are samples from the latent space rather than the final conditional distribution.",
    "text_chunks": [
      "Because the reconstruction loss in is the sum of the means of the losses imposed at each scale in a D-level pyramid of side predictions, we use a D level normalized wavelet decomposition (wherein images in [0,1] result in wavelet coefficients in [0,1]) and then scale each coefficient's loss by 2^d, where d is the coefficient\u2019s level. In Figure~ we visualize the final shape parameters for each output coefficient that were converged upon during training. These results provide some insight into why our adaptive model produces improved results compared to the ablations of our model in which we use a single fixed or annealed value for \\power for all output coefficients.",
      "\\newcommand{\\noisewidth{0.45\\linewidth See Figure~ for additional samples from our models, and see Figure~ for reconstructions from our models on validation-set images. As is common practice, the samples and reconstructions in those figures and in the paper are the means of the output distributions of the decoder, not samples from those distributions. That is, we draw samples from the latent encoded space and then decode them, but we do not draw samples in our output space.",
      "Our ``Pixels'' model is easy to visualize as each output coefficient simply corresponds to a pixel in a channel, and our ``Wavelets'' model can be visualized by flattening each wavelet scale and orientation into an image (our DCT-based model is difficult to visualize in any intuitive way). In both models we observe that training has determined that these face images should be modeled using normal-like distributions near the eyes and mouth, presumably because these structures are consistent and repeatable on human faces, and Cauchy-like distributions on the background and in flat regions of skin. Though our ``Pixels + RGB'' model has estimated similar distributions for each color channel, our ``Wavelets + YUV'' model has estimated very different behavior for luma and chroma: more Cauchy-like behavior is expected in luma variation, especially at fine frequencies, while chroma variation is modeled as being closer to a normal distribution across all scales.",
      "Samples drawn from these output distributions tend to look noisy and irregular across all distributions and image representations, but they provide a good intuition of how our general distribution behaves in each image representation, so in Figure~ we present side-by-side visualizations of decoded means and samples. Unsupervised Monocular Depth Estimation Our unsupervised monocular depth estimation experiments use the code from \\url{https://github.com/tinghuiz/SfMLearner, which appears to correspond to the ``Ours (w/o explainability)'' model from Table 1 of . The only changes we made to this code were: replacing its loss function with our own, reducing the number of training iterations from 200000 to 100000 (training converges faster when using our loss function) and disabling the smoothness term and multi-scale side predictions used by , as neither yielded much benefit when combined with our new loss function and they complicated experimentation by introducing hyperparameters.",
      "For all representations, VAEs trained with our general distribution produce sharper and more detailed samples than those trained with normal distributions. Models trained with Cauchy and t-distributions preserve high-frequency detail and work well on pixel representations, but systematically fail to synthesize low-frequency image content when given non-pixel representations, as evidenced by the gray backgrounds of those samples. Comparing performance across image representations shows that the ``Wavelets + YUV'' representation best maximizes validation set ELBO --- though if we were to limit our model to only normal distributions the ``DCT + YUV'' model would appear superior, suggesting that there is value in reasoning jointly about distributions and image representations.",
      "CelebA~ images are processed by extracting a square 160 \\times 160 image region at the center of each 178 \\times 218 image and downsampling it to 64 \\times 64 by a factor of 2.5 \\times using TensorFlow's bilinear interpolation implementation. Pixel intensities are scaled to [0, 1]. In the main paper we demonstrated that using our general distribution to independently model the robustness of each coefficient of our image representation works better than assuming a Cauchy (\\power=0) or normal distribution (\\power=2) for all coefficients (as those two distributions lie within our general distribution).",
      "Because the empirical mean of a set of values minimizes total squared error between the mean and the set, and the empirical median similarly minimizes absolute error, minimizing our loss with \\power=2 is equivalent to estimating a mean, and with \\power=1 is similar to estimating a median. Minimizing our loss with \\power=-\\infty is equivalent to local mode-finding . Values of \\power between these extents can be thought of as smoothly interpolating between these three kinds of averages during estimation.",
      "After training we see shape parameters \\{ \\power^{(i) \\ that span (0, 2.5), suggesting that an adaptive mixture of normal-like and Cauchy-like distributions is useful in modeling natural images, as has been observed previously~. Note that this adaptive robustness is just a consequence of allowing \\{\\power_{\\ell^{(i)\\ to be free variables during training, and requires no manual parameter tuning. See Appendix~ for more samples and reconstructions from these models, and a review of our experimental procedure."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 provides a visual comparison between 'Mean Reconstruction' and 'Sampled Reconstruction', which is directly relevant to the question. Text 1 explains that the reconstructions are means of the output distributions, not samples, which helps clarify the difference.\",\n    \"Answer\": \"The 'Mean Reconstruction' images are smoother and more consistent, representing the average of the output distributions. In contrast, the 'Sampled Reconstruction' images appear noisier and more varied, as they are samples from the latent space rather than the final conditional distribution.\"\n}"
  },
  "2": {
    "question": "How does the performance of the adaptive model compare to the fixed model with different values of \u03b1?",
    "ground_truth": "The adaptive model consistently outperforms the fixed model for all values of \u03b1.",
    "selected_figures_names": [
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure9-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Table1-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 3,
    "rationale": "Image 3 and Text 3 both discuss the comparison between adaptive and fixed models in terms of ELBO performance, showing that the adaptive model outperforms fixed models with any single value of \u03b1.",
    "answer": "The adaptive model outperforms the fixed model with any single value of \u03b1, as it allows each coefficient to adapt its shape parameter, leading to better performance.",
    "text_chunks": [
      "Note that though the ``fixed'' and ``annealed'' strategies only require our general loss, the ``adaptive'' strategy requires that we use the NLL of our general distribution as our loss --- otherwise training would simply drive \\power to be as small as possible due to the monotonicity of our loss with respect to \\power, causing performance to degrade to the ``fixed \\power=0'' model. Comparing the ``adaptive'' model's performance to that of the ``fixed'' models suggests that, as in Section~, no single setting of \\power is optimal for all wavelet coefficients. Overall, we see that just replacing the loss function of with our adaptive loss on wavelet coefficients reduces average error by \\sim\\!17\\%.",
      "For the ``fixed'' models we use a constant value for \\power for all wavelet coefficients, and observe that though performance is improved relative to the baseline, no single value of \\power is optimal. The \\power=1 entry is simply a smoothed version of the L1 loss used by the baseline model, suggesting that just using a wavelet representation improves performance. In the ``annealing \\power=2\\!\\rightarrow\\!0'' model we linearly interpolate \\power from 2 (L2) to 0",
      "(Cauchy) as a function of training iteration, which outperforms all ``fixed'' models. In the ``adaptive \\power \\in (0, 2)'' model we assign each wavelet coefficient its own shape parameter as a free variable and we allow those variables to be optimized alongside our network weights during training as was done in Section~, but with \\amin=0 and \\amax=2. This ``adaptive'' strategy outperforms the ``annealing'' and all ``fixed'' strategies, thereby demonstrating the value of allowing the model to adaptively determine the robustness of its loss during training.",
      "To further demonstrate the value of independently modeling the robustness of each individual coefficient, we ran a more thorough experiment in which we densely sampled values for \\power in [0, 2] that are used for all coefficients. In Figure~ we visualize the validation set ELBO for each fixed value of \\power compared to an independently-adapted model. As we can see, though quality can be improved by selecting a value for \\power in between 0 and 2, no single global setting of the shape parameter matches the performance achieved by allowing each coefficient's shape parameter to automatically adapt itself to the training data.",
      "In Figure~ we compare our ``adaptive'' model's output to the baseline model and the ground-truth depth, and demonstrate a substantial qualitative improvement. See Appendix~ for many more results, and for visualizations of the per-coefficient robustness selected by our model. Fast Global Registration Robustness is often a core component of geometric registration~.",
      "Because the reconstruction loss in is the sum of the means of the losses imposed at each scale in a D-level pyramid of side predictions, we use a D level normalized wavelet decomposition (wherein images in [0,1] result in wavelet coefficients in [0,1]) and then scale each coefficient's loss by 2^d, where d is the coefficient\u2019s level. In Figure~ we visualize the final shape parameters for each output coefficient that were converged upon during training. These results provide some insight into why our adaptive model produces improved results compared to the ablations of our model in which we use a single fixed or annealed value for \\power for all output coefficients.",
      "In Figures~ and we present many more results from the test split of the KITTI dataset, in which we compare our ``adaptive'' model's output to the baseline model and the ground-truth depth. The improvement we see is substantial and consistent across a variety of scenes. Fast Global Registration",
      "We see that our general distribution performs similarly to a Student's t-distribution, with both producing higher ELBOs than any fixed distribution across all representations. These two adaptive distributions appear to have complementary strengths: ours can be more platykurtic (\\alpha>2) while a t-distribution can be more leptokurtic (\\nu < 1), which may explain why neither model consistently outperforms the other across representations. Note that the t-distribution's NLL does not generalize the Charbonnier, L1, Geman-McClure, or Welsch losses, so unlike ours it will not generalize the losses used in the other tasks we will address."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 3,\n    \"Rationale\": \"Image 3 and Text 3 both discuss the comparison between adaptive and fixed models in terms of ELBO performance, showing that the adaptive model outperforms fixed models with any single value of \u03b1.\",\n    \"Answer\": \"The adaptive model outperforms the fixed model with any single value of \u03b1, as it allows each coefficient to adapt its shape parameter, leading to better performance.\"\n}"
  },
  "3": {
    "question": "On which dataset did the gRCC* algorithm achieve the largest relative improvement over the RCC algorithm, and by approximately how much did it improve?",
    "ground_truth": "The gRCC* algorithm achieved the largest relative improvement over the RCC algorithm on the YTF dataset, with a relative improvement of approximately 31.9%.",
    "selected_figures_names": [
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Table4-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure13-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 0,
    "rationale": "Image 3 provides a table comparing the AMI results of the RCC and gRCC* algorithms across various datasets, which directly answers the question. Text 0 discusses the performance improvements of gRCC* over RCC, which is relevant to understanding the context of the improvements.",
    "answer": "The gRCC* algorithm achieved the largest relative improvement on the YTF dataset, with an improvement of approximately 31.9%.",
    "text_chunks": [
      "In Figure~ we show the adjusted mutual information (AMI, the metric used by ) of the resulting clustering for each value of \\power on the datasets used in , and in Table~ we report the AMI for the best-performing value of \\power for each dataset as ``gRCC*''. On some datasets performance is insensitive to \\power, but on others adjusting \\power improves performance by as much as 32\\%. This improvement demonstrates the gains that can be achieved by introducing robustness as a hyperparameter and tuning it accordingly.",
      "Robust Continuous Clustering In robust losses are used for unsupervised clustering, by minimizing: where \\{ \\mathbf{x_i \\ is a set of input datapoints, \\{ \\mathbf{u_i \\ is a set of ``representatives'' (cluster centers), and \\mathcal{E is a mutual k-nearest neighbors (m-kNN) graph. As in Section~, \\rho_{gm(\\cdot) is Geman-McClure loss, which means that our loss can be used to generalize this algorithm. Using the RCC code provided by the authors (and keeping all hyperparameters fixed to their default values) we replace Geman-McClure loss with our general loss and then sweep over values of \\power.",
      "In Figure~ we set \\power to a wide range of values and report errors for each setting, using the same evaluation of . We see that for high-noise inputs more negative values of \\power are preferable, but for low-noise inputs values closer to 0 are optimal. We report the lowest-error entry for each noise level as ``gFGR*'' in Table~ where we see a significant reduction in error, thereby demonstrating the improvement that can be achieved from treating robustness as a hyperparameter.",
      "In that difference between images is defined as the absolute difference between RGB values. We will replace that loss with different varieties of our general loss, and demonstrate that using annealed or adaptive forms of our loss can improve performance. The absolute loss in is equivalent to maximizing the likelihood of a Laplacian distribution with a fixed scale on RGB pixel values.",
      "\\paragraph{Acknowledgements: Thanks to Rob Anderson, Jesse Engel, David Gallup, Ross Girshick, Jaesik Park, Ben Poole, Vivek Rathod, and Tinghui Zhou. {\\small \\bibliographystyle{ieee \\bibliography{robust \\nonumber \\\\ % & \\quad\\quad \\rho(x, \\power, c) = \\min_{0 \\leq z \\leq 1 \\left({1",
      "In Figures~ and we present many more results from the test split of the KITTI dataset, in which we compare our ``adaptive'' model's output to the baseline model and the ground-truth depth. The improvement we see is substantial and consistent across a variety of scenes. Fast Global Registration",
      "% \\end{equation As we have shown, this loss function is a superset of the Welsch/Leclerc, Geman-McClure, Cauchy/Lorentzian, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. To enable gradient-based optimization we can derive the derivative of \\lossfun{x, \\power, c with respect to x: \\end{equation Our loss and its derivative are visualized for different values of \\power in Figure~. The shape of the derivative gives some intuition as to how \\power affects behavior when our loss is being minimized by gradient descent or some related method. For all values of \\power the derivative is approximately linear when \\abs{x < c, so the effect of a small residual is always linearly proportional to that residual's magnitude.",
      "As we will show, this approach is particularly effective when combined with image representations such as wavelets, in which we expect to see non-Gaussian, heavy-tailed distributions. In Sections~ and we will build upon existing algorithms for two classic vision tasks (registration and clustering) that both work by minimizing a robust loss that is subsumed by our general loss. We will then replace each algorithm's fixed robust loss with our loss, thereby introducing a continuous tunable robustness parameter \\power."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 0,\n    \"Rationale\": \"Image 3 provides a table comparing the AMI results of the RCC and gRCC* algorithms across various datasets, which directly answers the question. Text 0 discusses the performance improvements of gRCC* over RCC, which is relevant to understanding the context of the improvements.\",\n    \"Answer\": \"The gRCC* algorithm achieved the largest relative improvement on the YTF dataset, with an improvement of approximately 31.9%.\"\n}"
  },
  "4": {
    "question": "What is the relationship between the shape parameter \u03b1 and the shape of the loss function?",
    "ground_truth": "The shape parameter \u03b1 controls the shape of the loss function. As \u03b1 increases, the loss function becomes more peaked, and as \u03b1 decreases, the loss function becomes more flat.",
    "selected_figures_names": [
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Table4-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 shows the impact of changing the loss function on depth estimation, which is relevant to understanding how the shape parameter affects the loss function's behavior. Text 2 discusses the derivative of the loss function with respect to the shape parameter, providing insight into how the shape parameter influences the loss function's behavior.",
    "answer": "The shape parameter \u03b1 affects the behavior of the loss function by determining its robustness and how it transitions between L1 and L2 loss characteristics. The derivative of the loss function with respect to \u03b1 provides intuition on how changes in \u03b1 influence the minimization process during gradient descent.",
    "text_chunks": [
      "Loss Function The simplest form of our loss function is: Here \\power \\in \\mathbb{R is a shape parameter that controls the robustness of the loss and c > 0 is a scale parameter that controls the size of the loss's quadratic bowl near x=0. Though our loss is undefined when \\power = 2, it approaches L2 loss (squared error) in the limit: When \\power=1 our loss is a smoothed form of L1 loss: This is often referred to as Charbonnier loss , pseudo-Huber loss (as it resembles Huber loss ), or L1-L2 loss (as it behaves like L2 loss near the origin and like L1 loss elsewhere). Our loss's ability to express L2 and smoothed L1 losses is shared by the ``generalized Charbonnier'' loss , which has been used in flow and depth estimation tasks that require robustness and is commonly defined as: Our loss has significantly more expressive power than the generalized Charbonnier loss, which we can see by setting our shape parameter \\power to nonpositive values.",
      "Because the reconstruction loss in is the sum of the means of the losses imposed at each scale in a D-level pyramid of side predictions, we use a D level normalized wavelet decomposition (wherein images in [0,1] result in wavelet coefficients in [0,1]) and then scale each coefficient's loss by 2^d, where d is the coefficient\u2019s level. In Figure~ we visualize the final shape parameters for each output coefficient that were converged upon during training. These results provide some insight into why our adaptive model produces improved results compared to the ablations of our model in which we use a single fixed or annealed value for \\power for all output coefficients.",
      "% \\end{equation As we have shown, this loss function is a superset of the Welsch/Leclerc, Geman-McClure, Cauchy/Lorentzian, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. To enable gradient-based optimization we can derive the derivative of \\lossfun{x, \\power, c with respect to x: \\end{equation Our loss and its derivative are visualized for different values of \\power in Figure~. The shape of the derivative gives some intuition as to how \\power affects behavior when our loss is being minimized by gradient descent or some related method. For all values of \\power the derivative is approximately linear when \\abs{x < c, so the effect of a small residual is always linearly proportional to that residual's magnitude.",
      "Our loss function has several useful properties that we will take advantage of. The loss is smooth (\\ie, in C^\\infty) with respect to x, \\power, and c>0, and is therefore well-suited to gradient-based optimization over its input and its parameters. The loss is zero at the origin, and increases monotonically with respect to \\abs{x:",
      "Our nonlinearity and our spline approximation to the true partition function for small values of \\power can be seen in Figure~. Motivation and Derivation Our loss function is derived from the ``generalized Charbonnier'' loss , which itself builds upon the Charbonnier loss function . To better motivate the construction of our loss function, and to clarify its relationship to prior work, here we work through how our loss function was constructed. Generalized Charbonnier loss can be defined as: Here we use a slightly different parametrization from and use {\\sfrac{\\power{2 as the exponent instead of just \\power.",
      "The ``Ours'' entries all use our general loss imposed on wavelet coefficients, but for each entry we use a different strategy for setting the shape parameter or parameters. We keep our loss's scale c fixed to 0.01, thereby matching the fixed scale assumption of the baseline model and roughly matching the shape of its L1 loss (Eq.~). To avoid exploding gradients we multiply the loss being minimized by c, thereby bounding gradient magnitudes by residual magnitudes (Eq.~).",
      "We replace that fixed Laplacian distribution with our general distribution, keeping our scale fixed but allowing the shape parameter \\power to vary. Following our observation from Section~ that YUV wavelet representations work well when modeling images with our loss function, we impose our loss on a YUV wavelet decomposition instead of the RGB pixel representation of . The only changes we made to the code from were to replace its loss function with our own and to remove the model components that stopped yielding any improvement after the loss function was replaced (see Appendix~ for details).",
      "Additionally, only after dropping this scale factor does setting \\power to a negative value yield a family of meaningful robust loss functions, such as Geman-McClure loss. But this loss function still has several unintuitive properties: the loss is non-zero when x=0 (assuming a non-zero value of c), and the curvature of the quadratic ``bowl'' near x=0 varies as a function of c and \\power. We therefore construct a shifted and scaled version of Equation~ that does not have these properties: This loss generalizes L2, Cauchy, and Geman-McClure loss, but it has the unfortunate side-effect of flattening out to 0 when \\power \\ll 0, thereby prohibiting many annealing strategies."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 shows the impact of changing the loss function on depth estimation, which is relevant to understanding how the shape parameter affects the loss function's behavior. Text 2 discusses the derivative of the loss function with respect to the shape parameter, providing insight into how the shape parameter influences the loss function's behavior.\",\n    \"Answer\": \"The shape parameter \u03b1 affects the behavior of the loss function by determining its robustness and how it transitions between L1 and L2 loss characteristics. The derivative of the loss function with respect to \u03b1 provides intuition on how changes in \u03b1 influence the minimization process during gradient descent.\"\n}"
  },
  "5": {
    "question": "What is the effect of replacing the loss function in the \"Baseline\" network with the \"adaptive\" loss over wavelet coefficients?",
    "ground_truth": "Replacing the loss function in the \"Baseline\" network with the \"adaptive\" loss over wavelet coefficients results in significantly improved depth estimates.",
    "selected_figures_names": [
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure11-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure3-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 5,
    "rationale": "Image 2 shows the results of using different distributions, including the adaptive loss, in variational autoencoders, which is relevant to understanding the effect of the adaptive loss. Text 5 explains that the adaptive strategy outperforms other strategies, demonstrating the value of allowing the model to adaptively determine the robustness of its loss during training.",
    "answer": "Replacing the loss function in the 'Baseline' network with the 'adaptive' loss over wavelet coefficients allows the model to adaptively determine the robustness of its loss during training, leading to improved performance compared to fixed or annealed strategies.",
    "text_chunks": [
      "Note that though the ``fixed'' and ``annealed'' strategies only require our general loss, the ``adaptive'' strategy requires that we use the NLL of our general distribution as our loss --- otherwise training would simply drive \\power to be as small as possible due to the monotonicity of our loss with respect to \\power, causing performance to degrade to the ``fixed \\power=0'' model. Comparing the ``adaptive'' model's performance to that of the ``fixed'' models suggests that, as in Section~, no single setting of \\power is optimal for all wavelet coefficients. Overall, we see that just replacing the loss function of with our adaptive loss on wavelet coefficients reduces average error by \\sim\\!17\\%.",
      "Because the reconstruction loss in is the sum of the means of the losses imposed at each scale in a D-level pyramid of side predictions, we use a D level normalized wavelet decomposition (wherein images in [0,1] result in wavelet coefficients in [0,1]) and then scale each coefficient's loss by 2^d, where d is the coefficient\u2019s level. In Figure~ we visualize the final shape parameters for each output coefficient that were converged upon during training. These results provide some insight into why our adaptive model produces improved results compared to the ablations of our model in which we use a single fixed or annealed value for \\power for all output coefficients.",
      "We replace that fixed Laplacian distribution with our general distribution, keeping our scale fixed but allowing the shape parameter \\power to vary. Following our observation from Section~ that YUV wavelet representations work well when modeling images with our loss function, we impose our loss on a YUV wavelet decomposition instead of the RGB pixel representation of . The only changes we made to the code from were to replace its loss function with our own and to remove the model components that stopped yielding any improvement after the loss function was replaced (see Appendix~ for details).",
      "When used as a negative log-likelihood, this loss gives a general probability distribution that includes normal and Cauchy distributions as special cases. This distribution lets us train neural networks in which the loss has an adaptive degree of robustness for each output dimension, which allows training to automatically determine how much robustness should be imposed by the loss without any manual parameter tuning. When this adaptive loss is paired with image representations in which variable degrees of heavy-tailed behavior occurs, such as wavelets, this adaptive training approach allows us to improve the performance of variational autoencoders for image synthesis and of neural networks for unsupervised monocular depth estimation.",
      "For the ``fixed'' models we use a constant value for \\power for all wavelet coefficients, and observe that though performance is improved relative to the baseline, no single value of \\power is optimal. The \\power=1 entry is simply a smoothed version of the L1 loss used by the baseline model, suggesting that just using a wavelet representation improves performance. In the ``annealing \\power=2\\!\\rightarrow\\!0'' model we linearly interpolate \\power from 2 (L2) to 0",
      "(Cauchy) as a function of training iteration, which outperforms all ``fixed'' models. In the ``adaptive \\power \\in (0, 2)'' model we assign each wavelet coefficient its own shape parameter as a free variable and we allow those variables to be optimized alongside our network weights during training as was done in Section~, but with \\amin=0 and \\amax=2. This ``adaptive'' strategy outperforms the ``annealing'' and all ``fixed'' strategies, thereby demonstrating the value of allowing the model to adaptively determine the robustness of its loss during training.",
      "The ``Ours'' entries all use our general loss imposed on wavelet coefficients, but for each entry we use a different strategy for setting the shape parameter or parameters. We keep our loss's scale c fixed to 0.01, thereby matching the fixed scale assumption of the baseline model and roughly matching the shape of its L1 loss (Eq.~). To avoid exploding gradients we multiply the loss being minimized by c, thereby bounding gradient magnitudes by residual magnitudes (Eq.~).",
      "Our loss is strictly convex iff \\power \\geq 1, non-convex iff \\power < 1, and pseudoconvex for all values of \\power. Wavelet Implementation Two of our experiments impose our loss on images reparametrized with the Cohen-Daubechies-Feauveau (CDF) 9/7 wavelet decomposition . The analysis filters used for these experiments are: Here the origin coefficient of the filter is listed first, and the rest of the filter is symmetric."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 5,\n    \"Rationale\": \"Image 2 shows the results of using different distributions, including the adaptive loss, in variational autoencoders, which is relevant to understanding the effect of the adaptive loss. Text 5 explains that the adaptive strategy outperforms other strategies, demonstrating the value of allowing the model to adaptively determine the robustness of its loss during training.\",\n    \"Answer\": \"Replacing the loss function in the 'Baseline' network with the 'adaptive' loss over wavelet coefficients allows the model to adaptively determine the robustness of its loss during training, leading to improved performance compared to fixed or annealed strategies.\"\n}"
  },
  "6": {
    "question": "Which method for setting the shape parameter of the proposed loss function achieved the best performance in terms of average error? How much improvement did it offer compared to the reproduced baseline?",
    "ground_truth": "The \"adaptive $\\power \\in (0, 2)$\" strategy, where each wavelet coefficient has its own shape parameter that is optimized during training, achieved the best performance in terms of average error. It reduced the average error by approximately 17% compared to the reproduced baseline.",
    "selected_figures_names": [
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure11-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure12-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 0,
    "rationale": "Image 2 provides a detailed comparison of different methods for setting the shape parameter of the proposed loss function, showing performance metrics including average error. Text 0 discusses the adaptive strategy's performance improvement, which is relevant to the question.",
    "answer": "The adaptive method for setting the shape parameter achieved the best performance in terms of average error, reducing it to 0.332. This offered an improvement compared to the reproduced baseline, which had an average error of 0.398.",
    "text_chunks": [
      "Note that though the ``fixed'' and ``annealed'' strategies only require our general loss, the ``adaptive'' strategy requires that we use the NLL of our general distribution as our loss --- otherwise training would simply drive \\power to be as small as possible due to the monotonicity of our loss with respect to \\power, causing performance to degrade to the ``fixed \\power=0'' model. Comparing the ``adaptive'' model's performance to that of the ``fixed'' models suggests that, as in Section~, no single setting of \\power is optimal for all wavelet coefficients. Overall, we see that just replacing the loss function of with our adaptive loss on wavelet coefficients reduces average error by \\sim\\!17\\%.",
      "Because the reconstruction loss in is the sum of the means of the losses imposed at each scale in a D-level pyramid of side predictions, we use a D level normalized wavelet decomposition (wherein images in [0,1] result in wavelet coefficients in [0,1]) and then scale each coefficient's loss by 2^d, where d is the coefficient\u2019s level. In Figure~ we visualize the final shape parameters for each output coefficient that were converged upon during training. These results provide some insight into why our adaptive model produces improved results compared to the ablations of our model in which we use a single fixed or annealed value for \\power for all output coefficients.",
      "Loss Function The simplest form of our loss function is: Here \\power \\in \\mathbb{R is a shape parameter that controls the robustness of the loss and c > 0 is a scale parameter that controls the size of the loss's quadratic bowl near x=0. Though our loss is undefined when \\power = 2, it approaches L2 loss (squared error) in the limit: When \\power=1 our loss is a smoothed form of L1 loss: This is often referred to as Charbonnier loss , pseudo-Huber loss (as it resembles Huber loss ), or L1-L2 loss (as it behaves like L2 loss near the origin and like L1 loss elsewhere). Our loss's ability to express L2 and smoothed L1 losses is shared by the ``generalized Charbonnier'' loss , which has been used in flow and depth estimation tasks that require robustness and is commonly defined as: Our loss has significantly more expressive power than the generalized Charbonnier loss, which we can see by setting our shape parameter \\power to nonpositive values.",
      "We replace that fixed Laplacian distribution with our general distribution, keeping our scale fixed but allowing the shape parameter \\power to vary. Following our observation from Section~ that YUV wavelet representations work well when modeling images with our loss function, we impose our loss on a YUV wavelet decomposition instead of the RGB pixel representation of . The only changes we made to the code from were to replace its loss function with our own and to remove the model components that stopped yielding any improvement after the loss function was replaced (see Appendix~ for details).",
      "All training and evaluation was performed on the KITTI dataset using the same training/test split as . \\newcommand{\\sfmwidth{0.93\\linewidth \\newcommand{\\nametag{153 Results can be seen in Table~. We present the error and accuracy metrics used in and our own ``average'' error measure: the geometric mean of the four errors and one minus the three accuracies. The ``Baseline`` models use the loss function of , and we present both the numbers in (``as reported'') and our own numbers from running the code from ourselves (``reproduced'').",
      "The ``Ours'' entries all use our general loss imposed on wavelet coefficients, but for each entry we use a different strategy for setting the shape parameter or parameters. We keep our loss's scale c fixed to 0.01, thereby matching the fixed scale assumption of the baseline model and roughly matching the shape of its L1 loss (Eq.~). To avoid exploding gradients we multiply the loss being minimized by c, thereby bounding gradient magnitudes by residual magnitudes (Eq.~).",
      "For the ``fixed'' models we use a constant value for \\power for all wavelet coefficients, and observe that though performance is improved relative to the baseline, no single value of \\power is optimal. The \\power=1 entry is simply a smoothed version of the L1 loss used by the baseline model, suggesting that just using a wavelet representation improves performance. In the ``annealing \\power=2\\!\\rightarrow\\!0'' model we linearly interpolate \\power from 2 (L2) to 0",
      "Conclusion We have presented a two-parameter loss function that generalizes many existing one-parameter robust loss functions: the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By reducing a family of discrete single-parameter losses to a single function with two continuous parameters, our loss enables the convenient exploration and comparison of different robust penalties. This allows us to generalize and improve algorithms designed around the minimization of some fixed robust loss function, which we have demonstrated for registration and clustering."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 0,\n    \"Rationale\": \"Image 2 provides a detailed comparison of different methods for setting the shape parameter of the proposed loss function, showing performance metrics including average error. Text 0 discusses the adaptive strategy's performance improvement, which is relevant to the question.\",\n    \"Answer\": \"The adaptive method for setting the shape parameter achieved the best performance in terms of average error, reducing it to 0.332. This offered an improvement compared to the reproduced baseline, which had an average error of 0.398.\"\n}"
  },
  "7": {
    "question": "Why did the authors choose to use a nonlinearity to curve \u03b1 before fitting the cubic hermite spline?",
    "ground_truth": "The authors chose to use a nonlinearity to curve \u03b1 before fitting the cubic hermite spline because it allows for increased knot density near \u03b1 = 2 and decreased knot density when \u03b1 > 4. This helps to better approximate the log partition function, which is difficult to evaluate for arbitrary inputs.",
    "selected_figures_names": [
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure16-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Figure15-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 0,
    "rationale": "Image 1 visually demonstrates the process of curving \u03b1 and fitting the cubic hermite spline, which is directly related to the question. Text 0 explains the rationale behind using a nonlinearity to curve \u03b1, focusing on increasing knot density near \u03b1 = 2 and decreasing it when \u03b1 > 4.",
    "answer": "The authors used a nonlinearity to curve \u03b1 to increase the density of spline knots near \u03b1 = 2, where the log partition function varies most rapidly, and to decrease knot density when \u03b1 > 4. This approach allows for a more precise approximation of the log partition function in critical regions.",
    "text_chunks": [
      "Because the analytical form of \\partition{\\power detailed in the paper is difficult to evaluate efficiently for any real number, and especially difficult to differentiate with respect to \\power, we approximate \\log(\\partition{\\power) using cubic hermite spline interpolation in a transformed space. Efficiently approximating \\log(\\partition{\\power) with a spline is difficult, as we would like a concise approximation that holds over the entire valid range \\power \\geq 0, and we would like to allocate more precision in our spline interpolation to values near \\power=2 (which is where \\log(\\partition{\\power) varies most rapidly). To accomplish this, we first apply a monotonic nonlinearity to \\power that stretches values near \\power=2 (thereby increasing the density of spline knots in this region) and compresses values as \\power \\gg 4, for which we use: \\end{equation This curve is roughly piecewise-linear in [0, 4] with a slope of \\sim\\!\\!\\!1 at \\power=0 and \\power=4, but with a slope of \\sim\\!\\!\\!10 at \\power=2.",
      "When \\power > 4 the curve becomes logarithmic. This function is continuously differentiable, as is required for our log-partition approximation to also be continuously differentiable. We transform \\power with this nonlinearity, and then approximate \\log(\\partition{\\power) in that transformed space using a spline with knots in the range of [0, 12] evenly spaced apart by \\sfrac{1{1024.",
      "Instead of annealing c, we set it to its terminal value and instead anneal \\power over the following values: Table~ shows results for the 3D point cloud registration task of (Table 1 in that paper), which shows that annealing shape produces moderately improved performance over FGR for high-noise inputs, and behaves equivalently in low-noise inputs. This suggests that performing graduated non-convexity by directly adjusting a shape parameter that controls non-convexity --- a procedure that is enabled by our general loss -- is preferable to indirectly controlling non-convexity by annealing a scale parameter. Another generalization is to continue using the c-annealing strategy of , but treat \\power as a hyperparameter and tune it independently for each noise level in this task.",
      "We use this procedure when reproducing the baseline performance of and when evaluating our own models. \\renewcommand{\\vaewidth{0.3\\linewidth \\renewcommand{\\vaewidth{0.072\\linewidth \\renewcommand{\\sfmwidth{0.41\\linewidth \\renewcommand{\\nametag{000 \\end{appendices",
      "Values for each knot are set to their true value, and tangents for each knot are set to minimize the squared error between the spline and the true log partition function. Because our spline knots are evenly spaced in this transformed space, spline interpolation can be performed in constant time with respect to the number of spline knots. For all values of \\power this approximation is accurate to within 10^{-6, which appears to be sufficient for our purposes.",
      "Our wavelet construction in Section~ satisfies this criteria, and we use the orthonormal version of the DCT which also satisfies this criteria. However, the standard RGB to YUV conversion matrix does not have a determinant of 1, so we scale it by the inverse of the cube root of the standard conversion matrix, thereby forcing its determinant to be 1. The resulting matrix is: \\[ \\]",
      "Because the reconstruction loss in is the sum of the means of the losses imposed at each scale in a D-level pyramid of side predictions, we use a D level normalized wavelet decomposition (wherein images in [0,1] result in wavelet coefficients in [0,1]) and then scale each coefficient's loss by 2^d, where d is the coefficient\u2019s level. In Figure~ we visualize the final shape parameters for each output coefficient that were converged upon during training. These results provide some insight into why our adaptive model produces improved results compared to the ablations of our model in which we use a single fixed or annealed value for \\power for all output coefficients.",
      "In Figure~ we compare our ``adaptive'' model's output to the baseline model and the ground-truth depth, and demonstrate a substantial qualitative improvement. See Appendix~ for many more results, and for visualizations of the per-coefficient robustness selected by our model. Fast Global Registration Robustness is often a core component of geometric registration~."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 0,\n    \"Rationale\": \"Image 1 visually demonstrates the process of curving \u03b1 and fitting the cubic hermite spline, which is directly related to the question. Text 0 explains the rationale behind using a nonlinearity to curve \u03b1, focusing on increasing knot density near \u03b1 = 2 and decreasing it when \u03b1 > 4.\",\n    \"Answer\": \"The authors used a nonlinearity to curve \u03b1 to increase the density of spline knots near \u03b1 = 2, where the log partition function varies most rapidly, and to decrease knot density when \u03b1 > 4. This approach allows for a more precise approximation of the log partition function in critical regions.\"\n}"
  },
  "8": {
    "question": "What is the range of values for the shape parameter \u03b1?",
    "ground_truth": "The range of values for the shape parameter \u03b1 is from 0 to 2.",
    "selected_figures_names": [
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure15-1.png",
      "1701.03077v10-Figure13-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure15-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure1-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 2,
    "rationale": "Text chunk 2 mentions the range of shape parameters \\( \\alpha \\) as spanning (0, 2.5), which directly answers the question. Image 3 visualizes the shape parameters \\( \\alpha \\) in the YUV + Wavelet output space, providing a visual representation of the range.",
    "answer": "The range of values for the shape parameter \\( \\alpha \\) is (0, 2.5).",
    "text_chunks": [
      "Again, our goal is not to advance the state of the art for any particular image synthesis task, but is instead to explore the value of our distribution in an experimentally controlled setting. In our baseline model we give each pixel's normal distribution a variable scale parameter \\sigma^{(i) that will be optimized over during training, thereby allowing the VAE to adjust the scale of its distribution for each output dimension. We can straightforwardly replace this per-pixel normal distribution with a per-pixel general distribution, in which each output dimension is given a distinct shape parameter \\power^{(i) in addition to its scale parameter c^{(i) (\\ie, \\sigma^{(i)).",
      "By letting the \\power^{(i) parameters be free variables alongside the scale parameters, training is able to adaptively select both the scale and robustness of the VAE's posterior distribution over pixel values. We restrict all \\power^{(i) to be in (0, 3), which allows our distribution to generalize Cauchy (\\power=0) and Normal (\\power = 2) distributions and anything in between, as well as more platykurtic distributions (\\power > 2) which helps for this task. We limit \\power to be less than 3 because of the increased risk of numerical instability during training as \\power increases.",
      "After training we see shape parameters \\{ \\power^{(i) \\ that span (0, 2.5), suggesting that an adaptive mixture of normal-like and Cauchy-like distributions is useful in modeling natural images, as has been observed previously~. Note that this adaptive robustness is just a consequence of allowing \\{\\power_{\\ell^{(i)\\ to be free variables during training, and requires no manual parameter tuning. See Appendix~ for more samples and reconstructions from these models, and a review of our experimental procedure.",
      "To further demonstrate the value of independently modeling the robustness of each individual coefficient, we ran a more thorough experiment in which we densely sampled values for \\power in [0, 2] that are used for all coefficients. In Figure~ we visualize the validation set ELBO for each fixed value of \\power compared to an independently-adapted model. As we can see, though quality can be improved by selecting a value for \\power in between 0 and 2, no single global setting of the shape parameter matches the performance achieved by allowing each coefficient's shape parameter to automatically adapt itself to the training data.",
      "Instead of annealing c, we set it to its terminal value and instead anneal \\power over the following values: Table~ shows results for the 3D point cloud registration task of (Table 1 in that paper), which shows that annealing shape produces moderately improved performance over FGR for high-noise inputs, and behaves equivalently in low-noise inputs. This suggests that performing graduated non-convexity by directly adjusting a shape parameter that controls non-convexity --- a procedure that is enabled by our general loss -- is preferable to indirectly controlling non-convexity by annealing a scale parameter. Another generalization is to continue using the c-annealing strategy of , but treat \\power as a hyperparameter and tune it independently for each noise level in this task.",
      "Naturally, its inverse maps from YUV to RGB. Because our model can adapt the shape and scale parameters of our general distribution to each output coefficient, after training we can inspect the shapes and scales that have emerged during training, and from them gain insight into how optimization has modeled our training data. In Figures~ and we visualize the shape and scale parameters for our ``Pixels + RGB'' and ``Wavelets + YUV'' VAEs respectively.",
      "Because the reconstruction loss in is the sum of the means of the losses imposed at each scale in a D-level pyramid of side predictions, we use a D level normalized wavelet decomposition (wherein images in [0,1] result in wavelet coefficients in [0,1]) and then scale each coefficient's loss by 2^d, where d is the coefficient\u2019s level. In Figure~ we visualize the final shape parameters for each output coefficient that were converged upon during training. These results provide some insight into why our adaptive model produces improved results compared to the ablations of our model in which we use a single fixed or annealed value for \\power for all output coefficients.",
      "The ``Ours'' entries all use our general loss imposed on wavelet coefficients, but for each entry we use a different strategy for setting the shape parameter or parameters. We keep our loss's scale c fixed to 0.01, thereby matching the fixed scale assumption of the baseline model and roughly matching the shape of its L1 loss (Eq.~). To avoid exploding gradients we multiply the loss being minimized by c, thereby bounding gradient magnitudes by residual magnitudes (Eq.~)."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 2,\n    \"Rationale\": \"Text chunk 2 mentions the range of shape parameters \\\\( \\\\alpha \\\\) as spanning (0, 2.5), which directly answers the question. Image 3 visualizes the shape parameters \\\\( \\\\alpha \\\\) in the YUV + Wavelet output space, providing a visual representation of the range.\",\n    \"Answer\": \"The range of values for the shape parameter \\\\( \\\\alpha \\\\) is (0, 2.5).\"\n}"
  },
  "9": {
    "question": "How does the performance of gFGR change as the shape parameter \u03b1 increases?",
    "ground_truth": "The performance of gFGR generally improves as the shape parameter \u03b1 increases.",
    "selected_figures_names": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure4-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 3,
    "rationale": "Image 1 shows the performance of the gFGR algorithm as the shape parameter \u03b1 varies, which directly relates to the question about performance changes with \u03b1. Text 3 discusses the effect of different values of \u03b1 on performance, indicating that more negative values are preferable for high-noise inputs, while values closer to 0 are optimal for low-noise inputs.",
    "answer": "As the shape parameter \u03b1 increases, the performance of gFGR generally worsens, especially for high-noise inputs. More negative values of \u03b1 are preferable for high-noise inputs, while values closer to 0 are optimal for low-noise inputs.",
    "text_chunks": [
      "This generalized FGR (gFGR) enables algorithmic improvements. FGR iteratively solves a linear system while annealing its scale parameter c, which has the effect of gradually introducing nonconvexity. gFGR enables an alternative strategy in which we directly manipulate convexity by annealing \\power instead of c. This ``shape-annealed gFGR'' follows the same procedure as : 64 iterations in which a parameter is annealed every 4 iterations.",
      "The Fast Global Registration (FGR) algorithm of finds the rigid transformation \\mathbf{T that aligns point sets \\{ \\mathbf{p \\ and \\{ \\mathbf{q \\ by minimizing the following loss: where \\rho_{gm(\\cdot) is Geman-McClure loss. By using the Black and Rangarajan duality between robust estimation and line processes FGR is capable of producing high-quality registrations at high speeds. Because Geman-McClure loss is a special case of our loss, and because we can formulate our loss as an outlier process (see Appendix~), we can generalize FGR to an arbitrary shape parameter \\power by replacing \\rho_{gm(\\cdot, c) with our \\rho(\\cdot, \\power, c) (where setting \\power=-2 reproduces FGR).",
      "Instead of annealing c, we set it to its terminal value and instead anneal \\power over the following values: Table~ shows results for the 3D point cloud registration task of (Table 1 in that paper), which shows that annealing shape produces moderately improved performance over FGR for high-noise inputs, and behaves equivalently in low-noise inputs. This suggests that performing graduated non-convexity by directly adjusting a shape parameter that controls non-convexity --- a procedure that is enabled by our general loss -- is preferable to indirectly controlling non-convexity by annealing a scale parameter. Another generalization is to continue using the c-annealing strategy of , but treat \\power as a hyperparameter and tune it independently for each noise level in this task.",
      "In Figure~ we set \\power to a wide range of values and report errors for each setting, using the same evaluation of . We see that for high-noise inputs more negative values of \\power are preferable, but for low-noise inputs values closer to 0 are optimal. We report the lowest-error entry for each noise level as ``gFGR*'' in Table~ where we see a significant reduction in error, thereby demonstrating the improvement that can be achieved from treating robustness as a hyperparameter.",
      "In Figure~ we compare our ``adaptive'' model's output to the baseline model and the ground-truth depth, and demonstrate a substantial qualitative improvement. See Appendix~ for many more results, and for visualizations of the per-coefficient robustness selected by our model. Fast Global Registration Robustness is often a core component of geometric registration~.",
      "To further demonstrate the value of independently modeling the robustness of each individual coefficient, we ran a more thorough experiment in which we densely sampled values for \\power in [0, 2] that are used for all coefficients. In Figure~ we visualize the validation set ELBO for each fixed value of \\power compared to an independently-adapted model. As we can see, though quality can be improved by selecting a value for \\power in between 0 and 2, no single global setting of the shape parameter matches the performance achieved by allowing each coefficient's shape parameter to automatically adapt itself to the training data.",
      "Naturally, its inverse maps from YUV to RGB. Because our model can adapt the shape and scale parameters of our general distribution to each output coefficient, after training we can inspect the shapes and scales that have emerged during training, and from them gain insight into how optimization has modeled our training data. In Figures~ and we visualize the shape and scale parameters for our ``Pixels + RGB'' and ``Wavelets + YUV'' VAEs respectively.",
      "In Figure~ we show the adjusted mutual information (AMI, the metric used by ) of the resulting clustering for each value of \\power on the datasets used in , and in Table~ we report the AMI for the best-performing value of \\power for each dataset as ``gRCC*''. On some datasets performance is insensitive to \\power, but on others adjusting \\power improves performance by as much as 32\\%. This improvement demonstrates the gains that can be achieved by introducing robustness as a hyperparameter and tuning it accordingly."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 3,\n    \"Rationale\": \"Image 1 shows the performance of the gFGR algorithm as the shape parameter \u03b1 varies, which directly relates to the question about performance changes with \u03b1. Text 3 discusses the effect of different values of \u03b1 on performance, indicating that more negative values are preferable for high-noise inputs, while values closer to 0 are optimal for low-noise inputs.\",\n    \"Answer\": \"As the shape parameter \u03b1 increases, the performance of gFGR generally worsens, especially for high-noise inputs. More negative values of \u03b1 are preferable for high-noise inputs, while values closer to 0 are optimal for low-noise inputs.\"\n}"
  },
  "10": {
    "question": "Which image representation results in the sharpest and highest-quality samples?",
    "ground_truth": "DCT and wavelet representations.",
    "selected_figures_names": [
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure15-1.png",
      "1701.03077v10-Table3-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure15-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure3-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 0,
    "rationale": "Image 1 shows samples from different image representations and distributions, allowing for a visual comparison of sharpness and quality. Text 0 discusses the performance of different image representations and distributions, noting that the 'Wavelets + YUV' representation produces sharper and more detailed samples.",
    "answer": "The 'Wavelets + YUV' representation results in the sharpest and highest-quality samples.",
    "text_chunks": [
      "For all representations, VAEs trained with our general distribution produce sharper and more detailed samples than those trained with normal distributions. Models trained with Cauchy and t-distributions preserve high-frequency detail and work well on pixel representations, but systematically fail to synthesize low-frequency image content when given non-pixel representations, as evidenced by the gray backgrounds of those samples. Comparing performance across image representations shows that the ``Wavelets + YUV'' representation best maximizes validation set ELBO --- though if we were to limit our model to only normal distributions the ``DCT + YUV'' model would appear superior, suggesting that there is value in reasoning jointly about distributions and image representations.",
      "CelebA~ images are processed by extracting a square 160 \\times 160 image region at the center of each 178 \\times 218 image and downsampling it to 64 \\times 64 by a factor of 2.5 \\times using TensorFlow's bilinear interpolation implementation. Pixel intensities are scaled to [0, 1]. In the main paper we demonstrated that using our general distribution to independently model the robustness of each coefficient of our image representation works better than assuming a Cauchy (\\power=0) or normal distribution (\\power=2) for all coefficients (as those two distributions lie within our general distribution).",
      "These representations resemble the JPEG and JPEG 2000 compression standards, respectively. \\definecolor{Yellow{rgb{1,1, 0.6 \\definecolor{Orange{rgb{1,0.8, 0.6 \\definecolor{Red{rgb{1, 0.6, 0.6 \\newcommand{\\boxnum[1]{\\makebox[\\widthof{37{,000][r]{#1 \\newcommand{\\vaewidth{0.238\\linewidth Our results can be seen in Table~, where we report the validation set evidence lower bound (ELBO) for all combinations of our four distributions and three image representations, and in Figure~, where we visualize samples from these models.",
      "Though modeling images using independent distributions on pixel intensities is a popular choice due to its simplicity, classic work in natural image statistics suggest that images are better modeled with heavy-tailed distributions on wavelet-like image decompositions . We therefore train additional models in which our decoded RGB per-pixel images are linearly transformed into spaces that better model natural images before computing the NLL of our distribution. For this we use the DCT and the CDF 9/7 wavelet decomposition , both with a YUV colorspace.",
      "Our ``Pixels'' model is easy to visualize as each output coefficient simply corresponds to a pixel in a channel, and our ``Wavelets'' model can be visualized by flattening each wavelet scale and orientation into an image (our DCT-based model is difficult to visualize in any intuitive way). In both models we observe that training has determined that these face images should be modeled using normal-like distributions near the eyes and mouth, presumably because these structures are consistent and repeatable on human faces, and Cauchy-like distributions on the background and in flat regions of skin. Though our ``Pixels + RGB'' model has estimated similar distributions for each color channel, our ``Wavelets + YUV'' model has estimated very different behavior for luma and chroma: more Cauchy-like behavior is expected in luma variation, especially at fine frequencies, while chroma variation is modeled as being closer to a normal distribution across all scales.",
      "\\newcommand{\\noisewidth{0.45\\linewidth See Figure~ for additional samples from our models, and see Figure~ for reconstructions from our models on validation-set images. As is common practice, the samples and reconstructions in those figures and in the paper are the means of the output distributions of the decoder, not samples from those distributions. That is, we draw samples from the latent encoded space and then decode them, but we do not draw samples in our output space.",
      "In that difference between images is defined as the absolute difference between RGB values. We will replace that loss with different varieties of our general loss, and demonstrate that using annealed or adaptive forms of our loss can improve performance. The absolute loss in is equivalent to maximizing the likelihood of a Laplacian distribution with a fixed scale on RGB pixel values.",
      "Samples drawn from these output distributions tend to look noisy and irregular across all distributions and image representations, but they provide a good intuition of how our general distribution behaves in each image representation, so in Figure~ we present side-by-side visualizations of decoded means and samples. Unsupervised Monocular Depth Estimation Our unsupervised monocular depth estimation experiments use the code from \\url{https://github.com/tinghuiz/SfMLearner, which appears to correspond to the ``Ours (w/o explainability)'' model from Table 1 of . The only changes we made to this code were: replacing its loss function with our own, reducing the number of training iterations from 200000 to 100000 (training converges faster when using our loss function) and disabling the smoothness term and multi-scale side predictions used by , as neither yielded much benefit when combined with our new loss function and they complicated experimentation by introducing hyperparameters."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 0,\n    \"Rationale\": \"Image 1 shows samples from different image representations and distributions, allowing for a visual comparison of sharpness and quality. Text 0 discusses the performance of different image representations and distributions, noting that the 'Wavelets + YUV' representation produces sharper and more detailed samples.\",\n    \"Answer\": \"The 'Wavelets + YUV' representation results in the sharpest and highest-quality samples.\"\n}"
  },
  "11": {
    "question": "Which dataset shows the greatest sensitivity to the choice of $\\power$?",
    "ground_truth": "RCV1",
    "selected_figures_names": [
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure16-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Figure3-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure7-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 0,
    "rationale": "Image 1 shows the performance of the gRCC algorithm on various datasets for different values of the shape parameter \u03b1, which is related to \u007fpower. Text 0 discusses the sensitivity of performance to \u007fpower, mentioning improvements up to 32%.",
    "answer": "The dataset with the greatest sensitivity to the choice of \u007fpower is the one with the largest performance variation in Image 1, which appears to be the 'YaleB' dataset.",
    "text_chunks": [
      "In Figure~ we show the adjusted mutual information (AMI, the metric used by ) of the resulting clustering for each value of \\power on the datasets used in , and in Table~ we report the AMI for the best-performing value of \\power for each dataset as ``gRCC*''. On some datasets performance is insensitive to \\power, but on others adjusting \\power improves performance by as much as 32\\%. This improvement demonstrates the gains that can be achieved by introducing robustness as a hyperparameter and tuning it accordingly.",
      "In Figure~ we set \\power to a wide range of values and report errors for each setting, using the same evaluation of . We see that for high-noise inputs more negative values of \\power are preferable, but for low-noise inputs values closer to 0 are optimal. We report the lowest-error entry for each noise level as ``gFGR*'' in Table~ where we see a significant reduction in error, thereby demonstrating the improvement that can be achieved from treating robustness as a hyperparameter.",
      "Because the analytical form of \\partition{\\power detailed in the paper is difficult to evaluate efficiently for any real number, and especially difficult to differentiate with respect to \\power, we approximate \\log(\\partition{\\power) using cubic hermite spline interpolation in a transformed space. Efficiently approximating \\log(\\partition{\\power) with a spline is difficult, as we would like a concise approximation that holds over the entire valid range \\power \\geq 0, and we would like to allocate more precision in our spline interpolation to values near \\power=2 (which is where \\log(\\partition{\\power) varies most rapidly). To accomplish this, we first apply a monotonic nonlinearity to \\power that stretches values near \\power=2 (thereby increasing the density of spline knots in this region) and compresses values as \\power \\gg 4, for which we use: \\end{equation This curve is roughly piecewise-linear in [0, 4] with a slope of \\sim\\!\\!\\!1 at \\power=0 and \\power=4, but with a slope of \\sim\\!\\!\\!10 at \\power=2.",
      "To further demonstrate the value of independently modeling the robustness of each individual coefficient, we ran a more thorough experiment in which we densely sampled values for \\power in [0, 2] that are used for all coefficients. In Figure~ we visualize the validation set ELBO for each fixed value of \\power compared to an independently-adapted model. As we can see, though quality can be improved by selecting a value for \\power in between 0 and 2, no single global setting of the shape parameter matches the performance achieved by allowing each coefficient's shape parameter to automatically adapt itself to the training data.",
      "For the ``fixed'' models we use a constant value for \\power for all wavelet coefficients, and observe that though performance is improved relative to the baseline, no single value of \\power is optimal. The \\power=1 entry is simply a smoothed version of the L1 loss used by the baseline model, suggesting that just using a wavelet representation improves performance. In the ``annealing \\power=2\\!\\rightarrow\\!0'' model we linearly interpolate \\power from 2 (L2) to 0",
      "% \\end{equation As we have shown, this loss function is a superset of the Welsch/Leclerc, Geman-McClure, Cauchy/Lorentzian, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. To enable gradient-based optimization we can derive the derivative of \\lossfun{x, \\power, c with respect to x: \\end{equation Our loss and its derivative are visualized for different values of \\power in Figure~. The shape of the derivative gives some intuition as to how \\power affects behavior when our loss is being minimized by gradient descent or some related method. For all values of \\power the derivative is approximately linear when \\abs{x < c, so the effect of a small residual is always linearly proportional to that residual's magnitude.",
      "By letting the \\power^{(i) parameters be free variables alongside the scale parameters, training is able to adaptively select both the scale and robustness of the VAE's posterior distribution over pixel values. We restrict all \\power^{(i) to be in (0, 3), which allows our distribution to generalize Cauchy (\\power=0) and Normal (\\power = 2) distributions and anything in between, as well as more platykurtic distributions (\\power > 2) which helps for this task. We limit \\power to be less than 3 because of the increased risk of numerical instability during training as \\power increases.",
      "For some values of \\power the partition function is relatively straightforward: where K_n(\\cdot) is the modified Bessel function of the second kind. For any rational positive \\power (excluding a singularity at \\power=2) where \\power = \\sfrac{n{d with n, d \\in \\mathbb{N, we see that \\right| \\left(\\frac{1{n - \\frac{1{2d\\right)^{2d \\right) \\nonumber \\end{equation \\vspace{-0.08in where G(\\cdot) is the Meijer G-function and \\mathbf{b_q is a multiset (items may occur twice). Because the partition function is difficult to evaluate or differentiate, in our experiments we approximate \\log(\\partition{\\power) with a cubic hermite spline (see Appendix~ for details)."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 0,\n    \"Rationale\": \"Image 1 shows the performance of the gRCC algorithm on various datasets for different values of the shape parameter \u03b1, which is related to \\u007Fpower. Text 0 discusses the sensitivity of performance to \\u007Fpower, mentioning improvements up to 32%.\",\n    \"Answer\": \"The dataset with the greatest sensitivity to the choice of \\u007Fpower is the one with the largest performance variation in Image 1, which appears to be the 'YaleB' dataset.\"\n}"
  },
  "12": {
    "question": "How does the shape of the negative log-likelihood (NLL) and probability density functions change as the value of \u03b1 increases?",
    "ground_truth": " As the value of \u03b1 increases, the NLL functions become more peaked and the probability density functions become more concentrated around the mean.",
    "selected_figures_names": [
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure4-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure5-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 3,
    "rationale": "Image 2 provides a visual representation of how the log partition function changes with respect to \u03b1, which is directly related to the shape of the NLL and probability density functions. Text 3 explains the behavior of the distribution and NLL as \u03b1 changes, highlighting the transition towards a Gaussian distribution near the origin.",
    "answer": "As the value of \u03b1 increases, the shape of the negative log-likelihood (NLL) and probability density functions transitions towards a Gaussian distribution near the origin. This is due to the smoothing effect of the distribution, which becomes more pronounced with higher \u03b1 values.",
    "text_chunks": [
      "The loss is invariant to a simultaneous scaling of c and x: The loss increases monotonically with respect to \\power: This is convenient for graduated non-convexity~: we can initialize \\power such that our loss is convex and then gradually reduce \\power (and therefore reduce convexity and increase robustness) during optimization, thereby enabling robust estimation that (often) avoids local minima. We can take the limit of the loss as \\power approaches infinity, which due to Eq.~ must be the upper bound of the loss: We can bound the magnitude of the gradient of the loss, which allows us to better reason about exploding gradients: \\end{equation L1 loss is not expressible by our loss, but if c is much smaller than x we can approximate it with \\power = 1: See Appendix~ for other potentially-useful properties that are not used in our experiments. Probability Density Function With our loss function we can construct a general probability distribution, such that the negative log-likelihood (NLL) of its PDF is a shifted version of our loss function: where \\prob{x \\;|\\; \\mu, \\power, c is only defined if \\power \\geq 0, as \\partition{\\power is divergent when \\power < 0.",
      "When used as a negative log-likelihood, this loss gives a general probability distribution that includes normal and Cauchy distributions as special cases. This distribution lets us train neural networks in which the loss has an adaptive degree of robustness for each output dimension, which allows training to automatically determine how much robustness should be imposed by the loss without any manual parameter tuning. When this adaptive loss is paired with image representations in which variable degrees of heavy-tailed behavior occurs, such as wavelets, this adaptive training approach allows us to improve the performance of variational autoencoders for image synthesis and of neural networks for unsupervised monocular depth estimation.",
      "Sampling from our distribution is straightforward given the observation that -\\log\\left(\\prob{x \\;|\\; 0, \\power, 1\\right) is bounded from below by \\rho(x, 0, 1) + \\log(Z(\\power)) (shifted Cauchy loss). See Figure~ for visualizations of this bound when \\power=\\infty, which also bounds the NLL for all values of \\power.",
      "Our distribution resembles the generalized Gaussian distribution~, except that it is ``smoothed'' so as to approach a Gaussian distribution near the origin regardless of the shape parameter \\power. The PDF and NLL of our distribution for different values of \\power can be seen in Figure~. In later experiments we will use the NLL of our general distribution -\\log(p(\\cdot | \\alpha, c)) as the loss for training our neural networks, not our general loss \\lossfun{\\cdot, \\power, c. Critically, using the NLL allows us to treat \\power as a free parameter, thereby allowing optimization to automatically determine the degree of robustness that should be imposed by the loss being used during training. To understand why the NLL must be used for this, consider a training procedure in which we simply minimize \\lossfun{\\cdot, \\power, c with respect to \\power and our model weights.",
      "To circumvent these issues we can slightly modify our loss (and its gradient and \\Psi-function) to guard against singularities and make implementation easier: b =& \\abs{\\power - 2 + \\epsilon \\quad\\quad d = \\begin{cases \\power + \\epsilon & \\text{if \\power \\geq 0 \\\\ \\power - \\epsilon & \\text{if \\power < 0 \\end{cases \\nonumber \\end{align* \\end{minipage Where \\epsilon is some small value, such as 10^{-5. Note that even very small values of \\epsilon can cause significant inaccuracy between our true partition function \\partition{\\power and the effective partition function of our approximate distribution when \\power is near 0, so this approximate implementation should be avoided when accurate values of \\partition{\\power are necessary. Partition Function Approximation Implementing the negative log-likelihood of our general distribution (ie, our adaptive loss) requires a tractable and differentiable approximation of its log partition function.",
      "At the origin the IRLS weight of our loss is \\frac{1{c^2: For all values of \\power, when \\abs{x is small with respect to c the loss is well-approximated by a quadratic bowl: Because the second derivative of the loss is maximized at x = 0, this quadratic approximation tells us that the second derivative is bounded from above: When \\power is negative the loss approaches a constant as |x| approaches infinity, letting us bound the loss: The loss's \\Psi-function increases monotonically with respect to \\power when \\power < 2 for all values of z in [0, 1]: The roots of the second derivative of \\lossfun{x, \\power, c are: This tells us at what value of x the loss begins to redescend. This point has a magnitude of c when \\power = -\\infty, and that magnitude increases as \\power increases. The root is undefined when \\power \\geq 1, as our loss is redescending iff \\power < 1.",
      "Additionally, only after dropping this scale factor does setting \\power to a negative value yield a family of meaningful robust loss functions, such as Geman-McClure loss. But this loss function still has several unintuitive properties: the loss is non-zero when x=0 (assuming a non-zero value of c), and the curvature of the quadratic ``bowl'' near x=0 varies as a function of c and \\power. We therefore construct a shifted and scaled version of Equation~ that does not have these properties: This loss generalizes L2, Cauchy, and Geman-McClure loss, but it has the unfortunate side-effect of flattening out to 0 when \\power \\ll 0, thereby prohibiting many annealing strategies.",
      "In this scenario, the monotonicity of our general loss with respect to \\power (Eq.~) means that optimization can trivially minimize the cost of outliers by setting \\power to be as small as possible. Now consider that same training procedure in which we minimize the NLL of our distribution instead of our loss. As can be observed in Figure~, reducing \\power will decrease the NLL of outliers but will increase the NLL of inliers."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 3,\n    \"Rationale\": \"Image 2 provides a visual representation of how the log partition function changes with respect to \u03b1, which is directly related to the shape of the NLL and probability density functions. Text 3 explains the behavior of the distribution and NLL as \u03b1 changes, highlighting the transition towards a Gaussian distribution near the origin.\",\n    \"Answer\": \"As the value of \u03b1 increases, the shape of the negative log-likelihood (NLL) and probability density functions transitions towards a Gaussian distribution near the origin. This is due to the smoothing effect of the distribution, which becomes more pronounced with higher \u03b1 values.\"\n}"
  },
  "13": {
    "question": "How do the results of the baseline and the proposed method compare in terms of accuracy?",
    "ground_truth": "The proposed method appears to be more accurate than the baseline method. The depth maps generated by the proposed method are more detailed and realistic than those generated by the baseline method.",
    "selected_figures_names": [
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Figure15-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Figure15-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure3-1.png",
      "1701.03077v10-Figure11-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 5,
    "rationale": "Image 1 shows a comparison of depth estimation results using the baseline and proposed methods, highlighting the improvements. Text 5 discusses the substantial and consistent improvements in the adaptive model's output compared to the baseline on the KITTI dataset.",
    "answer": "The proposed method shows significantly improved accuracy in depth estimation compared to the baseline, as evidenced by the more accurate depth maps in the images and the consistent improvements mentioned in the text.",
    "text_chunks": [
      "We use this procedure when reproducing the baseline performance of and when evaluating our own models. \\renewcommand{\\vaewidth{0.3\\linewidth \\renewcommand{\\vaewidth{0.072\\linewidth \\renewcommand{\\sfmwidth{0.41\\linewidth \\renewcommand{\\nametag{000 \\end{appendices",
      "Our registration results were produced using the code release corresponding to . Because the numbers presented in have low precision, we reproduced the performance of the baseline FGR algorithm using this code. This code included some evaluation details that were omitted from the paper that we determined through correspondence with the author: for each input, FGR is run 20 times with random initialization and the median error is reported.",
      "All training and evaluation was performed on the KITTI dataset using the same training/test split as . \\newcommand{\\sfmwidth{0.93\\linewidth \\newcommand{\\nametag{153 Results can be seen in Table~. We present the error and accuracy metrics used in and our own ``average'' error measure: the geometric mean of the four errors and one minus the three accuracies. The ``Baseline`` models use the loss function of , and we present both the numbers in (``as reported'') and our own numbers from running the code from ourselves (``reproduced'').",
      "For the ``fixed'' models we use a constant value for \\power for all wavelet coefficients, and observe that though performance is improved relative to the baseline, no single value of \\power is optimal. The \\power=1 entry is simply a smoothed version of the L1 loss used by the baseline model, suggesting that just using a wavelet representation improves performance. In the ``annealing \\power=2\\!\\rightarrow\\!0'' model we linearly interpolate \\power from 2 (L2) to 0",
      "Again, our goal is not to advance the state of the art for any particular image synthesis task, but is instead to explore the value of our distribution in an experimentally controlled setting. In our baseline model we give each pixel's normal distribution a variable scale parameter \\sigma^{(i) that will be optimized over during training, thereby allowing the VAE to adjust the scale of its distribution for each output dimension. We can straightforwardly replace this per-pixel normal distribution with a per-pixel general distribution, in which each output dimension is given a distinct shape parameter \\power^{(i) in addition to its scale parameter c^{(i) (\\ie, \\sigma^{(i)).",
      "In Figures~ and we present many more results from the test split of the KITTI dataset, in which we compare our ``adaptive'' model's output to the baseline model and the ground-truth depth. The improvement we see is substantial and consistent across a variety of scenes. Fast Global Registration",
      "Because the analytical form of \\partition{\\power detailed in the paper is difficult to evaluate efficiently for any real number, and especially difficult to differentiate with respect to \\power, we approximate \\log(\\partition{\\power) using cubic hermite spline interpolation in a transformed space. Efficiently approximating \\log(\\partition{\\power) with a spline is difficult, as we would like a concise approximation that holds over the entire valid range \\power \\geq 0, and we would like to allocate more precision in our spline interpolation to values near \\power=2 (which is where \\log(\\partition{\\power) varies most rapidly). To accomplish this, we first apply a monotonic nonlinearity to \\power that stretches values near \\power=2 (thereby increasing the density of spline knots in this region) and compresses values as \\power \\gg 4, for which we use: \\end{equation This curve is roughly piecewise-linear in [0, 4] with a slope of \\sim\\!\\!\\!1 at \\power=0 and \\power=4, but with a slope of \\sim\\!\\!\\!10 at \\power=2.",
      "We will demonstrate that our general distribution can be used to improve the log-likelihood performance of VAEs for image synthesis on the CelebA dataset~. A common design decision for VAEs is to model images using an independent normal distribution on a vector of RGB pixel values , and we use this design as our baseline model. Recent work has improved upon this model by using deep, learned, and adversarial loss functions . Though it's possible that our general loss or distribution can add value in these circumstances, to more precisely isolate our contribution we will explore the hypothesis that the baseline model of normal distributions placed on a per-pixel image representation can be improved significantly with the small change of just modeling a linear transformation of a VAE's output with our general distribution."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 5,\n    \"Rationale\": \"Image 1 shows a comparison of depth estimation results using the baseline and proposed methods, highlighting the improvements. Text 5 discusses the substantial and consistent improvements in the adaptive model's output compared to the baseline on the KITTI dataset.\",\n    \"Answer\": \"The proposed method shows significantly improved accuracy in depth estimation compared to the baseline, as evidenced by the more accurate depth maps in the images and the consistent improvements mentioned in the text.\"\n}"
  },
  "14": {
    "question": "How does the choice of distribution affect the quality of the reconstructions?",
    "ground_truth": "Reconstructions from models using general distributions tend to be sharper and more detailed than reconstructions from the corresponding model that uses normal distributions.",
    "selected_figures_names": [
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table1-1.png"
    ],
    "all_figures_names": [
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure8-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure14-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure15-1.png",
      "1701.03077v10-Figure11-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 1,
    "rationale": "Image 3 provides a comparison of ELBO scores for different distributions, showing how the choice of distribution affects model performance. Text 1 discusses the impact of different distributions on the quality of image samples, highlighting the advantages of using certain distributions over others.",
    "answer": "The choice of distribution significantly affects the quality of reconstructions. Models using the general distribution produce sharper and more detailed samples compared to those using normal distributions. The 'Wavelets + YUV' representation with the general distribution maximizes validation set ELBO, indicating better reconstruction quality.",
    "text_chunks": [
      "\\newcommand{\\noisewidth{0.45\\linewidth See Figure~ for additional samples from our models, and see Figure~ for reconstructions from our models on validation-set images. As is common practice, the samples and reconstructions in those figures and in the paper are the means of the output distributions of the decoder, not samples from those distributions. That is, we draw samples from the latent encoded space and then decode them, but we do not draw samples in our output space.",
      "For all representations, VAEs trained with our general distribution produce sharper and more detailed samples than those trained with normal distributions. Models trained with Cauchy and t-distributions preserve high-frequency detail and work well on pixel representations, but systematically fail to synthesize low-frequency image content when given non-pixel representations, as evidenced by the gray backgrounds of those samples. Comparing performance across image representations shows that the ``Wavelets + YUV'' representation best maximizes validation set ELBO --- though if we were to limit our model to only normal distributions the ``DCT + YUV'' model would appear superior, suggesting that there is value in reasoning jointly about distributions and image representations.",
      "We will demonstrate that our general distribution can be used to improve the log-likelihood performance of VAEs for image synthesis on the CelebA dataset~. A common design decision for VAEs is to model images using an independent normal distribution on a vector of RGB pixel values , and we use this design as our baseline model. Recent work has improved upon this model by using deep, learned, and adversarial loss functions . Though it's possible that our general loss or distribution can add value in these circumstances, to more precisely isolate our contribution we will explore the hypothesis that the baseline model of normal distributions placed on a per-pixel image representation can be improved significantly with the small change of just modeling a linear transformation of a VAE's output with our general distribution.",
      "Though modeling images using independent distributions on pixel intensities is a popular choice due to its simplicity, classic work in natural image statistics suggest that images are better modeled with heavy-tailed distributions on wavelet-like image decompositions . We therefore train additional models in which our decoded RGB per-pixel images are linearly transformed into spaces that better model natural images before computing the NLL of our distribution. For this we use the DCT and the CDF 9/7 wavelet decomposition , both with a YUV colorspace.",
      "We see that our general distribution performs similarly to a Student's t-distribution, with both producing higher ELBOs than any fixed distribution across all representations. These two adaptive distributions appear to have complementary strengths: ours can be more platykurtic (\\alpha>2) while a t-distribution can be more leptokurtic (\\nu < 1), which may explain why neither model consistently outperforms the other across representations. Note that the t-distribution's NLL does not generalize the Charbonnier, L1, Geman-McClure, or Welsch losses, so unlike ours it will not generalize the losses used in the other tasks we will address.",
      "We replace that fixed Laplacian distribution with our general distribution, keeping our scale fixed but allowing the shape parameter \\power to vary. Following our observation from Section~ that YUV wavelet representations work well when modeling images with our loss function, we impose our loss on a YUV wavelet decomposition instead of the RGB pixel representation of . The only changes we made to the code from were to replace its loss function with our own and to remove the model components that stopped yielding any improvement after the loss function was replaced (see Appendix~ for details).",
      "Because the reconstruction loss in is the sum of the means of the losses imposed at each scale in a D-level pyramid of side predictions, we use a D level normalized wavelet decomposition (wherein images in [0,1] result in wavelet coefficients in [0,1]) and then scale each coefficient's loss by 2^d, where d is the coefficient\u2019s level. In Figure~ we visualize the final shape parameters for each output coefficient that were converged upon during training. These results provide some insight into why our adaptive model produces improved results compared to the ablations of our model in which we use a single fixed or annealed value for \\power for all output coefficients.",
      "This lets us perform rejection sampling using a Cauchy as the proposal distribution. Because our distribution is a location-scale family, we sample from \\prob{x \\;|\\; 0, \\power, 1 and then scale and shift that sample by c and \\mu respectively. This sampling approach is efficient, with an acceptance rate between \\sim\\!45\\% (\\power=\\infty) and 100\\% (\\power = 0)."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 1,\n    \"Rationale\": \"Image 3 provides a comparison of ELBO scores for different distributions, showing how the choice of distribution affects model performance. Text 1 discusses the impact of different distributions on the quality of image samples, highlighting the advantages of using certain distributions over others.\",\n    \"Answer\": \"The choice of distribution significantly affects the quality of reconstructions. Models using the general distribution produce sharper and more detailed samples compared to those using normal distributions. The 'Wavelets + YUV' representation with the general distribution maximizes validation set ELBO, indicating better reconstruction quality.\"\n}"
  }
}