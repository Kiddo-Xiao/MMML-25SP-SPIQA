{
  "0": {
    "question": "How does the observed error compare to the underlying true error as CPU time increases?",
    "ground_truth": "The observed error is initially higher than the underlying true error, but it quickly decreases and converges to the true error as CPU time increases.",
    "selected_figures_names": [
      "1702.03584v3-Figure1-1.png"
    ],
    "all_figures_names": [
      "1702.03584v3-Table1-1.png",
      "1702.03584v3-Figure1-1.png",
      "1702.03584v3-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 shows the observed and underlying true error rates over CPU time, which directly addresses the question. Text 1 provides a description of how these error rates converge over time, complementing the visual data.",
    "answer": "As CPU time increases, both the observed error and the underlying true error decrease. The observed error is consistently lower than the underlying true error throughout the process.",
    "text_chunks": [
      "This figure clearly demonstrates that the proposed exact cyclic CD algorithm converges very fast -- it only takes 1 second and 8 iterations to converge. Besides, the reconstruction accuracy is also very encouraging. The observed error and the underlying true error rates are close to each other and both of them are only about 0.1\\%.",
      "We call the proposed algorithm to factorize matrix \\tilde \\A by setting the dimensionality d=15. To measure the performance of the proposed method, we compute two error rates, i.e., the observed error \\|P_{\\Omega (\\tilde\\A\\ -\\X\\X^\\top)\\|_F/\\|P_{\\Omega (\\tilde\\A)\\|_F and the underlying true error rate \\|\\A\\ -\\X\\X^\\top\\|_F/\\|\\A\\|_F, at each iteration. Figure~ shows how they converge as a function of time.",
      "To further demonstrate the effectiveness and efficiency of our proposed algorithm, we compare our model with the naive gradient descent . For the 85 UCR time series datasets~, our average running time is only 2.7 seconds, roughly 3 times more efficient than the gradient descent method with a learning rate 0.1 to converge to a similar accuracy. \\vspace{-3pt Experiments \\vspace{-2pt{\\bf Experimental setup: In this section, we thoroughly evaluate the proposed framework, i.e., Similarity PreservIng RepresentAtion Learning (SPIRAL for short), on the time series clustering task.",
      "However, computing all the pairwise similarities requires calling the DTW \\mathcal{O(n^2) times, which is time-consuming when n is large. As a concrete example, generating a full similarity matrix for n=150,000 time series of length 30 takes more than 28 hours on an Intel Xeon 2.40 GHz processor with 256 GB of main memory. To significantly reduce the running time, we note that time series similarity measures, including DTW, usually capture the co-movements of time series, which has shown to be driven by only a small number of latent factors~.",
      "Although it looks similar to the idea of our SPIRAL-kMeans that also utilizes the DTW and kMeans algorithms, it is less desirable than SPIRAL-kMeans mainly because: (i) kMeans-DTW suffers from a very high computational cost since it needs to compute the pairwise DTW distances between all the time series and all the cluster centers at each iteration; and (ii) the DTW distance does not satisfy the triangle inequality, thus can make the cluster centers computed by averaging multiple time series drift out of the cluster~. By designing an efficient algorithm that only needs to call the DTW function O(n \\log n) times and by embedding time series data to the Euclidean space while preserving their original similarities, the proposed method SPIRAL successfully addresses both these issues. % {\\bf Experimental results: We use the normalized mutual information (NMI for short) to measure the coherence between the inferred clustering and the ground truth categorization. NMI scales from 0 to 1, and a higher NMI score implies a better partition.",
      "The convergence criteria is defined as the objective decreases to be less than 1e-5 in one iteration. Given the learned features, we then feed them into some static clustering models and compare them with some state-of-the-art time series clustering methods. To conduct fair comparisons, in all DTW related algorithms and all datasets, we set the DTW window size to be the best warping size reported in .",
      "% These limitations inspire us to propose a problem-independent and similarity preserving representation learning framework for time series clustering. \\vspace{-3pt Similarity Preserving Representation Learning for Time Series Clustering \\vspace{-2pt In this section, we first present the general framework of our similarity preserving time series representation learning method, and then propose an extremely efficient algorithm that is significantly faster than a naive implementation. Problem Definition and General Framework Given a set of n time series \\mathcal{T=\\{T_1, \\cdots, T_n\\ with equal or unequal lengths, our goal is to convert them to a matrix \\X\\in \\R^{n\\times d such that the time series similarities are well preserved after the transformation.",
      "{\\bf Baseline methods: We feed the features learned by our proposed framework into the kMeans algorithm as our clustering method, denoted as SPIRAL-DTW-kMeans. To compare it with the existing time series representation methods, we respectively use the Laplacian transformation~ and the complex-valued linear dynamical systems (CLDS)~ for extracting the same number of features as ours. We then feed the learned features into the kMeans algorithm, forming two baselines Laplace-DTW-kMeans and CLDS-kMeans."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 shows the observed and underlying true error rates over CPU time, which directly addresses the question. Text 1 provides a description of how these error rates converge over time, complementing the visual data.\",\n    \"Answer\": \"As CPU time increases, both the observed error and the underlying true error decrease. The observed error is consistently lower than the underlying true error throughout the process.\"\n}"
  },
  "1": {
    "question": "How does the performance of SPIRAL-DTW-kMeans compare to k-Shape and CLDS?",
    "ground_truth": "SPIRAL-DTW-kMeans performs better than k-Shape and CLDS on most datasets.",
    "selected_figures_names": [
      "1702.03584v3-Table1-1.png",
      "1702.03584v3-Figure1-1.png",
      "1702.03584v3-Figure2-1.png"
    ],
    "all_figures_names": [
      "1702.03584v3-Table1-1.png",
      "1702.03584v3-Figure1-1.png",
      "1702.03584v3-Figure2-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 0,
    "rationale": "Image 2 provides a visual comparison of SPIRAL-DTW-kMeans with k-Shape and CLDS, showing datasets where SPIRAL-DTW-kMeans performs better. Text 0 provides specific performance statistics comparing SPIRAL-DTW-kMeans to k-Shape and CLDS.",
    "answer": "SPIRAL-DTW-kMeans performs better than k-Shape in 72.9% of datasets and better than CLDS in 61.2% of datasets.",
    "text_chunks": [
      "% Figure~ (a)(b) show that SPIRAL-DTW-kMeans performs better in 62 (which is 72.9\\%) and 52 (61.2\\%) out of 85 datasets in comparison to k-Shape and CLDS, respectively. Though not plotted, it is also higher in 57 (67.1\\%), 57 (67.1\\%) and 66 (77.6\\%) datasets compared with Laplace-DTW-kMeans, kMedoids-DTW, and kMeans-DTW. In all these comparisons, the statistical test demonstrates the superiority of SPIRAL-DTW-kMeans.",
      "The figures show that SPIRAL-MSM-kMeans performs better in 69 (which is 81.2\\%) and 64 (75.3\\%) out of 85 datasets in comparison to Laplace-MSM-kMeans and kMedoids-MSM, respectively. % Table reports the average NMIs of all the algorithms, and the percentage of datasets over which the baseline methods perform worse than our algorithm SPIRAL-MSM-kMeans. The table clearly shows that our proposed methods SPIRAL-MSM-kMeans and SPIRAL-DTW-kMeans yield the overall best performance.",
      "% For instance, clustering the ElectricDevice (ED) dataset with 16,637 time series takes k-Shape, kMeans-DTW and Laplace-DTW-kMeans 20 minutes, 169 minutes and 5 hours, respectively. As a comparison, our clustering algorithms SPIRAL-DTW-kMeans and SPIRAL-MSM-kMeans only spend less than 2 and 9 minutes, respectively, to partition the ED dataset. According to the extensive experimental results, we have the following observations: To sum up, the proposed framework is % effective, efficient, and flexible.",
      "Although it looks similar to the idea of our SPIRAL-kMeans that also utilizes the DTW and kMeans algorithms, it is less desirable than SPIRAL-kMeans mainly because: (i) kMeans-DTW suffers from a very high computational cost since it needs to compute the pairwise DTW distances between all the time series and all the cluster centers at each iteration; and (ii) the DTW distance does not satisfy the triangle inequality, thus can make the cluster centers computed by averaging multiple time series drift out of the cluster~. By designing an efficient algorithm that only needs to call the DTW function O(n \\log n) times and by embedding time series data to the Euclidean space while preserving their original similarities, the proposed method SPIRAL successfully addresses both these issues. % {\\bf Experimental results: We use the normalized mutual information (NMI for short) to measure the coherence between the inferred clustering and the ground truth categorization. NMI scales from 0 to 1, and a higher NMI score implies a better partition.",
      "Our source code and the detailed experimental results are publicly available. Since data clustering is an unsupervised learning problem, we merge the training and testing sets of all the datasets. % Given the features learned by the proposed framework SPIRAL with DTW similarity, we feed them into the kMeans algorithm as our clustering method, denoted as SPIRAL-DTW-kMeans.",
      "In addition to using DTW similarity in the SPIRAL framework, we also test our framework with another similarity measure move-split-merge (MSM), and denote this clustering method as SPIRAL-MSM-kMeans. % Figure~ (c)(d) summarize the performance of all the MSM-based algorithms. %",
      "{\\bf Baseline methods: We feed the features learned by our proposed framework into the kMeans algorithm as our clustering method, denoted as SPIRAL-DTW-kMeans. To compare it with the existing time series representation methods, we respectively use the Laplacian transformation~ and the complex-valued linear dynamical systems (CLDS)~ for extracting the same number of features as ours. We then feed the learned features into the kMeans algorithm, forming two baselines Laplace-DTW-kMeans and CLDS-kMeans.",
      "% Another baseline is the state-of-the-art time series clustering algorithm k-Shape~, which has been shown to outperform many state-of-the-art partitional, hierarchical, and spectral time series clustering approaches. Besides, we also compare our method with clustering algorithms kMeans-DTW and kMedoids-DTW since our ideas are similar in some respects. kMeans-DTW is a popular time series clustering algorithm that uses DTW algorithm to measure pairwise distances between data points."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 0,\n    \"Rationale\": \"Image 2 provides a visual comparison of SPIRAL-DTW-kMeans with k-Shape and CLDS, showing datasets where SPIRAL-DTW-kMeans performs better. Text 0 provides specific performance statistics comparing SPIRAL-DTW-kMeans to k-Shape and CLDS.\",\n    \"Answer\": \"SPIRAL-DTW-kMeans performs better than k-Shape in 72.9% of datasets and better than CLDS in 61.2% of datasets.\"\n}"
  },
  "2": {
    "question": "Which method performs the best in terms of NMI and what percentage of datasets does it outperform the other methods on?",
    "ground_truth": "The SPIRAL-MSM-kMeans method performs the best in terms of NMI with a score of 0.365. It outperforms the other methods on 89.4% of the datasets.",
    "selected_figures_names": [
      "1702.03584v3-Table1-1.png",
      "1702.03584v3-Figure2-1.png"
    ],
    "all_figures_names": [
      "1702.03584v3-Table1-1.png",
      "1702.03584v3-Figure1-1.png",
      "1702.03584v3-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides a table with NMI scores and percentages of datasets where SPIRAL-MSM-kMeans outperforms other methods. Text 0 discusses the performance of SPIRAL-MSM-kMeans compared to other methods, highlighting its superior performance.",
    "answer": "SPIRAL-MSM-kMeans performs the best in terms of NMI with a score of 0.365. It outperforms other methods on 81.2% of the datasets.",
    "text_chunks": [
      "The figures show that SPIRAL-MSM-kMeans performs better in 69 (which is 81.2\\%) and 64 (75.3\\%) out of 85 datasets in comparison to Laplace-MSM-kMeans and kMedoids-MSM, respectively. % Table reports the average NMIs of all the algorithms, and the percentage of datasets over which the baseline methods perform worse than our algorithm SPIRAL-MSM-kMeans. The table clearly shows that our proposed methods SPIRAL-MSM-kMeans and SPIRAL-DTW-kMeans yield the overall best performance.",
      "Although it looks similar to the idea of our SPIRAL-kMeans that also utilizes the DTW and kMeans algorithms, it is less desirable than SPIRAL-kMeans mainly because: (i) kMeans-DTW suffers from a very high computational cost since it needs to compute the pairwise DTW distances between all the time series and all the cluster centers at each iteration; and (ii) the DTW distance does not satisfy the triangle inequality, thus can make the cluster centers computed by averaging multiple time series drift out of the cluster~. By designing an efficient algorithm that only needs to call the DTW function O(n \\log n) times and by embedding time series data to the Euclidean space while preserving their original similarities, the proposed method SPIRAL successfully addresses both these issues. % {\\bf Experimental results: We use the normalized mutual information (NMI for short) to measure the coherence between the inferred clustering and the ground truth categorization. NMI scales from 0 to 1, and a higher NMI score implies a better partition.",
      "% Figure~ (a)(b) show that SPIRAL-DTW-kMeans performs better in 62 (which is 72.9\\%) and 52 (61.2\\%) out of 85 datasets in comparison to k-Shape and CLDS, respectively. Though not plotted, it is also higher in 57 (67.1\\%), 57 (67.1\\%) and 66 (77.6\\%) datasets compared with Laplace-DTW-kMeans, kMedoids-DTW, and kMeans-DTW. In all these comparisons, the statistical test demonstrates the superiority of SPIRAL-DTW-kMeans.",
      "Besides, the MSM-based methods perform slighly better than the DTW-based methods, which is consistent with the observation in~. Moreover, the results also verify that the proposed method () is a better choice than the Laplacian method for converting DTW and MSM distances. In addition to superior performance, our proposed framework has a significantly lower running time than all the baseline algorithms.",
      "Besides, the derivative \\nabla\\psi(\\X_{ji) is a third-degree polynomial, thus its roots can be computed in a closed form. By using Cardano's method~, the optimal solution of \\X_{ji can be calculated in a constant time given the computed p and q. Likewise, lines 6 and 12 of the algorithm also take \\mathcal{O(n\\log n) time since matrix R can be updated by only considering the observed entries. To sum up, the proposed algorithm has a per-iteration cost of \\mathcal{O(d n\\log n), % which is significantly faster than direct factorization of the whole matrix that take at least \\mathcal{O(dn^2) time in each iteration~.% The following theorem shows that Algorithm guarantees to converge to a stationary point of~\\eqref{eqn:fact.",
      "In addition to using DTW similarity in the SPIRAL framework, we also test our framework with another similarity measure move-split-merge (MSM), and denote this clustering method as SPIRAL-MSM-kMeans. % Figure~ (c)(d) summarize the performance of all the MSM-based algorithms. %",
      "% Another baseline is the state-of-the-art time series clustering algorithm k-Shape~, which has been shown to outperform many state-of-the-art partitional, hierarchical, and spectral time series clustering approaches. Besides, we also compare our method with clustering algorithms kMeans-DTW and kMedoids-DTW since our ideas are similar in some respects. kMeans-DTW is a popular time series clustering algorithm that uses DTW algorithm to measure pairwise distances between data points.",
      "% For instance, clustering the ElectricDevice (ED) dataset with 16,637 time series takes k-Shape, kMeans-DTW and Laplace-DTW-kMeans 20 minutes, 169 minutes and 5 hours, respectively. As a comparison, our clustering algorithms SPIRAL-DTW-kMeans and SPIRAL-MSM-kMeans only spend less than 2 and 9 minutes, respectively, to partition the ED dataset. According to the extensive experimental results, we have the following observations: To sum up, the proposed framework is % effective, efficient, and flexible."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides a table with NMI scores and percentages of datasets where SPIRAL-MSM-kMeans outperforms other methods. Text 0 discusses the performance of SPIRAL-MSM-kMeans compared to other methods, highlighting its superior performance.\",\n    \"Answer\": \"SPIRAL-MSM-kMeans performs the best in terms of NMI with a score of 0.365. It outperforms other methods on 81.2% of the datasets.\"\n}"
  }
}