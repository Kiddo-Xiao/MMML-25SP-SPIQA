{
  "0": {
    "question": "What are the three types of sentences that the annotators are asked to write?",
    "ground_truth": "The three types of sentences are: \n1. A sentence that is definitely correct about the situation or event in the line.\n2. A sentence that might be correct about the situation or event in the line.\n3. A sentence that is definitely incorrect about the situation or event in the line.",
    "selected_figures_names": [
      "1704.05426v4-Figure1-1.png"
    ],
    "all_figures_names": [
      "1704.05426v4-Table4-1.png",
      "1704.05426v4-Table1-1.png",
      "1704.05426v4-Figure1-1.png",
      "1704.05426v4-Table5-1.png",
      "1704.05426v4-Table2-1.png",
      "1704.05426v4-Table3-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 directly shows the prompt asking annotators to write three types of sentences. Text 3 provides context on how sentence pairs are collected, which aligns with the task described in the image.",
    "answer": "The annotators are asked to write one sentence that is definitely correct, one that might be correct, and one that is definitely incorrect about the situation or event in the line.",
    "text_chunks": [
      "There are five unique prompts in total: one for written non-fiction genres (\\textsc{Slate, OUP, Government, Verbatim, Travel; Figure~), one for spoken genres (\\textsc{Telephone, Face-to-Face), one for each of the less formal written genres (\\textsc{Fiction, Letters), and a specialized one for \\textsc{9/11, tailored to fit its potentially emotional content. Each prompt is accompanied by example premises and hypothesis that are specific to each genre. Below the instructions, we present three text fields---one for each label---followed by a field for reporting issues, and a link to the frequently asked questions (FAQ) page.",
      "Validation instructions are tailored by genre, based on the main data collection prompt (Figure ); a single FAQ, modeled after the validation FAQ from SNLI, is provided for reference. In order to encourage thoughtful labeling, we manually label one percent of the validation examples and offer a \\1 bonus each time a worker selects a label that matches ours. For each validated sentence pair, we assign a gold label representing a majority vote between the initial label assigned to the pair by the original annotator, and the four additional labels assigned by validation annotators.",
      "We used this platform to hire an organized group of workers. 387 annotators contributed through this group, and at no point was any identifying information about them, including demographic information, available to the authors. \\paragraph{Validation We perform an additional round of annotation on test and development examples to ensure accurate labelling.",
      "Although SNLI is collected in largely the same way as MultiNLI, and is also permissively licensed, we do not include SNLI in the MultiNLI corpus distribution. SNLI can be appended and treated as an unusually large additional \\textsc{captions genre, built on image captions from the Flickr30k corpus . \\paragraph{Hypothesis Collection To collect a sentence pair, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate whenever the premise is true (paired with the premise and labeled \\textsc{entailment), one which is necessarily false or inappropriate whenever the premise is true (\\textsc{contradiction), and one where neither condition applies (\\textsc{neutral).",
      "OANC data constitutes the following nine genres: {transcriptions from \\href{http://newsouthvoices.uncc.edu/{the Charlotte Narrative and Conversation Collection of two-sided, in-person conversations that took place in the early 2000s (\\textsc{Face-to-face); reports, speeches, letters, and press releases from public domain government websites (\\textsc{Government); letters from \\href{https://liberalarts.iupui.edu/icic/research/corpus_of_philanthropic_fundraising_discourse{the Indiana Center for Intercultural Communication of Philanthropic Fundraising Discourse written in the late 1990s--early 2000s (\\textsc{Letters); the public report from \\href{https://9-11commission.gov/{the National Commission on Terrorist \\-Attacks Upon the United States released on July 22, 2004 (\\textsc{9/11); five non-fiction works on the textile industry and child development published by the Oxford University Press (\\textsc{OUP); popular culture articles from the archives of Slate Magazine (\\textsc{Slate) written between 1996--2000; transcriptions from University of Pennsylvania's \\href{https://catalog.ldc.upenn.edu/LDC97S62{Linguistic Data Consortium Switchboard corpus of two-sided, telephone conversations that took place in 1990 or 1991 (\\textsc{Telephone); travel guides published by Berlitz Publishing in the early 2000s (\\textsc{Travel); and short posts about linguistics for non-specialists from the \\href{http://www.verbatimmag.com/{Verbatim archives written between 1990 and 1996 (\\textsc{Verbatim). For our tenth genre, \\textsc{Fiction, we compile several freely available works of contemporary fiction written between 1912 and 2010, spanning genres including mystery, humor, western, science fiction, and fantasy by authors Isaac Asimov, Agatha Christie, Ben Essex (Elliott Gesswell), Nick Name (Piotr Kowalczyk), Andre Norton, Lester del Ray, and \\href{http://mikeshea.net{Mike Shea. We construct premise sentences from these ten source texts with minimal preprocessing; unique the sentences within genres, exclude very short sentences (under eight characters), and manually remove certain types of non-narrative writing, such as mathematical formulae, bibliographic references, and lists.",
      "The validation phase follows the same procedure used for SICK and SNLI: Workers are presented with pairs of sentences and asked to supply a single label (\\textsc{entailment, \\textsc{contradiction, \\textsc{neutral) for the pair. Each pair is relabeled by four workers, yielding a total of five labels per example.",
      "We provide one FAQ page per prompt. FAQs are modeled on their SNLI counterparts (supplied by the authors of that work) and include additional curated examples, answers to genre-specific questions arising from our pilot phase, and information about logistical concerns like payment. For both hypothesis collection and validation, we present prompts to annotators using \\href{gethybrid.io{Hybrid (\\url{gethybrid.io), a crowdsoucring platform similar to the Amazon Mechanical Turk platform used for SNLI.",
      "This section discusses the sources of our premise sentences, our collection method for hypotheses, and our validation (relabeling) strategy. \\paragraph{Premise Text Sources The MultiNLI premise sentences are derived from ten sources of freely available text which are meant to be maximally diverse and roughly represent the full range of American English. We selected nine sources from the second release of the Open American National Corpus \\citep[OANC; ][downloaded 12/2016]{fillmore1998,macleod2000,ide2001, ide2006openANC, balancing the volume of source text roughly evenly across genres, and avoiding genres with content that would be too difficult for untrained annotators."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 3,\n    \"Rationale\": \"Image 0 directly shows the prompt asking annotators to write three types of sentences. Text 3 provides context on how sentence pairs are collected, which aligns with the task described in the image.\",\n    \"Answer\": \"The annotators are asked to write one sentence that is definitely correct, one that might be correct, and one that is definitely incorrect about the situation or event in the line.\"\n}"
  },
  "1": {
    "question": "Which type of word has the greatest difference in frequency of occurrence between MultiNLI and SNLI?",
    "ground_truth": "Negation (PTB)",
    "selected_figures_names": [
      "1704.05426v4-Table4-1.png",
      "1704.05426v4-Table1-1.png",
      "1704.05426v4-Figure1-1.png",
      "1704.05426v4-Table5-1.png"
    ],
    "all_figures_names": [
      "1704.05426v4-Table4-1.png",
      "1704.05426v4-Table1-1.png",
      "1704.05426v4-Figure1-1.png",
      "1704.05426v4-Table5-1.png",
      "1704.05426v4-Table2-1.png",
      "1704.05426v4-Table3-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 2,
    "rationale": "Image 3 provides a detailed comparison of the frequency of different types of words between MultiNLI and SNLI, which directly addresses the question. Text 2 mentions the length and completeness of sentences in MultiNLI, which is relevant to understanding the context of the data.",
    "answer": "Pronouns have the greatest difference in frequency of occurrence between MultiNLI and SNLI, with a difference of 34%.",
    "text_chunks": [
      "Similarly, in SNLI, 1.6\\% of hypotheses differ from their premises by addition, substitution, or shuffling a single word, while in MultiNLI this only happens in 1.2\\% of examples. The percentage of hypothesis-premise pairs with high token overlap (>37\\%) was comparable between MultiNLI (30\\% of pairs) and SNLI (29\\%). These statistics suggest that MultiNLI's annotations are comparable in quality to those of SNLI.",
      "We observe that the two spoken genres differ in this---with \\textsc{Face-to-face showing more complete sentences (91\\%) than \\textsc{Telephone (71\\%)---and speculate that the lack of visual feedback in a telephone setting may result in a high incidence of interrupted or otherwise incomplete sentences. Hypothesis sentences in MultiNLI generally cannot be derived from their premise sentences using only trivial editing strategies. While 2.5\\% of the hypotheses in SNLI differ from their premises by deletion, only 0.9\\% of those in MultiNLI (170 examples total) are constructed in this way.",
      "No premise sentence occurs in more than one set. \\paragraph{Statistics Table~ shows some additional statistics. Premise sentences in MultiNLI tend to be longer (max 401 words, mean 22.3 words) than their hypotheses (max 70 words), % and much longer, on average, than premises in SNLI (mean 14.1 words); premises in MultiNLI also tend to be parsed as complete sentences at a much higher rate on average (91\\%) than their SNLI counterparts (74\\%).",
      "These statistics indicate that the labels included in MultiNLI are about as reliable as those included in SNLI, despite MultiNLI's more diverse text contents. The Resulting Corpus Table shows randomly chosen development set examples from the collected corpus. Hypotheses tend to be fluent and correctly spelled, though not all are complete sentences.",
      "While its size (433k pairs) and mode of collection are modeled closely on SNLI, unlike that corpus, MultiNLI represents both written and spoken speech in a wide range of styles, degrees of formality, and topics. Our chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning. In many application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data \\citep{krizhevsky2012imagenet,zeiler2013visualizing,donahue2014decaf.",
      "First, the sentences in SNLI are derived from only a single text genre---image captions---and are thus limited to descriptions of concrete visual scenes, rendering the hypothesis sentences used to describe these scenes short and simple, and rendering many important phenomena---like temporal reasoning (e.g., yesterday), belief (e.g., know), and modality (e.g., should)---rare enough to be irrelevant to task performance. Second, because of these issues, SNLI is not sufficiently demanding to serve as an effective benchmark for NLU, with the best current model performance falling within a few percentage points of human accuracy and limited room left for fine-grained comparisons between strong models. This paper introduces a new challenge dataset, the Multi-Genre NLI Corpus (MultiNLI), whose chief purpose is to remedy these limitations by making it possible to run large-scale NLI evaluations that capture more of the complexity of modern English.",
      "For each genre represented in the training set, the model that performs best on it was trained on that genre; a model trained only on SNLI performs worse on every genre than comparable models trained on any genre from MultiNLI. Models trained on a single genre from MultiNLI perform well on similar genres; for example, the model trained on \\textsc{Telephone attains the best accuracy (63\\%) on \\textsc{Face-to-Face, which was nearly one point better than it received on itself. \\textsc{Slate seems to be a difficult and relatively unusual genre and performance on it is relatively poor in this setting; when averaging over runs trained on SNLI and all genres in the matched section of the training set, average performance on \\textsc{Slate was only 57.5\\%.",
      "Although SNLI is collected in largely the same way as MultiNLI, and is also permissively licensed, we do not include SNLI in the MultiNLI corpus distribution. SNLI can be appended and treated as an unusually large additional \\textsc{captions genre, built on image captions from the Flickr30k corpus . \\paragraph{Hypothesis Collection To collect a sentence pair, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate whenever the premise is true (paired with the premise and labeled \\textsc{entailment), one which is necessarily false or inappropriate whenever the premise is true (\\textsc{contradiction), and one where neither condition applies (\\textsc{neutral)."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 2,\n    \"Rationale\": \"Image 3 provides a detailed comparison of the frequency of different types of words between MultiNLI and SNLI, which directly addresses the question. Text 2 mentions the length and completeness of sentences in MultiNLI, which is relevant to understanding the context of the data.\",\n    \"Answer\": \"Pronouns have the greatest difference in frequency of occurrence between MultiNLI and SNLI, with a difference of 34%.\"\n}"
  },
  "2": {
    "question": "Which model performs better on the MultiNLI dataset when considering the percentage of individual labels that match the author's label?",
    "ground_truth": "SNLI performs better than MultiNLI when considering the percentage of individual labels that match the author's label. SNLI has a score of 85.8%, while MultiNLI has a score of 85.2%.",
    "selected_figures_names": [
      "1704.05426v4-Table4-1.png",
      "1704.05426v4-Table1-1.png",
      "1704.05426v4-Figure1-1.png",
      "1704.05426v4-Table5-1.png"
    ],
    "all_figures_names": [
      "1704.05426v4-Table4-1.png",
      "1704.05426v4-Table1-1.png",
      "1704.05426v4-Figure1-1.png",
      "1704.05426v4-Table5-1.png",
      "1704.05426v4-Table2-1.png",
      "1704.05426v4-Table3-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 7,
    "rationale": "Image 0 provides test set accuracies for different models on the MultiNLI dataset, which is directly relevant to evaluating model performance. Text 7 discusses the validation process and how labels are assigned, which is crucial for understanding how well models perform in terms of label matching.",
    "answer": "The ESIM model performs better on the MultiNLI dataset, achieving the highest accuracy in both matched (72.4%) and mismatched (71.9%) test sets.",
    "text_chunks": [
      "These statistics indicate that the labels included in MultiNLI are about as reliable as those included in SNLI, despite MultiNLI's more diverse text contents. The Resulting Corpus Table shows randomly chosen development set examples from the collected corpus. Hypotheses tend to be fluent and correctly spelled, though not all are complete sentences.",
      "Similarly, in SNLI, 1.6\\% of hypotheses differ from their premises by addition, substitution, or shuffling a single word, while in MultiNLI this only happens in 1.2\\% of examples. The percentage of hypothesis-premise pairs with high token overlap (>37\\%) was comparable between MultiNLI (30\\% of pairs) and SNLI (29\\%). These statistics suggest that MultiNLI's annotations are comparable in quality to those of SNLI.",
      "Sentences in \\textsc{Slate cover a wide range of topics and phenomena, making it hard to do well on, but also forcing models trained on it be broadly capable; the model trained on \\textsc{Slate achieves the highest accuracy of any model on \\textsc{9/11 (55.6\\%) and \\textsc{Verbatim (57.2\\%), and relatively high accuracy on \\textsc{Travel (57.4\\%) and \\textsc{Government (58.3\\%). We also observe that our models perform similarly on both the matched and mismatched test sets of MultiNLI. % We expect genre mismatch issues to become more conspicuous as models are developed that can better fit MultiNLI's training genres.",
      "For each genre represented in the training set, the model that performs best on it was trained on that genre; a model trained only on SNLI performs worse on every genre than comparable models trained on any genre from MultiNLI. Models trained on a single genre from MultiNLI perform well on similar genres; for example, the model trained on \\textsc{Telephone attains the best accuracy (63\\%) on \\textsc{Face-to-Face, which was nearly one point better than it received on itself. \\textsc{Slate seems to be a difficult and relatively unusual genre and performance on it is relatively poor in this setting; when averaging over runs trained on SNLI and all genres in the matched section of the training set, average performance on \\textsc{Slate was only 57.5\\%.",
      "For the third, we implement and evaluate \\citeauthor{chen2017esim's Enhanced Sequential Inference Model (ESIM), which is roughly tied for the state of the art on SNLI at the time of writing. We use the base ESIM without ensembling with a TreeLSTM (as in the `HIM' runs in that work). The first two models produce separate vector representations for each sentence and compute label predictions for pairs of representations.",
      "Models thus can be evaluated on both the matched test examples, which are derived from the same sources as those in the training set, and on the mismatched examples, which do not closely resemble any of those seen at training time. The Corpus Data Collection The data collection methodology for MultiNLI is similar to that of SNLI: We create each sentence pair by selecting a premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis.",
      "While its size (433k pairs) and mode of collection are modeled closely on SNLI, unlike that corpus, MultiNLI represents both written and spoken speech in a wide range of styles, degrees of formality, and topics. Our chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning. In many application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data \\citep{krizhevsky2012imagenet,zeiler2013visualizing,donahue2014decaf.",
      "Validation instructions are tailored by genre, based on the main data collection prompt (Figure ); a single FAQ, modeled after the validation FAQ from SNLI, is provided for reference. In order to encourage thoughtful labeling, we manually label one percent of the validation examples and offer a \\1 bonus each time a worker selects a label that matches ours. For each validated sentence pair, we assign a gold label representing a majority vote between the initial label assigned to the pair by the original annotator, and the four additional labels assigned by validation annotators."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 7,\n    \"Rationale\": \"Image 0 provides test set accuracies for different models on the MultiNLI dataset, which is directly relevant to evaluating model performance. Text 7 discusses the validation process and how labels are assigned, which is crucial for understanding how well models perform in terms of label matching.\",\n    \"Answer\": \"The ESIM model performs better on the MultiNLI dataset, achieving the highest accuracy in both matched (72.4%) and mismatched (71.9%) test sets.\"\n}"
  },
  "3": {
    "question": "Which genre in the MultiNLI corpus has the highest percentage of sentences where the Stanford Parser produced a parse rooted with an 'S' (sentence) node, and how does this compare to the overall average for the corpus?",
    "ground_truth": "The genre with the highest percentage of 'S' parses is **9/11**, with **99%** of its sentences receiving this parse. This is higher than the overall average for the MultiNLI corpus, which sits at **91%**.",
    "selected_figures_names": [
      "1704.05426v4-Table3-1.png"
    ],
    "all_figures_names": [
      "1704.05426v4-Table4-1.png",
      "1704.05426v4-Table1-1.png",
      "1704.05426v4-Figure1-1.png",
      "1704.05426v4-Table5-1.png",
      "1704.05426v4-Table2-1.png",
      "1704.05426v4-Table3-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 6,
    "rationale": "Image 0 provides a table with detailed statistics on the MultiNLI corpus, including the percentage of sentences with 'S' parses for each genre. Text 6 explains the sources and diversity of the premise sentences in the MultiNLI corpus, which is relevant to understanding the context of the data. Together, they help identify the genre with the highest 'S' parse percentage and compare it to the overall average.",
    "answer": "The '9/11' genre in the MultiNLI corpus has the highest percentage of sentences with 'S' parses at 98%. This is higher than the overall average for the corpus, which is 91%.",
    "text_chunks": [
      "With this in mind, we construct MultiNLI so as to make it possible to explicitly evaluate models both on the quality of their sentence representations within the training domain and on their ability to derive reasonable representations in unfamiliar domains. The corpus is derived from ten different genres of written and spoken English, which are collectively meant to approximate the full diversity of ways in which modern standard American English is used. All of the genres appear in the test and development sets, but only five are included in the training set.",
      "While its size (433k pairs) and mode of collection are modeled closely on SNLI, unlike that corpus, MultiNLI represents both written and spoken speech in a wide range of styles, degrees of formality, and topics. Our chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning. In many application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data \\citep{krizhevsky2012imagenet,zeiler2013visualizing,donahue2014decaf.",
      "In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, and lexical and syntactic ambiguity. As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus \\citep[SNLI;][]{snli:emnlp2015 has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention , memory , and the use of parse structure . However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways.",
      "First, the sentences in SNLI are derived from only a single text genre---image captions---and are thus limited to descriptions of concrete visual scenes, rendering the hypothesis sentences used to describe these scenes short and simple, and rendering many important phenomena---like temporal reasoning (e.g., yesterday), belief (e.g., know), and modality (e.g., should)---rare enough to be irrelevant to task performance. Second, because of these issues, SNLI is not sufficiently demanding to serve as an effective benchmark for NLU, with the best current model performance falling within a few percentage points of human accuracy and limited room left for fine-grained comparisons between strong models. This paper introduces a new challenge dataset, the Multi-Genre NLI Corpus (MultiNLI), whose chief purpose is to remedy these limitations by making it possible to run large-scale NLI evaluations that capture more of the complexity of modern English.",
      "Although SNLI is collected in largely the same way as MultiNLI, and is also permissively licensed, we do not include SNLI in the MultiNLI corpus distribution. SNLI can be appended and treated as an unusually large additional \\textsc{captions genre, built on image captions from the Flickr30k corpus . \\paragraph{Hypothesis Collection To collect a sentence pair, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate whenever the premise is true (paired with the premise and labeled \\textsc{entailment), one which is necessarily false or inappropriate whenever the premise is true (\\textsc{contradiction), and one where neither condition applies (\\textsc{neutral).",
      "Similarly, in SNLI, 1.6\\% of hypotheses differ from their premises by addition, substitution, or shuffling a single word, while in MultiNLI this only happens in 1.2\\% of examples. The percentage of hypothesis-premise pairs with high token overlap (>37\\%) was comparable between MultiNLI (30\\% of pairs) and SNLI (29\\%). These statistics suggest that MultiNLI's annotations are comparable in quality to those of SNLI.",
      "This section discusses the sources of our premise sentences, our collection method for hypotheses, and our validation (relabeling) strategy. \\paragraph{Premise Text Sources The MultiNLI premise sentences are derived from ten sources of freely available text which are meant to be maximally diverse and roughly represent the full range of American English. We selected nine sources from the second release of the Open American National Corpus \\citep[OANC; ][downloaded 12/2016]{fillmore1998,macleod2000,ide2001, ide2006openANC, balancing the volume of source text roughly evenly across genres, and avoiding genres with content that would be too difficult for untrained annotators.",
      "These statistics indicate that the labels included in MultiNLI are about as reliable as those included in SNLI, despite MultiNLI's more diverse text contents. The Resulting Corpus Table shows randomly chosen development set examples from the collected corpus. Hypotheses tend to be fluent and correctly spelled, though not all are complete sentences."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 6,\n    \"Rationale\": \"Image 0 provides a table with detailed statistics on the MultiNLI corpus, including the percentage of sentences with 'S' parses for each genre. Text 6 explains the sources and diversity of the premise sentences in the MultiNLI corpus, which is relevant to understanding the context of the data. Together, they help identify the genre with the highest 'S' parse percentage and compare it to the overall average.\",\n    \"Answer\": \"The '9/11' genre in the MultiNLI corpus has the highest percentage of sentences with 'S' parses at 98%. This is higher than the overall average for the corpus, which is 91%.\"\n}"
  },
  "4": {
    "question": "How does the performance of the ESIM model differ when trained on MNLI alone versus trained on both MNLI and SNLI combined?",
    "ground_truth": "When trained on MNLI alone, the ESIM model achieves an accuracy of 60.7% on SNLI, 72.3% on matched genres in MNLI, and 72.1% on mismatched genres in MNLI. However, when trained on both MNLI and SNLI combined, the ESIM model's performance improves across all tasks, reaching 79.7% accuracy on SNLI, 72.4% on matched MNLI genres, and 71.9% on mismatched MNLI genres.",
    "selected_figures_names": [
      "1704.05426v4-Table4-1.png",
      "1704.05426v4-Table1-1.png",
      "1704.05426v4-Figure1-1.png",
      "1704.05426v4-Table5-1.png"
    ],
    "all_figures_names": [
      "1704.05426v4-Table4-1.png",
      "1704.05426v4-Table1-1.png",
      "1704.05426v4-Figure1-1.png",
      "1704.05426v4-Table5-1.png",
      "1704.05426v4-Table2-1.png",
      "1704.05426v4-Table3-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 provides test set accuracies for models trained on different datasets, including ESIM on MNLI and MNLI+SNLI. Text 3 discusses the training and evaluation of models on SNLI and MultiNLI, which is relevant to understanding performance differences.",
    "answer": "The ESIM model achieves an accuracy of 72.3% on the matched MNLI test set and 72.1% on the mismatched test set when trained on MNLI alone. When trained on both MNLI and SNLI, the accuracy improves to 72.4% on the matched test set and 71.9% on the mismatched test set.",
    "text_chunks": [
      "For each genre represented in the training set, the model that performs best on it was trained on that genre; a model trained only on SNLI performs worse on every genre than comparable models trained on any genre from MultiNLI. Models trained on a single genre from MultiNLI perform well on similar genres; for example, the model trained on \\textsc{Telephone attains the best accuracy (63\\%) on \\textsc{Face-to-Face, which was nearly one point better than it received on itself. \\textsc{Slate seems to be a difficult and relatively unusual genre and performance on it is relatively poor in this setting; when averaging over runs trained on SNLI and all genres in the matched section of the training set, average performance on \\textsc{Slate was only 57.5\\%.",
      "For the third, we implement and evaluate \\citeauthor{chen2017esim's Enhanced Sequential Inference Model (ESIM), which is roughly tied for the state of the art on SNLI at the time of writing. We use the base ESIM without ensembling with a TreeLSTM (as in the `HIM' runs in that work). The first two models produce separate vector representations for each sentence and compute label predictions for pairs of representations.",
      "The models use 300D hidden states, as in most prior work on SNLI. We use Dropout \\citep{srivastava2014dropout for regularization. For ESIM, we use a dropout rate of 0.5, following the paper.",
      "For CBOW and BiLSTM models, we tune Dropout on the SNLI dev.~set and find that a drop rate of 0.1 works well. We use the Adam \\citep{kingma2014adam optimizer with default parameters. We train models on SNLI, MultiNLI, and a mixture; Table~ shows the results.",
      "In the mixed setting, we use the full MultiNLI training set and randomly select 15\\% of the SNLI training set at each epoch, ensuring that each available genre is seen during training with roughly equal frequency. % We also train a separate CBOW model on each individual genre to establish the degree to which simple models already allow for effective transfer across genres, using a dropout rate of 0.2. When training on SNLI, a single random sample of 15\\% of the original training set is used.",
      "In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, and lexical and syntactic ambiguity. As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus \\citep[SNLI;][]{snli:emnlp2015 has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention , memory , and the use of parse structure . However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways.",
      "Similarly, in SNLI, 1.6\\% of hypotheses differ from their premises by addition, substitution, or shuffling a single word, while in MultiNLI this only happens in 1.2\\% of examples. The percentage of hypothesis-premise pairs with high token overlap (>37\\%) was comparable between MultiNLI (30\\% of pairs) and SNLI (29\\%). These statistics suggest that MultiNLI's annotations are comparable in quality to those of SNLI.",
      "Models thus can be evaluated on both the matched test examples, which are derived from the same sources as those in the training set, and on the mismatched examples, which do not closely resemble any of those seen at training time. The Corpus Data Collection The data collection methodology for MultiNLI is similar to that of SNLI: We create each sentence pair by selecting a premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 3,\n    \"Rationale\": \"Image 0 provides test set accuracies for models trained on different datasets, including ESIM on MNLI and MNLI+SNLI. Text 3 discusses the training and evaluation of models on SNLI and MultiNLI, which is relevant to understanding performance differences.\",\n    \"Answer\": \"The ESIM model achieves an accuracy of 72.3% on the matched MNLI test set and 72.1% on the mismatched test set when trained on MNLI alone. When trained on both MNLI and SNLI, the accuracy improves to 72.4% on the matched test set and 71.9% on the mismatched test set.\"\n}"
  }
}