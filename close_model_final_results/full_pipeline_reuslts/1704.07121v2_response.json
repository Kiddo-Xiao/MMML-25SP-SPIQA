{
  "0": {
    "question": "Which model performed best on the \"All\" category of Visual7W, and how did its performance compare to human performance?",
    "ground_truth": "The MLP-IQA model achieved the highest accuracy in the \"All\" category of Visual7W, with a score of 45.1%. However, this performance still falls significantly short of human performance, which stands at 84.1% for the same category.",
    "selected_figures_names": [
      "1704.07121v2-Table1-1.png",
      "1704.07121v2-Table11-1.png",
      "1704.07121v2-Table3-1.png"
    ],
    "all_figures_names": [
      "1704.07121v2-Table1-1.png",
      "1704.07121v2-Figure5-1.png",
      "1704.07121v2-Table8-1.png",
      "1704.07121v2-Table10-1.png",
      "1704.07121v2-Table11-1.png",
      "1704.07121v2-Figure4-1.png",
      "1704.07121v2-Table3-1.png",
      "1704.07121v2-Figure7-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 5,
    "rationale": "Image 2 provides a detailed comparison of model performances across different categories, including 'All'. Text 5 discusses human performance on Visual7W, which is relevant for comparison.",
    "answer": "The MLP-IQA model performed best on the 'All' category of Visual7W with an accuracy of 45.1%. In comparison, human performance on the original decoys was 88.4%, indicating that humans performed significantly better than the model.",
    "text_chunks": [
      "The candidate c\\in\\text{A that has the highest f(c,i) score in prediction is selected as the model output. We use the standard training, validation, and test splits of Visual7W, where each contains 69,817, 28,020, and 42,031 examples respectively. Each question has 4 candidate answers.",
      "In VQA, the decoys consist of human-generated plausible answers as well as high-frequency and random answers from the datasets. In Visual7W, the decoys are all human-generated plausible ones. Note that, humans generate those decoys by only looking at the questions and the correct answers but not the images.",
      "This is also expected as it is difficult to have decoys that are simultaneously both \\IU\\ and \\QU\\ --- such answers tend to be the target answers. Nonetheless, we deem this as a future direction to explore. \\paragraph{Differences across datasets Contrasting Visual7W to VQA (on the column \\IU+\\QU), we notice that Visual7W tends to have bigger improvements in general.",
      "For each dataset, we randomly sample 1,000 image-question-target triplets together with the corresponding \\IU-decoys and \\QU-decoys to evaluate human performance. For each of these triplets, three workers are assigned to select the most correct candidate answer according to the image and the question. We compute the average accuracy of these workers and report them in Table~3, 4 and 5 of the main text and Table~. We also conduct human evaluation using the Orig decoys of Visual7W so as to investigate the difference between human-generated and automatically generated decoys.",
      "This is expected. In the Visual7W dataset, the decoys are generated by human annotators as plausible answers to the questions without being shown the images --- thus, many decoy answers do not have visual groundings. For instance, a question of ``what animal is running?''",
      "The total cost is \\215 --- the rate for every 20 triplets is \\0.25. On our \\IU-decoys and \\QU-decoys, humans achieve 84.1\\%, 89.0\\%, and 82.5\\% on Visual7W, VQA, and VG, respectively. Compared to the human performance on the Orig decoys that involve human effort in creation (\\ie, 88.4\\% on Visual7W, and 88.5\\% on VQA as reported in~), these results suggest that the ways we create the decoys and the filtering steps mentioned in Sect.",
      "We apply the procedures to revise both Visual7W~ and VQA~. Additionally, we create new multiple-choice based datasets from COCOQA~ and the recently released VQA2~ and Visual Genome datasets~. The one based on Visual Genome becomes the largest multiple-choice dataset for the Visual QA task, with more than one million image-question-candidate answers triplets. We conduct extensive empirical and human studies to demonstrate the effectiveness of our procedures in creating high-quality datasets for the Visual QA task.",
      "\\paragraph{Visual7W Telling (Visual7W)~ The dataset uses 47,300 images from MSCOCO and contains 139,868 IQA triplets. Each has 3 decoys generated by humans. \\paragraph{Visual Genome (VG)"
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 5,\n    \"Rationale\": \"Image 2 provides a detailed comparison of model performances across different categories, including 'All'. Text 5 discusses human performance on Visual7W, which is relevant for comparison.\",\n    \"Answer\": \"The MLP-IQA model performed best on the 'All' category of Visual7W with an accuracy of 45.1%. In comparison, human performance on the original decoys was 88.4%, indicating that humans performed significantly better than the model.\"\n}"
  },
  "1": {
    "question": "Which method performs best overall on VQA-2014val, and how does its performance compare to human performance on the same dataset?",
    "ground_truth": "MLP-IQA achieves the highest overall accuracy (46.5%) among the machine learning methods tested on VQA-2014val. However, this performance still falls short of human performance, which reaches an accuracy of 85.5% on the same dataset.",
    "selected_figures_names": [
      "1704.07121v2-Table7-1.png"
    ],
    "all_figures_names": [
      "1704.07121v2-Figure6-1.png",
      "1704.07121v2-Table11-1.png",
      "1704.07121v2-Figure8-1.png",
      "1704.07121v2-Table7-1.png",
      "1704.07121v2-Table2-1.png",
      "1704.07121v2-Table3-1.png",
      "1704.07121v2-Figure3-1.png",
      "1704.07121v2-Table8-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 provides a table with accuracy results for different methods on the VQA-2014val dataset, which is directly relevant to the question. Text 1 discusses human performance in comparison to learning models, which is useful for comparing the best method to human performance.",
    "answer": "The Attention method performs best overall on VQA-2014val with an accuracy of 58.6%. Human performance on the same dataset is 85.5%, which is higher than the best model performance.",
    "text_chunks": [
      "Table~ shows the experimental results on VQA2^-. Comparing to Table~, we see that the overall performance for each model decreases as the dataset becomes more challenging on average. Specifically, the model that observes question and answer on VQA2^- performs much worse than that on VQA2 (37.2\\% vs. 48.1\\%).",
      "We report the average human performance and compare it to the learning models'. See the Supplementary Material for more details. Results The performances of learning models and humans on the 3 datasets are reported in Table~, , and , the 4.3\\% drop of the human performance on \\IU+\\QU, compared to Orig, is likely due to that \\IU+\\QU has more candidates (7 per question).",
      "Both models utilize the attention mechanism. Details are listed in the Supplementary Material. \\paragraph{Evaluation metric For VQA and VQA2, we follow their protocols by comparing the picked answer to 10 human-generated targets.",
      "We apply the procedures to revise both Visual7W~ and VQA~. Additionally, we create new multiple-choice based datasets from COCOQA~ and the recently released VQA2~ and Visual Genome datasets~. The one based on Visual Genome becomes the largest multiple-choice dataset for the Visual QA task, with more than one million image-question-candidate answers triplets. We conduct extensive empirical and human studies to demonstrate the effectiveness of our procedures in creating high-quality datasets for the Visual QA task.",
      "For each dataset, we randomly sample 1,000 image-question-target triplets together with the corresponding \\IU-decoys and \\QU-decoys to evaluate human performance. For each of these triplets, three workers are assigned to select the most correct candidate answer according to the image and the question. We compute the average accuracy of these workers and report them in Table~3, 4 and 5 of the main text and Table~. We also conduct human evaluation using the Orig decoys of Visual7W so as to investigate the difference between human-generated and automatically generated decoys.",
      "This can be achieved either by human annotators, or with an automatic procedure that uses captions or question templates and detailed image annotations. We concentrate on 3 datasets: VQA~, Visual7W~, and Visual Genome~. All of them use images from MSCOCO~. Besides the pairs of questions and correct answers, VQA, Visual7W, and visual Madlibs~ provide decoy answers for each pair so that the task can be evaluated in multiple-choice selection accuracy. What decoy answers to use is the focus of our work.",
      "We use the MLP-IQA trained on qaVG with both \\IU\\ and \\QU\\ decoys to initialize the models for the Visual7W and VQA datasets. We report the accuracies before and after fine-tuning, together with the best results learned solely on those two datasets. As shown in Table~, fine-tuning largely improves the performance, justifying the finding by Fukui et al.~\\shortcite{fukui2016multimodal.",
      "We suggest a set of principles for creating decoy answers. In light of the amount of human efforts in curating existing datasets for the Visual QA task, we propose two procedures that revise those datasets such that the decoy answers are better designed. In contrast to some earlier works, the procedures are fully automatic and do not incur additional human annotator efforts."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 provides a table with accuracy results for different methods on the VQA-2014val dataset, which is directly relevant to the question. Text 1 discusses human performance in comparison to learning models, which is useful for comparing the best method to human performance.\",\n    \"Answer\": \"The Attention method performs best overall on VQA-2014val with an accuracy of 58.6%. Human performance on the same dataset is 85.5%, which is higher than the best model performance.\"\n}"
  },
  "2": {
    "question": "Which model performs the best on qaVG when considering both image understanding (IU) and question understanding (QU) individually, and how does its performance compare to humans?",
    "ground_truth": "MLP-QA performs the best when considering both IU and QU individually, achieving 89.3% accuracy on IU and 45.6% accuracy on QU. However, its combined performance (IU+QU) of 43.9% is still significantly lower than human performance, which stands at 82.5%.",
    "selected_figures_names": [
      "1704.07121v2-Table10-1.png",
      "1704.07121v2-Figure5-1.png",
      "1704.07121v2-Figure4-1.png",
      "1704.07121v2-Figure8-1.png"
    ],
    "all_figures_names": [
      "1704.07121v2-Table10-1.png",
      "1704.07121v2-Figure5-1.png",
      "1704.07121v2-Figure4-1.png",
      "1704.07121v2-Figure8-1.png",
      "1704.07121v2-Figure7-1.png",
      "1704.07121v2-Table4-1.png",
      "1704.07121v2-Table6-1.png",
      "1704.07121v2-Table5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 provides a table with test accuracy for different models, which is directly relevant to evaluating model performance on qaVG. Text 2 discusses the performance improvements when using \\\\IU-decoys and \\\\QU-decoys, which is crucial for understanding how models perform with both image and question understanding.",
    "answer": "The MLP-IQA model performs the best on qaVG when considering both image understanding (IU) and question understanding (QU) individually, with an accuracy of 85.9% for IU and 56.1% for QU. Compared to humans, the model shows substantial improvement when both image and question information are used together.",
    "text_chunks": [
      "We see that on VQA^-, the improvement by our \\IU-decoys and \\QU-decoys becomes significant. The gain brought by images on \\QU\\ (from 39.3\\% to 56.6\\%) is much larger than that on Orig (from 45.8\\% to 55.6\\%). Similarly, the gain brought by questions on \\IU\\ (from 44.8\\% to 81.8\\%) is much larger than that on Orig (from 43.0\\% to 55.6\\%).",
      "We report the average human performance and compare it to the learning models'. See the Supplementary Material for more details. Results The performances of learning models and humans on the 3 datasets are reported in Table~, , and , the 4.3\\% drop of the human performance on \\IU+\\QU, compared to Orig, is likely due to that \\IU+\\QU has more candidates (7 per question).",
      "With the Orig decoys, the relatively small gain from MLP-IA to MLP-IQA suggests that the question information can be ignored to attain good performance. However, with the \\IU-decoys which require questions to help to resolve (as image itself is inadequate to resolve), the gain is substantial (from 27.3\\% to 84.1\\%). Likewise, with the \\QU-decoys (question itself is not adequate to resolve), including images information improves from 40.7\\% (MLP-QA) substantially to 57.6\\% (MLP-IQA).",
      "We use the MLP-IQA trained on qaVG with both \\IU\\ and \\QU\\ decoys to initialize the models for the Visual7W and VQA datasets. We report the accuracies before and after fine-tuning, together with the best results learned solely on those two datasets. As shown in Table~, fine-tuning largely improves the performance, justifying the finding by Fukui et al.~\\shortcite{fukui2016multimodal.",
      "Since each image corresponds to multiple questions or have multiple objects, solely relying on the image itself will not work well in principle. Such difference clearly indicates that in the Visual QA model, the language component is weak as the model cannot fully exploit the information in ``Q'', making a smaller relative improvement 5.3\\% (from 62.4\\% to 65.7\\%) where humans improved relatively 17.4\\%. \\paragraph{Shortcuts are due to design deficiencies We probe deeper on how the decoy answers have impacted the performance of learning models. As explained above, the decoys are drawn from all plausible answers to a question, irrespective of whether they are visually grounded or not.",
      "\\AUflat is comparably easier to achieve by reusing terms in the whole set of targets as decoys. On the contrary, a decoy may hardly meet \\QU and \\IU simultaneously, for the question ``What vehicle is pictured?'', the only answer that meets both principles is ``train'', which is the correct answer instead of being a decoy.. However, as long as all decoys of an IQA triplet meet \\AUflat and some meet \\QU and others meet \\IU, the triplet as a whole still achieves the three principles --- a machine ignoring either images or questions will likely perform poorly.",
      "As shown in Table~, we clearly see that with only answers being visible to the model (MLP-A), the performance is close to random (on the column of \\IU+\\QU -decoys), and far from observing all three sources of information (MLP-IQA). Meanwhile, models that can observe either images and answers (MLP-IA) or questions and answers (MLP-QA) fail to predict as good as the model that observe all three sources of information. Results in Table~ also shows a similar trend.",
      "After combining \\IU-decoys and \\QU-decoys as in \\IU+\\QU\\ and All, the improvement by either including images to MLP-QA or including questions to MLP-IA is noticeable higher than that on Orig. Moreover, even with only 6 decoys, the performance by MLP-A on \\IU+\\QU\\ is already lower than that on Orig, which has 17 decoys, demonstrating the effectiveness of our decoys in preventing machines from overfitting to the incidental statistics. These observations together demonstrate how our proposed ways for creating decoys improve the quality of multiple-choice Visual QA datasets."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 provides a table with test accuracy for different models, which is directly relevant to evaluating model performance on qaVG. Text 2 discusses the performance improvements when using \\\\\\\\IU-decoys and \\\\\\\\QU-decoys, which is crucial for understanding how models perform with both image and question understanding.\",\n    \"Answer\": \"The MLP-IQA model performs the best on qaVG when considering both image understanding (IU) and question understanding (QU) individually, with an accuracy of 85.9% for IU and 56.1% for QU. Compared to humans, the model shows substantial improvement when both image and question information are used together.\"\n}"
  },
  "3": {
    "question": "How can the shortcuts in the Visual7W dataset be remedied?",
    "ground_truth": "The shortcuts in the Visual7W dataset can be remedied by creating alternative decoys that are more likely to be correct, based on either the image or the question alone. This forces the machine to consider all of the information together in order to select the correct answer.",
    "selected_figures_names": [
      "1704.07121v2-Figure3-1.png",
      "1704.07121v2-Figure6-1.png",
      "1704.07121v2-Figure1-1.png",
      "1704.07121v2-Table8-1.png"
    ],
    "all_figures_names": [
      "1704.07121v2-Figure3-1.png",
      "1704.07121v2-Figure6-1.png",
      "1704.07121v2-Figure1-1.png",
      "1704.07121v2-Table8-1.png",
      "1704.07121v2-Figure2-1.png",
      "1704.07121v2-Table2-1.png",
      "1704.07121v2-Figure7-1.png",
      "1704.07121v2-Table12-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 3,
    "rationale": "Image 2 illustrates the concept of IoU and QoU decoys, showing how they prevent machines from exploiting shortcuts by requiring consideration of both image and question. Text 3 explains the procedures to revise datasets like Visual7W to avoid overfitting on incidental statistics, aligning with the question about remedying shortcuts.",
    "answer": "Shortcuts in the Visual7W dataset can be remedied by using IoU and QoU decoys, which require a learning model to consider both the image and the question to make the correct choice. This prevents the model from exploiting shortcuts by overfitting on incidental statistics.",
    "text_chunks": [
      "This is also expected as it is difficult to have decoys that are simultaneously both \\IU\\ and \\QU\\ --- such answers tend to be the target answers. Nonetheless, we deem this as a future direction to explore. \\paragraph{Differences across datasets Contrasting Visual7W to VQA (on the column \\IU+\\QU), we notice that Visual7W tends to have bigger improvements in general.",
      "This can be achieved either by human annotators, or with an automatic procedure that uses captions or question templates and detailed image annotations. We concentrate on 3 datasets: VQA~, Visual7W~, and Visual Genome~. All of them use images from MSCOCO~. Besides the pairs of questions and correct answers, VQA, Visual7W, and visual Madlibs~ provide decoy answers for each pair so that the task can be evaluated in multiple-choice selection accuracy. What decoy answers to use is the focus of our work.",
      "We apply the procedures to revise both Visual7W~ and VQA~. Additionally, we create new multiple-choice based datasets from COCOQA~ and the recently released VQA2~ and Visual Genome datasets~. The one based on Visual Genome becomes the largest multiple-choice dataset for the Visual QA task, with more than one million image-question-candidate answers triplets. We conduct extensive empirical and human studies to demonstrate the effectiveness of our procedures in creating high-quality datasets for the Visual QA task.",
      "Thus, similarly, a learning model can examine the image alone and deduce the correct answer. We propose automatic procedures to revise VQA and Visual7W (and to create new datasets based on COCOQA~, VQA2~, and Visual Genome) such that the decoy generation is carefully orchestrated to prevent learning algorithms from exploiting the shortcuts in the datasets by overfitting on incidental statistics. In particular, our design goal is that a learning machine needs to understand all the 3 components of an image-question-candidate answers triplet in order to make the right choice --- ignoring either one or two components will result in drastic degradation in performance.",
      "In VQA, the decoys consist of human-generated plausible answers as well as high-frequency and random answers from the datasets. In Visual7W, the decoys are all human-generated plausible ones. Note that, humans generate those decoys by only looking at the questions and the correct answers but not the images.",
      "This is expected. In the Visual7W dataset, the decoys are generated by human annotators as plausible answers to the questions without being shown the images --- thus, many decoy answers do not have visual groundings. For instance, a question of ``what animal is running?''",
      "\\paragraph{Visual7W Telling (Visual7W)~ The dataset uses 47,300 images from MSCOCO and contains 139,868 IQA triplets. Each has 3 decoys generated by humans. \\paragraph{Visual Genome (VG)",
      "Analysis of Decoy Answers' Effects In this section, we examine in detail the dataset Visual7W~, a popular choice for the Visual QA task. We demonstrate how the deficiencies in designing decoy questions impact the performance of learning algorithms. In multiple-choice Visual QA datasets, a training or test example is a triplet that consists of an image I, a question Q, and a candidate answer set A."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 3,\n    \"Rationale\": \"Image 2 illustrates the concept of IoU and QoU decoys, showing how they prevent machines from exploiting shortcuts by requiring consideration of both image and question. Text 3 explains the procedures to revise datasets like Visual7W to avoid overfitting on incidental statistics, aligning with the question about remedying shortcuts.\",\n    \"Answer\": \"Shortcuts in the Visual7W dataset can be remedied by using IoU and QoU decoys, which require a learning model to consider both the image and the question to make the correct choice. This prevents the model from exploiting shortcuts by overfitting on incidental statistics.\"\n}"
  },
  "4": {
    "question": "Which dataset presents the biggest challenge for a model trying to distinguish true triplets from decoys, and why?",
    "ground_truth": "The VQA dataset presents the biggest challenge.",
    "selected_figures_names": [
      "1704.07121v2-Figure1-1.png",
      "1704.07121v2-Figure8-1.png",
      "1704.07121v2-Table2-1.png",
      "1704.07121v2-Table12-1.png"
    ],
    "all_figures_names": [
      "1704.07121v2-Figure1-1.png",
      "1704.07121v2-Figure8-1.png",
      "1704.07121v2-Table2-1.png",
      "1704.07121v2-Table12-1.png",
      "1704.07121v2-Table3-1.png",
      "1704.07121v2-Figure3-1.png",
      "1704.07121v2-Figure5-1.png",
      "1704.07121v2-Table9-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 illustrates the challenge of distinguishing true triplets from decoys in the Visual7W dataset by showing how decoys can be misleading if not well-designed. Text 3 discusses the impact of decoy design on learning algorithms, specifically in the Visual7W dataset, highlighting its deficiencies.",
    "answer": "The Visual7W dataset presents the biggest challenge for a model trying to distinguish true triplets from decoys. This is because the design of decoy questions can significantly impact the performance of learning algorithms, making it difficult for models to accurately gauge the correct answers without considering all information together.",
    "text_chunks": [
      "\\paragraph{Other details For \\QU-decoys, we sort and keep for each triplet the top N (e.g., 10,000) similar triplets from the entire dataset according to the question similarity. Then for each triplet, we compute the WUP-based similarity of each potential decoy to the target successively, and accept those with similarity below 0.9 until we have K decoys. We choose 0.9 according to~.",
      "For each dataset, we randomly sample 1,000 image-question-target triplets together with the corresponding \\IU-decoys and \\QU-decoys to evaluate human performance. For each of these triplets, three workers are assigned to select the most correct candidate answer according to the image and the question. We compute the average accuracy of these workers and report them in Table~3, 4 and 5 of the main text and Table~. We also conduct human evaluation using the Orig decoys of Visual7W so as to investigate the difference between human-generated and automatically generated decoys.",
      "These empirical observations meet our expectation and again verify the effectiveness of our proposed methods for creating decoys. Besides, we also perform a more in-depth experiment on VQA2, removing triplets with Yes/No as the target. We name this subset as VQA2^-.",
      "Analysis of Decoy Answers' Effects In this section, we examine in detail the dataset Visual7W~, a popular choice for the Visual QA task. We demonstrate how the deficiencies in designing decoy questions impact the performance of learning algorithms. In multiple-choice Visual QA datasets, a training or test example is a triplet that consists of an image I, a question Q, and a candidate answer set A.",
      "More experimental results on VQA2 and COCOQA Dataset descriptions \\paragraph{COCOQA~ This dataset contains in total 117,684 auto-generated IQT triplets with no decoy answers. Therefore, we create decoys using our proposed approach and follow the original data split, leading to a training set and a testing set with 78,736 IQA triplets and 38,948 IQA triplets, respectfully.",
      "For completeness, we include the results on the Visual Genome dataset in Table~. This dataset has no ``Orig'' decoys, and we have created a multiple-choice based dataset qaVG from it for the task --- it has over 1 million triplets, the largest dataset on this task to our knowledge. On the combined \\IU+\\QU -decoys, we again clearly see that machines need to use all the information to succeed. With qaVG, we also investigate whether it can help improve the multiple-choice performances on the other two datasets.",
      "Creating Better Visual QA Datasets In this section, we describe our approaches of remedying design deficiencies in the existing datasets for the Visual QA task. We introduce two automatic and widely-applicable procedures to create new decoys that can prevent learning models from exploiting incident statistics in the datasets. Methods \\paragraph{Main ideas Our procedures operate on a dataset that already contains image-question-target (IQT) triplets, \\ie, we do not assume it has decoys already.",
      "For each triplet, we randomly choose to use \\QU-decoys or \\IU-decoys when training on \\IU+\\QU, or \\QU-decoys or \\IU-decoys or Orig when training on All. We then take the target and 3 decoys for each triplet to train the binary classifier (\\ie, minimize the logistic loss). Specifically on VQA, which has 17 Orig decoys for a triplet, we randomly choose 3 decoys out of them."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 3,\n    \"Rationale\": \"Image 0 illustrates the challenge of distinguishing true triplets from decoys in the Visual7W dataset by showing how decoys can be misleading if not well-designed. Text 3 discusses the impact of decoy design on learning algorithms, specifically in the Visual7W dataset, highlighting its deficiencies.\",\n    \"Answer\": \"The Visual7W dataset presents the biggest challenge for a model trying to distinguish true triplets from decoys. This is because the design of decoy questions can significantly impact the performance of learning algorithms, making it difficult for models to accurately gauge the correct answers without considering all information together.\"\n}"
  },
  "5": {
    "question": "Which embedding method performed the best overall across all model architectures on the Visual7W dataset with IoU + QoU decoys? Was there a significant difference in performance compared to the other methods?",
    "ground_truth": "The MLP-IQA model achieved the highest accuracy across all embedding methods, reaching 52.5% with GloVe, 51.4% with Translation embeddings, and 52.0% with word2vec. However, the passage notes that there was no significant difference in performance between the different embedding methods.",
    "selected_figures_names": [
      "1704.07121v2-Figure2-1.png",
      "1704.07121v2-Figure7-1.png",
      "1704.07121v2-Table10-1.png",
      "1704.07121v2-Table2-1.png"
    ],
    "all_figures_names": [
      "1704.07121v2-Figure2-1.png",
      "1704.07121v2-Figure7-1.png",
      "1704.07121v2-Table10-1.png",
      "1704.07121v2-Table2-1.png",
      "1704.07121v2-Table3-1.png",
      "1704.07121v2-Figure6-1.png",
      "1704.07121v2-Table11-1.png",
      "1704.07121v2-Table12-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 5,
    "rationale": "Image 2 provides a table with test accuracy percentages for different methods, which directly relates to the performance of embedding methods. Text 5 discusses the analysis of different embeddings, noting no significant differences, which addresses the question about performance comparison.",
    "answer": "The MLP-IQA method performed the best overall with IoU + QoU decoys on the Visual7W dataset. There was no significant difference in performance compared to other methods, as noted in the text.",
    "text_chunks": [
      "This is also expected as it is difficult to have decoys that are simultaneously both \\IU\\ and \\QU\\ --- such answers tend to be the target answers. Nonetheless, we deem this as a future direction to explore. \\paragraph{Differences across datasets Contrasting Visual7W to VQA (on the column \\IU+\\QU), we notice that Visual7W tends to have bigger improvements in general.",
      "We use the MLP-IQA trained on qaVG with both \\IU\\ and \\QU\\ decoys to initialize the models for the Visual7W and VQA datasets. We report the accuracies before and after fine-tuning, together with the best results learned solely on those two datasets. As shown in Table~, fine-tuning largely improves the performance, justifying the finding by Fukui et al.~\\shortcite{fukui2016multimodal.",
      "Analysis of Decoy Answers' Effects In this section, we examine in detail the dataset Visual7W~, a popular choice for the Visual QA task. We demonstrate how the deficiencies in designing decoy questions impact the performance of learning algorithms. In multiple-choice Visual QA datasets, a training or test example is a triplet that consists of an image I, a question Q, and a candidate answer set A.",
      "In VQA, the decoys consist of human-generated plausible answers as well as high-frequency and random answers from the datasets. In Visual7W, the decoys are all human-generated plausible ones. Note that, humans generate those decoys by only looking at the questions and the correct answers but not the images.",
      "In the cases that we cannot find 3 decoys, we include random ones from the original set of decoys for VQA and Visual7W; for other datasets, we randomly include those from the top 10 frequently-occurring targets. Setup \\paragraph{Visual QA models We utilize the MLP models mentioned in Sect.~ for all the experiments. We denote MLP-A, MLP-QA, MLP-IA, MLP-IQA as the models using A (Answers only), Q+A (Question plus Answers), I+A (Image plus Answers), and I+Q+A (Image, Question and Answers) for multimodal information, respectively.",
      "4.2 lead to high-quality datasets with limited ambiguity. Analysis on different question and answer embeddings We consider \\textsc{GloVe~ and the embedding learned from translation~ on both question and answer embeddings. The results on Visual7W (IoU + QoU, compared to Table 3 of the main text that uses \\textsc{word2vec) are in Table~. We do not observe significant difference among different embeddings, which is likely due to that both the questions and answers are short (averagely 7 words for questions and 2 for answers).",
      "After combining \\IU-decoys and \\QU-decoys as in \\IU+\\QU\\ and All, the improvement by either including images to MLP-QA or including questions to MLP-IA is noticeable higher than that on Orig. Moreover, even with only 6 decoys, the performance by MLP-A on \\IU+\\QU\\ is already lower than that on Orig, which has 17 decoys, demonstrating the effectiveness of our decoys in preventing machines from overfitting to the incidental statistics. These observations together demonstrate how our proposed ways for creating decoys improve the quality of multiple-choice Visual QA datasets.",
      "Analysis on random decoys We conduct the analysis on sampling random decoys, instead of our \\IU-decoys and \\QU-decoys, on Visual7W. We collect 6 additional random decoys for each Orig IQA triplet so the answer set will contain 10 candidates, the same as All in Table 3 of the main text. We consider two strategies: (A) uniformly random decoys from unique correct answers, and (B) weighted random decoys w.r.t."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 5,\n    \"Rationale\": \"Image 2 provides a table with test accuracy percentages for different methods, which directly relates to the performance of embedding methods. Text 5 discusses the analysis of different embeddings, noting no significant differences, which addresses the question about performance comparison.\",\n    \"Answer\": \"The MLP-IQA method performed the best overall with IoU + QoU decoys on the Visual7W dataset. There was no significant difference in performance compared to other methods, as noted in the text.\"\n}"
  },
  "6": {
    "question": "Which model performs the best on COCOQA dataset when considering the combined accuracy of identifying irrelevant image-question pairs (IU) and irrelevant question-answer pairs (QU)? How does this compare to the performance of the model that only observes answers?",
    "ground_truth": "The MLP-IQA model performs the best when considering the combined accuracy of identifying irrelevant image-question and question-answer pairs, achieving an accuracy of 75.9%. This is significantly higher than the MLP-A model, which only observes answers and achieves a combined accuracy of 26.6%, close to random performance.",
    "selected_figures_names": [
      "1704.07121v2-Table8-1.png"
    ],
    "all_figures_names": [
      "1704.07121v2-Figure7-1.png",
      "1704.07121v2-Figure3-1.png",
      "1704.07121v2-Figure6-1.png",
      "1704.07121v2-Figure8-1.png",
      "1704.07121v2-Figure4-1.png",
      "1704.07121v2-Table12-1.png",
      "1704.07121v2-Figure5-1.png",
      "1704.07121v2-Table8-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 provides a table with accuracy percentages for different models on the COCOQA dataset, specifically showing the combined accuracy for irrelevant image-question and question-answer pairs. Text 3 discusses the performance of models with partial information and highlights the effectiveness of using all three sources of information (MLP-IQA).",
    "answer": "The MLP-IQA model performs the best on the COCOQA dataset with a combined accuracy of 75.9% for identifying irrelevant image-question and question-answer pairs. In comparison, the model that only observes answers (MLP-A) has a much lower combined accuracy of 26.6%.",
    "text_chunks": [
      "As shown in Table~, we clearly see that with only answers being visible to the model (MLP-A), the performance is close to random (on the column of \\IU+\\QU -decoys), and far from observing all three sources of information (MLP-IQA). Meanwhile, models that can observe either images and answers (MLP-IA) or questions and answers (MLP-QA) fail to predict as good as the model that observe all three sources of information. Results in Table~ also shows a similar trend.",
      "With the Orig decoys, the relatively small gain from MLP-IA to MLP-IQA suggests that the question information can be ignored to attain good performance. However, with the \\IU-decoys which require questions to help to resolve (as image itself is inadequate to resolve), the gain is substantial (from 27.3\\% to 84.1\\%). Likewise, with the \\QU-decoys (question itself is not adequate to resolve), including images information improves from 40.7\\% (MLP-QA) substantially to 57.6\\% (MLP-IQA).",
      "\\AUflat is comparably easier to achieve by reusing terms in the whole set of targets as decoys. On the contrary, a decoy may hardly meet \\QU and \\IU simultaneously, for the question ``What vehicle is pictured?'', the only answer that meets both principles is ``train'', which is the correct answer instead of being a decoy.. However, as long as all decoys of an IQA triplet meet \\AUflat and some meet \\QU and others meet \\IU, the triplet as a whole still achieves the three principles --- a machine ignoring either images or questions will likely perform poorly.",
      "What is particularly appealing is that MLP-IQA improves noticeably over models learned with partial information on the combined \\IU+\\QU -decoys (and ``All'' decoys but not the other two principles, to demonstrate the effectiveness of our methods in the Supplementary Material.). Furthermore, using answer information only (MLP-A) attains about the chance-level accuracy. On the VQA dataset (Table~), the same observations hold, though to a lesser degree.",
      "\\paragraph{Comparison on Visual QA models As presented in Table~ and , MLP-IQA is on par with or even outperforms Attention and HieCoAtt on the Orig decoys, showing how the shortcuts make it difficult to compare different models. By eliminating the shortcuts (\\ie, on the combined \\IU+\\QU -decoys), the advantage of using sophisticated models becomes obvious (Attention outperforms MLP-IQA by 3\\% in Table ), indicating the importance to design advanced models for achieving human-level performance on Visual QA. \\vspace{10pt",
      "On any of the \\IU\\ or \\QU\\ columns, we observe substantial gains when the complementary information is added to the model (such as MLP-IA to MLP-IQA). All these improvements are much more visible than those observed on the original decoy sets. Combining both Table~ and , we notice that the improvements from MLP-QA to MLP-IQA tend to be lower when facing \\IU-decoys.",
      "After combining \\IU-decoys and \\QU-decoys as in \\IU+\\QU\\ and All, the improvement by either including images to MLP-QA or including questions to MLP-IA is noticeable higher than that on Orig. Moreover, even with only 6 decoys, the performance by MLP-A on \\IU+\\QU\\ is already lower than that on Orig, which has 17 decoys, demonstrating the effectiveness of our decoys in preventing machines from overfitting to the incidental statistics. These observations together demonstrate how our proposed ways for creating decoys improve the quality of multiple-choice Visual QA datasets.",
      "We use the MLP-IQA trained on qaVG with both \\IU\\ and \\QU\\ decoys to initialize the models for the Visual7W and VQA datasets. We report the accuracies before and after fine-tuning, together with the best results learned solely on those two datasets. As shown in Table~, fine-tuning largely improves the performance, justifying the finding by Fukui et al.~\\shortcite{fukui2016multimodal."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 3,\n    \"Rationale\": \"Image 0 provides a table with accuracy percentages for different models on the COCOQA dataset, specifically showing the combined accuracy for irrelevant image-question and question-answer pairs. Text 3 discusses the performance of models with partial information and highlights the effectiveness of using all three sources of information (MLP-IQA).\",\n    \"Answer\": \"The MLP-IQA model performs the best on the COCOQA dataset with a combined accuracy of 75.9% for identifying irrelevant image-question and question-answer pairs. In comparison, the model that only observes answers (MLP-A) has a much lower combined accuracy of 26.6%.\"\n}"
  },
  "7": {
    "question": "On the VQAv2-2017 validation set, which model performs best when considering all three sources of information (images, questions, and answers) and how does its performance compare to the model that only uses answers?",
    "ground_truth": "The model that performs best on VQAv2-2017 val when considering all three sources of information is MLP-IQA. It achieves an accuracy of 61.1% on the \\IU+\\QU -decoys metric, significantly outperforming the model that only uses answers (MLP-A) which achieves only 27.7% on the same metric. This demonstrates the importance of incorporating all available information for accurate prediction.",
    "selected_figures_names": [
      "1704.07121v2-Figure2-1.png",
      "1704.07121v2-Table11-1.png",
      "1704.07121v2-Table3-1.png",
      "1704.07121v2-Table2-1.png"
    ],
    "all_figures_names": [
      "1704.07121v2-Figure2-1.png",
      "1704.07121v2-Table11-1.png",
      "1704.07121v2-Table3-1.png",
      "1704.07121v2-Table2-1.png",
      "1704.07121v2-Figure4-1.png",
      "1704.07121v2-Table9-1.png",
      "1704.07121v2-Figure3-1.png",
      "1704.07121v2-Table8-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 6,
    "rationale": "Image 2 provides test accuracy for different models on Visual7W, including those using all three sources of information. Text 6 explains the setup of Visual QA models, including the use of all three components (I+Q+A).",
    "answer": "The MLP-IQA model performs best when considering all three sources of information, with an accuracy of 65.7%. In comparison, the MLP-A model, which only uses answers, has an accuracy of 52.9%.",
    "text_chunks": [
      "Table~ shows the experimental results on VQA2^-. Comparing to Table~, we see that the overall performance for each model decreases as the dataset becomes more challenging on average. Specifically, the model that observes question and answer on VQA2^- performs much worse than that on VQA2 (37.2\\% vs. 48.1\\%).",
      "As shown in Table~, we clearly see that with only answers being visible to the model (MLP-A), the performance is close to random (on the column of \\IU+\\QU -decoys), and far from observing all three sources of information (MLP-IQA). Meanwhile, models that can observe either images and answers (MLP-IA) or questions and answers (MLP-QA) fail to predict as good as the model that observe all three sources of information. Results in Table~ also shows a similar trend.",
      "Since each image corresponds to multiple questions or have multiple objects, solely relying on the image itself will not work well in principle. Such difference clearly indicates that in the Visual QA model, the language component is weak as the model cannot fully exploit the information in ``Q'', making a smaller relative improvement 5.3\\% (from 62.4\\% to 65.7\\%) where humans improved relatively 17.4\\%. \\paragraph{Shortcuts are due to design deficiencies We probe deeper on how the decoy answers have impacted the performance of learning models. As explained above, the decoys are drawn from all plausible answers to a question, irrespective of whether they are visually grounded or not.",
      "The model learns the specific statistics of the candidate answers in the dataset and exploits those. Adding the information about the image (\\ie, the row of ``I+A''), the machine improves significantly and gets close to the performance when all information is used (62.4\\% vs. 65.7\\%). There is a weaker correlation between the question and the answers as ``Q+A'' improves over ``A'' only modestly.",
      "Besides, the human performance on qaVG cannot be directly compared to that on the other datasets, since the questions on qaVG tend to focus on local image regions and are considered harder.. \\paragraph{Effectiveness of new decoys A better set of decoys will force learning models to integrate all 3 pieces of information --- images, questions and answers --- to make the correct selection from multiple-choices. In particular, they should prevent learning algorithms from exploiting shortcuts such that partial information is sufficient for performing well on the Visual QA task. Table~ clearly indicates that those goals have been achieved.",
      "This can be achieved either by human annotators, or with an automatic procedure that uses captions or question templates and detailed image annotations. We concentrate on 3 datasets: VQA~, Visual7W~, and Visual Genome~. All of them use images from MSCOCO~. Besides the pairs of questions and correct answers, VQA, Visual7W, and visual Madlibs~ provide decoy answers for each pair so that the task can be evaluated in multiple-choice selection accuracy. What decoy answers to use is the focus of our work.",
      "In the cases that we cannot find 3 decoys, we include random ones from the original set of decoys for VQA and Visual7W; for other datasets, we randomly include those from the top 10 frequently-occurring targets. Setup \\paragraph{Visual QA models We utilize the MLP models mentioned in Sect.~ for all the experiments. We denote MLP-A, MLP-QA, MLP-IA, MLP-IQA as the models using A (Answers only), Q+A (Question plus Answers), I+A (Image plus Answers), and I+Q+A (Image, Question and Answers) for multimodal information, respectively.",
      "Thus, similarly, a learning model can examine the image alone and deduce the correct answer. We propose automatic procedures to revise VQA and Visual7W (and to create new datasets based on COCOQA~, VQA2~, and Visual Genome) such that the decoy generation is carefully orchestrated to prevent learning algorithms from exploiting the shortcuts in the datasets by overfitting on incidental statistics. In particular, our design goal is that a learning machine needs to understand all the 3 components of an image-question-candidate answers triplet in order to make the right choice --- ignoring either one or two components will result in drastic degradation in performance."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 6,\n    \"Rationale\": \"Image 2 provides test accuracy for different models on Visual7W, including those using all three sources of information. Text 6 explains the setup of Visual QA models, including the use of all three components (I+Q+A).\",\n    \"Answer\": \"The MLP-IQA model performs best when considering all three sources of information, with an accuracy of 65.7%. In comparison, the MLP-A model, which only uses answers, has an accuracy of 52.9%.\"\n}"
  }
}