{
  "0": {
    "question": "Which model achieved the highest performance score on the Breakout game?",
    "ground_truth": "A3C-CTS",
    "selected_figures_names": [
      "1707.00524v2-Table2-1.png"
    ],
    "all_figures_names": [
      "1707.00524v2-Figure5-1.png",
      "1707.00524v2-Table1-1.png",
      "1707.00524v2-Figure3-1.png",
      "1707.00524v2-Figure1-1.png",
      "1707.00524v2-Figure2-1.png",
      "1707.00524v2-Figure4-1.png",
      "1707.00524v2-Table2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 provides a table with performance scores for different models on the Breakout game. Text 1 discusses the performance of DQN-Informed-Hash, which is relevant to understanding the context of the scores.",
    "answer": "A3C-CTS achieved the highest performance score on the Breakout game with a score of 473.93.",
    "text_chunks": [
      "Breakout, though not classified as hard exploration game, demonstrates significant exploration bottleneck with standard exploration attempt, as the state space is changing rapidly as agent learn, and the performance with standard exploration falls far behind advanced exploration technique. Hence it is also included as a test domain. For all the tasks, we use the state representation that concatenates 4 consequent image frames of size 84\\times 84.",
      "We report the result in Table~. Among all the test domains, {DQN-Informed-Hash outperforms {DQN-Informed, and there are significant performance gains observed in each domain. Note that in Breakout, the agent fails to progress with {DQN-Informed and always scores almost 0. It may due to that the kernel-based pixel distance evaluation metric used in {DQN-Informed encourages the agent to explore states that is dissimilar from the recent history, which is insufficient to let the agent explore.",
      "We choose 5 representative games that require significant exploration to learn the policy: Breakout, Freeway, Frostbite, Ms-Pacman and Q-bert. Four of the five games, Freeway, Frostbite, Ms-Pacman and Q-bert are classified as hard exploration games, based on the taxonomy of exploration proposed in~, where Freeway has sparse reward and the others have dense reward.",
      "We discount the gradient by multiplying the gradient value by 5\\times 10^{-3. \\end{subfloatrow % \\captionsetup[subfigure]{justification=centering \\floatbox{figure{% \\vspace{0.5mm \\caption{% Left: the predicted future trajectories for each action in Breakout.",
      "\\subsubsection{Computing Novelty for States Once the prediction model f(\\cdot) and the autoencoder model g(\\cdot) are both trained, the agent could perform informed exploration as illustrated in Figure~. At each step, the agent performs exploration with a probability less than \\epsilon and performs greedy action selection otherwise. When performing exploration, the agent strategically selects the most novel action direction to explore. Given the state \\mathcal{S_t, the agent first performs roll-out with length H for the future trajectories predicted by the prediction model, for all the possible actions \\mathbf{a_j\\in \\mathcal{A. Formally, the novelty score for an action {a_j given state \\mathcal{S_t, denoted by \\rho({a_j|\\mathcal{S_t), is computed as, where \\psi_{t+i is the count for the future state \\mathcal{S_{t+i derived from () by hashing over the predicted frame \\hat{s_{t+i, H is a predefined roll-out length, and \\beta is a real-valued discount rate.",
      "Moreover, we use Breakout as an illustrative example to demonstrate the hashing can generate meaningful hash codes for predicted future frames (see Figure ~). For a given ground-truth frame, we show the predicted frames with length 5 for taking each action. It can be found that different actions lead to different trajectories of board positions.",
      "After evaluating the novelty for all the possible actions, the agent selects the one with the highest novelty score to explore. Overall, the policy for the RL agent with the proposed informed exploration strategy is defined as: \\[ {a_t= \\end{cases \\] where p is a random value sampled from Uniform (0,1), and \\mathcal{Q(\\mathcal{S_t,a) is the output of the Q-value function. Experiments In the empirical evaluation, we use the Arcade Learning Environment (ALE)~, which consists of Atari 2600 video games as the testing domain.",
      "By incorporating a deep action-conditional prediction model over future transitions and a hashing mechanism based on a deep autoencoder model and LSH, we enable the agent to predict over the future trajectories and intuitively evaluate the novelty for each future action direction based on the hashing result. Empirical results on Atari 2600 domain show that the proposed informed exploration framework could significantly improve the exploration efficiency in several challenging deep RL domains. \\subsubsection*{Acknowledgments This work is funded by NTU Singapore Nanyang Assistant Professorship (NAP) grant M4081532.020, MOE AcRF Tier-1 grant 2016-T1-001-159, and Microsoft Research Asia."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 provides a table with performance scores for different models on the Breakout game. Text 1 discusses the performance of DQN-Informed-Hash, which is relevant to understanding the context of the scores.\",\n    \"Answer\": \"A3C-CTS achieved the highest performance score on the Breakout game with a score of 473.93.\"\n}"
  },
  "1": {
    "question": "Which game has the highest code loss in phase 2?",
    "ground_truth": "Pacman",
    "selected_figures_names": [
      "1707.00524v2-Figure5-1.png"
    ],
    "all_figures_names": [
      "1707.00524v2-Figure5-1.png",
      "1707.00524v2-Table1-1.png",
      "1707.00524v2-Figure3-1.png",
      "1707.00524v2-Figure1-1.png",
      "1707.00524v2-Figure2-1.png",
      "1707.00524v2-Figure4-1.png",
      "1707.00524v2-Table2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 provides a direct visual comparison of code loss across different games for both phases, which is essential to identify the game with the highest code loss in phase 2. Text 1 discusses code loss but does not specify values for individual games.",
    "answer": "Frostbite",
    "text_chunks": [
      "First, the result shows that without the second phase, it is impossible to perform hashing with autoencoder trained only by the reconstruction loss, since the average code losses are above 1 in all the domains and with distinct hash codes, the count values returned from querying the hash table are meaningless. Second, the result shows that after the training of the second phase, the code loss is significantly reduced. \\linebreak \\end{table*",
      "\\vspace{-3mm Overall, it is extremely challenging to match the state codes for the predicted frames and their corresponding seen frames while maintaining a satisfying reconstruction performance. We demonstrate this in Figure~ (a) by showing the code loss, which is measured in terms of the number of mismatch in binary codes between a pair of predicted frame and its corresponding ground-truth frame. The presented result is derived by averaging over 10,000 pairs of codes.",
      "We choose 5 representative games that require significant exploration to learn the policy: Breakout, Freeway, Frostbite, Ms-Pacman and Q-bert. Four of the five games, Freeway, Frostbite, Ms-Pacman and Q-bert are classified as hard exploration games, based on the taxonomy of exploration proposed in~, where Freeway has sparse reward and the others have dense reward.",
      "Breakout, though not classified as hard exploration game, demonstrates significant exploration bottleneck with standard exploration attempt, as the state space is changing rapidly as agent learn, and the performance with standard exploration falls far behind advanced exploration technique. Hence it is also included as a test domain. For all the tasks, we use the state representation that concatenates 4 consequent image frames of size 84\\times 84.",
      "We also show the reconstruction errors measured in terms of MSE after the training of the two phases for each domain in Figure~ (b). By incorporating the code matching loss, the reconstruction behavior for the autoencoder receives slightly negative effect. A comparison of frame reconstruction effect after the training of the two phases are shown in Figure~. It is shown that after training to match the state codes, the reconstructed frames are slightly blurred, but still able to reflect the essential features in each problem domain.",
      "Training the autoencoder from scratch with both \\mathcal{L_{\\mbox{rec and \\mathcal{L_{\\mbox{mat is also difficult, as \\mathcal{L_{\\mbox{mat is initially very low and \\mathcal{L_{\\mbox{rec is very high. The network can hardly find a direction to consistently decrease \\mathcal{L_{\\mbox{rec with such an imbalance. Therefore, in this work, we propose to train the autoencoder for two phases, where the first phase uses \\mathcal{L_{\\mbox{rec to train on seen frames until convergence, and the second phase uses the composed loss function \\mathcal{L as in () to address the requirement for matching the prediction with reality.",
      "We use Adam as optimization algorithm, 10^{-3 as learning rate, a mini-batch size of 100 and discount the gradient by multiplying 10^{-2. In the second phase, we train the autoencoder based on the loss in (). Specifically, we use Adam as optimization algorithm, 10^{-4 as learning rate, a mini-batch size of 100, and \\lambda value of 0.01.",
      "Therefore, the effort for matching the codes is necessary. However, matching the state code while guaranteeing a satisfying reconstruction behavior is extremely challenging. Fine tuning an autoencoder fully trained with \\mathcal{L_{\\mbox{rec by involving the additional code matching loss \\mathcal{L_{\\mbox{mat would fast disrupt the reconstruction behavior before the code loss could decrease to the expected level."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 provides a direct visual comparison of code loss across different games for both phases, which is essential to identify the game with the highest code loss in phase 2. Text 1 discusses code loss but does not specify values for individual games.\",\n    \"Answer\": \"Frostbite\"\n}"
  },
  "2": {
    "question": "What is the difference between the predicted frame and the reconstructed frame for each task domain?",
    "ground_truth": "The predicted frame is generated by the prediction model, while the reconstructed frame is generated by the autoencoder. The predicted frame is typically more accurate than the reconstructed frame, as the prediction model is trained to predict the future state of the environment, while the autoencoder is only trained to reconstruct the input image.",
    "selected_figures_names": [
      "1707.00524v2-Figure5-1.png",
      "1707.00524v2-Table1-1.png",
      "1707.00524v2-Figure3-1.png",
      "1707.00524v2-Figure1-1.png"
    ],
    "all_figures_names": [
      "1707.00524v2-Figure5-1.png",
      "1707.00524v2-Table1-1.png",
      "1707.00524v2-Figure3-1.png",
      "1707.00524v2-Figure1-1.png",
      "1707.00524v2-Figure2-1.png",
      "1707.00524v2-Figure4-1.png",
      "1707.00524v2-Table2-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 0,
    "rationale": "Image 2 provides a visual comparison of the predicted and reconstructed frames for each task domain, showing the differences directly. Text 0 discusses the reconstruction errors and the effect of training phases on the autoencoder, which is relevant to understanding the differences in frame quality.",
    "answer": "The predicted frames are generated by the prediction model and are generally accurate. The reconstructed frames, especially after the second phase of training with both reconstruction and code matching loss, are slightly blurred but still capture essential features. The autoencoder's reconstruction is affected by the additional code matching loss, leading to some blurring.",
    "text_chunks": [
      "We also show the reconstruction errors measured in terms of MSE after the training of the two phases for each domain in Figure~ (b). By incorporating the code matching loss, the reconstruction behavior for the autoencoder receives slightly negative effect. A comparison of frame reconstruction effect after the training of the two phases are shown in Figure~. It is shown that after training to match the state codes, the reconstructed frames are slightly blurred, but still able to reflect the essential features in each problem domain.",
      "\\vspace{-3mm Overall, it is extremely challenging to match the state codes for the predicted frames and their corresponding seen frames while maintaining a satisfying reconstruction performance. We demonstrate this in Figure~ (a) by showing the code loss, which is measured in terms of the number of mismatch in binary codes between a pair of predicted frame and its corresponding ground-truth frame. The presented result is derived by averaging over 10,000 pairs of codes.",
      "Note that our prediction on frame-to-frame level is more precise than those on feature level because the prediction target of frame-to-frame is exact image from game environment while feature prediction target is encoded feature extracted from a trained neural network. \\subsubsection{Hashing over the State Space with Autoencoder and LSH To evaluate the novelty of a state, we adopt a hashing model to count over the state space. We first train an autoencoder model on frames, g\\!:\\! \\mathbf{s \\!\\in\\!",
      "To make the hash codes to be the same, the derived feature vectors of the predicted frames and the ground-truth seen fames through \\phi(\\cdot) need to be close to each other. We introduce an additional loss function of a pair of ground-truth seen frame and predicted frame (\\mathbf{s_t,\\boldsymbol{\\hat{s_t) as follows, Finally, by combing () and (), we define the following overall loss function, where \\theta is the parameter for the autoencoder. Note that even though the prediction model could generate almost identical frames, training the autoencoder with only the reconstruction loss may lead to distinct state codes in all the task domains (details will be shown in Section~).",
      "For the hash codes, as three of the actions, no-op, fire and left, lead to little change in frames, most of the future frames are hashed into the same code. The action right leads to the most significant changes in board position, so the codes for future frames are much more distinct than the rest. Meanwhile, Figure~ also shows that the multi-step prediction model generates realistic future frames.",
      "For multi-step prediction, as expected, the prediction loss increases with the increase of the prediction length. We demonstrate that the trained prediction models are able to generate realistic future frames which are visualized to be very close to the ground-truth frames in Figure~. \\vspace{-3mm Evaluation on Hashing with Autoencoder and LSH The architecture for the autoencoder model is identical to that shown in Figure~ (down).",
      "In our work, we use the counts derived from hashing over the state space to reliably infer the novelty of a state. In~, which is a work mostly related to ours, an action-conditional prediction model is trained to predict the future frame given a state-action pair. Then they compute the Gaussian kernel distance between each predicted future frame and a set of history frames, and inform the agent to take the action that leads to the frames which are most dissimilar compared to a window of recently seen frames.",
      "In our work, we use the same frame prediction architecture as in~, but our work additionally performs hashing over the predicted future frames to reliably infer the novelty of a possible action direction for the agent to explore. \\paragraph{Hashing for Deep RL Domain Running RL algorithms on the discretized features yields faster learning and promising performance. It is shown that the latent features learned by autoencoder trained in an unsupervised manner is of great promise to efficiently discretize the high-dimensional state space~. In~, the state space for Atari 2600 domain is first discretized using the latent features derived from an autoencoder model."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 0,\n    \"Rationale\": \"Image 2 provides a visual comparison of the predicted and reconstructed frames for each task domain, showing the differences directly. Text 0 discusses the reconstruction errors and the effect of training phases on the autoencoder, which is relevant to understanding the differences in frame quality.\",\n    \"Answer\": \"The predicted frames are generated by the prediction model and are generally accurate. The reconstructed frames, especially after the second phase of training with both reconstruction and code matching loss, are slightly blurred but still capture essential features. The autoencoder's reconstruction is affected by the additional code matching loss, leading to some blurring.\"\n}"
  },
  "3": {
    "question": "What is the difference between the encoder and decoder networks in the action-conditional prediction model?",
    "ground_truth": "The encoder network takes a one-hot action and the current state as input and outputs a latent representation of the state. The decoder network takes the latent representation and outputs a prediction of the next state.",
    "selected_figures_names": [
      "1707.00524v2-Figure2-1.png"
    ],
    "all_figures_names": [
      "1707.00524v2-Figure5-1.png",
      "1707.00524v2-Table1-1.png",
      "1707.00524v2-Figure3-1.png",
      "1707.00524v2-Figure1-1.png",
      "1707.00524v2-Figure2-1.png",
      "1707.00524v2-Figure4-1.png",
      "1707.00524v2-Table2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 6,
    "rationale": "Image 0 shows the architecture of the action-conditional prediction model, which includes both encoder and decoder networks. Text 6 describes the input and output of the prediction model, providing details on how the encoder and decoder function in predicting future states.",
    "answer": "The encoder network in the action-conditional prediction model processes a set of recent image frames and a one-hot encoded action to create a compact representation. The decoder network then uses this representation to predict future transition frames, effectively reconstructing the next state from the encoded information.",
    "text_chunks": [
      "\\subsubsection{Learning Transition Model with Prediction Network The architecture for action-conditional prediction is shown in Figure~ (up). To be specific, we train a deep prediction model f:(\\mathcal{S_t,a_t)",
      "The autoencoder is trained on a dataset collected in an identical manner as that for the prediction model. It is trained under two phases. In the first phase, it is trained with only reconstruction loss.",
      "To make the hash codes to be the same, the derived feature vectors of the predicted frames and the ground-truth seen fames through \\phi(\\cdot) need to be close to each other. We introduce an additional loss function of a pair of ground-truth seen frame and predicted frame (\\mathbf{s_t,\\boldsymbol{\\hat{s_t) as follows, Finally, by combing () and (), we define the following overall loss function, where \\theta is the parameter for the autoencoder. Note that even though the prediction model could generate almost identical frames, training the autoencoder with only the reconstruction loss may lead to distinct state codes in all the task domains (details will be shown in Section~).",
      "Note that our prediction on frame-to-frame level is more precise than those on feature level because the prediction target of frame-to-frame is exact image from game environment while feature prediction target is encoded feature extracted from a trained neural network. \\subsubsection{Hashing over the State Space with Autoencoder and LSH To evaluate the novelty of a state, we adopt a hashing model to count over the state space. We first train an autoencoder model on frames, g\\!:\\! \\mathbf{s \\!\\in\\!",
      "Training the autoencoder from scratch with both \\mathcal{L_{\\mbox{rec and \\mathcal{L_{\\mbox{mat is also difficult, as \\mathcal{L_{\\mbox{mat is initially very low and \\mathcal{L_{\\mbox{rec is very high. The network can hardly find a direction to consistently decrease \\mathcal{L_{\\mbox{rec with such an imbalance. Therefore, in this work, we propose to train the autoencoder for two phases, where the first phase uses \\mathcal{L_{\\mbox{rec to train on seen frames until convergence, and the second phase uses the composed loss function \\mathcal{L as in () to address the requirement for matching the prediction with reality.",
      "\\subsubsection{Computing Novelty for States Once the prediction model f(\\cdot) and the autoencoder model g(\\cdot) are both trained, the agent could perform informed exploration as illustrated in Figure~. At each step, the agent performs exploration with a probability less than \\epsilon and performs greedy action selection otherwise. When performing exploration, the agent strategically selects the most novel action direction to explore. Given the state \\mathcal{S_t, the agent first performs roll-out with length H for the future trajectories predicted by the prediction model, for all the possible actions \\mathbf{a_j\\in \\mathcal{A. Formally, the novelty score for an action {a_j given state \\mathcal{S_t, denoted by \\rho({a_j|\\mathcal{S_t), is computed as, where \\psi_{t+i is the count for the future state \\mathcal{S_{t+i derived from () by hashing over the predicted frame \\hat{s_{t+i, H is a predefined roll-out length, and \\beta is a real-valued discount rate.",
      "\\rightarrow \\mathcal{S_{t+1 to make the agent able to predict over the future transitions given a state-action pair. The state input \\mathcal{S_t is a set of r recent image frames, and the action input a_t is represented by \\mathbf{a_t \\in \\mathbb{R^{l, which is a one-hot vector, where l is the number of actions for the task domain. To predict a new state, the model predicts one single frame at a time, denoted as \\boldsymbol{\\hat{s\\in \\mathbb{R^{m\\times n.",
      "In our work, we use the counts derived from hashing over the state space to reliably infer the novelty of a state. In~, which is a work mostly related to ours, an action-conditional prediction model is trained to predict the future frame given a state-action pair. Then they compute the Gaussian kernel distance between each predicted future frame and a set of history frames, and inform the agent to take the action that leads to the frames which are most dissimilar compared to a window of recently seen frames."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 6,\n    \"Rationale\": \"Image 0 shows the architecture of the action-conditional prediction model, which includes both encoder and decoder networks. Text 6 describes the input and output of the prediction model, providing details on how the encoder and decoder function in predicting future states.\",\n    \"Answer\": \"The encoder network in the action-conditional prediction model processes a set of recent image frames and a one-hot encoded action to create a compact representation. The decoder network then uses this representation to predict future transition frames, effectively reconstructing the next state from the encoded information.\"\n}"
  }
}