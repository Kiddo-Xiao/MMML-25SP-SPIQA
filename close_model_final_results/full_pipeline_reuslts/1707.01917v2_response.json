{
  "0": {
    "question": "What does the induced schema Win <A4, B3, C2> represent?",
    "ground_truth": "The induced schema Win <A4, B3, C2> represents the fact that player A4 won tournament C2, defeating player B3.",
    "selected_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png"
    ],
    "all_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png",
      "1707.01917v2-Table1-1.png",
      "1707.01917v2-Table4-1.png",
      "1707.01917v2-Table5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 7,
    "rationale": "Image 0 provides a visual representation of the schema induction process, showing how the schema Win <A4, B3, C2> is formed. Text 7 explains the concept of inducing relation schemata involving multiple arguments, which directly relates to the schema in question.",
    "answer": "The induced schema Win <A4, B3, C2> represents a high-order relation schema for a win event, where A4 corresponds to the winning player, B3 to the opponent player, and C2 to the tournament or location.",
    "text_chunks": [
      "Non-negative updates for the variables can be obtained following . Similar to , schemata induced will be of the form relation \\langle \\mat{A_i, \\mat{B_j, \\mat{C_k \\rangle. Here, \\mat{P_i represents the i^{\\text{th column of a matrix \\mat{P. \\mat{A is the embedding matrix of subject NPs in \\tuples{ (i.e., mode-1 of \\ten{X), r_1 is the embedding rank in mode-1 which is the number of latent categories of subject NPs.",
      "Each slice of the core tensor \\ten{G^3 corresponds to one of the m relations. Each cell in a slice corresponds to an induced schema in terms of the latent factors from matrices \\mat{A and \\mat{B. In other words, \\ten{G^3_{i,j,k is an induced binary schema for relation k involving induced categories represented by columns \\mat{A_{i and \\mat{B_{j. Cells in \\ten{G^1 and \\ten{G^2 may be interpreted accordingly.",
      "% Induced schemata for each dataset and method are evaluated by three human evaluators, E1, E2, and E3. \\system{ performs better than \\HCB{ for Shootings and NYT Sports datasets. Even though \\HCB{ achieves better accuracy on MUC dataset, it has several limitations, see \\refsec{sec:exp for more details.",
      "Whereas for \\system{, the number of categories depends on the rank of factorization, which is a user provided parameter, thus providing more flexibility to choose the latent categories. Using Event Schema Induction for \\problem{ Event schema induction is defined as the task of learning high-level representations of events, like a tournament, and their entity roles, like winning-player etc, from unlabeled text. Even though the main focus of event schema induction is to induce the important roles of the events, as a side result most of the algorithms also provide schemata for the relations.",
      "In this section, we investigate the effectiveness of these schemata compared to the ones induced by \\system{. Event schemata are represented as a set of {\\it (Actor, Rel, Actor) triples in . {\\it Actors represent groups of noun phrases and {\\it Rels represent relations.",
      "However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus . Recently, a model for event representations is proposed in .",
      "Schema Induction: Most work on inducing schemata for relations has been in the binary setting . \\citet{McDonald:2005:SAC:1219840.1219901 and \\citet{Peng2017 extract \\nary{ relations from Biomedical documents, but do not induce the schema, i.e., type signature of the \\nary{ relations. There has been significant amount of work on Semantic Role Labeling , which can be considered as \\nary{ relation extraction.",
      "For example, in the sports domain, one may be interested in beliefs of the form win(Roger Federer, Nadal, Wimbledon, London), which is an instance of the high-order (or n-ary) relation win whose schema is given by win(WinningPlayer, OpponentPlayer, Tournament, Location). We refer to the problem of inducing such relation schemata involving multiple arguments as \\problemfull{ (\\problem{). In spite of its importance, \\problem{ is mostly unexplored."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 7,\n    \"Rationale\": \"Image 0 provides a visual representation of the schema induction process, showing how the schema Win <A4, B3, C2> is formed. Text 7 explains the concept of inducing relation schemata involving multiple arguments, which directly relates to the schema in question.\",\n    \"Answer\": \"The induced schema Win <A4, B3, C2> represents a high-order relation schema for a win event, where A4 corresponds to the winning player, B3 to the opponent player, and C2 to the tournament or location.\"\n}"
  },
  "1": {
    "question": "Which dataset has the highest value for the hyperparameter  \u03bba?",
    "ground_truth": "The NYT Sports dataset has the highest value for \u03bba (0.9).",
    "selected_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png"
    ],
    "all_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png",
      "1707.01917v2-Table1-1.png",
      "1707.01917v2-Table4-1.png",
      "1707.01917v2-Table5-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 1,
    "rationale": "Image 1 provides the hyperparameter values for each dataset, and Text 1 discusses the selection of hyperparameters, which is relevant to understanding their significance.",
    "answer": "The NYT Sports dataset has the highest value for the hyperparameter \u03bba, which is 0.9.",
    "text_chunks": [
      "\\lambda_a, \\lambda_b and \\lambda_c are the regularization weights. However, the 4-order tensors are heavily sparse for all the datasets we consider in this work. The sparsity ratio of this 4-order tensor for all the datasets is of the order 1e-7.",
      "Model Selection: In order to select appropriate \\system{ parameters, we perform a grid search over the space of hyper-parameters, and select the set of hyper-parameters that give best Average FIT score (\\mathrm{AvgFIT). % where, We perform a grid search for the rank parameters between 5 and 20, for the regularization weights we perform a grid search over 0 and 1. \\reftbl{tbl:hyperparameters provides the details of hyper-parameters set for different datasets.",
      "And the third dataset (MUC) is a set of 1300 Latin American newswire documents about terrorism events. After performing the processing steps described in \\refsec{sec:method, we obtained 357,914 unique OpenIE extractions from the NYT Sports dataset, 10,847 from Shootings dataset, and 8,318 from the MUC dataset. However, in order to properly analyze and evaluate the model, we consider only the 50 most frequent relations in the datasets and their corresponding OpenIE extractions.",
      "Datasets: We run our experiments on three datasets. The first dataset (Shootings) is a collection of 1,335 documents constructed from a publicly available database of mass shootings in the United States. The second is New York Times Sports (NYT Sports) dataset which is a collection of 20,940 sports documents from the period 2005 and 2007.",
      "We present evaluation results from three evaluators represented as E1, E2 and E3. As can be observed from \\reftbl{tbl:accuracy, \\system{ achieves better results than \\HCB{ for the Shootings and NYT Sports datasets, however \\HCB{ achieves better results for the MUC dataset. Percentage agreement of the evaluators for \\system{ is 72\\%, 70\\% and 60\\% for Shootings, NYT Sports and MUC datasets respectively.",
      "% Induced schemata for each dataset and method are evaluated by three human evaluators, E1, E2, and E3. \\system{ performs better than \\HCB{ for Shootings and NYT Sports datasets. Even though \\HCB{ achieves better accuracy on MUC dataset, it has several limitations, see \\refsec{sec:exp for more details.",
      "In our experiments, we use human judgments from three evaluators. For every relation, the first and second columns given in \\reftbl{tbl:predicate_examples are presented to the evaluators and they are asked to validate the schema. We present top 50 schemata based on the score of the constrained maximal clique induced by \\system{ to the evaluators.",
      "\\HCB{ also performs better than Chambers-13 on all the datasets. From this analysis we infer that there is a need for algorithms which induce higher-order schemata for relations, a gap we fill in this paper. Please note that the experimental results provided in for MUC dataset are for the task of event schema induction, but in this work we evaluate the relation schemata."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 1,\n    \"Rationale\": \"Image 1 provides the hyperparameter values for each dataset, and Text 1 discusses the selection of hyperparameters, which is relevant to understanding their significance.\",\n    \"Answer\": \"The NYT Sports dataset has the highest value for the hyperparameter \u03bba, which is 0.9.\"\n}"
  },
  "2": {
    "question": "What is the shape of the tensor $x^1$ for the Shootings dataset?",
    "ground_truth": "The shape of the tensor $x^1$ for the Shootings dataset is 3365 x 1295 x 50.",
    "selected_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png"
    ],
    "all_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png",
      "1707.01917v2-Table1-1.png",
      "1707.01917v2-Table4-1.png",
      "1707.01917v2-Table5-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 2,
    "rationale": "Image 2 provides the dimensions of tensors constructed for each dataset, including the shape of tensor X^1 for the Shootings dataset. Text 2 discusses the sparsity of the tensors, which is relevant to understanding their structure.",
    "answer": "The shape of the tensor X^1 for the Shootings dataset is 3365 \u00d7 1295 \u00d7 50.",
    "text_chunks": [
      "\\subsubsection{Step 1: Back-off Tensor Factorization A schematic overview of this step is shown in \\reffig{fig:arch. \\system{ first preprocesses the corpus and extracts OpenIE tuple set \\tuples{ out of it. The 4-mode tensor \\ten{X is constructed out of \\tuples{.",
      "Datasets: We run our experiments on three datasets. The first dataset (Shootings) is a collection of 1,335 documents constructed from a publicly available database of mass shootings in the United States. The second is New York Times Sports (NYT Sports) dataset which is a collection of 20,940 sports documents from the period 2005 and 2007.",
      "\\lambda_a, \\lambda_b and \\lambda_c are the regularization weights. However, the 4-order tensors are heavily sparse for all the datasets we consider in this work. The sparsity ratio of this 4-order tensor for all the datasets is of the order 1e-7.",
      "And the third dataset (MUC) is a set of 1300 Latin American newswire documents about terrorism events. After performing the processing steps described in \\refsec{sec:method, we obtained 357,914 unique OpenIE extractions from the NYT Sports dataset, 10,847 from Shootings dataset, and 8,318 from the MUC dataset. However, in order to properly analyze and evaluate the model, we consider only the 50 most frequent relations in the datasets and their corresponding OpenIE extractions.",
      "Similarly, \\mat{B and \\mat{C are the embedding matrices of object NPs and other NPs respectively. r_2 and r_3 are the number of latent categories of object NPs and other NPs respectively. \\ten{G is the core tensor.",
      "As a result of the extreme sparsity, this approach fails to learn any schemata. Therefore, we propose a more successful back-off strategy for higher-order RSI in the next section. \\system{: Proposed Framework To alleviate the problem of sparsity, we construct three tensors \\ten{X^3, \\ten{X^2, and \\ten{X^1 from \\mathbb{T as follows: The proposed framework \\system{ for inducing higher order schemata involves the following two steps.",
      "We present evaluation results from three evaluators represented as E1, E2 and E3. As can be observed from \\reftbl{tbl:accuracy, \\system{ achieves better results than \\HCB{ for the Shootings and NYT Sports datasets, however \\HCB{ achieves better results for the MUC dataset. Percentage agreement of the evaluators for \\system{ is 72\\%, 70\\% and 60\\% for Shootings, NYT Sports and MUC datasets respectively.",
      "This is done to avoid noisy OpenIE extractions to yield better data quality and to aid subsequent manual evaluation of the data. We construct input tensors following the procedure described in \\refsec{sec:model. Details on the dimensions of tensors obtained are given in \\reftbl{tbl:datasets."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 2,\n    \"Rationale\": \"Image 2 provides the dimensions of tensors constructed for each dataset, including the shape of tensor X^1 for the Shootings dataset. Text 2 discusses the sparsity of the tensors, which is relevant to understanding their structure.\",\n    \"Answer\": \"The shape of the tensor X^1 for the Shootings dataset is 3365 \u00d7 1295 \u00d7 50.\"\n}"
  },
  "3": {
    "question": "What is the definition of a non-negative tensor?",
    "ground_truth": "A non-negative tensor is a tensor whose elements are all non-negative real numbers.",
    "selected_figures_names": [
      "1707.01917v2-Table1-1.png"
    ],
    "all_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png",
      "1707.01917v2-Table1-1.png",
      "1707.01917v2-Table4-1.png",
      "1707.01917v2-Table5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides a table with definitions of notations used in the paper, including the definition of a non-negative tensor. Text 0 discusses non-negative tensors in the context of tensor factorization.",
    "answer": "A non-negative tensor is defined as an Nth-order tensor with non-negative real values, denoted as \\( \\mathcal{X} \\in \\mathbb{R}_{+}^{n_1 \\times n_2 \\times ... \\times n_N} \\).",
    "text_chunks": [
      "Instead of performing factorization of the higher-order tensor \\ten{X as in \\refsec{sec:failed_attempt, \\system{ creates three tensors out of \\ten{X: \\ten{X^1_{n_2\\times n_3 \\times m, \\ten{X^2_{n_1\\times n_3 \\times m and \\ten{X^3_{n_1\\times n_2 \\times m. \\system{ performs a coupled non-negative Tucker factorization of the input tensors \\ten{X^1, \\ten{X^2 and \\ten{X^3 by solving the following optimization problem. where, We enforce non-negativity constraints on the matrices \\mat{A, \\mat{B, \\mat{C and the core tensors \\ten{G^i (i \\in \\{1, 2, 3\\). Non-negativity is essential for learning interpretable latent factors .",
      "We lemmatize the relations in the tuples and only consider the noun phrases as arguments. Let \\mathbb{T represent the set of these 4-tuples. We can construct a 4-order tensor \\ten{X \\in \\mathbb{R_{+^{n_1",
      "Similarly, \\mat{B and \\mat{C are the embedding matrices of object NPs and other NPs respectively. r_2 and r_3 are the number of latent categories of object NPs and other NPs respectively. \\ten{G is the core tensor.",
      "In this paper, we proposed \\system{, a tensor factorization-based method for higher-order RSI. To the best of our knowledge, this is the first attempt at inducing higher-order (n-ary) schemata for relations from unlabeled text. Rather than factorizing a severely sparse higher-order tensor directly, \\system{ performs back-off and jointly factorizes multiple lower-order tensors derived out of the higher-order tensor.",
      "\\times n_2 \\times n_3 \\times m from \\tuples{. Here, n_1 is the number of {\\it subject noun phrases (NPs), n_2 is the number of {\\it object NPs, n_3 is the number of {\\it other NPs, and m is the number of relations in \\tuples{. Values in the tensor correspond to the frequency of the tuples.",
      "Each slice of the core tensor \\ten{G^3 corresponds to one of the m relations. Each cell in a slice corresponds to an induced schema in terms of the latent factors from matrices \\mat{A and \\mat{B. In other words, \\ten{G^3_{i,j,k is an induced binary schema for relation k involving induced categories represented by columns \\mat{A_{i and \\mat{B_{j. Cells in \\ten{G^1 and \\ten{G^2 may be interpreted accordingly.",
      "\\lambda_a, \\lambda_b and \\lambda_c are the regularization weights. However, the 4-order tensors are heavily sparse for all the datasets we consider in this work. The sparsity ratio of this 4-order tensor for all the datasets is of the order 1e-7.",
      "As a result of the extreme sparsity, this approach fails to learn any schemata. Therefore, we propose a more successful back-off strategy for higher-order RSI in the next section. \\system{: Proposed Framework To alleviate the problem of sparsity, we construct three tensors \\ten{X^3, \\ten{X^2, and \\ten{X^1 from \\mathbb{T as follows: The proposed framework \\system{ for inducing higher order schemata involves the following two steps."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides a table with definitions of notations used in the paper, including the definition of a non-negative tensor. Text 0 discusses non-negative tensors in the context of tensor factorization.\",\n    \"Answer\": \"A non-negative tensor is defined as an Nth-order tensor with non-negative real values, denoted as \\\\( \\\\mathcal{X} \\\\in \\\\mathbb{R}_{+}^{n_1 \\\\times n_2 \\\\times ... \\\\times n_N} \\\\).\"\n}"
  },
  "4": {
    "question": "What is the role of OpenIE in Step 1 of TFBA?",
    "ground_truth": "OpenIE is used to extract tuples from the unlabeled text corpus. These tuples are then used to create the 3-mode tensors X1, X2, and X3.",
    "selected_figures_names": [
      "1707.01917v2-Figure1-1.png"
    ],
    "all_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png",
      "1707.01917v2-Table1-1.png",
      "1707.01917v2-Table4-1.png",
      "1707.01917v2-Table5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 provides a visual overview of the process involving OpenIE in Step 1 of TFBA, showing how tuples are extracted and used in tensor factorization. Text 2 explains the role of OpenIE in extracting tuples from a text corpus, which is crucial for constructing the tensors used in TFBA.",
    "answer": "OpenIE extracts tuples from a text corpus, which are then used to construct tensors for factorization in Step 1 of TFBA.",
    "text_chunks": [
      "This is done to avoid noisy OpenIE extractions to yield better data quality and to aid subsequent manual evaluation of the data. We construct input tensors following the procedure described in \\refsec{sec:model. Details on the dimensions of tensors obtained are given in \\reftbl{tbl:datasets.",
      "\\subsubsection{Step 1: Back-off Tensor Factorization A schematic overview of this step is shown in \\reffig{fig:arch. \\system{ first preprocesses the corpus and extracts OpenIE tuple set \\tuples{ out of it. The 4-mode tensor \\ten{X is constructed out of \\tuples{.",
      "Factorizing a Higher-order Tensor Given a text corpus, we use OpenIEv5 to extract tuples. Consider the following sentence ``Federer won against Nadal at Wimbledon.''. Given this sentence, OpenIE extracts the 4-tuple (Federer, won, against Nadal, at Wimbledon).",
      "We also propose a baseline model for \\problem{ called HardClust. \\\\ {\\bf HardClust: We propose a baseline model called the Hard Clustering Baseline (\\HCB{) for the task of higher order relation schema induction. This model induces schemata by grouping per-relation NP arguments from OpenIE extractions.",
      "And the third dataset (MUC) is a set of 1300 Latin American newswire documents about terrorism events. After performing the processing steps described in \\refsec{sec:method, we obtained 357,914 unique OpenIE extractions from the NYT Sports dataset, 10,847 from Shootings dataset, and 8,318 from the MUC dataset. However, in order to properly analyze and evaluate the model, we consider only the 50 most frequent relations in the datasets and their corresponding OpenIE extractions.",
      "Due to data sparsity, straightforward adaptation of tensor factorization from to \\problem{ % is not feasible, as we shall see in Section~. We overcome this challenge in this paper, and make the following contributions. Source code of \\system{ is available at \\url{https://github.com/madhavcsa/TFBA. The remainder of the paper is organized as follows.",
      "Whereas for \\system{, the number of categories depends on the rank of factorization, which is a user provided parameter, thus providing more flexibility to choose the latent categories. Using Event Schema Induction for \\problem{ Event schema induction is defined as the task of learning high-level representations of events, like a tournament, and their entity roles, like winning-player etc, from unlabeled text. Even though the main focus of event schema induction is to induce the important roles of the events, as a side result most of the algorithms also provide schemata for the relations.",
      "In our experiments, we use human judgments from three evaluators. For every relation, the first and second columns given in \\reftbl{tbl:predicate_examples are presented to the evaluators and they are asked to validate the schema. We present top 50 schemata based on the score of the constrained maximal clique induced by \\system{ to the evaluators."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 provides a visual overview of the process involving OpenIE in Step 1 of TFBA, showing how tuples are extracted and used in tensor factorization. Text 2 explains the role of OpenIE in extracting tuples from a text corpus, which is crucial for constructing the tensors used in TFBA.\",\n    \"Answer\": \"OpenIE extracts tuples from a text corpus, which are then used to construct tensors for factorization in Step 1 of TFBA.\"\n}"
  },
  "5": {
    "question": "Which method achieves the highest accuracy on the Shootings dataset?",
    "ground_truth": "TFBA",
    "selected_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png"
    ],
    "all_figures_names": [
      "1707.01917v2-Figure2-1.png",
      "1707.01917v2-Table3-1.png",
      "1707.01917v2-Table2-1.png",
      "1707.01917v2-Figure1-1.png",
      "1707.01917v2-Table1-1.png",
      "1707.01917v2-Table4-1.png",
      "1707.01917v2-Table5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 provides an overview of the schema induction process, which is relevant to understanding the methods being compared. Text 1 directly states that the system performs better than the alternative method for the Shootings dataset.",
    "answer": "The system achieves the highest accuracy on the Shootings dataset.",
    "text_chunks": [
      "We present evaluation results from three evaluators represented as E1, E2 and E3. As can be observed from \\reftbl{tbl:accuracy, \\system{ achieves better results than \\HCB{ for the Shootings and NYT Sports datasets, however \\HCB{ achieves better results for the MUC dataset. Percentage agreement of the evaluators for \\system{ is 72\\%, 70\\% and 60\\% for Shootings, NYT Sports and MUC datasets respectively.",
      "% Induced schemata for each dataset and method are evaluated by three human evaluators, E1, E2, and E3. \\system{ performs better than \\HCB{ for Shootings and NYT Sports datasets. Even though \\HCB{ achieves better accuracy on MUC dataset, it has several limitations, see \\refsec{sec:exp for more details.",
      "Datasets: We run our experiments on three datasets. The first dataset (Shootings) is a collection of 1,335 documents constructed from a publicly available database of mass shootings in the United States. The second is New York Times Sports (NYT Sports) dataset which is a collection of 20,940 sports documents from the period 2005 and 2007.",
      "And the third dataset (MUC) is a set of 1300 Latin American newswire documents about terrorism events. After performing the processing steps described in \\refsec{sec:method, we obtained 357,914 unique OpenIE extractions from the NYT Sports dataset, 10,847 from Shootings dataset, and 8,318 from the MUC dataset. However, in order to properly analyze and evaluate the model, we consider only the 50 most frequent relations in the datasets and their corresponding OpenIE extractions.",
      "Each event template also provides a cluster of relations, which is most likely to appear in the context of the aforementioned slots. We evaluate the schemata of these relation clusters. As can be observed from \\reftbl{tbl:accuracy, the proposed \\system{ performs much better than Chambers-13.",
      "However, we note that solved a different problem, viz., n-ary relation instance extraction, while our focus is on inducing schemata. Though we discuss the case of back-off from 4-order to 3-order, ideas presented above can be extended for even higher orders depending on the sparsity of the tensors. Experiments \\end{center \\end{table* \\end{table* \\caption{Higher-order RSI accuracies of various methods on the three datasets.",
      "For this investigation we experiment with the following algorithm. {\\bf Chambers-13 : This model learns event templates from text documents. Each event template provides a distribution over slots, where slots are clusters of NPs.",
      "Model Selection: In order to select appropriate \\system{ parameters, we perform a grid search over the space of hyper-parameters, and select the set of hyper-parameters that give best Average FIT score (\\mathrm{AvgFIT). % where, We perform a grid search for the rank parameters between 5 and 20, for the regularization weights we perform a grid search over 0 and 1. \\reftbl{tbl:hyperparameters provides the details of hyper-parameters set for different datasets."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 provides an overview of the schema induction process, which is relevant to understanding the methods being compared. Text 1 directly states that the system performs better than the alternative method for the Shootings dataset.\",\n    \"Answer\": \"The system achieves the highest accuracy on the Shootings dataset.\"\n}"
  }
}