{
  "0": {
    "question": "What is the probability of finding the pattern {head=F, ant=NAM} in the data?",
    "ground_truth": "1",
    "selected_figures_names": [
      "1708.00160v2-Figure2-1.png",
      "1708.00160v2-Table5-1.png",
      "1708.00160v2-Table6-1.png",
      "1708.00160v2-Figure4-1.png"
    ],
    "all_figures_names": [
      "1708.00160v2-Figure2-1.png",
      "1708.00160v2-Table5-1.png",
      "1708.00160v2-Table6-1.png",
      "1708.00160v2-Figure4-1.png",
      "1708.00160v2-Table4-1.png",
      "1708.00160v2-Figure3-1.png",
      "1708.00160v2-Figure5-1.png",
      "1708.00160v2-Table2-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 5,
    "rationale": "Image 3 shows the conditional FP-Tree for the pattern {head=F, ant=NAM}, which is directly related to the question. Text 5 discusses the probability of patterns in coreference relations, which is relevant for calculating the probability of a specific pattern.",
    "answer": "The probability of finding the pattern {head=F, ant=NAM} can be determined by examining the support values in the conditional FP-Tree and applying the coreference probability formula from Text 5. However, specific numerical values are not provided in the text or image.",
    "text_chunks": [
      "As an example, assume D contains the following two samples: Based on these samples A=\\{ana-type=NAM, ant-type=NAM, head-match=F, head-match=T\\, support(a_i,0)_{a_i \\in A= \\{1,1,1,0\\, and support(a_i,1)_{a_i \\in A=\\{1,1,0,1\\. If we sort A based on a_i's frequencies (support(a_i,0)+support(a_i,1)), the ordering of A's items will remain the same. The FP-Tree construction steps for the above samples are demonstrated in Figure~. ana-type, ant-type, and head-match features are abbreviated as ana, ant, and head, respectively.",
      "\\item A=\\{a_1,\\dots,a_l\\: set of all feature-values present in D. Each a_i \\in A is called an item, e.g.\\ a_i=``anaphor type=proper''. \\item p: pattern p=\\{a_{i_1,\\dots, a_{i_k\\ is a set of one or more items, e.g.\\ p=\\{``anaphor type=proper'', ``antecedent type=proper''\\. \\item support(p,c_i): the number of samples that contain pattern p and are labeled with c_i. \\squishend Data Structure For representing the input samples, we use the Frequent Pattern Tree (FP-Tree) structure that is the data structure of the FP-Growth algorithm , i.e.\\ one of the most common algorithms for frequent pattern mining.",
      "Initially, p is empty and the FP-Tree is constructed based on all frequent items of data and A_j = A. Resulting patterns are collected in P. For each a_i \\in A_j, the algorithm builds new pattern q by combining a_i with p. frequent(q) checks whether q meets the frequency condition. If q is frequent, the algorithm continues the search process. Otherwise, it is not possible to build any frequent pattern out of a non-frequent one.",
      "Due to its efficiency, EPM can handle large datasets similar to ones that are commonly used in various NLP tasks. Experimental Setup We use the same FP-Tree implementation for DDPMine and EPM. In all algorithms, we consider a pattern as frequent if it occurs in 10\\% of the samples of one of the classes.",
      "We run the JIM algorithm on the same data and with the same setup as that of EPM.\\footnote{ We set the minimum frequency, maximum pattern length and score^+ threshold parameters of JIM to 20, 5 and 0.6. This results in nine pairwise features, 260 POS tags, 38 dependency relations, 32 mention types, and 18 named entity tags. The ``+JIM'' row shows the results of deep-coref top-pairs model in which these feature-values are incorporated.",
      "Impact of Informative Feature-values Experimental Setup For determining informative feature-values, we extract all features for all mention-pairs of the CoNLL training data and then apply EPM on this data. In order to prevent learning annotation errors and specific properties of the training data, we consider a pattern as frequent if it occurs in coreference relations of at least m different coreferring anaphors (m=20). Since the majority of mention-pairs are non-coreferent and we are not interested in patterns for non-coreferring relations, we also consider the coreference probability of each pattern p, i.e.\\ \\frac{|\\{X_i|p \\in X_i \\land c(X_i)=coreferent\\|{|\\{X_i|p \\in X_i\\|, in the post-pruning function.",
      "Frequency of an item corresponds to \\sum_{c_i \\in C \\: support(a_i, c_i). Except for the root, which is a null node, each node n contains an item a_i \\in A. It also contains the support values of a_i in the subpath of the tree that starts from the root and ends with n, i.e.\\ support_n(a_i,c_j). The FP-Tree construction method is as follows: (a) scan D to collect the set of all items, i.e.\\ A. Compute support(a_i,c_j) for each item a_i \\in A and label c_j \\in C. Sort A's members in descending order according to their frequencies, i.e. \\sum_{c_i \\in C \\: support(a_i, c_i).",
      "Similar to \\newcite{batal10, we employ a binomial distribution to determine information novelty. Mining Algorithm The EPM algorithm is summarized in Algorithm~. It takes FP-Tree T, pattern p on which T is conditioned, and set of items (A_j \\subset A) whose combinations with p will be examined."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 5,\n    \"Rationale\": \"Image 3 shows the conditional FP-Tree for the pattern {head=F, ant=NAM}, which is directly related to the question. Text 5 discusses the probability of patterns in coreference relations, which is relevant for calculating the probability of a specific pattern.\",\n    \"Answer\": \"The probability of finding the pattern {head=F, ant=NAM} can be determined by examining the support values in the conditional FP-Tree and applying the coreference probability formula from Text 5. However, specific numerical values are not provided in the text or image.\"\n}"
  },
  "1": {
    "question": "What is the support value of the node \"ana=NAM\"?",
    "ground_truth": "2",
    "selected_figures_names": [
      "1708.00160v2-Figure2-1.png"
    ],
    "all_figures_names": [
      "1708.00160v2-Table6-1.png",
      "1708.00160v2-Table5-1.png",
      "1708.00160v2-Figure2-1.png",
      "1708.00160v2-Figure4-1.png",
      "1708.00160v2-Table1-1.png",
      "1708.00160v2-Figure1-1.png",
      "1708.00160v2-Table2-1.png",
      "1708.00160v2-Table7-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 shows the FP-Tree with support values, and Text 1 explains the FP-Tree structure and support values. Together, they provide the necessary information to determine the support value of 'ana=NAM'.",
    "answer": "The support value of the node 'ana=NAM' is (2,1).",
    "text_chunks": [
      "As an example, assume D contains the following two samples: Based on these samples A=\\{ana-type=NAM, ant-type=NAM, head-match=F, head-match=T\\, support(a_i,0)_{a_i \\in A= \\{1,1,1,0\\, and support(a_i,1)_{a_i \\in A=\\{1,1,0,1\\. If we sort A based on a_i's frequencies (support(a_i,0)+support(a_i,1)), the ordering of A's items will remain the same. The FP-Tree construction steps for the above samples are demonstrated in Figure~. ana-type, ant-type, and head-match features are abbreviated as ana, ant, and head, respectively.",
      "Frequency of an item corresponds to \\sum_{c_i \\in C \\: support(a_i, c_i). Except for the root, which is a null node, each node n contains an item a_i \\in A. It also contains the support values of a_i in the subpath of the tree that starts from the root and ends with n, i.e.\\ support_n(a_i,c_j). The FP-Tree construction method is as follows: (a) scan D to collect the set of all items, i.e.\\ A. Compute support(a_i,c_j) for each item a_i \\in A and label c_j \\in C. Sort A's members in descending order according to their frequencies, i.e. \\sum_{c_i \\in C \\: support(a_i, c_i).",
      "These resulting feature-values are the union of informative feature-values for all types of anaphora. Based on the observation in Section~, we use the top-pairs model of deep-coref as the baseline to employ additional features, i.e. ``+EPM'' is the top-pairs model in which EPM feature-values are incorporated. Impact on In-domain Performance",
      "We run the JIM algorithm on the same data and with the same setup as that of EPM.\\footnote{ We set the minimum frequency, maximum pattern length and score^+ threshold parameters of JIM to 20, 5 and 0.6. This results in nine pairwise features, 260 POS tags, 38 dependency relations, 32 mention types, and 18 named entity tags. The ``+JIM'' row shows the results of deep-coref top-pairs model in which these feature-values are incorporated.",
      "\\item A=\\{a_1,\\dots,a_l\\: set of all feature-values present in D. Each a_i \\in A is called an item, e.g.\\ a_i=``anaphor type=proper''. \\item p: pattern p=\\{a_{i_1,\\dots, a_{i_k\\ is a set of one or more items, e.g.\\ p=\\{``anaphor type=proper'', ``antecedent type=proper''\\. \\item support(p,c_i): the number of samples that contain pattern p and are labeled with c_i. \\squishend Data Structure For representing the input samples, we use the Frequent Pattern Tree (FP-Tree) structure that is the data structure of the FP-Growth algorithm , i.e.\\ one of the most common algorithms for frequent pattern mining.",
      "We add these feature-values as binary features. By setting \\Theta_l to five, EPM results in 13 pairwise feature-values, 112 POS tags, i.e.\\ 53 POS for anaphors and 59 for antecedents, 25 dependency relations, 26 mention types (mention types or fine mention types), and finally, 14 named entity tags. and \\newcite{moosavi17a, we mine a separate set of patterns for each type of anaphor.",
      "We use \\Theta_l=3 for both MPP and EPM. We perform 5-times repeated 5-fold cross validation and the results are averaged. In each validation, all experiments are performed on the same split.",
      "\\end{table* \\paragraph{Feature Ablation Table~ shows the effect of each group of EPM feature-values, i.e.\\ pairwise features, mention types, dependency relations, named entity tags and POS tags, on the performance of ``+EPM''. The performance of ``+EPM'' from which each of the above feature groups is removed, one feature group at a time, is represented as ``-pairwise'', ``-types'', ``-dep'', ``-NER'', and ``-POS'', respectively. The POS and named entity tags have the least and the pairwise features have the most significant effect."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 shows the FP-Tree with support values, and Text 1 explains the FP-Tree structure and support values. Together, they provide the necessary information to determine the support value of 'ana=NAM'.\",\n    \"Answer\": \"The support value of the node 'ana=NAM' is (2,1).\"\n}"
  },
  "2": {
    "question": "Which coreference model performs best on the CoNLL test set according to the F$_1$ score? Is this performance statistically significant compared to all other models in the table?",
    "ground_truth": "The \"ensemble\" model of e2ef achieves the highest F$_1$ score of 68.83 on the CoNLL test set. Yes, this performance is statistically significant compared to all other models listed in the table, as indicated by the caption and footnote referencing the approximate randomization test.",
    "selected_figures_names": [
      "1708.00160v2-Table3-1.png",
      "1708.00160v2-Table5-1.png",
      "1708.00160v2-Figure2-1.png",
      "1708.00160v2-Figure4-1.png"
    ],
    "all_figures_names": [
      "1708.00160v2-Table3-1.png",
      "1708.00160v2-Table5-1.png",
      "1708.00160v2-Figure2-1.png",
      "1708.00160v2-Figure4-1.png",
      "1708.00160v2-Figure1-1.png",
      "1708.00160v2-Table4-1.png",
      "1708.00160v2-Figure3-1.png",
      "1708.00160v2-Table2-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 6,
    "rationale": "Image 1 provides a table with F1 scores for different coreference models on the CoNLL test set, which directly addresses the question. Text 6 discusses the performance of the EPM model compared to other state-of-the-art models on the CoNLL test set, providing context for the scores.",
    "answer": "The +EPM model performs best on the CoNLL test set with an F1 score of 66.95. The performance is not statistically significant compared to all other models, as the differences in scores are relatively small.",
    "text_chunks": [
      "However, the superiority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate that it also performs better on new datasets. For instance, the ranking model of \\newcite{clarkkevin16a, the reinforcement learning model of \\newcite{clarkkevin16b and the end-to-end model of \\newcite{leekenton17 are three recent coreference resolvers, among which the model of \\newcite{leekenton17 performs the best and that of \\newcite{clarkkevin16b performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset , which is consistent with CoNLL with regard to coreference definition and annotation scheme, the performance ranking would be in a reverse order is used here..",
      "In \\newcite{moosavi17b, we investigate the generalization problem in coreference resolution and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily indicate a better coreference model. They may be due to better memorization of the training data.",
      "Baseline Coreference Resolver deep-coref and e2e-coref are among the best performing coreference resolvers from which e2e-coref performs better on the CoNLL test set. deep-coref is a pipelined system, i.e.\\ a mention detection first determines the list of candidate mentions with their corresponding features. It contains various coreference models including the mention-pair, mention-ranking, and entity-based models.",
      "We use MUC , B^3 , CEAF_e , LEA , and the CoNLL score , i.e.\\ the average F_1 value of MUC, B^3, and CEAF_e, for evaluations. The results of employing those features in deep-coref's ``ranking'' and ``top-pairs'' models on the CoNLL development set are reported in Table~. The rows ``ranking'' and ``top-pairs'' show the base results of deep-coref's ``ranking'' and ``top-pairs'' models, respectively. ``+linguistic'' rows represents the results for each of the mention-ranking models in which the feature set of Section~ is employed.",
      "As we can see, ``+EPM'' generalizes best, and in out-of-domain evaluations, it considerably outperforms the ensemble model of e2e-coref, which has the best performance on the CoNLL test set. Conclusions In this paper, we show that employing linguistic features in a neural coreference resolver significantly improves generalization. However, the incorporated features should be informative enough to be taken into account in the presence of lexical features, which are very strong features in the CoNLL dataset.",
      "EPM is notably faster in comparison to the other two approaches. It is notable that the examined datasets are considerably smaller than the coreference data, which includes more than 33 million samples and 200 frequent feature-values. \\caption{\\footnotesize Comparisons on the CoNLL test set.",
      "The performance of the ``+EPM'' model compared to recent state-of-the-art coreference models on the CoNLL test set is presented in Table~. The ``single'' and ``ensemble'' rows represent the results of the single and ensemble models of e2e-coref. We also compare EPM with the pattern mining approach used by \\newcite{uryupina15, i.e.\\ Jaccard Item Mining (JIM). For a fair comparison, while \\newcite{uryupina15 used mined patterns for extracting feature templates, we use them for selecting feature-values.",
      "We also observe that if we limit e2e-coref candidate spans to those that correspond to deep-coref's detected mentions, the performance of e2e-coref drops to a level on-par with deep-coref\\footnote{ The CoNLL score of the e2e-coref single model on the CoNLL development set drops from 67.36 to 65.81, while that of the deep-coref ``ranking'' model is 66.09.. Examined Features The examined linguistic features include string match, syntactic, shallow semantic and discourse features. Mention-based features include: \\squishlist \\item Mention type: proper, nominal or pronominal \\item Fine mention type: proper, definite or indefinite nominal, or the citation form of pronouns \\item Gender: female, male, neutral, unknown \\item Number: singular, plural, unknown \\item Animacy: animate, inanimate, unknown \\item Named entity type: person, location, organization, date, time, number, etc. \\item Dependency relation: enhanced dependency relation of the head word to its parent \\item POS tags of the first, last, head, two words preceding and following of each mention \\squishend Pairwise features include: \\squishlist \\item Head match: both mentions have the same head, e.g.\\ ``red hat'' and ``the hat'' \\item String of one mention is contained in the other, e.g.\\ ``Mary's hat'' and ``Mary'' \\item Head of one mention is contained in the other, e.g.\\ ``Mary's hat'' and ``hat'' \\item Acronym, e.g.\\ ``Heidelberg Institute for Theoretical Studies'' and ``HITS'' \\item Compatible pre-modifiers: the set of pre-modifiers of one mention is contained in that of the other, e.g.\\ ``the red hat that she is wearing'' and ``the red hat'' \\item Compatible\\ gender,\\ e.g.\\ ``Mary'' and ``women'' \\item Compatible number, e.g.\\ ``Mary'' and ``John'' \\item Compatible animacy, e.g.\\ ``those hats'' and ``it'' \\item Compatible attributes: compatible gender, number and animacy, e.g.\\ ``Mary'' and ``she'' \\item Closest antecedent that has the same head and compatible premodifiers, e.g.\\ ``this new book'' and ``This book'' in ``Take a look at this new book."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 6,\n    \"Rationale\": \"Image 1 provides a table with F1 scores for different coreference models on the CoNLL test set, which directly addresses the question. Text 6 discusses the performance of the EPM model compared to other state-of-the-art models on the CoNLL test set, providing context for the scores.\",\n    \"Answer\": \"The +EPM model performs best on the CoNLL test set with an F1 score of 66.95. The performance is not statistically significant compared to all other models, as the differences in scores are relatively small.\"\n}"
  },
  "3": {
    "question": "Which deep-coref model performed best on the WikiCoref dataset, according to the table?",
    "ground_truth": "The CoNLL model performed best on the WikiCoref dataset, with an F1 score of 53.40 when using the +linguistic evaluation metric.",
    "selected_figures_names": [
      "1708.00160v2-Table2-1.png"
    ],
    "all_figures_names": [
      "1708.00160v2-Figure2-1.png",
      "1708.00160v2-Table3-1.png",
      "1708.00160v2-Table7-1.png",
      "1708.00160v2-Figure3-1.png",
      "1708.00160v2-Table4-1.png",
      "1708.00160v2-Figure1-1.png",
      "1708.00160v2-Table2-1.png",
      "1708.00160v2-Table5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 7,
    "rationale": "Image 0 provides the performance metrics of different deep-coref models on the WikiCoref dataset, which directly answers the question. Text 7 discusses the evaluation of these models, providing context for the results shown in the image.",
    "answer": "+linguistic model performed best on the WikiCoref dataset.",
    "text_chunks": [
      "Similar to \\newcite{moosavi17b, we use the pt and wb genres for the latter evaluation setup. The results of the first evaluation setup are shown in Table~. The best performance on WikiCoref is achieved by \\newcite{ghaddar16a (``G\\&L'' in Table~) who introduced WikiCoref and design a domain-specific coreference resolver that makes use of the Wikipedia markups of a document as well as links to Freebase, which are annotated in WikiCoref. Incorporating EPM feature-values improves the performance by about three points.",
      "We show that while the informativeness of EPM mined patterns is on-par with those of its counterparts, it scales best to large datasets. \\item By improving generalization, we achieve state-of-the-art performance on all examined out-of-domain evaluations. Our out-of-domain performance on WikiCoref is on-par with that of \\newcite{ghaddar16b's coreference resolver, which is a system specifically designed for WikiCoref and uses its domain knowledge.",
      "Baseline Coreference Resolver deep-coref and e2e-coref are among the best performing coreference resolvers from which e2e-coref performs better on the CoNLL test set. deep-coref is a pipelined system, i.e.\\ a mention detection first determines the list of candidate mentions with their corresponding features. It contains various coreference models including the mention-pair, mention-ranking, and entity-based models.",
      "We use MUC , B^3 , CEAF_e , LEA , and the CoNLL score , i.e.\\ the average F_1 value of MUC, B^3, and CEAF_e, for evaluations. The results of employing those features in deep-coref's ``ranking'' and ``top-pairs'' models on the CoNLL development set are reported in Table~. The rows ``ranking'' and ``top-pairs'' show the base results of deep-coref's ``ranking'' and ``top-pairs'' models, respectively. ``+linguistic'' rows represents the results for each of the mention-ranking models in which the feature set of Section~ is employed.",
      "However, the superiority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate that it also performs better on new datasets. For instance, the ranking model of \\newcite{clarkkevin16a, the reinforcement learning model of \\newcite{clarkkevin16b and the end-to-end model of \\newcite{leekenton17 are three recent coreference resolvers, among which the model of \\newcite{leekenton17 performs the best and that of \\newcite{clarkkevin16b performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset , which is consistent with CoNLL with regard to coreference definition and annotation scheme, the performance ranking would be in a reverse order is used here..",
      "As we see, EPM feature-values result in significantly better performance than those of JIM while the number of EPM feature-values is considerably less than JIM. \\caption{Out-of-domain evaluation on the WikiCoref dataset. The highest F_1 scores are boldfaced.",
      "The mention-ranking model of deep-coref has three variations: (1) ``ranking'' uses the slack-rescaled max-margin training objective of \\newcite{wiseman15, (2) ``reinforce'' is a variation of the ``ranking'' model in which the hyper-parameters are set in a reinforcement learning framework , and (3) ``top-pairs'' is a simple variation of the ``ranking'' model that uses a probabilistic objective function and is used for pretraining the ``ranking'' model. e2e-coref is an end-to-end system that jointly models mention detection and coreference resolution. It considers all possible (start, end) word spans of each sentence as candidate mentions.",
      "However, it does not improve significantly over ``ranking''. Henceforth, we use the ``top-pairs'' model of deep-coref as the baseline model to incorporate linguistic features. To assess the impact on generalization, we evaluate ``top-pairs'' and ``+linguistic'' models that are trained on CoNLL, on WikiCoref (see Table~)."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 7,\n    \"Rationale\": \"Image 0 provides the performance metrics of different deep-coref models on the WikiCoref dataset, which directly answers the question. Text 7 discusses the evaluation of these models, providing context for the results shown in the image.\",\n    \"Answer\": \"+linguistic model performed best on the WikiCoref dataset.\"\n}"
  }
}