{
  "0": {
    "question": "What is the role of the code layer in the HDMF architecture?",
    "ground_truth": "The code layer is responsible for generating a compressed representation of the input data. This compressed representation is then used by the decoder to reconstruct the original data.",
    "selected_figures_names": [
      "1708.03797v1-Figure1-1.png"
    ],
    "all_figures_names": [
      "1708.03797v1-Figure1-1.png",
      "1708.03797v1-Table1-1.png",
      "1708.03797v1-Table2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 provides a visual overview of the HDMF architecture, highlighting the code layer's position. Text 2 explains the role of the code layer in generating deep-semantic user and item matrices.",
    "answer": "The code layer in the HDMF architecture generates deep-semantic user and item matrices, which are crucial for connecting users and items to discover underlying preferences and optimizing the model.",
    "text_chunks": [
      "Figure~ shows an overview of the HDMF model. Generally, HDMF takes the tag-based user and item matrices X and Y (defined in Section~) as inputs of two deep autoencoders, consisting of encoders and decoders. These inputs are then passed through multiple hidden layers and projected to the deep-semantic user and item matrices \\widetilde X and \\widetilde Y at the code layers, and to the reconstructed user and item matrices X' and Y' at the output layers.",
      "We thus call this model hybrid deep-semantic matrix factorization (HDMF). The intuition behind it % is as follows: (i) the reconstruction-error-based signal can learn better representations for both users and items; (ii) the collaborative learning signal from deep-semantic matrix factorization can connect users and items to discover the underlying users' preferences; (iii) furthermore, the reconstruction-error-based signal can complement deep-semantic matrix factorization to provide sufficient gradients for better optimizing the model and escaping the local minima. Specifically, as shown in Figure~, we adopt autoencoders with tied weights in HDMF, i.e., the weight matrices in the decoder are the transposes of weight matrices in the encoder.",
      "Generally, HDMF uses a tag-based user matrix and a tag-based item matrix as respective inputs of two deep autoencoders to generate deep-semantic user and item matrices at the code layers, and also reconstructed user and item matrices at the output layers. The deep model is then trained by using a hybrid learning signal to minimize both reconstruction errors and deep-semantic matrix factorization errors, i.e., the squared differences between the user-item rating matrix (seeing tags as positive ratings) and the dot product of deep-semantic user and item matrices (seeing deep-semantic matrices as the decomposed matrices in matrix factorization). The intuitions of using the hybrid learning signal are: (i) minimizing reconstruction errors can learn better representations for both users and items; (ii) deep-semantic matrix factorization offers a learning signal that connects users and items to discover the underlying users' preferences; (iii) two signals can complement each other to provide sufficient gradients for better model optimization and escaping the local minima.",
      "To ensure a fair comparison, the experiments are performed on the same real-world social-tagging dataset as used in~, which is gathered from the Delicious bookmarking system and released in HetRec 2011~. After using the same pre-processing to remove the infrequent tags that are used less than 15 times, the resulting dataset is as shown in Table~. We randomly select 80\\% of assignments as training set, 5\\% as validation set, and 15\\% as test set. All models are implemented using Python and Theano and run on a GPU server with an NVIDIA Tesla K40 GPU and 12GB GPU memory. The parameters of HDMF are selected by grid search and the values are set as follows: (i) \\# of hidden layers is 5; (ii)~\\# of neurons from 1st to 5th hidden layer are 2\\,000, 300, 128, 300, and 2\\,000, respectively; (iii) the parameters \\lambda_\\theta and \\lambda_e are set to 0.01 and 0.2; (iv)~the learning rate for model training is 0.002.",
      "By integrating the reconstruction errors with the deep-semantic matrix factorizations errors (as defined in Equation~), the HDMF model is thus trained by minimizing the following hybrid learning signal: \\vspace{-2em \\end{small \\vspace{-1em \\noindent Experiments We have conducted extensive experimental studies and compar\\-ed our proposed hybrid deep-semantic matrix factorization (HD\\-MF) model with a number of state-of-the-art baselines, which are grouped into two categories and summarized as follows: \\noindentContent-based tag-aware models. Four state-of-the-art models that utilize social tags as the content information to conduct tag-aware personalized recommendation are selected as the baselines.",
      "In this paper, we have briefly analyzed the state-of-the-art tag-aware personalized recommendation models that use content-based filtering or matrix factorization, and identified their existing problems. We thus have proposed a hybrid deep-semantic matrix factorization (HDMF) model to tackle these problems and to further enhance the performance of tag-aware personalized recommendation. We have also conducted extensive experimental studies and compared HDMF with seven state-of-the-art baselines; the results show that, by integrating the techniques of deep-semantic modeling, hybrid learning, and matrix factorization, HDMF greatly outperforms the state-of-the-art baselines in tag-aware personalized recommendation, in terms of all evaluation metrics.",
      "(iii)~Differently from AMF models, the additional tag information in HD\\-MF is directly used to model the decomposed user and item matrices in matrix factorization; this thus maximizes the effect of the additional tag information on the model optimization. (iv) HDMF remedies the non-interpretability problem in matrix factorization: considering deep-semantic matrices as the decomposed matrices and finding the most influential input tags for each dimension, the decomposed user and item matrices in HDMF become interpretable. The main contributions of this paper are briefly as follows: \\eat{ \\vspace*{-1.5ex Related Work Many systems have been proposed for tag-aware personalized recommendation on the Social Web.",
      "MF_{sf and MF_{ct ``slightly'' outperform MF, respectively. More importantly, we note that our proposed model, HDMF, significantly outperforms all seven baselines in all metrics; e.g., the MRR (resp., MAP) of HDMF are 1.52 (resp., 1.66) times as high as that of the best baseline, DSPR (resp., MF_{sf), while the relative performances in \\!P@k, \\!R@k,~and \\!F@k are also similar. This finding strongly proves that by integrating the techniques of deep-semantic modeling, hybrid learning, and matrix factorization, HDMF overcomes the existing problems (as presented in Section~) of the state-of-the-art recommendation models and achieves very superior performance in tag-aware personalized recommendation."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 provides a visual overview of the HDMF architecture, highlighting the code layer's position. Text 2 explains the role of the code layer in generating deep-semantic user and item matrices.\",\n    \"Answer\": \"The code layer in the HDMF architecture generates deep-semantic user and item matrices, which are crucial for connecting users and items to discover underlying preferences and optimizing the model.\"\n}"
  },
  "1": {
    "question": "Which model achieved the best overall performance in terms of ranking relevant tags for users?",
    "ground_truth": "HDMF achieved the best overall performance.",
    "selected_figures_names": [
      "1708.03797v1-Table2-1.png"
    ],
    "all_figures_names": [
      "1708.03797v1-Figure1-1.png",
      "1708.03797v1-Table1-1.png",
      "1708.03797v1-Table2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 7,
    "rationale": "Image 0 provides a detailed comparison of various models' performance metrics, including MAP and MRR, which are crucial for evaluating ranking performance. Text 7 highlights that HDMF significantly outperforms all baselines in all metrics, confirming its superior performance.",
    "answer": "The HDMF model achieved the best overall performance in terms of ranking relevant tags for users.",
    "text_chunks": [
      "Content-based systems~ aim at recommending items that are similar to those that a user liked previously, where the similarity is usually measured by the cosine similarity between user and item profiles in the tag space. Collaborative systems recommend users with items liked by similar users using machine learning techniques, such as nearest neighbor modeling~ and matrix factorization~. Due to uncontrolled vocabularies, social tags are usually redundant, sparse, and ambiguous.",
      "The common solution is to apply machine learning techniques, e.g., clustering~ or {autoencoders~, to learn more abstract and representative features from raw tags. Recently, Xu et al.~ propose a deep-semantic model called DSPR which utilizes deep neural networks to model abstract and recommendation-oriented representations for social tags. DSPR is reported to achieve better performance than the clustering and autoencoder solutions.",
      "Similarly, a tag-based item profile is a vector y=[g^d_{j]^{|T|_{j=1, where g^d_{j=|\\{(u,t_j,d)\\,{\\in\\, A\\mid u \\,{\\in\\, U\\| is the number of times that item d is annotated with tag~t_j; while the tag-based item matrix is defined as Y=[y_j]^{|D|_{j=1, where y_j is the profile vector of the jth item, and |D| is the total number of items. The user-item rating matrix is R=[r_{i,j]^{|U|,|D|_{i=1,j=1, where r_{i,j is the number of tags annotated by user i to item j. Given R, traditional matrix-facto\\-rization-based recommender systems aim to approximate R using the decomposed latent matrices of users and items, i.e., X^l and Y^l, respectively, which are optimized by minimizing the squared differences between R and {X^l^T \\cdot Y^l on a set of observed ratings; formally, \\vspace{-1.2em \\end{small \\vspace{-1em",
      "(iii)~Differently from AMF models, the additional tag information in HD\\-MF is directly used to model the decomposed user and item matrices in matrix factorization; this thus maximizes the effect of the additional tag information on the model optimization. (iv) HDMF remedies the non-interpretability problem in matrix factorization: considering deep-semantic matrices as the decomposed matrices and finding the most influential input tags for each dimension, the decomposed user and item matrices in HDMF become interpretable. The main contributions of this paper are briefly as follows: \\eat{ \\vspace*{-1.5ex Related Work Many systems have been proposed for tag-aware personalized recommendation on the Social Web.",
      "In this paper, we have briefly analyzed the state-of-the-art tag-aware personalized recommendation models that use content-based filtering or matrix factorization, and identified their existing problems. We thus have proposed a hybrid deep-semantic matrix factorization (HDMF) model to tackle these problems and to further enhance the performance of tag-aware personalized recommendation. We have also conducted extensive experimental studies and compared HDMF with seven state-of-the-art baselines; the results show that, by integrating the techniques of deep-semantic modeling, hybrid learning, and matrix factorization, HDMF greatly outperforms the state-of-the-art baselines in tag-aware personalized recommendation, in terms of all evaluation metrics.",
      "To ensure a fair comparison, the experiments are performed on the same real-world social-tagging dataset as used in~, which is gathered from the Delicious bookmarking system and released in HetRec 2011~. After using the same pre-processing to remove the infrequent tags that are used less than 15 times, the resulting dataset is as shown in Table~. We randomly select 80\\% of assignments as training set, 5\\% as validation set, and 15\\% as test set. All models are implemented using Python and Theano and run on a GPU server with an NVIDIA Tesla K40 GPU and 12GB GPU memory. The parameters of HDMF are selected by grid search and the values are set as follows: (i) \\# of hidden layers is 5; (ii)~\\# of neurons from 1st to 5th hidden layer are 2\\,000, 300, 128, 300, and 2\\,000, respectively; (iii) the parameters \\lambda_\\theta and \\lambda_e are set to 0.01 and 0.2; (iv)~the learning rate for model training is 0.002.",
      "By contrast, as shown in Table~, HDMF dramatically outperforms MF: the MAP and MRR of HDMF are about 70\\% and 130\\%, respectively, better than those of MF. This is mainly because that the additional social tag information in HDMF is utilized to model the deep-semantic user and item matrices, which are then used directly as the decomposed user and item matrices in matrix factorization; since the decomposed matrices have dominant contribution on the optimizing gradient, HDMF maximizes the effect of the additional social tag information on the model optimization, making it possible to achieve significant improvements. \\vspace*{-1.5ex Summary and Outlook",
      "MF_{sf and MF_{ct ``slightly'' outperform MF, respectively. More importantly, we note that our proposed model, HDMF, significantly outperforms all seven baselines in all metrics; e.g., the MRR (resp., MAP) of HDMF are 1.52 (resp., 1.66) times as high as that of the best baseline, DSPR (resp., MF_{sf), while the relative performances in \\!P@k, \\!R@k,~and \\!F@k are also similar. This finding strongly proves that by integrating the techniques of deep-semantic modeling, hybrid learning, and matrix factorization, HDMF overcomes the existing problems (as presented in Section~) of the state-of-the-art recommendation models and achieves very superior performance in tag-aware personalized recommendation."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 7,\n    \"Rationale\": \"Image 0 provides a detailed comparison of various models' performance metrics, including MAP and MRR, which are crucial for evaluating ranking performance. Text 7 highlights that HDMF significantly outperforms all baselines in all metrics, confirming its superior performance.\",\n    \"Answer\": \"The HDMF model achieved the best overall performance in terms of ranking relevant tags for users.\"\n}"
  }
}