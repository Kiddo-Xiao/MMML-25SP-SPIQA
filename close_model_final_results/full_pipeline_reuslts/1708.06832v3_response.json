{
  "0": {
    "question": "Which method achieves the lowest error rate on ILSVRC at 1/4 of the total cost?",
    "ground_truth": "MSDNNet38",
    "selected_figures_names": [
      "1708.06832v3-Figure3-1.png",
      "1708.06832v3-Figure1-1.png",
      "1708.06832v3-Figure4-1.png",
      "1708.06832v3-Figure5-1.png"
    ],
    "all_figures_names": [
      "1708.06832v3-Figure3-1.png",
      "1708.06832v3-Figure1-1.png",
      "1708.06832v3-Figure4-1.png",
      "1708.06832v3-Figure5-1.png",
      "1708.06832v3-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 4,
    "rationale": "Image 0 provides a table with error rates on ILSVRC at different fractions of the total cost, which directly answers the question. Text 4 discusses comparisons of AdaLoss and CONST on ILSVRC, which is relevant to understanding the context of the error rates.",
    "answer": "MSDNet38+AdaLoss achieves the lowest error rate on ILSVRC at 1/4 of the total cost.",
    "text_chunks": [
      "We also conduct similar comparisons on ILSVRC using Res\\anns, and MSDNets, as shown in Fig.~ and Fig.~, and observe that the smaller networks with \\adaloss can achieve accuracy levels faster than the large ones with \\const, without sacrificing much final accuracy. For instance, MSDNet~\\citep{msdense is the state-of-the-art anytime predictor and is specially designed for anytime predictions, but by simply switching from their \\const scheme to \\adaloss, we significantly improve MSDNet32, which costs about 4.0e9 FLOPS (details in the appendix), to be about as accurate as the published result of MSDNet38, which has 6.6e9 total FLOPS in convolutions, and 72e6 parameters. Various base networks on ILSVRC.",
      "On CIFAR and SVHN, we divide the learning rate by 10 at 1/2 and 3/4 of the total epochs. We train for 300 epochs on CIFAR and 60 epochs on SVHN. On ILSVRC, we train for 90 epochs, and divide the learning rate by 10 at epoch 30 and 60.",
      "This is also equivalent to having a distribution of test-time interruption, where the interruption happens at all layers equally likely, except on the final layer. In our experiments, we do not use extra final weights on CIFAR10, CIFAR100 and SVHN to keep the weights simple, and we double the final weight on ILSVRC because the final accuracy there is critical for comparing against other non-anytime networks. Implementation Details of \\anns CIFAR and SVHN Res\\anns.",
      "MSDNet38 is described in the appendix of~\\citep{msdense. We set the four blocks to have 10, 9, 10 and 9 layers, and drop the feature maps of the finest resolution after each block as suggest in the original paper. We successfully reproduced the published results to 24.3\\% error rate on ILSVRC using our Tensorflow implementation.",
      "In contrast, \\adaloss enjoys small performance gaps in the later half of layers. On ILSVRC, we compare \\adaloss against \\const on Res\\annnp50, Dense\\annnp169, and MSDNet38, which have similar final errors and total computational costs (See Fig.~). In Table~, we observe the trade-offs between early and late accuracy on Res\\annnp50 and MSDNet38.",
      "As training an OPT for each depth is too expensive, we instead report the average relative comparison at 1/4, 1/2, 3/4, and 1 of the total \\ann costs. We observe that the \\const scheme makes 15\\sim 18\\% more errors than the OPT, and the relative gap widens at later layers. The \\linear scheme also has about 13\\% relative gap in later layers.",
      "For each (n,c) pair such that n < 25, we set the anytime prediction period s to be 1, i.e., every residual block leads to an auxiliary prediction. We set the prediction period s=3 for n=25. Res\\anns on ILSVRC.",
      "We report the top-1 error rates on the validation set using a single-crop of size 224x224, after scaling the smaller side of the image to 256, following~\\citep{resnet.. Training details. We optimize the models using stochastic gradient descent, with initial learning rate of 0.1, momentum of 0.9 and a weight decay of 1e-4."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 4,\n    \"Rationale\": \"Image 0 provides a table with error rates on ILSVRC at different fractions of the total cost, which directly answers the question. Text 4 discusses comparisons of AdaLoss and CONST on ILSVRC, which is relevant to understanding the context of the error rates.\",\n    \"Answer\": \"MSDNet38+AdaLoss achieves the lowest error rate on ILSVRC at 1/4 of the total cost.\"\n}"
  },
  "1": {
    "question": "What is the effect of increasing the budget in FLOPS on the test Top-1 error rate for the three different training strategies?",
    "ground_truth": "The test Top-1 error rate decreases as the budget in FLOPS increases for all three training strategies.",
    "selected_figures_names": [
      "1708.06832v3-Figure1-1.png"
    ],
    "all_figures_names": [
      "1708.06832v3-Figure3-1.png",
      "1708.06832v3-Figure1-1.png",
      "1708.06832v3-Figure4-1.png",
      "1708.06832v3-Figure5-1.png",
      "1708.06832v3-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 4,
    "rationale": "Image 0 provides a visual comparison of test Top-1 error rates against budget in FLOPS for different training strategies, which directly addresses the question. Text 4 discusses weight scheme comparisons and error rates, providing context for the strategies shown in the image.",
    "answer": "Increasing the budget in FLOPS generally decreases the test Top-1 error rate for all three training strategies. The strategy with adaptive loss weights (orange) achieves lower error rates compared to the large ANN with non-adaptive weights (green), and approaches the lowest error at each budget (blue).",
    "text_chunks": [
      "As training an OPT for each depth is too expensive, we instead report the average relative comparison at 1/4, 1/2, 3/4, and 1 of the total \\ann costs. We observe that the \\const scheme makes 15\\sim 18\\% more errors than the OPT, and the relative gap widens at later layers. The \\linear scheme also has about 13\\% relative gap in later layers.",
      "We report the top-1 error rates on the validation set using a single-crop of size 224x224, after scaling the smaller side of the image to 256, following~\\citep{resnet.. Training details. We optimize the models using stochastic gradient descent, with initial learning rate of 0.1, momentum of 0.9 and a weight decay of 1e-4.",
      "With large n, since almost all budgets are consumed by the last few networks, we know the overall expectation E_{B\\sim Uniform(0, L)[C] approaches 1.5 + \\frac{b-1{2 + \\frac{1{b-1, which is at least 1.5 + \\sqrt{2. Additional Details of \\adaloss for Experiments Prevent tiny weights. In practice, early \\hat{\\ell_i could be poor estimates of \\ell_i, and we may have a feed-back loop where large losses incur small weights, and in turn, results in poorly optimized large losses.",
      "As expected, the final loss is improved, but this is at the cost of significant increases of early training losses. In contrast, the adaptive weight scheme that we propose next (\\adaloss), achieves roughly even relative increases in training losses automatically, and is much better than the \\const scheme in the late layers. Adaptive Loss Balancing (\\adaloss).",
      "On CIFAR and SVHN, we also compare \\adaloss against LINEAR and OPT, defined in Sec.~. We evaluate the weights on multiple models including ResNet~\\citep{resnet and DenseNet~\\citep{densenet, and MSDNet~\\citep{msdense. For ResNet and DenseNet, we augment them with auxiliary predictors and losses, and call the resulting models Res\\ann and Dense\\ann, and defer the details of these models to the appendix Sec.~. Weight Scheme Comparisons \\adaloss vs. \\const on the same models. Table~ presents the average relative test error rate increase from OPT on 12 Res\\anns on CIFAR10, CIFAR100 and SVHN \\times \\{ 16, 32 \\ and \\{(9,64), (9,128)\\, where n represents the number of residual units in each of the three blocks of the network, and c is the filter size of the first convolution..",
      "Both Res\\annnp50 and Dense\\annnp169 see improvements switching from \\const to \\adaloss, which is also shown in Table~. Thanks to \\adaloss, Dense\\annnp169 achieves the same final error using similar FLOPS as the original published results of MSDNet38~\\citep{msdense. This suggests that~ improve over Dense\\anns by having better early predictions without sacrificing the final cost efficiency via impressive architecture insight. Our \\adaloss brings a complementary improvement to MSDNets, as it enables smaller MSDNets to reach the final error rates of bigger MSDNets, while having similar or better early predictions, as shown in the previous paragraph and Fig.~. EANN:",
      "To prevent such loops, we mix the adaptive weights with the constant weights. More precisely, we regularize Eq.~ with the arithmetic mean of the losses: where \\alpha >0 and \\gamma >0 are hyper parameters. In practice, since DNNs often have elaborate learning rate schedules that assume B_L=1, we choose \\alpha = \\min_i \\hat{\\ell_i(\\theta) at each iteration to scale the max weight to 1.",
      "An anytime predictor is attached to each residual block, and we estimate the optimal performance (OPT) in training cross entropy of predictor i by training a network that has weight only on \\ell_i to convergence. Then for each weight scheme we train an \\ann to measure the relative increase in training loss at each depth i from the OPT. In Fig.~, we observe that the intuitive \\const scheme has high relative losses in late layers."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 4,\n    \"Rationale\": \"Image 0 provides a visual comparison of test Top-1 error rates against budget in FLOPS for different training strategies, which directly addresses the question. Text 4 discusses weight scheme comparisons and error rates, providing context for the strategies shown in the image.\",\n    \"Answer\": \"Increasing the budget in FLOPS generally decreases the test Top-1 error rate for all three training strategies. The strategy with adaptive loss weights (orange) achieves lower error rates compared to the large ANN with non-adaptive weights (green), and approaches the lowest error at each budget (blue).\"\n}"
  },
  "2": {
    "question": "Which model performs the best on CIFAR100 and ILSVRC datasets?",
    "ground_truth": "EANN with AdaLoss performs the best on both CIFAR100 and ILSVRC datasets.",
    "selected_figures_names": [
      "1708.06832v3-Figure3-1.png",
      "1708.06832v3-Figure1-1.png",
      "1708.06832v3-Figure4-1.png",
      "1708.06832v3-Figure5-1.png"
    ],
    "all_figures_names": [
      "1708.06832v3-Figure3-1.png",
      "1708.06832v3-Figure1-1.png",
      "1708.06832v3-Figure4-1.png",
      "1708.06832v3-Figure5-1.png",
      "1708.06832v3-Figure2-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 5,
    "rationale": "Image 3 shows performance comparisons of EANNs and ANNs with AdaLoss and CONST on CIFAR100 and ILSVRC, highlighting the effectiveness of AdaLoss. Text 5 discusses how EANNs outperform linear ensembles on ILSVRC, providing context for the performance of different models.",
    "answer": "EANN with AdaLoss performs the best on both CIFAR100 and ILSVRC datasets.",
    "text_chunks": [
      "On CIFAR and SVHN, we divide the learning rate by 10 at 1/2 and 3/4 of the total epochs. We train for 300 epochs on CIFAR and 60 epochs on SVHN. On ILSVRC, we train for 90 epochs, and divide the learning rate by 10 at epoch 30 and 60.",
      "This is also equivalent to having a distribution of test-time interruption, where the interruption happens at all layers equally likely, except on the final layer. In our experiments, we do not use extra final weights on CIFAR10, CIFAR100 and SVHN to keep the weights simple, and we double the final weight on ILSVRC because the final accuracy there is critical for comparing against other non-anytime networks. Implementation Details of \\anns CIFAR and SVHN Res\\anns.",
      "On CIFAR100, we average the relative comparison of six such pairs of Res\\anns \\times \\{16, 32\\, and \\const takes (n,c) from \\{13,17,25\\ \\times \\{16, 32\\. in Fig.~. E.g., the location (0.5, 200) in the plot means using half computation of the small \\ann, and having 200\\% extra errors than it. We observe small \\anns with \\adaloss to achieve the same accuracy levels faster than large ones with \\const, because \\const neglects the late predictions and large networks, and early predictions of large networks are not as accurate of those of a small ones. The same comparisons using Res\\anns result in similar results on CIFAR10 and SVHN (Fig.~ and~).",
      "SVHN contains around 600000 training and around 26032 testing 32x32 images of numeric digits from the Google Street Views. We adopt the same pad-and-crop augmentations of CIFAR for SVHN, and also add Gaussian blur. and ILSVRC~\\citep{ILSVRC15\\footnote{ ILSVRC2012~\\citep{ILSVRC15 is a visual recognition data-set containing around 1.2 million natural and 50000 validation images for 1000 classes.",
      "On CIFAR and SVHN, we also compare \\adaloss against LINEAR and OPT, defined in Sec.~. We evaluate the weights on multiple models including ResNet~\\citep{resnet and DenseNet~\\citep{densenet, and MSDNet~\\citep{msdense. For ResNet and DenseNet, we augment them with auxiliary predictors and losses, and call the resulting models Res\\ann and Dense\\ann, and defer the details of these models to the appendix Sec.~. Weight Scheme Comparisons \\adaloss vs. \\const on the same models. Table~ presents the average relative test error rate increase from OPT on 12 Res\\anns on CIFAR10, CIFAR100 and SVHN \\times \\{ 16, 32 \\ and \\{(9,64), (9,128)\\, where n represents the number of residual units in each of the three blocks of the network, and c is the filter size of the first convolution..",
      "However, in Fig.~, an EANN using Res\\anns of depths 26, 50 and 101 outperforms the linear ensembles of ResNets and DenseNets significantly on ILSVRC. In particular, this drastically reduces the gap between ensembles and the state-of-the-art anytime predictor MSDNet~\\citep{msdense. Comparing Res\\ann50 and the EANN, we note that the EANN achieves better early accuracy but delays final predictions.",
      "We observe that from the easiest data-set, SVHN, to the hardest, CIFAR100, the weights are more concentrated on the final layers. This suggests that \\adaloss can automatically decide that harder data-sets need more concentrated final weights to have near-optimal final performance, whereas on easy data-sets, more efforts are directed to early predictions. Hence, \\adaloss weights may provide information for practitioners to design and choose models based on data-sets.",
      "We add linear anytime predictions every 14 dense layers, starting from layer 12 (1-based indexing). The original DenseNet paper~\\citep{densenet mentioned that they use drop-out with keep rate 0.9 after each conv in CIFAR and SVHN, but we found drop-out to be detrimental to performance on ILSVRC. MSDNet on ILSVRC."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 5,\n    \"Rationale\": \"Image 3 shows performance comparisons of EANNs and ANNs with AdaLoss and CONST on CIFAR100 and ILSVRC, highlighting the effectiveness of AdaLoss. Text 5 discusses how EANNs outperform linear ensembles on ILSVRC, providing context for the performance of different models.\",\n    \"Answer\": \"EANN with AdaLoss performs the best on both CIFAR100 and ILSVRC datasets.\"\n}"
  }
}