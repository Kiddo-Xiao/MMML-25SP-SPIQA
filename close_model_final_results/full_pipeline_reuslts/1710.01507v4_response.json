{
  "0": {
    "question": "What is the role of the LSTM network in the model architecture?",
    "ground_truth": "The LSTM network is used to process the post text and generate a post text embedding.",
    "selected_figures_names": [
      "1710.01507v4-Figure1-1.png"
    ],
    "all_figures_names": [
      "1710.01507v4-Figure1-1.png",
      "1710.01507v4-Table1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 4,
    "rationale": "Image 0 provides a visual representation of the model architecture, showing how different components, including LSTM, are integrated. Text 4 explains the use of BiLSTM for modeling sequential data and dependencies, which is crucial for understanding the role of LSTM in the architecture.",
    "answer": "The LSTM network in the model architecture is used to model sequential data and its dependencies. It processes textual features, such as article titles, by converting them into embeddings and capturing the sequential information through its bidirectional structure. This helps in tasks like fake news detection and sentiment analysis by understanding the context and importance of words in a sequence.",
    "text_chunks": [
      "\\smallskip We now go into detail about the components of the model, individual and combined, and how the parameters are learned. \\smallskip Bidirectional LSTM with Attention Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers. A standard RNN has an internal state whose output at every time-step which can be expressed in terms of that of previous time-steps.",
      "These components are (1) BiLSTM with attention, (2) Siamese Network on Text Embeddings, and (3) Siamese Network on Visual Embeddings. An overview of the architecture can be seen in Figure 1. We start with an explanation of the features used in the first component of the model.",
      "However, it has been seen that standard RNNs suffer from a problem of vanishing gradients . This means it will not be able to efficiently model dependencies and interactions between words that are a few steps apart. LSTMs are able to tackle this issue by their use of gating mechanisms.",
      "The first leverages neural networks for sequential modeling of text. Article title is represented as a sequence of word vectors and each word of the title is further converted into character level embeddings. These features serve as input to a bidirectional LSTM model.",
      "They used distributional semantics to represent article titles, and BiLSTM to model sequential data and its dependencies. Since then, has also experimented with Twitter data deploying a BiLSTM for each of the textual features (post-text, target-title, target-paragraphs, target-description, target-keywords, post-time) available in the corpus, and finally concatenating the dense output layers of the network before forwarding it to a fully connected layer. Since it was proposed in , the attention mechanism has been used for a variety of text-classification tasks, such as fake news detection and aspect-based sentiment analysis.",
      "used a self-attentive BiGRU to infer the importance of tweet tokens in predicting the annotation distribution of the task. One common point in all the approaches yet has been the use of only textual features available in the dataset. Our model not only incorporates textual features, modeled using BiLSTM and augmented with an attention mechanism, but also considers related images for the task.",
      "For each record in the dataset, the content of the post as well as the content of the related web page is available. We convert the words from the title of both attributes into the previously mentioned types of embeddings to act as input to our bidirectional LSTMs. (\\overrightarrow{h_1, \\overrightarrow{h_2, \\dots, \\overrightarrow{h_R) represent forward states of the LSTM and its state updates satisfy the following equations: here \\sigma is the logistic sigmoid function, \\overrightarrow{f_t, \\overrightarrow{i_t, \\overrightarrow{o_t represent the forget, input and output gates respectively.",
      "(\\overleftarrow{h_1, \\overleftarrow{h_2, \\dots, \\overleftarrow{h_R) denote the backward states and its updates can be computed similarly. The number of bidirectional LSTM units is set to a constant K, which is the maximum length of all title lengths of records used in training. The forward and backward states are then concatenated to obtain (h_1, h_2, \\dots, h_K), where \\end{equation Finally, we are left with the task of figuring out the significance of each word in the sequence i.e. how much a particular word influences the clickbait-y nature of the post."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 4,\n    \"Rationale\": \"Image 0 provides a visual representation of the model architecture, showing how different components, including LSTM, are integrated. Text 4 explains the use of BiLSTM for modeling sequential data and dependencies, which is crucial for understanding the role of LSTM in the architecture.\",\n    \"Answer\": \"The LSTM network in the model architecture is used to model sequential data and its dependencies. It processes textual features, such as article titles, by converting them into embeddings and capturing the sequential information through its bidirectional structure. This helps in tasks like fake news detection and sentiment analysis by understanding the context and importance of words in a sequence.\"\n}"
  }
}