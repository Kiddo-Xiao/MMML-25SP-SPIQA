{
  "0": {
    "question": "Which dataset has the most 4-hop triples?",
    "ground_truth": "Bing-News.",
    "selected_figures_names": [
      "1803.03467v4-Table1-1.png"
    ],
    "all_figures_names": [
      "1803.03467v4-Table1-1.png",
      "1803.03467v4-Table2-1.png",
      "1803.03467v4-Figure7-1.png",
      "1803.03467v4-Figure9-1.png",
      "1803.03467v4-Figure8-1.png",
      "1803.03467v4-Figure3-1.png",
      "1803.03467v4-Table3-1.png",
      "1803.03467v4-Figure4-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "The image provides the basic statistics of the datasets, including the number of 4-hop triples. The text chunk 0 mentions the datasets and their setup, which is relevant to understanding the context of the image.",
    "answer": "The Bing-News dataset has the most 4-hop triples.",
    "text_chunks": [
      "The basic statistics of the three datasets are presented in Table . Baselines We compare the proposed RippleNet with the following state-of-the-art baselines: Experiment Setup In RippleNet, we set the hop number H = 2 for MovieLens-1M/Book-Crossing and H = 3 for Bing-News. A larger number of hops hardly improves performance but does incur heavier computational overhead according to experiment results.",
      "But notice that the performance drops when the size is too large. In general, a size of 16 or 32 is enough for most datasets according to the experiment results. \\noindent Hop number.",
      "Several observations stand out: \\noindent Size of ripple set in each hop. We vary the size of a user's ripple set in each hop to further investigate the robustness of RippleNet. The results of AUC on the three datasets are presented in Table , from which we observe that with the increase of the size of ripple set, the performance of RippleNet is improved at first because a larger ripple set can encode more knowledge from the KG.",
      "Given the item embedding {\\bf v and the 1-hop ripple set \\mathcal S_u^1 of user u, each triple (h_i, r_i, t_i) in \\mathcal S_u^1 is assigned a relevance probability by comparing item v to the head h_i and the relation r_i in this triple: where {\\bf R_i \\in \\mathbb R^{d \\times d and {\\bf h_i \\in \\mathbb R^d are the embeddings of relation r_i and head h_i, respectively.",
      "We also vary the maximal hop number H to see how performance changes in RippleNet. The results are shown in Table , which shows that the best performance is achieved when H is 2 or 3. We attribute the phenomenon to the trade-off between the positive signals from long-distance dependency and negative signals from noises: too small of an H can hardly explore inter-entity relatedness and dependency of long distance, while too large of an H brings much more noises than useful signals, as stated in Section .",
      "Case Study To intuitively demonstrate the preference propagation in RippleNet, we randomly sample a user with 4 clicked pieces of news, and select one candidate news from his test set with label 1. For each of the user's k-hop relevant entities, we calculate the (unnormalized) relevance probability between the entity and the candidate news or its k-order responses. The results are presented in Figure , in which the darker shade of blue indicates larger values, and we omit names of relations for clearer presentation.",
      "A ripple set \\mathcal S_u^k is the set of knowledge triples that are k-hop(s) away from the seed set \\mathcal V_u. These ripple sets are used to interact with the item embedding (the yellow block) iteratively for obtaining the responses of user u with respect to item v (the green blocks), which are then combined to form the final user embedding (the grey block). Lastly, we use the embeddings of user u and item v together to compute the predicted probability \\hat y_{uv.",
      "In addition to the interaction matrix \\bf Y, we also have a knowledge graph \\mathcal G available, which consists of massive entity-relation-entity triples (h, r, t). Here h \\in \\mathcal E, r \\in \\mathcal R, and t \\in \\mathcal E denote the head, relation, and tail of a knowledge triple, respectively, \\mathcal E and \\mathcal R denote the set of entities and relations in the KG. For example, the triple (Jurassic Park, film.film.director, Steven Spielberg) states the fact that Steven Spielberg is the director of the film \"Jurassic Park\"."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"The image provides the basic statistics of the datasets, including the number of 4-hop triples. The text chunk 0 mentions the datasets and their setup, which is relevant to understanding the context of the image.\",\n    \"Answer\": \"The Bing-News dataset has the most 4-hop triples.\"\n}"
  },
  "1": {
    "question": "How does the number of common k-hop neighbors change as the hop distance increases for items with and without common raters?",
    "ground_truth": "The number of common k-hop neighbors generally decreases as the hop distance increases for both items with and without common raters. However, the number of common k-hop neighbors is consistently higher for items with common raters than for items without common raters.",
    "selected_figures_names": [
      "1803.03467v4-Figure8-1.png",
      "1803.03467v4-Table2-1.png",
      "1803.03467v4-Table3-1.png",
      "1803.03467v4-Figure3-1.png"
    ],
    "all_figures_names": [
      "1803.03467v4-Figure8-1.png",
      "1803.03467v4-Table2-1.png",
      "1803.03467v4-Table3-1.png",
      "1803.03467v4-Figure3-1.png",
      "1803.03467v4-Figure1-1.png",
      "1803.03467v4-Figure9-1.png",
      "1803.03467v4-Table4-1.png",
      "1803.03467v4-Figure4-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 visualizes the relevance probabilities and connections between entities, which is relevant to understanding how proximity structures change with hop distance. Text 1 discusses how the proximity structures of two items become more similar with increased hop distance, even if there is no direct similarity, which directly addresses the question.",
    "answer": "As the hop distance increases, the number of common k-hop neighbors for items with and without common raters becomes more similar. This is because items are likely to share more k-hop neighbors at larger hop distances, even if they don't have direct similarities.",
    "text_chunks": [
      "For each dataset, we first randomly sample one million item pairs, then count the average number of k-hop neighbors that the two items share in the KG under the following two circumstances: (1) the two items have at least one common rater in RS; (2) the two items have no common rater in RS. The results are presented in Figures , , , respectively, which clearly show that if two items have common rater(s) in RS, they likely share more common k-hop neighbors in the KG for fixed k. The above findings empirically demonstrate that the similarity of proximity structures of two items in the KG could assist in measuring their relatedness in RS.",
      "In addition, we plot the ratio of the two average numbers with different hops (i.e., dividing the higher bar by its immediate lower bar for each hop number) in Figure , from which we observe that the proximity structures of two items under the two circumstances become more similar with the increase of the hop number. This is because any two items are probable to share a large amount of k-hop neighbors in the KG for a large k, even if there is no direct similarity between them in reality. The result motivates us to find a moderate hop number in RippleNet to explore users' potential interests as far as possible while avoiding introducing too much noise.",
      "We also vary the maximal hop number H to see how performance changes in RippleNet. The results are shown in Table , which shows that the best performance is achieved when H is 2 or 3. We attribute the phenomenon to the trade-off between the positive signals from long-distance dependency and negative signals from noises: too small of an H can hardly explore inter-entity relatedness and dependency of long distance, while too large of an H brings much more noises than useful signals, as stated in Section .",
      "Given the definition of relevant entities, we then define the k-hop ripple set of user u as follows: \\end{definition The word \"ripple\" has two meanings: (1) Analogous to real ripples created by multiple raindrops, a user's potential interest in entities is activated by his historical preferences, then propagates along the links in KG layer by layer, from near to distant. We visualize the analogy by the concentric circles illustrated in Figure . (2) The strength of a user's potential preferences in ripple sets weakens with the increase of the hop number k, which is similar to the gradually attenuated amplitude of real ripples.",
      "Case Study To intuitively demonstrate the preference propagation in RippleNet, we randomly sample a user with 4 clicked pieces of news, and select one candidate news from his test set with label 1. For each of the user's k-hop relevant entities, we calculate the (unnormalized) relevance probability between the entity and the candidate news or its k-order responses. The results are presented in Figure , in which the darker shade of blue indicates larger values, and we omit names of relations for clearer presentation.",
      "The fading blue in Figure shows the decreasing relatedness between the center and surrounding entities. One concern about ripple sets is their sizes may get too large with the increase of hop number k. To address the concern, note that: (1) A large number of entities in a real KG are sink entities, meaning they only have incoming links but no outgoing links, such as \"2004\" and \"PG-13\" in Figure . (2) In specific recommendation scenarios such as movie or book recommendations, relations can be limited to scenario-related categories to reduce the size of ripple sets and improve relevance among entities.",
      "For example, in Figure , all relations are movie-related and contain the word \"film\" in their names. (3) The number of maximal hop H is usually not too large in practice, since entities that are too distant from a user's history may bring more noise than positive signals. We will discuss the choice of H in the experiments part.",
      "We use Accuracy and AUC to evaluate the performance of CTR prediction. (2) In top-K recommendation, we use the trained model to select K items with highest predicted click probability for each user in the test set, and choose Precision@K, Recall@K, F1@K to evaluate the recommended sets. Empirical Study We conduct an empirical study to investigate the correlation between the average number of common neighbors of an item pair in the KG and whether they have common rater(s) in RS."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 visualizes the relevance probabilities and connections between entities, which is relevant to understanding how proximity structures change with hop distance. Text 1 discusses how the proximity structures of two items become more similar with increased hop distance, even if there is no direct similarity, which directly addresses the question.\",\n    \"Answer\": \"As the hop distance increases, the number of common k-hop neighbors for items with and without common raters becomes more similar. This is because items are likely to share more k-hop neighbors at larger hop distances, even if they don't have direct similarities.\"\n}"
  },
  "2": {
    "question": "Which model performs the best in terms of AUC on the MovieLens-1M dataset?",
    "ground_truth": "RippleNet*",
    "selected_figures_names": [
      "1803.03467v4-Table3-1.png"
    ],
    "all_figures_names": [
      "1803.03467v4-Figure1-1.png",
      "1803.03467v4-Figure7-1.png",
      "1803.03467v4-Table3-1.png",
      "1803.03467v4-Figure3-1.png",
      "1803.03467v4-Figure6-1.png",
      "1803.03467v4-Table2-1.png",
      "1803.03467v4-Figure8-1.png",
      "1803.03467v4-Figure4-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 provides a direct comparison of AUC values for different models on the MovieLens-1M dataset. Text 2 discusses the performance gains of RippleNet, which is relevant to understanding its effectiveness.",
    "answer": "RippleNet performs the best in terms of AUC on the MovieLens-1M dataset with an AUC of 0.921.",
    "text_chunks": [
      "We vary d from 2 to 64 and \\lambda_2 from 0.0 to 1.0, respectively, while keeping other parameters fixed. The results of AUC on MovieLens-1M are presented in Figure . We observe from Figure that, with the increase of d, the performance is boosted at first since embeddings with a larger dimension can encode more useful information, but drops after d",
      "Results \\hfill \\hfill \\hfill \\caption{Precision@K, Recall@K, and F1@K in top-K recommendation for MovieLens-1M. \\end{figure* \\hfill \\hfill \\caption{Precision@K, Recall@K, and F1@K in top-K recommendation for Book-Crossing. \\end{figure*",
      "The experiment results show that RippleNet achieves AUC gains of 2.0\\% to 40.6\\%, 2.5\\% to 17.4\\%, and 2.6\\% to 22.4\\% in movie, book, and news recommendations, respectively, compared with state-of-the-art baselines for recommendation. We also find that RippleNet provides a new perspective of explainability for the recommended results in terms of the knowledge graph. In summary, our contributions in this paper are as follows:",
      "We will also give a case study of visualization and discuss the choice of hyper-parameters in this section. Datasets We utilize the following three datasets in our experiments for movie, book, and news recommendation: Since MovieLens-1M and Book-Crossing are explicit feedback data, we transform them into implicit feedback where each entry is marked with 1 indicating that the user has rated the item (the threshold of rating is 4 for MovieLens-1M, while no threshold is set for Book-Crossing due to its sparsity), and sample an unwatched set marked as 0 for each user, which is of equal size with the rated ones. For MovieLens-1M and Book-Crossing, we use the ID embeddings of users and items as raw input, while for Bing-News, we concatenate the ID embedding of a piece of news and the averaged word embedding of its title as raw input for the item, since news titles are typically much longer than names of movies or books, hence providing more useful information for recommendation.",
      "The basic statistics of the three datasets are presented in Table . Baselines We compare the proposed RippleNet with the following state-of-the-art baselines: Experiment Setup In RippleNet, we set the hop number H = 2 for MovieLens-1M/Book-Crossing and H = 3 for Bing-News. A larger number of hops hardly improves performance but does incur heavier computational overhead according to experiment results.",
      "Several observations stand out: \\noindent Size of ripple set in each hop. We vary the size of a user's ripple set in each hop to further investigate the robustness of RippleNet. The results of AUC on the three datasets are presented in Table , from which we observe that with the increase of the size of ripple set, the performance of RippleNet is improved at first because a larger ripple set can encode more knowledge from the KG.",
      "We use Microsoft Satori to construct the knowledge graph for each dataset. For MovieLens-1M and Book-Crossing, we first select a subset of triples from the whole KG whose relation name contains \"movie\" or \"book\" and the confidence level is greater than 0.9. Given the sub-KG, we collect IDs of all valid movies/books by matching their names with tail of triples (head, film.film.name, tail) or (head, book.book.title, tail).",
      "We conduct extensive experiments in three recommendation scenarios. The results demonstrate the significant superiority of RippleNet over strong baselines. For future work, we plan to (1) further investigate the methods of characterizing entity-relation interactions; (2) design non-uniform samplers during preference propagation to better explore users' potential interests and improve the performance."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 provides a direct comparison of AUC values for different models on the MovieLens-1M dataset. Text 2 discusses the performance gains of RippleNet, which is relevant to understanding its effectiveness.\",\n    \"Answer\": \"RippleNet performs the best in terms of AUC on the MovieLens-1M dataset with an AUC of 0.921.\"\n}"
  },
  "3": {
    "question": "What is the role of the ripple sets in the RippleNet framework?",
    "ground_truth": "The ripple sets are used to propagate a user's preferences from his or her click history to his or her relevant entities.",
    "selected_figures_names": [
      "1803.03467v4-Figure2-1.png"
    ],
    "all_figures_names": [
      "1803.03467v4-Figure6-1.png",
      "1803.03467v4-Figure2-1.png",
      "1803.03467v4-Table4-1.png",
      "1803.03467v4-Table5-1.png",
      "1803.03467v4-Table2-1.png",
      "1803.03467v4-Figure4-1.png",
      "1803.03467v4-Figure8-1.png",
      "1803.03467v4-Figure3-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 visually represents the RippleNet framework, showing how ripple sets are formed and used. Text 2 explains the role of ripple sets in interacting with item embeddings to form user embeddings, which are crucial for predicting user-item interactions.",
    "answer": "In the RippleNet framework, ripple sets are used to interact with item embeddings iteratively to obtain user responses with respect to an item. These interactions help form the final user embedding, which is then used to compute the predicted probability of user engagement with the item.",
    "text_chunks": [
      "We also give some discussions on the model and introduce the related work. Framework The framework of RippleNet is illustrated in Figure . RippleNet takes a user u and an item v as input, and outputs the predicted probability that user u will click item v. For the input user u, his historical set of interests \\mathcal V_u is treated as seeds in the KG, then extended along links to form multiple ripple sets \\mathcal S_u^k (k = 1, 2, ..., H).",
      "Several observations stand out: \\noindent Size of ripple set in each hop. We vary the size of a user's ripple set in each hop to further investigate the robustness of RippleNet. The results of AUC on the three datasets are presented in Table , from which we observe that with the increase of the size of ripple set, the performance of RippleNet is improved at first because a larger ripple set can encode more knowledge from the KG.",
      "A ripple set \\mathcal S_u^k is the set of knowledge triples that are k-hop(s) away from the seed set \\mathcal V_u. These ripple sets are used to interact with the item embedding (the yellow block) iteratively for obtaining the responses of user u with respect to item v (the green blocks), which are then combined to form the final user embedding (the grey block). Lastly, we use the embeddings of user u and item v together to compute the predicted probability \\hat y_{uv.",
      "The basic statistics of the three datasets are presented in Table . Baselines We compare the proposed RippleNet with the following state-of-the-art baselines: Experiment Setup In RippleNet, we set the hop number H = 2 for MovieLens-1M/Book-Crossing and H = 3 for Bing-News. A larger number of hops hardly improves performance but does incur heavier computational overhead according to experiment results.",
      "To address the limitations of existing methods, we propose RippleNet, an end-to-end framework for knowledge-graph-aware recommendation. RippleNet is designed for click-through rate (CTR) prediction, which takes a user-item pair as input and outputs the probability of the user engaging (e.g., clicking, browsing) the item. The key idea behind RippleNet is preference propagation: For each user, RippleNet treats his historical interests as a seed set in the KG, then extends the user's interests iteratively along KG links to discover his hierarchical potential interests with respect to a candidate item.",
      "Note that different from path-based methods where the patterns of path are manually designed, RippleNet automatically discovers the possible explanation paths according to relevance probability. We will further present a visualized example in the experiments section to intuitively demonstrate the explainability of RippleNet. \\subsubsection{Ripple Superposition A common phenomenon in RippleNet is that a user's ripple sets may be large in size, which dilutes his potential interests inevitably in preference propagation.",
      "Conclusion and Future Work In this paper, we propose RippleNet, an end-to-end framework that naturally incorporates the knowledge graph into recommender systems. RippleNet overcomes the limitations of existing embedding-based and path-based KG-aware recommendation methods by introducing preference propagation, which automatically propagates users' potential preferences and explores their hierarchical interests in the KG. RippleNet unifies the preference propagation with regularization of KGE in a Bayesian framework for click-through rate prediction.",
      "From this point of view, RippleNet can be seen as a specially designed KGE method that serves recommendation directly. Experiments In this section, we evaluate RippleNet on three real-world scenarios: movie, book, and news recommendations .. We first introduce the datasets, baselines, and experiment setup, then present the experiment results."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 visually represents the RippleNet framework, showing how ripple sets are formed and used. Text 2 explains the role of ripple sets in interacting with item embeddings to form user embeddings, which are crucial for predicting user-item interactions.\",\n    \"Answer\": \"In the RippleNet framework, ripple sets are used to interact with item embeddings iteratively to obtain user responses with respect to an item. These interactions help form the final user embedding, which is then used to compute the predicted probability of user engagement with the item.\"\n}"
  },
  "4": {
    "question": "What is the relationship between the movies \"Forrest Gump\" and \"Cast Away\"?",
    "ground_truth": "The movies \"Forrest Gump\" and \"Cast Away\" are connected by the actor Tom Hanks.",
    "selected_figures_names": [
      "1803.03467v4-Figure3-1.png"
    ],
    "all_figures_names": [
      "1803.03467v4-Table2-1.png",
      "1803.03467v4-Figure2-1.png",
      "1803.03467v4-Figure3-1.png",
      "1803.03467v4-Figure8-1.png",
      "1803.03467v4-Figure7-1.png",
      "1803.03467v4-Figure4-1.png",
      "1803.03467v4-Table1-1.png",
      "1803.03467v4-Figure6-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 visually represents the knowledge graph connections between 'Forrest Gump' and 'Cast Away', showing their relationship through the actor 'Tom Hanks'. Text 0 provides a description of these connections, specifically mentioning 'Tom Hanks' as a common link between the two movies.",
    "answer": "The relationship between 'Forrest Gump' and 'Cast Away' is that both movies star Tom Hanks.",
    "text_chunks": [
      "Ripple Set A knowledge graph usually contains fruitful facts and connections among entities. For example, as illustrated in Figure , the film \"Forrest Gump\" is linked with \"Robert Zemeckis\" (director), \"Tom Hanks\" (star), \"U.S.\" (country) and \"Drama\" (genre), while \"Tom Hanks\" is further linked with films \"The Terminal\" and \"Cast Away\" which he starred in. These complicated connections in KG provide us a deep and latent perspective to explore user preferences.",
      "The relevance probability p_i can be regarded as the similarity of item {\\bf v and the entity {\\bf h_i measured in the space of relation {\\bf R_i. Note that it is necessary to take the embedding matrix {\\bf R_i into consideration when calculating the relevance of item {\\bf v and entity {\\bf h_i, since an item-entity pair may have different similarities when measured by different relations. For example, \"Forrest Gump\" and \"Cast Away\" are highly similar when considering their directors or stars, but have less in common if measured by genre or writer.",
      "For example, if a user has ever watched \"Forrest Gump\", he may possibly become a fan of Tom Hanks and be interested in \"The Terminal\" or \"Cast Away\". To characterize users' hierarchically extended preferences in terms of KG, in RippleNet, we recursively define the set of k-hop relevant entities for user u as follows: where \\mathcal E_u^0 = \\mathcal V_u = \\{ v \\ | \\ y_{uv = 1 \\ is the set of user's clicked items in the past, which can be seen as the seed set of user u in KG. \\end{definition Relevant entities can be regarded as natural extensions of a user's historical interests with respect to the KG.",
      "However, we observe that relevant entities of different items in a user's click history often highly overlap. In other words, an entity could be reached by multiple paths in the KG starting from a user's click history. For example, \"Saving Private Ryan\" is connected to a user who has watched \"The Terminal\", \"Jurassic Park\" and \"Braveheart\" through actor \"Tom Hanks\", director \"Steven Spielberg\" and genre \"War\", respectively.",
      "In addition to the interaction matrix \\bf Y, we also have a knowledge graph \\mathcal G available, which consists of massive entity-relation-entity triples (h, r, t). Here h \\in \\mathcal E, r \\in \\mathcal R, and t \\in \\mathcal E denote the head, relation, and tail of a knowledge triple, respectively, \\mathcal E and \\mathcal R denote the set of entities and relations in the KG. For example, the triple (Jurassic Park, film.film.director, Steven Spielberg) states the fact that Steven Spielberg is the director of the film \"Jurassic Park\".",
      "Discussion \\subsubsection{Explainability Explainable recommender systems aim to reveal why a user might like a particular item, which helps improve their acceptance or satisfaction of recommendations and increase trust in RS. The explanations are usually based on community tags , social networks , aspect , and phrase sentiment Since RippleNet explores users' interests based on the KG, it provides a new point of view of explainability by tracking the paths from a user's history to an item with high relevance probability (Eq. ()) in the KG. For example, a user's interest in film \"Back to the Future\" might be explained by the path \"user \\xrightarrow{watched Forrest \\ Gump \\xrightarrow{directed \\ by Robert \\ Zemeckis \\xrightarrow{directs Back \\ to \\ the Future\", if the item \"Back to the Future\" is of high relevance probability with \"Forrest Gump\" and \"Robert Zemeckis\" in the user's 1-hop and 2-hop ripple set, respectively.",
      "In many recommendation scenarios, an item v \\in \\mathcal V may associate with one or more entities in \\mathcal G. For example, the movie \"Jurassic Park\" is linked with its namesake in KG, while news with title \"France's Baby Panda Makes Public Debut\" is linked with entities \"France\" and \"panda\". Given interaction matrix \\bf Y as well as knowledge graph \\mathcal G, we aim to predict whether user u has potential interest in item v with which he has had no interaction before. Our goal is to learn a prediction function {\\hat y_{uv = \\mathcal F(u, v ; \\Theta), where {\\hat y_{uv denotes the probability that user u will click item v, and \\Theta denotes the model parameters of function \\mathcal F. RippleNet In this section, we discuss the proposed RippleNet in detail.",
      "The fading blue in Figure shows the decreasing relatedness between the center and surrounding entities. One concern about ripple sets is their sizes may get too large with the increase of hop number k. To address the concern, note that: (1) A large number of entities in a real KG are sink entities, meaning they only have incoming links but no outgoing links, such as \"2004\" and \"PG-13\" in Figure . (2) In specific recommendation scenarios such as movie or book recommendations, relations can be limited to scenario-related categories to reduce the size of ripple sets and improve relevance among entities."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 visually represents the knowledge graph connections between 'Forrest Gump' and 'Cast Away', showing their relationship through the actor 'Tom Hanks'. Text 0 provides a description of these connections, specifically mentioning 'Tom Hanks' as a common link between the two movies.\",\n    \"Answer\": \"The relationship between 'Forrest Gump' and 'Cast Away' is that both movies star Tom Hanks.\"\n}"
  },
  "5": {
    "question": "Which dataset has the highest AUC for all ripple set sizes?",
    "ground_truth": "MovieLens-1M",
    "selected_figures_names": [
      "1803.03467v4-Figure1-1.png",
      "1803.03467v4-Figure4-1.png",
      "1803.03467v4-Table4-1.png",
      "1803.03467v4-Figure5-1.png"
    ],
    "all_figures_names": [
      "1803.03467v4-Figure1-1.png",
      "1803.03467v4-Figure4-1.png",
      "1803.03467v4-Table4-1.png",
      "1803.03467v4-Figure5-1.png",
      "1803.03467v4-Figure7-1.png",
      "1803.03467v4-Figure8-1.png",
      "1803.03467v4-Figure3-1.png",
      "1803.03467v4-Table1-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 0,
    "rationale": "Image 2 provides the AUC results for different sizes of a user's ripple set across datasets, which directly answers the question. Text 0 explains the relationship between ripple set size and AUC performance, supporting the data in the image.",
    "answer": "MovieLens-1M has the highest AUC for all ripple set sizes.",
    "text_chunks": [
      "Several observations stand out: \\noindent Size of ripple set in each hop. We vary the size of a user's ripple set in each hop to further investigate the robustness of RippleNet. The results of AUC on the three datasets are presented in Table , from which we observe that with the increase of the size of ripple set, the performance of RippleNet is improved at first because a larger ripple set can encode more knowledge from the KG.",
      "The experiment results show that RippleNet achieves AUC gains of 2.0\\% to 40.6\\%, 2.5\\% to 17.4\\%, and 2.6\\% to 22.4\\% in movie, book, and news recommendations, respectively, compared with state-of-the-art baselines for recommendation. We also find that RippleNet provides a new perspective of explainability for the recommended results in terms of the knowledge graph. In summary, our contributions in this paper are as follows:",
      "A ripple set \\mathcal S_u^k is the set of knowledge triples that are k-hop(s) away from the seed set \\mathcal V_u. These ripple sets are used to interact with the item embedding (the yellow block) iteratively for obtaining the responses of user u with respect to item v (the green blocks), which are then combined to form the final user embedding (the grey block). Lastly, we use the embeddings of user u and item v together to compute the predicted probability \\hat y_{uv.",
      "The basic statistics of the three datasets are presented in Table . Baselines We compare the proposed RippleNet with the following state-of-the-art baselines: Experiment Setup In RippleNet, we set the hop number H = 2 for MovieLens-1M/Book-Crossing and H = 3 for Bing-News. A larger number of hops hardly improves performance but does incur heavier computational overhead according to experiment results.",
      "\\subsection*{\\small{ACM Reference Format: \\vspace{-0.05in{\\small Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018. RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. In The 27th ACM International Conference on Information and Knowledge Management (CIKM '18), October 22--26, 2018, Torino, Italy. ACM, New York, NY, USA, 10 pages.",
      "We also vary the maximal hop number H to see how performance changes in RippleNet. The results are shown in Table , which shows that the best performance is achieved when H is 2 or 3. We attribute the phenomenon to the trade-off between the positive signals from long-distance dependency and negative signals from noises: too small of an H can hardly explore inter-entity relatedness and dependency of long distance, while too large of an H brings much more noises than useful signals, as stated in Section .",
      "We vary d from 2 to 64 and \\lambda_2 from 0.0 to 1.0, respectively, while keeping other parameters fixed. The results of AUC on MovieLens-1M are presented in Figure . We observe from Figure that, with the increase of d, the performance is boosted at first since embeddings with a larger dimension can encode more useful information, but drops after d",
      "= 16 due to possible overfitting. From Figure , we can see that RippleNet achieves the best performance when \\lambda_2 = 0.01. This is because the KGE term with a small weight cannot provide enough regularization constraints, while a large weight will mislead the objective function."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 0,\n    \"Rationale\": \"Image 2 provides the AUC results for different sizes of a user's ripple set across datasets, which directly answers the question. Text 0 explains the relationship between ripple set size and AUC performance, supporting the data in the image.\",\n    \"Answer\": \"MovieLens-1M has the highest AUC for all ripple set sizes.\"\n}"
  },
  "6": {
    "question": "How does the dimension of embedding affect the AUC of RippleNet on MovieLens-1M?",
    "ground_truth": "The AUC of RippleNet first increases and then decreases with the increase of the dimension of embedding.",
    "selected_figures_names": [
      "1803.03467v4-Figure9-1.png"
    ],
    "all_figures_names": [
      "1803.03467v4-Table4-1.png",
      "1803.03467v4-Figure9-1.png",
      "1803.03467v4-Figure7-1.png",
      "1803.03467v4-Table5-1.png",
      "1803.03467v4-Figure5-1.png",
      "1803.03467v4-Table3-1.png",
      "1803.03467v4-Figure3-1.png",
      "1803.03467v4-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 shows the effect of embedding dimension on AUC for MovieLens-1M, which directly answers the question. Text 0 discusses the impact of embedding dimension on performance, providing additional context.",
    "answer": "The dimension of embedding affects the AUC of RippleNet on MovieLens-1M by initially boosting performance as the dimension increases, but performance drops after a certain point due to overfitting.",
    "text_chunks": [
      "We vary d from 2 to 64 and \\lambda_2 from 0.0 to 1.0, respectively, while keeping other parameters fixed. The results of AUC on MovieLens-1M are presented in Figure . We observe from Figure that, with the increase of d, the performance is boosted at first since embeddings with a larger dimension can encode more useful information, but drops after d",
      "The experiment results show that RippleNet achieves AUC gains of 2.0\\% to 40.6\\%, 2.5\\% to 17.4\\%, and 2.6\\% to 22.4\\% in movie, book, and news recommendations, respectively, compared with state-of-the-art baselines for recommendation. We also find that RippleNet provides a new perspective of explainability for the recommended results in terms of the knowledge graph. In summary, our contributions in this paper are as follows:",
      "The basic statistics of the three datasets are presented in Table . Baselines We compare the proposed RippleNet with the following state-of-the-art baselines: Experiment Setup In RippleNet, we set the hop number H = 2 for MovieLens-1M/Book-Crossing and H = 3 for Bing-News. A larger number of hops hardly improves performance but does incur heavier computational overhead according to experiment results.",
      "We will also give a case study of visualization and discuss the choice of hyper-parameters in this section. Datasets We utilize the following three datasets in our experiments for movie, book, and news recommendation: Since MovieLens-1M and Book-Crossing are explicit feedback data, we transform them into implicit feedback where each entry is marked with 1 indicating that the user has rated the item (the threshold of rating is 4 for MovieLens-1M, while no threshold is set for Book-Crossing due to its sparsity), and sample an unwatched set marked as 0 for each user, which is of equal size with the rated ones. For MovieLens-1M and Book-Crossing, we use the ID embeddings of users and items as raw input, while for Bing-News, we concatenate the ID embedding of a piece of news and the averaged word embedding of its title as raw input for the item, since news titles are typically much longer than names of movies or books, hence providing more useful information for recommendation.",
      "Several observations stand out: \\noindent Size of ripple set in each hop. We vary the size of a user's ripple set in each hop to further investigate the robustness of RippleNet. The results of AUC on the three datasets are presented in Table , from which we observe that with the increase of the size of ripple set, the performance of RippleNet is improved at first because a larger ripple set can encode more knowledge from the KG.",
      "Results \\hfill \\hfill \\hfill \\caption{Precision@K, Recall@K, and F1@K in top-K recommendation for MovieLens-1M. \\end{figure* \\hfill \\hfill \\caption{Precision@K, Recall@K, and F1@K in top-K recommendation for Book-Crossing. \\end{figure*",
      "= 16 due to possible overfitting. From Figure , we can see that RippleNet achieves the best performance when \\lambda_2 = 0.01. This is because the KGE term with a small weight cannot provide enough regularization constraints, while a large weight will mislead the objective function.",
      "In RippleNet, to model the interactions between users and items in a more fine-grained way, we propose a preference propagation technique to explore users' potential interests in his ripple sets. As shown in Figure , each item v is associated with an item embedding {\\bf v \\in {\\mathbb R^d, where d is the dimension of embeddings. Item embedding can incorporate one-hot ID , attributes , bag-of-words or context information of an item, based on the application scenario."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 shows the effect of embedding dimension on AUC for MovieLens-1M, which directly answers the question. Text 0 discusses the impact of embedding dimension on performance, providing additional context.\",\n    \"Answer\": \"The dimension of embedding affects the AUC of RippleNet on MovieLens-1M by initially boosting performance as the dimension increases, but performance drops after a certain point due to overfitting.\"\n}"
  }
}