{
  "0": {
    "question": "How does the performance of the proposed model compare to other models when trained on the LDC2017T10 dataset, and what does this suggest about the effectiveness of incorporating syntax into the model?",
    "ground_truth": "When trained on the LDC2017T10 dataset, the proposed model achieves the highest BLEU scores on both Dev and Test sets compared to other models listed in the table. This suggests that incorporating syntax into the model significantly improves its performance in generating text from AMR representations.",
    "selected_figures_names": [
      "1804.07707v2-Table1-1.png"
    ],
    "all_figures_names": [
      "1804.07707v2-Figure1-1.png",
      "1804.07707v2-Table3-1.png",
      "1804.07707v2-Table2-1.png",
      "1804.07707v2-Table1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 provides parsing scores which are relevant for comparing model performance. Text 2 discusses the improved performance of the model when incorporating syntax, which directly addresses the question.",
    "answer": "The proposed model shows improved performance compared to a syntax-agnostic baseline, as it generates more meaning-preserving realisations. This suggests that incorporating syntax into the model enhances its effectiveness, as evidenced by the higher parsing scores on the LDC2017T10 dataset.",
    "text_chunks": [
      "We train our models on the two most recent AMR releases. LDC2017T10 has roughly 36k training sentences, while LDC2015E86 is about half this size. Both share dev and test sets, facilitating comparison.",
      "As a baseline, we train a straight AMR-to-text model with the same architecture as above to control for the extra regularisation in our model compared to previous work. Our results show that adding syntax into the model dramatically boosts performance, resulting in state-of-the-art single model performance on both datasets without using external training data. As an oracle experiment, we also generate from the realisation model conditioned on the ground truth parse.",
      "Our results in Table show that the joint model can generate more meaning-preserving realisations compared to a syntax-agnostic baseline. This shows the utility of separating out syntactic and lexical variation: we model explicitly meaning-preserving invariances, and can therefore generate better paraphrases. Conclusions and further work We present an AMR generation model that factors the generation process through a syntactic decision, and show that this leads to improved AMR generation performance.",
      "In addition, we show that separating the syntactic decisions from the lexicalisation decisions allows the model to generate higher quality paraphrases of a given AMR graph. In future work, we would like to integrate a semantic parser into our model \\citep{Yin:18. Annotating data with AMR is expensive, and existing AMR treebanks are small.",
      "Then we feed this into a stacked bidirectional LSTM encoder to obtain contextualised representations of each input token c_i. As far as possible, we share parameters between our two models. Concretely, this means that the syntax model uses the same AMR and parse embeddings, and AMR encoder, as the lexicalisation model.",
      "to h_t before calculating the logits over the distribution of tokens: For the syntax model, we further constrain the decoder to only produce valid parse trees; as we build the parse tree left-to-right according to a depth-first traversal, the permissible actions at any stage are to open a new constituent, produce a terminal (i.e. a POS tag), or close the currently open constituent. We implement this constraint by setting the logits of all impermissible actions to negative infinity before taking the softmax. We find that this improves both training speed and final model performance, as we imbue the decoder with an intrinsic bias towards producing well-formed parse trees.",
      "We hypothesise that this procedure generates more meaning-preserving realisations than just sampling from a straight AMR-to-text model, which can result in incoherent output \\citep{Cao:17. We selected the first 50 AMR graphs in the dev set on linearised length between 15 and 40 with coherent reference realisations and generated 5 different realisations with our joint model and our baseline model. For our joint model, we first sampled 3 parse structures from the syntax model with temperature 0.3.",
      "In our experiments, we find that considering realisations of the 2 best parses (i.e. setting n = 2 above) gives the highest BLEU score on the dev set. Experiment 1: AMR and syntax We first investigate how much information AMR contains about possible syntactic realisations. We train two seq2seq models of the above architecture to predict the delexicalised constituency tree of an example given either the AMR graph or the text."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 provides parsing scores which are relevant for comparing model performance. Text 2 discusses the improved performance of the model when incorporating syntax, which directly addresses the question.\",\n    \"Answer\": \"The proposed model shows improved performance compared to a syntax-agnostic baseline, as it generates more meaning-preserving realisations. This suggests that incorporating syntax into the model enhances its effectiveness, as evidenced by the higher parsing scores on the LDC2017T10 dataset.\"\n}"
  },
  "1": {
    "question": "How does explicitly modeling meaning-preserving invariances impact the generation of paraphrases?",
    "ground_truth": "Explicitly modeling meaning-preserving invariances leads to the generation of better paraphrases.",
    "selected_figures_names": [
      "1804.07707v2-Table2-1.png"
    ],
    "all_figures_names": [
      "1804.07707v2-Figure1-1.png",
      "1804.07707v2-Table3-1.png",
      "1804.07707v2-Table2-1.png",
      "1804.07707v2-Table1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides quantitative evidence of the improved performance of a syntax-aware model over a baseline, which supports the claim in Text 0 about the benefits of modeling meaning-preserving invariances. Text 0 explains how this approach leads to better paraphrases.",
    "answer": "Explicitly modeling meaning-preserving invariances improves the generation of paraphrases by allowing the model to generate more meaning-preserving realisations, as shown by the higher number of acceptable realisations in the syntax-aware model compared to the baseline.",
    "text_chunks": [
      "Our results in Table show that the joint model can generate more meaning-preserving realisations compared to a syntax-agnostic baseline. This shows the utility of separating out syntactic and lexical variation: we model explicitly meaning-preserving invariances, and can therefore generate better paraphrases. Conclusions and further work We present an AMR generation model that factors the generation process through a syntactic decision, and show that this leads to improved AMR generation performance.",
      "Breaking down the AMR generation process this way provides us with several advantages: we disentangle the variance caused by the choice of syntax from that caused by the choice of words. We can therefore realise the same AMR graph with a variety of syntactic structures by sampling from the syntax model, and deterministically decoding using the lexicalisation model. We hypothesise that this generates better paraphrases of the reference realisation than sampling from a single-step model.",
      "By integrating a component which parses into AMR into our model, we can do semi-supervised learning on plentiful unannotated natural language sentences, and improve our AMR generation performance even further. In addition, we would be able to generate text-to-text paraphrases by parsing into AMR first and then carrying out the paraphrase generation procedure described in this paper \\citep{Iyyer:18. This opens up scope for data augmentation for downstream NLP tasks, such as machine translation.",
      "We hypothesise that this procedure generates more meaning-preserving realisations than just sampling from a straight AMR-to-text model, which can result in incoherent output \\citep{Cao:17. We selected the first 50 AMR graphs in the dev set on linearised length between 15 and 40 with coherent reference realisations and generated 5 different realisations with our joint model and our baseline model. For our joint model, we first sampled 3 parse structures from the syntax model with temperature 0.3.",
      "In addition, we show that separating the syntactic decisions from the lexicalisation decisions allows the model to generate higher quality paraphrases of a given AMR graph. In future work, we would like to integrate a semantic parser into our model \\citep{Yin:18. Annotating data with AMR is expensive, and existing AMR treebanks are small.",
      "We believe that we can improve our results even further if we use these techniques. Experiment 3: Generating varied realisations Our model explicitly disentangles variation caused by syntax choice from that caused by lexical choice. This means that we can generate diverse realisations of the same AMR graph by sampling from the syntax model and deterministically decoding from the realisation model.",
      "We therefore incorporate a copy mechanism \\citep{Vinyals:15b,See:17 into our lexicalisation model to make use of these alignments. We implement this by decomposing the word generation probability into a weighted sum of two terms. One is the vocabulary generation term.",
      "We linearise both the AMR graphs \\citep{Konstas:17 and constituency trees \\citep{Vinyals:15 to allow us to use sequence-to-sequence models \\citep{Sutskever:14,Bahdanau:15 for the syntax and lexicalisation models. Further, as the AMR dataset is relatively small, we have issues with data sparsity causing poor parameter estimation for rarely seen words. We deal with this by anonymizing named entities, and including a copy mechanism \\citep{Vinyals:15b,See:17,Song:18 into our decoder, which allows open-vocabulary token generation."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides quantitative evidence of the improved performance of a syntax-aware model over a baseline, which supports the claim in Text 0 about the benefits of modeling meaning-preserving invariances. Text 0 explains how this approach leads to better paraphrases.\",\n    \"Answer\": \"Explicitly modeling meaning-preserving invariances improves the generation of paraphrases by allowing the model to generate more meaning-preserving realisations, as shown by the higher number of acceptable realisations in the syntax-aware model compared to the baseline.\"\n}"
  },
  "2": {
    "question": "Which model performs the best at predicting the delexicalised constituency tree of an example, and how much better does it perform compared to the baseline model in terms of unlabelled F1 score?",
    "ground_truth": "The Text-to-parse model performs the best at predicting the delexicalised constituency tree, achieving an unlabelled F1 score of 87.5. This is significantly higher than the baseline Unconditional model, which achieves an unlabelled F1 score of 38.5. The Text-to-parse model therefore performs approximately 49 points better than the baseline.",
    "selected_figures_names": [
      "1804.07707v2-Table1-1.png"
    ],
    "all_figures_names": [
      "1804.07707v2-Figure1-1.png",
      "1804.07707v2-Table3-1.png",
      "1804.07707v2-Table2-1.png",
      "1804.07707v2-Table1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 provides the F1 scores for different models, which directly answers the question about model performance. Text 2 discusses the evaluation of models using F1 scores, aligning with the information in the image.",
    "answer": "The Text-to-parse model performs the best at predicting the delexicalised constituency tree with an unlabelled F1 score of 87.5. It performs 49.0 points better than the baseline Unconditional model, which has an unlabelled F1 score of 38.5.",
    "text_chunks": [
      "In our experiments, we find that considering realisations of the 2 best parses (i.e. setting n = 2 above) gives the highest BLEU score on the dev set. Experiment 1: AMR and syntax We first investigate how much information AMR contains about possible syntactic realisations. We train two seq2seq models of the above architecture to predict the delexicalised constituency tree of an example given either the AMR graph or the text.",
      "After this, we linearise the delexicalised constituency trees with depth-first traversal, following \\citet{Vinyals:15. Model implementation and training Model details We wish to estimate P(Y, Z|X), the joint probability of a parse Y and surface form Z given an AMR graph X. We model this in two parts, using the chain rule to decompose the joint distribution.",
      "We then evaluate both models on labelled and unlabelled F1 score on the dev split of the corpus. As neither model is guaranteed to produce trees with the right number of terminals, we first run an insert/delete aligner between the predicted and reference terminals (i.e. POS tags) before calculating span F1s. We also report the results of running our aligner on the most probable parse tree as estimated by an unconditional LSTM as a baseline both to control for our aligner and also to see how much extra signal is in the AMR graph.",
      "\\paragraph{Constituency syntax While there are many syntactic annotation formalisms, we use delexicalised Penn treebank-style constituency trees to represent syntax. Constituency trees have the advantage of a well-defined linearization order compared to dependency trees. Further, constituency trees may be easier to realise, as they effectively correspond to a bracketing of the surface form.",
      "The first model, which we call the syntax model, approximates P(Y|X), the probability of a particular syntactic structure for a meaning representation. The second is P(Z|X, Y), the lexicalisation model. This calculates the probability of a surface realisation given a parse tree and an AMR graph.",
      "As a baseline, we train a straight AMR-to-text model with the same architecture as above to control for the extra regularisation in our model compared to previous work. Our results show that adding syntax into the model dramatically boosts performance, resulting in state-of-the-art single model performance on both datasets without using external training data. As an oracle experiment, we also generate from the realisation model conditioned on the ground truth parse.",
      "We instead explicitly try and capture this syntactic variation and factor the generation process through a syntactic representation \\citep{Walker:03,Dusek:16,Gardent:17,Currey:18. First, we generate a delexicalised constituency structure from the AMR graph using a syntax model. Then, we fill out the constituency structure with the semantic content in the AMR graph using a lexicalisation model to generate the final surface form.",
      "The outstanding result here -- BLEU scores in the 50s -- demonstrates that being able to predict the gold reference parse tree is a bottleneck in the performance of our model. However, given the inherent difficulty of predicting a single syntax realisation (cf. Section ), we suspect that there is an intrinsic limit to how well generating from an AMR graph can replicate the reference realisation. We further note that we do not use models tailored to graph-structured data or character-level features as in \\citet{Song:18,Beck:18, or additional unlabelled data to perform semi-supervised learning \\citep{Konstas:17."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 provides the F1 scores for different models, which directly answers the question about model performance. Text 2 discusses the evaluation of models using F1 scores, aligning with the information in the image.\",\n    \"Answer\": \"The Text-to-parse model performs the best at predicting the delexicalised constituency tree with an unlabelled F1 score of 87.5. It performs 49.0 points better than the baseline Unconditional model, which has an unlabelled F1 score of 38.5.\"\n}"
  }
}