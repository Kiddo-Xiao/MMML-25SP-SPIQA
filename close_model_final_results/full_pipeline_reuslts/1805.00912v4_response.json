{
  "0": {
    "question": "Which model has the lowest memory consumption and time cost on synthetic data?",
    "ground_truth": "MTSA",
    "selected_figures_names": [
      "1805.00912v4-Table6-1.png",
      "1805.00912v4-Table3-1.png",
      "1805.00912v4-Table4-1.png",
      "1805.00912v4-Table1-1.png"
    ],
    "all_figures_names": [
      "1805.00912v4-Table6-1.png",
      "1805.00912v4-Table3-1.png",
      "1805.00912v4-Table4-1.png",
      "1805.00912v4-Table1-1.png",
      "1805.00912v4-Table5-1.png",
      "1805.00912v4-Figure3-1.png",
      "1805.00912v4-Table2-1.png",
      "1805.00912v4-Figure1-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 0,
    "rationale": "Image 3 provides detailed information on memory consumption and time cost for various models, including MTSA, on synthetic data. Text 0 discusses memory consumption and time cost comparisons, making it relevant to the question.",
    "answer": "The Multi-CNN model has the lowest memory consumption and time cost on synthetic data.",
    "text_chunks": [
      "To summarize the comparison of MTSA with recently popular models, we show the memory consumption and time cost vs. sequence length respectively in Figure and on synthetic data (batch size of 64 and feature channels of 300). On the SNLI~, a public dataset for language inference, as shown in Figure , MTSA achieves the best result but is as fast and as memory-efficient as the CNNs (all baselines and the benchmark are detailed in Section ). Notations: 1) lowercase denotes a vector; 2) bold lowercase denotes a sequence of vectors (stored as a matrix); and 3) uppercase denotes a matrix or tensor.",
      "Compared to baselines, MTSA is 4\\!\\sim\\!5\\!\\times faster than RNN-based models and outperforms CNN-based models given a similar number of parameters and computation time. Moreover, compared to the dot-product self-attention (Multi-head), MTSA costs similar time and memory but performs more expressively powerful self-attention, and thus achieves better performance. Furthermore, compared to the multi-dim self-attention (DiSA and Bi-BloSA), MTSA uses much less memory and time but even produces much better prediction quality.",
      "MTSA achieves state-of-the-art performance with less time and memory cost. Compared to the methods from the leaderboard, MTSA outperforms RNN-based encoders (e.g., Residual stacked enc.) , RNN+attention encoders (e.g., Deep Gated Attn.)",
      "Therefore, it produces the expressively powerful tensorized alignment scores but is computed as fast and as memory-efficiently as a CNN. More Details about Algorithm 1 Memory-Efficiency (illustrated in Figure~1(a)): Compared to multi-dim token2token self-attention that inherently requires 4-D tensors (with the shape of [batch size, sequence length, sequence length, feature channels]) to store the alignment scores during the training phase, MTSA does not use any high-rank tensor operation but only matrix multiplications to avoid memory explosion . Time-Efficiency (illustrated in Figure~1(b)):",
      "Codes are implemented in Python with Tensorflow and executed on a single NVIDIA GTX 1080Ti graphics card. In addition, data for both time cost and memory consumption are collected under Tensorflow-1.7 with CUDA9 and cuDNN7. The fair and reliable experiment setups are elaborated in Appendix B. The context fusion baselines include 1) Bi-LSTM : 600D bi-directional LSTM consisting of 300D forward plus 300D backward LSTMs, 2) Bi-GRU : 600D bi-directional GRU, 3) Multi-CNN : three CNNs with 200D kernels to model 3/4/5-grams respectively, 4) Hrchy-CNN : 3-layer 300D stacked CNN with kernel size 5, gated linear units and residual connections , 5) Multi-head : 600D multi-head self-attention with 8 heads (75-dim subspace per head) and positional embedding used by \\citeauthor{vaswani2017attention~\\shortcite{vaswani2017attention, 6) DiSA :",
      "Conclusion In conclusion, MTSA is highly parallelizable with more expressive power since it efficiently captures the pairwise dependency at token level, but delicately models the global dependency at feature level, and distributes computations to multiple heads, each equipped with a distinct positional mask. These lead to a sweet spot of the trade-off between performance and efficiency, and make MTSA as memory-efficient as CNN and scalable to long sequences but outperform previous (and even multi-dim) self-attention mechanisms in terms of prediction quality. The experiments conducted on nine NLP tasks verify that the MTSA can reach state-of-the-art performance with appealing efficiency.",
      "For SNLI dataset (natural language inference), we set p_{kp = 0.65 and \\gamma = 5\\times10^{-5, and use Adadelta as the optimizer with mini batch size of 128. And, we do not use the attention dropout for this benchmark. Besides, the activation function for fully-connected layer is set to \\elu .",
      "Different from the other experiments in this paper, we here use Adam as the optimizer to train the models, and do not use any learning rate decay trick. The training procedure is completed within 1000 batch steps. For TREC dataset (question-type classification), we set p_{kp = 0.5 and \\gamma = 10^{-4 and do not apply the attention dropout."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 0,\n    \"Rationale\": \"Image 3 provides detailed information on memory consumption and time cost for various models, including MTSA, on synthetic data. Text 0 discusses memory consumption and time cost comparisons, making it relevant to the question.\",\n    \"Answer\": \"The Multi-CNN model has the lowest memory consumption and time cost on synthetic data.\"\n}"
  },
  "1": {
    "question": "Which model has the highest test accuracy on the SNLI dataset, and how does its training time per epoch compare to the MTSA model?",
    "ground_truth": "The model with the highest test accuracy is MTSA, with an accuracy of 86.3%. Its training time per epoch is 180s, which is faster than the training time of several other models with lower accuracy, such as Bi-LSTM (854s), Bi-GRU (850s), and DiSA (390s).",
    "selected_figures_names": [
      "1805.00912v4-Figure1-1.png",
      "1805.00912v4-Figure2-1.png",
      "1805.00912v4-Table4-1.png",
      "1805.00912v4-Table2-1.png"
    ],
    "all_figures_names": [
      "1805.00912v4-Figure1-1.png",
      "1805.00912v4-Figure2-1.png",
      "1805.00912v4-Table4-1.png",
      "1805.00912v4-Table2-1.png",
      "1805.00912v4-Table6-1.png",
      "1805.00912v4-Table1-1.png",
      "1805.00912v4-Table5-1.png",
      "1805.00912v4-Figure3-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 2,
    "rationale": "Image 3 provides the test accuracy for the SNLI dataset, showing that 'Transfer + MTSA' achieves the highest accuracy. Text 2 discusses the integration of MTSA with a pretrained language model, which is relevant to understanding its performance. However, the training time per epoch is not directly compared in the provided text or images.",
    "answer": "The 'Transfer + MTSA' model has the highest test accuracy on the SNLI dataset with 86.9%. The training time per epoch is not specified in the provided information.",
    "text_chunks": [
      "To summarize the comparison of MTSA with recently popular models, we show the memory consumption and time cost vs. sequence length respectively in Figure and on synthetic data (batch size of 64 and feature channels of 300). On the SNLI~, a public dataset for language inference, as shown in Figure , MTSA achieves the best result but is as fast and as memory-efficient as the CNNs (all baselines and the benchmark are detailed in Section ). Notations: 1) lowercase denotes a vector; 2) bold lowercase denotes a sequence of vectors (stored as a matrix); and 3) uppercase denotes a matrix or tensor.",
      "The prediction accuracies achieved on these five benchmarks are shown in Table . MTSA achieves the best prediction accuracy on CR, MPQA, TREC and SST-5 benchmarks with better time efficiency and a lower memory load. Machine Translation We also evaluate proposed model on WMT 2014 English-German translation task for exhaustive comparisons with multi-head attention.",
      "In addition, to further improve the state-of-the-art performance, in contrast to training from scratch, a language model built on the Transformer unsupervisedly pretrained on large English corpus (detailed by \\citeauthor{radford2018improving~\\shortcite{radford2018improving) is transfered for the baseline and proposed models for sentence-encoding based NLI tasks. As shown in Table~, MTSA integrated with pretrained language model can achieve new state-of-the-art accuracy on both SNLI and Multi-Genre Natural Language Inference (MultiNLI) among all sentence-encoding models. An ablation study of MTSA is shown in Table to verify the capability of its each part in context fusion.",
      "Compared to baselines, MTSA is 4\\!\\sim\\!5\\!\\times faster than RNN-based models and outperforms CNN-based models given a similar number of parameters and computation time. Moreover, compared to the dot-product self-attention (Multi-head), MTSA costs similar time and memory but performs more expressively powerful self-attention, and thus achieves better performance. Furthermore, compared to the multi-dim self-attention (DiSA and Bi-BloSA), MTSA uses much less memory and time but even produces much better prediction quality.",
      "Conclusion In conclusion, MTSA is highly parallelizable with more expressive power since it efficiently captures the pairwise dependency at token level, but delicately models the global dependency at feature level, and distributes computations to multiple heads, each equipped with a distinct positional mask. These lead to a sweet spot of the trade-off between performance and efficiency, and make MTSA as memory-efficient as CNN and scalable to long sequences but outperform previous (and even multi-dim) self-attention mechanisms in terms of prediction quality. The experiments conducted on nine NLP tasks verify that the MTSA can reach state-of-the-art performance with appealing efficiency.",
      "The results achieved by previous methods, baselines, and MTSA are shown in Table , which demonstrates that MTSA achieves new state-of-the-art performance on the CoNLL-05 dataset by costing similar training time as CNN and multi-head self-attention baselines. Sentence Classifications The goal of sentence classification is to predict the correct label for a sentence in various scenarios. We evaluate the models on five sentence classification benchmarks for different NLP tasks, which include 1) CR : customer reviews of various products to predict whether the review is positive or negative, 2) MPQA : an opinion polarity detection subtask of the MPQA dataset, 3) SUBJ : subjectivity dataset where a label indicates whether a sentence is subjective or objective, 4) TREC : question-type classification dataset which classifies the question sentences into six classes, 5) SST-5 : the Stanford Sentiment Treebank dataset with five sentiment labels.",
      "Therefore, it produces the expressively powerful tensorized alignment scores but is computed as fast and as memory-efficiently as a CNN. More Details about Algorithm 1 Memory-Efficiency (illustrated in Figure~1(a)): Compared to multi-dim token2token self-attention that inherently requires 4-D tensors (with the shape of [batch size, sequence length, sequence length, feature channels]) to store the alignment scores during the training phase, MTSA does not use any high-rank tensor operation but only matrix multiplications to avoid memory explosion . Time-Efficiency (illustrated in Figure~1(b)):",
      "Following the method of applying sentence-encoding model to NLI given by \\citeauthor{bowman2016fast~\\shortcite{bowman2016fast, two parameter-tied sentence-encoding models are used to generate embeddings for premise and hypothesis, resulting in s^p and s^h respectively. The concatenation of s^p, s^h, s^p - s^h and s^p \\odot s^h representing the relationship is passed into a 3-way neural classifier for final prediction. The experimental results of the models from the official leaderboard, baselines, and MTSA are shown in Table ."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 2,\n    \"Rationale\": \"Image 3 provides the test accuracy for the SNLI dataset, showing that 'Transfer + MTSA' achieves the highest accuracy. Text 2 discusses the integration of MTSA with a pretrained language model, which is relevant to understanding its performance. However, the training time per epoch is not directly compared in the provided text or images.\",\n    \"Answer\": \"The 'Transfer + MTSA' model has the highest test accuracy on the SNLI dataset with 86.9%. The training time per epoch is not specified in the provided information.\"\n}"
  },
  "2": {
    "question": "Which model performed best on the SNLI test set?",
    "ground_truth": "The Transfer + MTSA model performed best on the SNLI test set with an accuracy of 86.9%.",
    "selected_figures_names": [
      "1805.00912v4-Figure1-1.png",
      "1805.00912v4-Table1-1.png",
      "1805.00912v4-Figure3-1.png",
      "1805.00912v4-Figure2-1.png"
    ],
    "all_figures_names": [
      "1805.00912v4-Figure1-1.png",
      "1805.00912v4-Table1-1.png",
      "1805.00912v4-Figure3-1.png",
      "1805.00912v4-Figure2-1.png",
      "1805.00912v4-Table6-1.png",
      "1805.00912v4-Table3-1.png",
      "1805.00912v4-Table2-1.png",
      "1805.00912v4-Table4-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 2,
    "rationale": "Image 1 provides a detailed table comparing the performance of different models on the SNLI test set, including test accuracy. Text 2 discusses the integration of MTSA with a pretrained language model, achieving state-of-the-art accuracy on SNLI. Both sources are directly relevant to determining the best-performing model.",
    "answer": "MTSA integrated with a pretrained language model performed best on the SNLI test set.",
    "text_chunks": [
      "For SNLI dataset (natural language inference), we set p_{kp = 0.65 and \\gamma = 5\\times10^{-5, and use Adadelta as the optimizer with mini batch size of 128. And, we do not use the attention dropout for this benchmark. Besides, the activation function for fully-connected layer is set to \\elu .",
      "To summarize the comparison of MTSA with recently popular models, we show the memory consumption and time cost vs. sequence length respectively in Figure and on synthetic data (batch size of 64 and feature channels of 300). On the SNLI~, a public dataset for language inference, as shown in Figure , MTSA achieves the best result but is as fast and as memory-efficient as the CNNs (all baselines and the benchmark are detailed in Section ). Notations: 1) lowercase denotes a vector; 2) bold lowercase denotes a sequence of vectors (stored as a matrix); and 3) uppercase denotes a matrix or tensor.",
      "In addition, to further improve the state-of-the-art performance, in contrast to training from scratch, a language model built on the Transformer unsupervisedly pretrained on large English corpus (detailed by \\citeauthor{radford2018improving~\\shortcite{radford2018improving) is transfered for the baseline and proposed models for sentence-encoding based NLI tasks. As shown in Table~, MTSA integrated with pretrained language model can achieve new state-of-the-art accuracy on both SNLI and Multi-Genre Natural Language Inference (MultiNLI) among all sentence-encoding models. An ablation study of MTSA is shown in Table to verify the capability of its each part in context fusion.",
      "Besides, for the language model based transfer learning for SNLI and MultiNLI tasks, we use the pretrained model provided by \\citeauthor{radford2018improving~\\shortcite{radford2018improving. And, we use the language model as the auxiliary task for models' universality with the coefficient of 0.3, and use other hyper-parameters (e.g., initial learning rate, optimizer, leaning rate schedule, epoch number) given by \\citeauthor{radford2018improving~\\shortcite{radford2018improving. Finally, We give the details about hyper-parameter selection which leads the proposed model to achieve the optimal performance for each NLP benchmark in the following.",
      "Following the method of applying sentence-encoding model to NLI given by \\citeauthor{bowman2016fast~\\shortcite{bowman2016fast, two parameter-tied sentence-encoding models are used to generate embeddings for premise and hypothesis, resulting in s^p and s^h respectively. The concatenation of s^p, s^h, s^p - s^h and s^p \\odot s^h representing the relationship is passed into a 3-way neural classifier for final prediction. The experimental results of the models from the official leaderboard, baselines, and MTSA are shown in Table .",
      "The experimental results show that MTSA empirically achieves state-of-the-art performance on a wide range of NLP tasks, and is as fast and as memory-efficient as CNN baselines. This indicates that a stacked version of MTSA might improve the performance on more NLP tasks. There are various intriguing works that are worth exploring based on the proposed model, such as 1) integrating MTSA with Transformer for more complicated and high-level NLP tasks (e.g., neural machine translation and summarization), 2) applying more types of positional masks or distance-aware masks to different heads, and thus taking into account richer structure information, and 3) integrating the light-weight and time-efficient MTSA with a hierarchical self-attention structure for context fusion on long text (e.g., passage and document), rather than using single self-attention mechanism or the non-parallelizable RNN-based models.",
      "The activation function \\sigma _t (\\cdot) applied to token2token alignment scores is set to \\log (\\sigmoid(\\cdot)) unless otherwise specified. For fair and reliable comparisons with baselines and prior works, on SNLI and sentence classification tasks, we follow training setup and hyperparameters used in corresponding prior works, and only tune the dropout probability for different baseline or ablation models, without any other trick (e.g., learning rate schedule, batch/layer normalization, etc.) ; on SRL, we directly employ the training setup and the well-tuned hyperparameters used in the prior state-of-the-art work based on multi-head self-attention mechanism, without tuning them specifically for our proposed model.",
      "Conclusion In conclusion, MTSA is highly parallelizable with more expressive power since it efficiently captures the pairwise dependency at token level, but delicately models the global dependency at feature level, and distributes computations to multiple heads, each equipped with a distinct positional mask. These lead to a sweet spot of the trade-off between performance and efficiency, and make MTSA as memory-efficient as CNN and scalable to long sequences but outperform previous (and even multi-dim) self-attention mechanisms in terms of prediction quality. The experiments conducted on nine NLP tasks verify that the MTSA can reach state-of-the-art performance with appealing efficiency."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 2,\n    \"Rationale\": \"Image 1 provides a detailed table comparing the performance of different models on the SNLI test set, including test accuracy. Text 2 discusses the integration of MTSA with a pretrained language model, achieving state-of-the-art accuracy on SNLI. Both sources are directly relevant to determining the best-performing model.\",\n    \"Answer\": \"MTSA integrated with a pretrained language model performed best on the SNLI test set.\"\n}"
  },
  "3": {
    "question": "Based on the table, how does MTSA compare to the Bi-LSTM and Multi-CNN baselines in terms of performance and training time?",
    "ground_truth": "MTSA outperforms both Bi-LSTM and Multi-CNN baselines across all evaluation metrics (P, R, F1, and Comp.) on all three test sets (Development, WSJ Test, and Brown Test). While MTSA achieves the highest scores, its training time is comparable to Multi-head and Multi-CNN, and significantly faster than Bi-LSTM.",
    "selected_figures_names": [
      "1805.00912v4-Table3-1.png",
      "1805.00912v4-Table6-1.png"
    ],
    "all_figures_names": [
      "1805.00912v4-Table3-1.png",
      "1805.00912v4-Table6-1.png",
      "1805.00912v4-Table4-1.png",
      "1805.00912v4-Figure3-1.png",
      "1805.00912v4-Figure1-1.png",
      "1805.00912v4-Table5-1.png",
      "1805.00912v4-Table2-1.png",
      "1805.00912v4-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 6,
    "rationale": "Image 0 provides a comparison of MTSA with different configurations, showing inference time and test accuracy, which is relevant for performance evaluation. Text 6 discusses MTSA's performance compared to RNN-based encoders, which are similar to Bi-LSTM, and mentions its efficiency compared to CNNs.",
    "answer": "MTSA achieves state-of-the-art performance with less time and memory cost compared to RNN-based encoders like Bi-LSTM. It is as fast and memory-efficient as CNN baselines, indicating superior performance and efficiency.",
    "text_chunks": [
      "Compared to baselines, MTSA is 4\\!\\sim\\!5\\!\\times faster than RNN-based models and outperforms CNN-based models given a similar number of parameters and computation time. Moreover, compared to the dot-product self-attention (Multi-head), MTSA costs similar time and memory but performs more expressively powerful self-attention, and thus achieves better performance. Furthermore, compared to the multi-dim self-attention (DiSA and Bi-BloSA), MTSA uses much less memory and time but even produces much better prediction quality.",
      "Therefore, it produces the expressively powerful tensorized alignment scores but is computed as fast and as memory-efficiently as a CNN. More Details about Algorithm 1 Memory-Efficiency (illustrated in Figure~1(a)): Compared to multi-dim token2token self-attention that inherently requires 4-D tensors (with the shape of [batch size, sequence length, sequence length, feature channels]) to store the alignment scores during the training phase, MTSA does not use any high-rank tensor operation but only matrix multiplications to avoid memory explosion . Time-Efficiency (illustrated in Figure~1(b)):",
      "and even parsing trees based encoders (e.g., Gumbel TreeLSTM enc.) by a large margin. Compared to the two competitive self-attention networks with complicated and expensive training computations, MTSA trained in end-to-end manner achieves the same state-of-the-art performance by using much fewer parameters and less computational time.",
      "Conclusion In conclusion, MTSA is highly parallelizable with more expressive power since it efficiently captures the pairwise dependency at token level, but delicately models the global dependency at feature level, and distributes computations to multiple heads, each equipped with a distinct positional mask. These lead to a sweet spot of the trade-off between performance and efficiency, and make MTSA as memory-efficient as CNN and scalable to long sequences but outperform previous (and even multi-dim) self-attention mechanisms in terms of prediction quality. The experiments conducted on nine NLP tasks verify that the MTSA can reach state-of-the-art performance with appealing efficiency.",
      "To summarize the comparison of MTSA with recently popular models, we show the memory consumption and time cost vs. sequence length respectively in Figure and on synthetic data (batch size of 64 and feature channels of 300). On the SNLI~, a public dataset for language inference, as shown in Figure , MTSA achieves the best result but is as fast and as memory-efficient as the CNNs (all baselines and the benchmark are detailed in Section ). Notations: 1) lowercase denotes a vector; 2) bold lowercase denotes a sequence of vectors (stored as a matrix); and 3) uppercase denotes a matrix or tensor.",
      "The experimental results show that MTSA empirically achieves state-of-the-art performance on a wide range of NLP tasks, and is as fast and as memory-efficient as CNN baselines. This indicates that a stacked version of MTSA might improve the performance on more NLP tasks. There are various intriguing works that are worth exploring based on the proposed model, such as 1) integrating MTSA with Transformer for more complicated and high-level NLP tasks (e.g., neural machine translation and summarization), 2) applying more types of positional masks or distance-aware masks to different heads, and thus taking into account richer structure information, and 3) integrating the light-weight and time-efficient MTSA with a hierarchical self-attention structure for context fusion on long text (e.g., passage and document), rather than using single self-attention mechanism or the non-parallelizable RNN-based models.",
      "MTSA achieves state-of-the-art performance with less time and memory cost. Compared to the methods from the leaderboard, MTSA outperforms RNN-based encoders (e.g., Residual stacked enc.) , RNN+attention encoders (e.g., Deep Gated Attn.)",
      "For Setup2, we use the hyperparameter set of transformer\\_base with the modification of 1) using 4\\times instead of 8\\times\\text{P100, 2) increasing batch size from 4096 to 6144 per GPU, and 3) using training step of 133K. More details of the training setups for translation task are described in Appendix B.1. As shown in Table , with small p-value for both training setup 1 and 2, the encoder with MTSA significantly outperforms that with multi-head self-attention, which demonstrates that multi-dim based MTSA modeling both pairwise and global dependencies is more expressive than dot-product based multi-head self-attention. Although the results do not improve state-of-the-art BLEU value of machine translation task, the purpose of this experiment to verify the effectiveness of MTSA in contrast to dot-product based multi-head self-attention is accomplished."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 6,\n    \"Rationale\": \"Image 0 provides a comparison of MTSA with different configurations, showing inference time and test accuracy, which is relevant for performance evaluation. Text 6 discusses MTSA's performance compared to RNN-based encoders, which are similar to Bi-LSTM, and mentions its efficiency compared to CNNs.\",\n    \"Answer\": \"MTSA achieves state-of-the-art performance with less time and memory cost compared to RNN-based encoders like Bi-LSTM. It is as fast and memory-efficient as CNN baselines, indicating superior performance and efficiency.\"\n}"
  },
  "4": {
    "question": "What is the purpose of the positional mask in the TSA mechanism?",
    "ground_truth": "The positional mask is used to provide information about the relative position of tokens in the input sequence. This information is used to compute the attention weights, which determine how much each token attends to each other token.",
    "selected_figures_names": [
      "1805.00912v4-Figure2-1.png"
    ],
    "all_figures_names": [
      "1805.00912v4-Table3-1.png",
      "1805.00912v4-Table5-1.png",
      "1805.00912v4-Table2-1.png",
      "1805.00912v4-Figure1-1.png",
      "1805.00912v4-Table4-1.png",
      "1805.00912v4-Figure2-1.png",
      "1805.00912v4-Table6-1.png",
      "1805.00912v4-Table1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 4,
    "rationale": "Image 0 visually represents the TSA mechanism, showing how positional masks are applied. Text 4 explains the role of positional masks in encoding sequential and structural information, which directly answers the question.",
    "answer": "The purpose of the positional mask in the TSA mechanism is to encode sequential and structural information.",
    "text_chunks": [
      "% In addition, to encode different kinds of sequential or structural information, multiple different positional masks (e.g., forward, backward and multi-length window) can be further applied to the multiple heads. The memory-/time-efficiency and expressive power of TSA can be improved by using the combination of the multi-head and multi-mask techniques introduced above.",
      "By writing TSA mechanism as a function \\tsa (\\bm{x, M) with input sequence \\bm{x \\in \\mathbb{R ^{d_e \\times n and a positional mask M \\in \\mathbb{R ^{n \\times n, and the output given by Eq.(), multi-mask tensorized self-attention (MTSA) produces where W^{(o)\\in\\mathbb{R^{h\\cdot d_h \\times h\\cdot d_h, h is the number of heads, \\tsa\\nolimits^c denotes the c^{th parameter-independent TSA block that produces a d_h-dim representation in the c^{th subspace, M^{c represents the positional mask applied to attention in the c^{th subspace, [\\cdot; \\dots;\\cdot] denotes a vertical concatenation operation, and \\bm{s\\in\\mathbb{R^{h\\cdot d_h \\times n is the output of MTSA.",
      "For example, the compatibility function of additive source2token self-attention mechanism is to simply remove q from Eq.(). Proposed Models In this section, we firstly elaborate on tensorized self-attention (TSA) in Section , which captures both pairwise and global dependencies by combining the two types of self-attention mechanisms introduced in Section~. Then, we extend TSA to multi-mask tensorized self-attention (MTSA) in Section~ by applying different positional masks to TSA in multiple subspaces (multi-head fashion).",
      "In our experiments, we apply forward mask to half of the heads and apply backward mask to the other half to encode bi-directional order information of the input sequence. Computation-Optimized MTSA \\end{algorithm As shown in Eq.() and Eq.(), TSA or each head of MTSA needs to compute the attention scores and probabilities as n\\times n\\times d_h tensors. In accordance with multi-dim self-attention , this makes TSA more expressively powerful and improves the final performance for sequence modeling, but terribly leads to memory explosion and computational bottleneck on long sequences with large n and d_h.",
      "The compatibility function used in TSA broadcasts the scalar alignment score f^t(k_i, q_j) \\in\\mathbb{R computed by the token2token self-attention to all d_h feature dimensions, and then adds them to the feature-wise score vector f^s(k_i) \\in\\mathbb{R^{d_h computed by the source2token self-attention. In addition, the positional masks from masked self-attention (in Section~) are also integrated to encode sequential and structural information. These yield following compatibility function of TSA. where \\forall i,j \\in[n],~\\forall l\\in[d_h].",
      "The tokens in x-axis and y-axis are the dependent and governor tokens, respectively. \\end{figure* Discussion In this paper, we propose a novel multi-dim self-attention mechanism, called multi-mask tensorized self-attention (MTSA), for context fusion purpose.",
      "\\end{equation Furthermore, directional self-attention (DiSA) concatenates the features produced by masked self-attention mechanisms with the forward and backward positional masks (i.e., M^{fw, M^{bw), leading to context-ware representations with bi-directional information encoded. B) Source2token self-attention mechanism is designed for sentence embedding or sequence compression, which is based on the importance of each token x_i to the entire source sequence \\bm{x for a specific task. Specifically, it removes the query q from the compatibility function f(x_i, q) when computing the alignment score.",
      "TSA then integrates two kinds of compatibility functions from two self-attention mechanisms respectively. Firstly, the scaled dot-product self-attention is used to capture dependency between every two tokens. Dot-product operations are fast, and sufficient to model the pairwise dependency in most tasks."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 4,\n    \"Rationale\": \"Image 0 visually represents the TSA mechanism, showing how positional masks are applied. Text 4 explains the role of positional masks in encoding sequential and structural information, which directly answers the question.\",\n    \"Answer\": \"The purpose of the positional mask in the TSA mechanism is to encode sequential and structural information.\"\n}"
  }
}