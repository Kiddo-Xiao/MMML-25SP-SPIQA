{
  "0": {
    "question": "Which task-oriented dialog system performs the best when the percentage of unseen information in the KB is high?",
    "ground_truth": "BoSsNet",
    "selected_figures_names": [
      "1805.01216v3-Figure1-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Figure3-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 shows the performance of various dialog systems as the percentage of unseen entities increases, and Text 2 discusses \\\\sys\\\\ achieving state-of-the-art results, indicating its robustness to unseen entities.",
    "answer": "\\sys\\ performs the best when the percentage of unseen information in the KB is high.",
    "text_chunks": [
      "Unfortunately, this loses the contextual information around a word, which is needed to decipher an unseen word. In contrast, \\sys\\ uses a bag of sequences encoding, where KB tuples are a set for easier inference, and also each utterance is a sequence for effectively learning when to copy. Conclusions We propose \\sys\\ for training task-oriented dialog systems in an end-to-end fashion.",
      "Introduction Task-oriented dialog agents converse with a user with the goal of accomplishing a specific task and often interact with a knowledge-base (KB). For example, a restaurant reservation agent will be grounded to a KB that contains the names of restaurants, and their details. In real-world applications, the KB information could change over time.",
      "\\sys\\ achieves the state of the art results on bAbI dialog dataset, outperforming existing models by 10 points or more in its OOV conditions. In the knowledge adaptability test, we find that \\sys\\ is highly robust to increasing the percentage of unseen entities at test time, suggesting a good language-knowledge disentanglement. Human evaluations show that \\sys\\ responses are highly informative and slightly more grammatical compared to baselines.",
      "This clearly shows that multi-hop inference and the copy mechanism are essentials for task-oriented dialogs. Despite gains from the Mem2Seq model, the performance difference between the non-OOV and OOV test sets remains large. \\sys\\ succeeds to bridge this gap with its ability to better interpret unseen words, using their surrounding context.",
      "SMD consists of dialogs with a large KB and a highly varying response pattern. This makes it very difficult to learn the language model -- reflected in the low BLEU scores for all the systems. \\sys\\ still provides the best F1 entity score due to its ability to inference efficiently on the large KB (Figure ).",
      "We systematically picked random KB entities and replaced all their occurrences in the dialog with new entity names. We will refer to these generated dialogs as the Knowledge Adaptability (KA) test sets. Baselines We compare \\sys\\ against several existing end-to-end task-oriented dialog systems.",
      "\\sys\\ has the best performance with even a slight increase in both BLEU and Entity F1 metrics as more OOV content is injected in the dialog, probably because it is clear that it needs to copy when processing unseen entities. Seq2Seq+Copy is unable to perform well in CamRest as the length of the input (dialog history + KB tuples) is long and the size of the training set is also small. We believe that Seq2Seq+Copy works best in an environment with an abundance of short dialog training data (e.g., bAbI task 1 in Figure ).",
      "In the current dialog context, the system is required to provide the address of the selected restaurant, but since more than one restaurant in the KB is unseen, it becomes ambiguous for the network to identify the correct restaurant and infer its address. In the end, the system is forced to pick a random address -- the probability of which being correct decreases as more restaurants become unseen. The performance on the CamRest KA test sets is illustrated in Figures and ."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 shows the performance of various dialog systems as the percentage of unseen entities increases, and Text 2 discusses \\\\\\\\sys\\\\\\\\ achieving state-of-the-art results, indicating its robustness to unseen entities.\",\n    \"Answer\": \"\\\\sys\\\\ performs the best when the percentage of unseen information in the KB is high.\"\n}"
  },
  "1": {
    "question": "Why did Seq2Seq and Mem2Seq models perform poorly when the percentage of unseen entities in the knowledge base (KB) increased?",
    "ground_truth": "Seq2Seq and Mem2Seq models performed poorly because they struggled to capture the semantic representations of unseen entities. This means they couldn't understand the meaning and relationships of new restaurants introduced in the KB. As a result, they were unable to accurately identify the correct restaurant and provide its address when faced with unseen entities.",
    "selected_figures_names": [
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Figure7-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Figure9-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 5,
    "rationale": "Image 0 shows the performance of different models, including Seq2Seq and Mem2Seq, as the percentage of unseen entities increases. Text 5 explains that Mem2Seq's flat memory prevents it from capturing context, leading to performance deterioration with more unseen information.",
    "answer": "Seq2Seq and Mem2Seq models performed poorly with increased unseen entities because Mem2Seq's flat memory structure fails to capture context, and Seq2Seq+Copy struggles with long input sequences, leading to a drop in performance.",
    "text_chunks": [
      "From Figure we observe that \\sys\\ remains immune to any variablity in the KB content, whereas the performance of Mem2Seq and Seq2Seq models drops drastically due to their inability to capture semantic representations of the injected KB entities. We see a similar trend in Figure , but here all the models show a drop in performance, with \\sys\\ appearing the most steady. We explain this trend using the example dialog in Table .",
      "Table contains only unseen entities. This example highlights the shortcomings of the Seq2Seq model as it ends up predicting a restaurant encountered during training. Mem2Seq copies a restaurant name without learning to sort the restaurants based on rating.",
      "Mem2Seq shows the best BLEU score performance on the original test set, but its performance drop of 42.5\\%, from 10.3 at 0\\% unseen to 5.93 at 100\\% unseen, is a lot heavier than that of \\sys\\ which only drops 7.6\\% -- 8.27 at 0\\% unseen to 7.64 at 100\\% unseen. \\noindent Human Evaluation: We summarize the human evaluation results for real-world datasets on the 50\\% unseen KA test set in Table .",
      "Examples from KA test set with percentage of unseen entites set to 100 from bAbI dialog Task 1 is shown in Table . Dataset Preprocessing and Faults Mem2Seq Preprocessing Mem2Seq paper used the following pre-processing on the data: The pre-processing has major impact on the performance of Mem2Seq, as it can only copy objects of a KB tuple, while the subject and relation can never be copied.",
      "For instance, Mem2Seq exhibits satisfactory performance when tested on the training KB. It represents the dialog history and the KB knowledge as a bag of words in a flat memory arrangement. This enables Mem2Seq to revisit each word several times, as needed, obtaining good performance.",
      "But at the same time, flat memory prevents it from capturing any surrounding context -- this deteriorates its performance rapidly when the amount of new unseen information in the KB increases, as shown in Figure . On the other hand, the performance of copy augmented sequence-to-sequence network (Seq2Seq+Copy) , is robust to changes in the KB, but fails to achieve acceptable task-oriented performance. It captures context by representing the entire dialog history as one continuous sequence.",
      "\\sys\\ again outperforms the baselines and is labeled successful twice more often than the next best model on both Camrest and SMD. Seq2Seq appears to produce better sentence structures on the SMD dataset, primarily because it does not attempt to learn inference on the KB, allowing it to solely focus on learning the language model better. \\end{figure* \\end{figure*",
      "\\sys\\ has the best performance with even a slight increase in both BLEU and Entity F1 metrics as more OOV content is injected in the dialog, probably because it is clear that it needs to copy when processing unseen entities. Seq2Seq+Copy is unable to perform well in CamRest as the length of the input (dialog history + KB tuples) is long and the size of the training set is also small. We believe that Seq2Seq+Copy works best in an environment with an abundance of short dialog training data (e.g., bAbI task 1 in Figure )."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 5,\n    \"Rationale\": \"Image 0 shows the performance of different models, including Seq2Seq and Mem2Seq, as the percentage of unseen entities increases. Text 5 explains that Mem2Seq's flat memory prevents it from capturing context, leading to performance deterioration with more unseen information.\",\n    \"Answer\": \"Seq2Seq and Mem2Seq models performed poorly with increased unseen entities because Mem2Seq's flat memory structure fails to capture context, and Seq2Seq+Copy struggles with long input sequences, leading to a drop in performance.\"\n}"
  },
  "2": {
    "question": "Which model achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset?",
    "ground_truth": "The BOSSNET model achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset.",
    "selected_figures_names": [
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table3-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Table11-1.png"
    ],
    "chosen_image_idx": 1,
    "chosen_text_idx": 0,
    "rationale": "Image 1 provides a direct comparison of informativeness and grammatical correctness scores for different models on the CamRest dataset. Text 0 supports this by stating that \\sys\\ has the highest average grammatical correctness score and is judged useful 77 times out of 100.",
    "answer": "BoSSNET achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset.",
    "text_chunks": [
      "\\sys\\ shows the best performance on Camrest, and is judged useful 77 times out of 100. Also, it has the highest average grammatical correctness score of 2.28 (very close to Seq2Seq and Mem2Seq). \\sys\\ performs on par with Mem2Seq and Seq2Seq in its ability to relay appropriate information to solve SMD dialog tasks, and has a slightly higher grammaticality score.",
      "\\noindent CamRest is a human-human dialog dataset, collected using the Wiz-of-Oz framework, also aimed at restaurant reservation. It is typically used to evaluate traditional slot filling systems. In order to make it suitable for end-to-end learning, we stripped the handcrafted state representations and annotations in each dialog, and divided the 676 available dialogs into train, validation, and test sets (406, 135, and 135 dialogs, respectively).",
      "It obtains significant improvements on average of about 34\\% per-dialog accuracy and 10\\% per-response accuracy for the bAbI OOV test sets. In Table , we report results on the real-world datasets. \\sys\\ greatly outperforms other models in both Entity F1 metric and BLEU scores on CamRest.",
      "\\sys\\ again outperforms the baselines and is labeled successful twice more often than the next best model on both Camrest and SMD. Seq2Seq appears to produce better sentence structures on the SMD dataset, primarily because it does not attempt to learn inference on the KB, allowing it to solely focus on learning the language model better. \\end{figure* \\end{figure*",
      "We will also make our end-to-end reconstruced Camrest dataset along with our whole batch of knowledge adaptability test sets available. Example Predictions of \\sys\\ and Baselines Examples from SMD is shown in Table respectively. Examples from KA test set with percentage of unseen entites set to 50 from CamRest and SMD are shown in Table and Table respectively.",
      "\\sys\\ has the best performance with even a slight increase in both BLEU and Entity F1 metrics as more OOV content is injected in the dialog, probably because it is clear that it needs to copy when processing unseen entities. Seq2Seq+Copy is unable to perform well in CamRest as the length of the input (dialog history + KB tuples) is long and the size of the training set is also small. We believe that Seq2Seq+Copy works best in an environment with an abundance of short dialog training data (e.g., bAbI task 1 in Figure ).",
      "We perform evaluations over three datasets -- bAbI , CamRest , and Stanford Multi-Domain Dataset . Of these, the last two are real-world datasets. We find that \\sys\\ is competitive or significantly better on standard metrics in all datasets as compared to state-of-the-art baselines.",
      "Note) the sentences were not asked to be rated with respect to each other, but instead as individual occurrences. A total of 200 such annotations were collected for Camrest and SMD each. Multi-Hop vs 1-Hop Encoders Table shows the performance of bAbI tasks and CamRest on two \\sys\\ encoder settings."
    ],
    "raw_gpt": "{\n    \"Image\": 1,\n    \"Text\": 0,\n    \"Rationale\": \"Image 1 provides a direct comparison of informativeness and grammatical correctness scores for different models on the CamRest dataset. Text 0 supports this by stating that \\\\sys\\\\ has the highest average grammatical correctness score and is judged useful 77 times out of 100.\",\n    \"Answer\": \"BoSSNET achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset.\"\n}"
  },
  "3": {
    "question": "Can you explain why the BOSSNET with multi-hop encoder performs better on bAbI tasks 3 and 5 compared to the 1-hop encoder, and how this relates to the tasks themselves?",
    "ground_truth": "The multi-hop encoder performs better on bAbI tasks 3 and 5 because these tasks specifically require inferencing over multiple KB tuples. In other words, the model needs to \"hop\" between different pieces of information in the knowledge base to make the correct inferences and recommendations.\n\nTask 3 involves sorting restaurants by rating, and task 5 requires recommending a restaurant based on user preferences. Both tasks necessitate the model to consider various restaurant attributes and their relationships, which the multi-hop encoder facilitates by capturing longer-range dependencies within the knowledge base.",
    "selected_figures_names": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Table12-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Table14-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 0,
    "rationale": "Image 2 provides information on the hyperparameters used for BOSSNET, including the number of hops, which is relevant to understanding the performance differences. Text 0 discusses the role of multi-hops in improving performance on bAbI tasks 3 and 5, which directly addresses the question.",
    "answer": "BOSSNET with a multi-hop encoder performs better on bAbI tasks 3 and 5 because these tasks require inferencing over knowledge base tuples, such as sorting restaurants by rating. The multi-hop mechanism allows the model to perform more complex reasoning, which is necessary for these tasks, leading to improved performance compared to a 1-hop encoder.",
    "text_chunks": [
      "Multi-hops in encoder helps in bAbI task 3 and 5, as they require inferencing over the KB tuples (sorting restaurants by rating) to recommend a restaurant. We also see substantial improvements on CamRest in both BLEU and entity F1 metric.",
      "Note) the sentences were not asked to be rated with respect to each other, but instead as individual occurrences. A total of 200 such annotations were collected for Camrest and SMD each. Multi-Hop vs 1-Hop Encoders Table shows the performance of bAbI tasks and CamRest on two \\sys\\ encoder settings.",
      "This result is expected as these models are trained to retrieve from a pre-defined set of responses. Their poor non-OOV performance on tasks 3 and 4 is attributed to an error in the bAbI dataset construction, due to which, the non-OOV and OOV test conditions are the same for these tasks (see Appendix). A simple generative model (Seq2Seq) achieves accuracies comparable to the multi-hop retrieval models.",
      "bAbI Dataset Faults The KB entities present in validation and non-OOV test sets for task 3 and 4 do not overlap with those in the train set. This effectively means that non-OOV and OOV test conditions are the same for tasks 3 and 4. This explains the low performance of baseline models on task 3 and 4 non-OOV test sets.",
      "Experimental Results Our experiments evaluate three research questions. Performance Study Table reports the per-response and per-dialog (in parentheses) accuracies on the bAbI dialog tasks. The multi-hop retrieval-based models such as QRN, MN and GMN perform well on the non-OOV test sets for tasks 1, 2, and 5, but fail to exhibit similar performance on the corresponding OOV test sets.",
      "Enabling it with the ability to copy from the context (Seq2Seq+Copy) shows a considerable increase in performance, especially on the OOV test sets (and non-OOV tests for tasks 3 and 4). The strong performance of simple sequence encoders when compared with multi-hop encoders (in retrieval models) raises a question about the value of multi-hop inference. Mem2Seq answers this question, by obtaining improvements in several tasks, specifically on their OOV test sets.",
      "In the example of a sample dialog from bAbI Task 3, shown in Figure , the decoder is aimed at predicting the second best restaurant 3 stars, given that the restaurant with rating 8 stars has already been suggested and rejected. We show attention only on the KB entries for brevity. The models share some similarities in their distribution of attention.",
      "Without the disentangled loss, the model sometimes learns to generate KB words. This severely affects OOV performance. As described earlier, an error in bAbI dataset construction tasks 3 and 4 effectively injects the validation set with a lot of OOVs."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 0,\n    \"Rationale\": \"Image 2 provides information on the hyperparameters used for BOSSNET, including the number of hops, which is relevant to understanding the performance differences. Text 0 discusses the role of multi-hops in improving performance on bAbI tasks 3 and 5, which directly addresses the question.\",\n    \"Answer\": \"BOSSNET with a multi-hop encoder performs better on bAbI tasks 3 and 5 because these tasks require inferencing over knowledge base tuples, such as sorting restaurants by rating. The multi-hop mechanism allows the model to perform more complex reasoning, which is necessary for these tasks, leading to improved performance compared to a 1-hop encoder.\"\n}"
  },
  "4": {
    "question": "How does the encoder understand the last user utterance?",
    "ground_truth": "The encoder understands the last user utterance by using the memory cell representations of the dialog history and KB tuples.",
    "selected_figures_names": [
      "1805.01216v3-Figure2-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table9-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 visually represents the encoder-decoder architecture, showing how the dialog history and KB tuples are stored and used. Text 1 explains the encoding process, detailing how the input is represented at two levels of abstraction, which is crucial for understanding the encoder's function.",
    "answer": "The encoder understands the last user utterance by encoding the conversational input as a bag of sequences (BoSs) memory, using memory cell representations to process the dialog history and KB tuples.",
    "text_chunks": [
      "The encoder in \\citeauthor{sukhbaatar2015end (\\citeyear{sukhbaatar2015end) uses two different embedding matrices, whereas we use just one to reduce the number of parameters. The encoder considers the last user utterance as the query q=\\psi(c_n^u) and computes the reduced representation q_r using the memory M as follows: where W_r, W_o \\in \\mathbb{R^{d \\times d are learnable parameters. The hop step can be re-iterated, by assigning the output of the previous hop as the new input query, i.e., setting q=q_r.",
      "First, it encodes the conversational input as a {\\em bag of sequences (\\textsc{BoSs) memory, in which the input representation is built at two levels of abstraction. The higher level flat memory encodes the KB tuples and utterances to facilitate effective inferencing over them. The lower level encoding of each individual utterance and tuple is constructed via a sequence encoder (Bi-GRU).",
      "We discuss end-to-end approaches along two verticals: 1) decoder: whether the response is retrieved or generated and 2) encoder: how the dialog history and KB tuples are encoded. Most of the existing end-to-end approaches {\\em retrieve a response from a pre-defined set . These methods are generally successful when they have to provide boilerplate responses -- they cannot construct responses by using words in KB not seen during training.",
      "The output of the encoder after K hops, q_r^k, is assigned as the initial state of the \\sys\\ decoder. The \\sys\\ Decoder \\sys\\ models a copy-augmented sequence decoder, which generates the response one word at a time. At any decode time step t, the decoder can either generate a word from the decode vocabulary or copy a word from the memory.",
      "\\in [0,1] as in \\citeauthor{see2017get (\\citeyear{see2017get). g_s^t is a function of the decoder state at time t and the word decoded in the previous time step. Loss The decoder is trained using cross-entropy loss.",
      "Other approaches represent the dialog context as a bag. Original Memory Networks and its extensions encode each memory element (utterance) as an average of all constituent words -- this cannot point to individual words, and hence cannot be used with a copy mechanism. Mem2Seq encodes each word individually in a flat memory.",
      "Overall, our contributions are: We release our code and {\\em knowledge adaptability (KA) test sets for further use by the research community. The \\sys\\ Architecture The proposed Bag-of-Sequences Memory Network has an encoder-decoder architecture that takes as input (1) dialog history, which includes a sequence of previous user utterances \\{c_1^u, \\ldots, c_{n^u\\ and system responses \\{c_1^s, \\ldots, c_{n-1^s\\, and (2) KB tuples \\{kb_1, \\ldots, kb_{N\\. The network then generates the next system response c_n^s=\\langle y_1 y_2 \\ldots y_T \\rangle word-by-word.",
      "as an ordered sequence of tokens \\langle w^1_i w^2_i \\ldots w^{|m_i|_i\\rangle. For an utterance, the word tokens are followed by a temporal indicator and a speaker indicator \\{\\u, \\s\\. For example, \\{\\texttt{good, morning, \\#1, \\s\\\\ indicates this was the first utterance by the system."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 visually represents the encoder-decoder architecture, showing how the dialog history and KB tuples are stored and used. Text 1 explains the encoding process, detailing how the input is represented at two levels of abstraction, which is crucial for understanding the encoder's function.\",\n    \"Answer\": \"The encoder understands the last user utterance by encoding the conversational input as a bag of sequences (BoSs) memory, using memory cell representations to process the dialog history and KB tuples.\"\n}"
  },
  "5": {
    "question": "Which model performs best on tasks T3 and T3-OOV in terms of per-dialog accuracy, and how does its performance differ between the two test sets?",
    "ground_truth": "The proposed system model (BOSSNET) performs best on both tasks T3 and T3-OOV in terms of per-dialog accuracy. However, its performance is significantly higher on the T3-OOV test set (95.7%) compared to the non-OOV T3 test set (95.2%).",
    "selected_figures_names": [
      "1805.01216v3-Table1-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Figure9-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 provides the per-dialog accuracies for tasks T3 and T3-OOV, which directly answers the question. Text 2 explains the importance of multi-hop inference and the copy mechanism, which are relevant to understanding performance differences between non-OOV and OOV test sets.",
    "answer": "The model BOSSNET performs best on tasks T3 and T3-OOV in terms of per-dialog accuracy, with scores of 63.8 and 66.6 respectively. The performance is slightly better on the T3-OOV test set compared to the T3 test set.",
    "text_chunks": [
      "Experimental Results Our experiments evaluate three research questions. Performance Study Table reports the per-response and per-dialog (in parentheses) accuracies on the bAbI dialog tasks. The multi-hop retrieval-based models such as QRN, MN and GMN perform well on the non-OOV test sets for tasks 1, 2, and 5, but fail to exhibit similar performance on the corresponding OOV test sets.",
      "It obtains significant improvements on average of about 34\\% per-dialog accuracy and 10\\% per-response accuracy for the bAbI OOV test sets. In Table , we report results on the real-world datasets. \\sys\\ greatly outperforms other models in both Entity F1 metric and BLEU scores on CamRest.",
      "This clearly shows that multi-hop inference and the copy mechanism are essentials for task-oriented dialogs. Despite gains from the Mem2Seq model, the performance difference between the non-OOV and OOV test sets remains large. \\sys\\ succeeds to bridge this gap with its ability to better interpret unseen words, using their surrounding context.",
      "bAbI Dataset Faults The KB entities present in validation and non-OOV test sets for task 3 and 4 do not overlap with those in the train set. This effectively means that non-OOV and OOV test conditions are the same for tasks 3 and 4. This explains the low performance of baseline models on task 3 and 4 non-OOV test sets.",
      "This result is expected as these models are trained to retrieve from a pre-defined set of responses. Their poor non-OOV performance on tasks 3 and 4 is attributed to an error in the bAbI dataset construction, due to which, the non-OOV and OOV test conditions are the same for these tasks (see Appendix). A simple generative model (Seq2Seq) achieves accuracies comparable to the multi-hop retrieval models.",
      "Hence, the performance on OOV test sets is much inferior compared to the non-OOV setting. Overall, we notice that combining all three model elements is necessary in obtaining the best performance across all tasks. Qualitative Evaluation We qualitatively compare the performance of \\sys\\ with other baselines using examples.",
      "Knowledge Adaptability (KA) Test Sets Each bAbI dialog task has an additional OOV test set, which helps to evaluate a model's robustness to change in information in the KB. A model that perfectly disentangles language and knowledge should have no drop in accuracy on the OOV test set when compared to the non-OOV test set. To measure the degree of disentanglement in a model, we generated 10 additional test sets for each real-world corpus by varying the percentage (in multiples of 10) of unseen entities in the KB.",
      "\\sys\\ has the best performance with even a slight increase in both BLEU and Entity F1 metrics as more OOV content is injected in the dialog, probably because it is clear that it needs to copy when processing unseen entities. Seq2Seq+Copy is unable to perform well in CamRest as the length of the input (dialog history + KB tuples) is long and the size of the training set is also small. We believe that Seq2Seq+Copy works best in an environment with an abundance of short dialog training data (e.g., bAbI task 1 in Figure )."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 provides the per-dialog accuracies for tasks T3 and T3-OOV, which directly answers the question. Text 2 explains the importance of multi-hop inference and the copy mechanism, which are relevant to understanding performance differences between non-OOV and OOV test sets.\",\n    \"Answer\": \"The model BOSSNET performs best on tasks T3 and T3-OOV in terms of per-dialog accuracy, with scores of 63.8 and 66.6 respectively. The performance is slightly better on the T3-OOV test set compared to the T3 test set.\"\n}"
  },
  "6": {
    "question": "Why might the authors claim that although BOSSNET achieves a lower BLEU score than Mem2Seq on the SMD dataset, it still performs better in conveying necessary entity information?",
    "ground_truth": "While BOSSNET has a lower BLEU score than Mem2Seq on SMD, it achieves the highest Entity F1 score on that dataset. This suggests that BOSSNET is better at capturing and including the relevant entities in its responses, even though it may not have as much lexical overlap with the gold responses as Mem2Seq.",
    "selected_figures_names": [
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure2-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Table13-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 1,
    "rationale": "Image 2 provides a comparison of BLEU and Entity F1 scores, showing BOSSNET's performance. Text 1 explains that BOSSNET conveys necessary entity information effectively, despite lower BLEU scores, due to meaningful phrases with less lexical overlap.",
    "answer": "BOSSNET achieves a lower BLEU score because its responses have less lexical overlap with the gold response. However, it performs better in conveying necessary entity information by using meaningful phrases that effectively incorporate entities from the knowledge base.",
    "text_chunks": [
      "Mem2Seq shows the best BLEU score performance on the original test set, but its performance drop of 42.5\\%, from 10.3 at 0\\% unseen to 5.93 at 100\\% unseen, is a lot heavier than that of \\sys\\ which only drops 7.6\\% -- 8.27 at 0\\% unseen to 7.64 at 100\\% unseen. \\noindent Human Evaluation: We summarize the human evaluation results for real-world datasets on the 50\\% unseen KA test set in Table .",
      "On SMD, \\sys\\ achieves the best only in Entity F1. On further analysis of the generated responses we observe that \\sys\\ responses often convey the necessary entity information from the KB. However, they consist of meaningful phrases with little lexical overlap with the gold response, reducing the BLEU scores.",
      "\\sys\\ again outperforms the baselines and is labeled successful twice more often than the next best model on both Camrest and SMD. Seq2Seq appears to produce better sentence structures on the SMD dataset, primarily because it does not attempt to learn inference on the KB, allowing it to solely focus on learning the language model better. \\end{figure* \\end{figure*",
      "We noticed that the reported results in the Mem2Seq paper are not directly comparable, as they pre-processed as the object. See Appendix. training data in SMD and bAbI datasets.",
      "SMD consists of dialogs with a large KB and a highly varying response pattern. This makes it very difficult to learn the language model -- reflected in the low BLEU scores for all the systems. \\sys\\ still provides the best F1 entity score due to its ability to inference efficiently on the large KB (Figure ).",
      "From Figure we observe that \\sys\\ remains immune to any variablity in the KB content, whereas the performance of Mem2Seq and Seq2Seq models drops drastically due to their inability to capture semantic representations of the injected KB entities. We see a similar trend in Figure , but here all the models show a drop in performance, with \\sys\\ appearing the most steady. We explain this trend using the example dialog in Table .",
      "For fair comparisons, we re-run Mem2Seq on the original training datasets. For completeness we mention their reported results (with pre-processing) as Mem2Seq*. Evaluation Metrics We evaluate \\sys\\ and other models based on their ability to generate valid responses.",
      "\\sys\\ has the best performance with even a slight increase in both BLEU and Entity F1 metrics as more OOV content is injected in the dialog, probably because it is clear that it needs to copy when processing unseen entities. Seq2Seq+Copy is unable to perform well in CamRest as the length of the input (dialog history + KB tuples) is long and the size of the training set is also small. We believe that Seq2Seq+Copy works best in an environment with an abundance of short dialog training data (e.g., bAbI task 1 in Figure )."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 1,\n    \"Rationale\": \"Image 2 provides a comparison of BLEU and Entity F1 scores, showing BOSSNET's performance. Text 1 explains that BOSSNET conveys necessary entity information effectively, despite lower BLEU scores, due to meaningful phrases with less lexical overlap.\",\n    \"Answer\": \"BOSSNET achieves a lower BLEU score because its responses have less lexical overlap with the gold response. However, it performs better in conveying necessary entity information by using meaningful phrases that effectively incorporate entities from the knowledge base.\"\n}"
  },
  "7": {
    "question": "Which model performs the best in terms of Entity F1 score when the percentage of unseen entities in the response is low?",
    "ground_truth": "BoSsNet",
    "selected_figures_names": [
      "1805.01216v3-Figure5-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table14-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 7,
    "rationale": "Image 0 provides a visual comparison of Entity F1 scores across different models, including BoSsNet, Mem2Seq, Seq2Seq+Copy, and Seq2Seq, specifically showing performance at varying levels of unseen entity percentages. Text 7 highlights that \\\\sys\\\\ achieves state-of-the-art results, indicating its robustness to unseen entities. Together, they help identify which model performs best at low unseen entity percentages.",
    "answer": "BoSsNet performs the best in terms of Entity F1 score when the percentage of unseen entities in the response is low.",
    "text_chunks": [
      "On SMD, \\sys\\ achieves the best only in Entity F1. On further analysis of the generated responses we observe that \\sys\\ responses often convey the necessary entity information from the KB. However, they consist of meaningful phrases with little lexical overlap with the gold response, reducing the BLEU scores.",
      "SMD consists of dialogs with a large KB and a highly varying response pattern. This makes it very difficult to learn the language model -- reflected in the low BLEU scores for all the systems. \\sys\\ still provides the best F1 entity score due to its ability to inference efficiently on the large KB (Figure ).",
      "\\sys\\ has the best performance with even a slight increase in both BLEU and Entity F1 metrics as more OOV content is injected in the dialog, probably because it is clear that it needs to copy when processing unseen entities. Seq2Seq+Copy is unable to perform well in CamRest as the length of the input (dialog history + KB tuples) is long and the size of the training set is also small. We believe that Seq2Seq+Copy works best in an environment with an abundance of short dialog training data (e.g., bAbI task 1 in Figure ).",
      "Mem2Seq shows the best BLEU score performance on the original test set, but its performance drop of 42.5\\%, from 10.3 at 0\\% unseen to 5.93 at 100\\% unseen, is a lot heavier than that of \\sys\\ which only drops 7.6\\% -- 8.27 at 0\\% unseen to 7.64 at 100\\% unseen. \\noindent Human Evaluation: We summarize the human evaluation results for real-world datasets on the 50\\% unseen KA test set in Table .",
      "It obtains significant improvements on average of about 34\\% per-dialog accuracy and 10\\% per-response accuracy for the bAbI OOV test sets. In Table , we report results on the real-world datasets. \\sys\\ greatly outperforms other models in both Entity F1 metric and BLEU scores on CamRest.",
      "Table contains only unseen entities. This example highlights the shortcomings of the Seq2Seq model as it ends up predicting a restaurant encountered during training. Mem2Seq copies a restaurant name without learning to sort the restaurants based on rating.",
      "From Figure we observe that \\sys\\ remains immune to any variablity in the KB content, whereas the performance of Mem2Seq and Seq2Seq models drops drastically due to their inability to capture semantic representations of the injected KB entities. We see a similar trend in Figure , but here all the models show a drop in performance, with \\sys\\ appearing the most steady. We explain this trend using the example dialog in Table .",
      "\\sys\\ achieves the state of the art results on bAbI dialog dataset, outperforming existing models by 10 points or more in its OOV conditions. In the knowledge adaptability test, we find that \\sys\\ is highly robust to increasing the percentage of unseen entities at test time, suggesting a good language-knowledge disentanglement. Human evaluations show that \\sys\\ responses are highly informative and slightly more grammatical compared to baselines."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 7,\n    \"Rationale\": \"Image 0 provides a visual comparison of Entity F1 scores across different models, including BoSsNet, Mem2Seq, Seq2Seq+Copy, and Seq2Seq, specifically showing performance at varying levels of unseen entity percentages. Text 7 highlights that \\\\\\\\sys\\\\\\\\ achieves state-of-the-art results, indicating its robustness to unseen entities. Together, they help identify which model performs best at low unseen entity percentages.\",\n    \"Answer\": \"BoSsNet performs the best in terms of Entity F1 score when the percentage of unseen entities in the response is low.\"\n}"
  },
  "8": {
    "question": " \n\nWhat is the difference between the original and pre-processed SMD Navigate data? ",
    "ground_truth": " \n\nThe pre-processed SMD Navigate data combines all the properties (such as distance, address) of a point of interest (POI) into a single subject with the object being \"poi\". The original data had separate entries for each property. ",
    "selected_figures_names": [
      "1805.01216v3-Figure9-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Figure9-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table7-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "The image shows the original and pre-processed SMD Navigate data, highlighting changes in the data structure. Text 0 mentions pre-processing in the context of the Mem2Seq paper, which is relevant to understanding the changes.",
    "answer": "The original SMD Navigate data lists subjects like 'the_westin' and 'toms_house' with associated predicates and objects. The pre-processed data reorganizes this information, combining attributes like distance, traffic info, and poi_type into a single subject entry, simplifying the structure.",
    "text_chunks": [
      "We noticed that the reported results in the Mem2Seq paper are not directly comparable, as they pre-processed as the object. See Appendix. training data in SMD and bAbI datasets.",
      "\\noindent Stanford Multi-Domain Dataset (SMD) is another human-human dialog dataset collected using the Wiz-of-Oz framework. Each conversation is between a driver and an in-car assistant. The other datasets consist of dialogs from just one domain (restaurant reservation), whereas SMD consists of dialogs from multiple domains (calendar scheduling, weather information retrieval, and navigation).",
      "\\sys\\ again outperforms the baselines and is labeled successful twice more often than the next best model on both Camrest and SMD. Seq2Seq appears to produce better sentence structures on the SMD dataset, primarily because it does not attempt to learn inference on the KB, allowing it to solely focus on learning the language model better. \\end{figure* \\end{figure*",
      "SMD consists of dialogs with a large KB and a highly varying response pattern. This makes it very difficult to learn the language model -- reflected in the low BLEU scores for all the systems. \\sys\\ still provides the best F1 entity score due to its ability to inference efficiently on the large KB (Figure ).",
      "On SMD, \\sys\\ achieves the best only in Entity F1. On further analysis of the generated responses we observe that \\sys\\ responses often convey the necessary entity information from the KB. However, they consist of meaningful phrases with little lexical overlap with the gold response, reducing the BLEU scores.",
      "We will release our code and all curated datasets for further research. \\section*{Acknowledgments We thank Danish Contractor, Gaurav Pandey and Sachindra Joshi for their comments on an earlier version of this work. This work is supported by IBM AI Horizons Network grant, an IBM SUR award, grants by Google, Bloomberg and 1MG, and a Visvesvaraya faculty award by Govt.",
      "\\noindent Without BoSs Memory: This configuration uses the Bag-of-Bags (BoB) Memory rather than {\\sc BoSs memory. The BoB memory is a simplified representation, similar to the one in the original Memory Networks.",
      "The encoder in \\citeauthor{sukhbaatar2015end (\\citeyear{sukhbaatar2015end) uses two different embedding matrices, whereas we use just one to reduce the number of parameters. The encoder considers the last user utterance as the query q=\\psi(c_n^u) and computes the reduced representation q_r using the memory M as follows: where W_r, W_o \\in \\mathbb{R^{d \\times d are learnable parameters. The hop step can be re-iterated, by assigning the output of the previous hop as the new input query, i.e., setting q=q_r."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"The image shows the original and pre-processed SMD Navigate data, highlighting changes in the data structure. Text 0 mentions pre-processing in the context of the Mem2Seq paper, which is relevant to understanding the changes.\",\n    \"Answer\": \"The original SMD Navigate data lists subjects like 'the_westin' and 'toms_house' with associated predicates and objects. The pre-processed data reorganizes this information, combining attributes like distance, traffic info, and poi_type into a single subject entry, simplifying the structure.\"\n}"
  },
  "9": {
    "question": "Which model performs best when the percentage of unseen entities in the response is low?",
    "ground_truth": "BoSsNet",
    "selected_figures_names": [
      "1805.01216v3-Figure3-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides a visual comparison of model performance based on unseen entity percentage, which directly addresses the question. Text 0 discusses \\\\sys\\\\ achieving state-of-the-art results, indicating its strong performance.",
    "answer": "The model BoSsNet performs best when the percentage of unseen entities in the response is low.",
    "text_chunks": [
      "\\sys\\ achieves the state of the art results on bAbI dialog dataset, outperforming existing models by 10 points or more in its OOV conditions. In the knowledge adaptability test, we find that \\sys\\ is highly robust to increasing the percentage of unseen entities at test time, suggesting a good language-knowledge disentanglement. Human evaluations show that \\sys\\ responses are highly informative and slightly more grammatical compared to baselines.",
      "\\sys\\ has the best performance with even a slight increase in both BLEU and Entity F1 metrics as more OOV content is injected in the dialog, probably because it is clear that it needs to copy when processing unseen entities. Seq2Seq+Copy is unable to perform well in CamRest as the length of the input (dialog history + KB tuples) is long and the size of the training set is also small. We believe that Seq2Seq+Copy works best in an environment with an abundance of short dialog training data (e.g., bAbI task 1 in Figure ).",
      "From Figure we observe that \\sys\\ remains immune to any variablity in the KB content, whereas the performance of Mem2Seq and Seq2Seq models drops drastically due to their inability to capture semantic representations of the injected KB entities. We see a similar trend in Figure , but here all the models show a drop in performance, with \\sys\\ appearing the most steady. We explain this trend using the example dialog in Table .",
      "Mem2Seq shows the best BLEU score performance on the original test set, but its performance drop of 42.5\\%, from 10.3 at 0\\% unseen to 5.93 at 100\\% unseen, is a lot heavier than that of \\sys\\ which only drops 7.6\\% -- 8.27 at 0\\% unseen to 7.64 at 100\\% unseen. \\noindent Human Evaluation: We summarize the human evaluation results for real-world datasets on the 50\\% unseen KA test set in Table .",
      "Table contains only unseen entities. This example highlights the shortcomings of the Seq2Seq model as it ends up predicting a restaurant encountered during training. Mem2Seq copies a restaurant name without learning to sort the restaurants based on rating.",
      "Alternatively, generative approaches are used where the response is {\\em generated one word at a time . These approaches mitigate the unseen entity problem by incorporating the ability to copy words from the input . The copy mechanism has also found success in summarization and machine translation .",
      "We also introduce a {\\em knowledge adaptability (KA) evaluation, in which we systematically increase the percentage of previously unseen entities in the KB. We find that \\sys\\ is highly robust across all percentage levels. Finally, we also report a human-based evaluation and find that \\sys\\ responses are frequently rated higher than other baselines.",
      "This clearly shows that multi-hop inference and the copy mechanism are essentials for task-oriented dialogs. Despite gains from the Mem2Seq model, the performance difference between the non-OOV and OOV test sets remains large. \\sys\\ succeeds to bridge this gap with its ability to better interpret unseen words, using their surrounding context."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides a visual comparison of model performance based on unseen entity percentage, which directly addresses the question. Text 0 discusses \\\\\\\\sys\\\\\\\\ achieving state-of-the-art results, indicating its strong performance.\",\n    \"Answer\": \"The model BoSsNet performs best when the percentage of unseen entities in the response is low.\"\n}"
  },
  "10": {
    "question": "Which task required the highest learning rate and how does this compare to the learning rate used for CamRest?",
    "ground_truth": "Task T1 and T2 required the highest learning rate of 0.001. This is twice the learning rate used for CamRest, which was trained with a learning rate of 0.0005.",
    "selected_figures_names": [
      "1805.01216v3-Table9-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure10-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 5,
    "rationale": "Image 0 provides the hyperparameters, including learning rates, for different tasks. Text 5 mentions the datasets used, including CamRest, which is relevant for comparison.",
    "answer": "The tasks T1, T2, T3, and T4 required the highest learning rate of 0.001. In comparison, CamRest used a lower learning rate of 0.0005.",
    "text_chunks": [
      "\\sys\\ has the best performance with even a slight increase in both BLEU and Entity F1 metrics as more OOV content is injected in the dialog, probably because it is clear that it needs to copy when processing unseen entities. Seq2Seq+Copy is unable to perform well in CamRest as the length of the input (dialog history + KB tuples) is long and the size of the training set is also small. We believe that Seq2Seq+Copy works best in an environment with an abundance of short dialog training data (e.g., bAbI task 1 in Figure ).",
      "\\sys\\ shows the best performance on Camrest, and is judged useful 77 times out of 100. Also, it has the highest average grammatical correctness score of 2.28 (very close to Seq2Seq and Mem2Seq). \\sys\\ performs on par with Mem2Seq and Seq2Seq in its ability to relay appropriate information to solve SMD dialog tasks, and has a slightly higher grammaticality score.",
      "\\noindent CamRest is a human-human dialog dataset, collected using the Wiz-of-Oz framework, also aimed at restaurant reservation. It is typically used to evaluate traditional slot filling systems. In order to make it suitable for end-to-end learning, we stripped the handcrafted state representations and annotations in each dialog, and divided the 676 available dialogs into train, validation, and test sets (406, 135, and 135 dialogs, respectively).",
      "Note) the sentences were not asked to be rated with respect to each other, but instead as individual occurrences. A total of 200 such annotations were collected for Camrest and SMD each. Multi-Hop vs 1-Hop Encoders Table shows the performance of bAbI tasks and CamRest on two \\sys\\ encoder settings.",
      "Disentanglement Study We use our generated knowledge adaptability (KA) test sets to measure the robustness of \\sys\\ and the other baselines to changes in the KB. We perform this experiment on 4 different tasks, namely bAbI tasks 1 and 5, CamRest, and SMD. Figures and show the per-response accuracies of the two bAbI dialog tasks plotted against the percentage of unseen entities in KA sets.",
      "The dropout rate is also a hyper-parameter. Experimental Setup We perform experiments on three task-oriented dialog datasets: bAbI Dialog , CamRest , and Stanford Multi-Domain Dataset . \\noindent bAbI Dialog consists of synthetically generated dialogs with the goal of restaurant reservation.",
      "We will also make our end-to-end reconstruced Camrest dataset along with our whole batch of knowledge adaptability test sets available. Example Predictions of \\sys\\ and Baselines Examples from SMD is shown in Table respectively. Examples from KA test set with percentage of unseen entites set to 50 from CamRest and SMD are shown in Table and Table respectively.",
      "Multi-hops in encoder helps in bAbI task 3 and 5, as they require inferencing over the KB tuples (sorting restaurants by rating) to recommend a restaurant. We also see substantial improvements on CamRest in both BLEU and entity F1 metric."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 5,\n    \"Rationale\": \"Image 0 provides the hyperparameters, including learning rates, for different tasks. Text 5 mentions the datasets used, including CamRest, which is relevant for comparison.\",\n    \"Answer\": \"The tasks T1, T2, T3, and T4 required the highest learning rate of 0.001. In comparison, CamRest used a lower learning rate of 0.0005.\"\n}"
  },
  "11": {
    "question": "What is the difference between the attention weights in the two-level attention model and the one-level attention model?",
    "ground_truth": " The two-level attention model has higher attention weights on the relevant information in the memory, while the one-level attention model has more uniform attention weights.",
    "selected_figures_names": [
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table1-1.png"
    ],
    "all_figures_names": [
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Figure9-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Text chunk 2 discusses the ability of two-level attention to sort restaurant entries based on ratings, which is a key difference from one-level attention. Image 0 provides performance metrics for models, including those using different attention mechanisms, which supports understanding the effectiveness of these models.",
    "answer": "The two-level attention model can sort restaurant entries based on ratings, giving more weight to higher-rated entries, unlike the one-level attention model.",
    "text_chunks": [
      "First, the attention weights are localized over the restaurant names, indicating the preference of the system to point to a specific restaurant. This is supported by the g_s values, 3.14 x 10^{-5 and 1.15 x 10^{-4 for two-level attention and one-level attention respectively, i.e., both models prefer to copy rather than generate. Moreover, entries with the same restaurant name have similar attention weights, reflecting the robustness of the distribution.",
      "of India. We thank Microsoft Azure sponsorships, and the IIT Delhi HPC facility for computational resources. \\bibliography{naaclhlt2019 \\bibliographystyle{acl_natbib \\appendix Two-Level attention on BoSs Memory To visualize the benefit of two-level attention used on {\\sc BoSs memory by the decoder, we compare attention weights for two models: our proposed two-level attention and a variant with just one-level attention (over all the words in the memory).",
      "We also observe that two-level attention is able to perform the difficult task of {\\em sorting the restaurant entries based on decreasing order of rating (number of stars). It gives more weight to entries with a high rating (3 stars > 2 stars > 1 star) and suppresses the weights of any previously suggested restaurant. The attention over memory cells provides \\sys\\ with the ability to infer over multiple sets of tuples.",
      "The ability to sort the restaurants and reject a previously seen restaurant can be observed by the attention heat map of Memory cells. Attention over tokens on the other hand can push the attention weights towards either the subject or object in the KB tuple, based on the query's request. Thus using both in conjunction helps \\sys\\ perform significantly better than the baselines and illustrates the importance of the {\\sc BoSs memory in comparison to a flat memory layout.",
      "Table contains only unseen entities. This example highlights the shortcomings of the Seq2Seq model as it ends up predicting a restaurant encountered during training. Mem2Seq copies a restaurant name without learning to sort the restaurants based on rating.",
      "In the example of a sample dialog from bAbI Task 3, shown in Figure , the decoder is aimed at predicting the second best restaurant 3 stars, given that the restaurant with rating 8 stars has already been suggested and rejected. We show attention only on the KB entries for brevity. The models share some similarities in their distribution of attention.",
      "Note) the sentences were not asked to be rated with respect to each other, but instead as individual occurrences. A total of 200 such annotations were collected for Camrest and SMD each. Multi-Hop vs 1-Hop Encoders Table shows the performance of bAbI tasks and CamRest on two \\sys\\ encoder settings.",
      "But at the same time, flat memory prevents it from capturing any surrounding context -- this deteriorates its performance rapidly when the amount of new unseen information in the KB increases, as shown in Figure . On the other hand, the performance of copy augmented sequence-to-sequence network (Seq2Seq+Copy) , is robust to changes in the KB, but fails to achieve acceptable task-oriented performance. It captures context by representing the entire dialog history as one continuous sequence."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Text chunk 2 discusses the ability of two-level attention to sort restaurant entries based on ratings, which is a key difference from one-level attention. Image 0 provides performance metrics for models, including those using different attention mechanisms, which supports understanding the effectiveness of these models.\",\n    \"Answer\": \"The two-level attention model can sort restaurant entries based on ratings, giving more weight to higher-rated entries, unlike the one-level attention model.\"\n}"
  }
}