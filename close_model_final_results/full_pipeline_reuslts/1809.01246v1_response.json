{
  "0": {
    "question": "How does the square hashing process work?",
    "ground_truth": "Square hashing is a process that uses two hash functions to map a source/destination pair to a bucket in a two-dimensional array. The first hash function, h_i(s), maps the source address to a row in the array, and the second hash function, h_i(d), maps the destination address to a column in the array. The intersection of the row and column is the bucket where the fingerprint is stored.",
    "selected_figures_names": [
      "1809.01246v1-Figure5-1.png"
    ],
    "all_figures_names": [
      "1809.01246v1-TableI-1.png",
      "1809.01246v1-Figure5-1.png",
      "1809.01246v1-Figure13-1.png",
      "1809.01246v1-Figure11-1.png",
      "1809.01246v1-Figure10-1.png",
      "1809.01246v1-Figure9-1.png",
      "1809.01246v1-Figure1-1.png",
      "1809.01246v1-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 visually represents the concept of square hashing with mapped, occupied, and empty buckets, which is crucial for understanding the process. Text 3 explains the process of inserting an edge using square hashing, including the computation of fingerprint and hash address pairs, and how buckets are checked and updated.",
    "answer": "Square hashing works by computing a fingerprint pair and a hash address pair for an edge. It then locates multiple buckets in a matrix using these pairs. If the fingerprint pair is already stored in one of the buckets, the values are updated. Otherwise, an empty bucket is found to store the pair and its attributes.",
    "text_chunks": [
      "The above results show that the decrement in buffer size brought by using square hashing and multiple rooms is significant, especially the square hashing. The results also show that the buffer percentage in the fully improved GSS (2 rooms each bucket, with square hashing) becomes 0 in most experiments when the matrix size is close to |E|. In this case, the overhead brought by the insertion failure in the matrix is nearly 0.",
      "The length of the matrix is m, and each bucket in the matrix has l rooms. For each node we compute a hash address sequence with length r. For each edge we choose k candidate buckets among the r^2 mapped buckets.",
      "\\nop{In order to store the relationships between different edges, it uses one hash function to map the two vertexes of each edge separately. Then it uses the two hash addresses as two dimensions to locate a mapped bucket in the matrix, and accumulates the attribute of the edge in the bucket. Usually the accumulation function is addition.",
      "When inserting an edge (s, d, v), it computes a fingerprint pair (f(s), f(d)) and a hash address pair (h(s), h(d)), and finds r buckets in the matrix, which is located with a novel technique called \\hashname. Then it checks these buckets. If pair (f(s), f(d)) is already stored in one of the buckets, it updates the values in this bucket with v. Otherwise, it finds an empty one to store the fingerprint pair and the attributes.",
      "In this section, we design several solutions to reduce the size of buffer B. \\presub Square Hashing \\postsub In the basic version, an edge is pushed into buffer B if and only if its mapped position in the matrix X has been occupied. The most intuitive solution is to find another bucket for it.",
      "In \\hashname, this hash address sequence of H(v) is generated with a pseudo random algorithm using both h(v) and f(v) as input. The procedure is as following: To meet the above requirements, we propose to use linear congruence method to generate a sequence of r random values \\left \\{q_i(v)|1 \\leqslant i \\leqslant r \\right \\ with f(v) as seeds. We call this sequence the linear congruential (LR) sequence for convenience.",
      "\\nop{(The following two paraphrases can be removed, something can be moved above..) The method to generate the hash address sequence is not easy to find. On one hand, we hope that the addresses in the sequence are independent with each other, thus each edge has higher probability to find an empty bucket.",
      "Multiple hash functions can be applied in TCM. In this circumstance a TCM sketch is made up of multiple matrices, and each matrix is relevant with a hash function. These matrices work separately when updating, and report the most accurate value in them when querying."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 3,\n    \"Rationale\": \"Image 0 visually represents the concept of square hashing with mapped, occupied, and empty buckets, which is crucial for understanding the process. Text 3 explains the process of inserting an edge using square hashing, including the computation of fingerprint and hash address pairs, and how buckets are checked and updated.\",\n    \"Answer\": \"Square hashing works by computing a fingerprint pair and a hash address pair for an edge. It then locates multiple buckets in a matrix using these pairs. If the fingerprint pair is already stored in one of the buckets, the values are updated. Otherwise, an empty bucket is found to store the pair and its attributes.\"\n}"
  },
  "1": {
    "question": "What is the relationship between the buffer percentage and the width of the room?",
    "ground_truth": "The buffer percentage decreases as the width of the room increases.",
    "selected_figures_names": [
      "1809.01246v1-Figure13-1.png"
    ],
    "all_figures_names": [
      "1809.01246v1-Figure12-1.png",
      "1809.01246v1-Figure6-1.png",
      "1809.01246v1-Figure8-1.png",
      "1809.01246v1-Figure11-1.png",
      "1809.01246v1-Figure1-1.png",
      "1809.01246v1-Figure13-1.png",
      "1809.01246v1-Figure2-1.png",
      "1809.01246v1-Figure5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 shows graphs of buffer percentage against width for different room configurations, which directly relates to the question. Text 0 discusses the impact of square hashing and multiple rooms on buffer size, providing context for understanding the graph.",
    "answer": "The buffer percentage decreases as the width increases, especially when using two rooms and square hashing. This indicates that larger widths and certain configurations can reduce buffer usage.",
    "text_chunks": [
      "The above results show that the decrement in buffer size brought by using square hashing and multiple rooms is significant, especially the square hashing. The results also show that the buffer percentage in the fully improved GSS (2 rooms each bucket, with square hashing) becomes 0 in most experiments when the matrix size is close to |E|. In this case, the overhead brought by the insertion failure in the matrix is nearly 0.",
      "It measures buffer size of \\fname. Buffer percentage is defined as the number of edges that the buffer contains divided by the total number of edges in the graph stream. \\presub Experiments settings \\postsub",
      "However, when the rooms in each bucket are stored separately, the speed will probably decrease, as we can not fetch the l rooms in one memory access in most cases, and multiple memory accesses increase the time cost. As shown in Fig. , we separate the bucket into 3 area: the index area, the fingerprint area, and the weight area.",
      "In \\fname, the memory cost and update speed are greatly influenced by the size of the buffer B. As the buffer takes additional memory, and update speed in an adjacency list is linear with its size. In order to restrict its size, we propose a technique called \\hashname.",
      "Each bucket in the matrix contains l=2 rooms. The length of the address sequences is r=16, and the number of candidate buckets for each edge is k=16 (r=8,k=8 for the small data set \\dataseta and \\datasetb). As for TCM, we apply 4 graph sketches to improve its accuracy, and allow it to use larger memory, because otherwise the gap between it and \\fname\\ will be so huge that we can hardly compare them in one figure.",
      "We call each segment a room for convenience. When performing the basic operators, we use the same process as above the find the buckets we need to check, and search all the rooms in them to find qualified edges or empty rooms. \\nop{This modification enables each bucket to contain multiple edges, and decreases the size of the buffer further.",
      "We can also see that the accuracy of TCM is so poor that it can barely support this query. \\subfigure[\\datasetd]{ \\subfigure[\\datasete]{ \\caption{Buffer Percentage \\end{figure* \\presub Experiments on Buffer Size \\postsub Figure shows the buffer percentage for the three larger data sets \\datasetc,\\datasetd and \\datasete.",
      "We choose the k buckets with address \\{h_i(s)\\ and \\{h_i(d)\\ are the hash address sequence of the source node and the destination node, respectively. \\subsubsection{Multiple Rooms When the memory is sufficient, we do not need to use multiple matrices to increase accuracy as TCM, as the accuracy is already very high. Instead, in order to further decrease the buffer size, we can separate each bucket in the matrix into l segments, and each segments contains an edge, including the weight, the fingerprint pair and the index pair."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 shows graphs of buffer percentage against width for different room configurations, which directly relates to the question. Text 0 discusses the impact of square hashing and multiple rooms on buffer size, providing context for understanding the graph.\",\n    \"Answer\": \"The buffer percentage decreases as the width increases, especially when using two rooms and square hashing. This indicates that larger widths and certain configurations can reduce buffer usage.\"\n}"
  },
  "2": {
    "question": "How are edges aggregated in the graph sketch $G_h$?",
    "ground_truth": "Edges are aggregated by adding their weights together.",
    "selected_figures_names": [
      "1809.01246v1-Figure1-1.png"
    ],
    "all_figures_names": [
      "1809.01246v1-Figure3-1.png",
      "1809.01246v1-Figure8-1.png",
      "1809.01246v1-Figure9-1.png",
      "1809.01246v1-Figure1-1.png",
      "1809.01246v1-Figure6-1.png",
      "1809.01246v1-Figure10-1.png",
      "1809.01246v1-Figure11-1.png",
      "1809.01246v1-TableI-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 5,
    "rationale": "Image 0 provides a visual representation of a graph stream, which helps in understanding how nodes and edges are structured. Text 5 explains the process of mapping nodes and edges from the original graph G to the graph sketch G_h, focusing on the aggregation of edge weights.",
    "answer": "Edges in the graph sketch G_h are aggregated by mapping each edge from the original graph G to an edge in G_h. The weight of an edge in G_h is the aggregation of the weights of all edges mapped to it.",
    "text_chunks": [
      "Each node v in G is mapped into a hash value H(v). Nodes with the same hash value are combined into one node in G_h, and the edges connected to them are also aggregated. An example of the graph stream and the graph sketch can be referred in Fig. and Fig..",
      "The gSketch reduces their relative errors by classifying the edges with their weights and storing those with similar values together. The gSketch assumes that edges emitted by the same nodes has similar weights. It uses a small sampling of the data stream to predict of weights of edges emitted from different nodes, then divides a CM sketch into several small local sketches with different sizes, and maintains a hash table that maps the vertexes in the graph into different local sketches according to their predicted edge weights.",
      "Edges in the graph is maintained and queried in the local sketch their source vertexes are mapped to. As Only edges with similar weights will have hash collisions, the relative error of the gSketch is much smaller than the CM sketch. But like the CM sketch, it only stores the weights of edges but not any information about the topology of the graph.",
      "First, edge \\overrightarrow{H(s),H(d) has been in the graph sketch G_h already, we only update the edge weight. Second, if both nodes H(s) and H(d) have been in the graph sketch G_h but edge \\overrightarrow{H(s),H(d) does not occur, we introduce edge \\overrightarrow{H(s),H(d) with the weight w to G_h directly. Third, if either of H(s) or H(d) does not occur in V(G_h), we introduce the node and then add the edge \\overrightarrow{H(s),H(d) with the weight w into G_h.",
      "V and V_h are the sets of nodes in graph G and G_h, respectively, and E and E_h are the sets of edges in graph G and G_h, respectively. The second part is to design a data structure DS to store G_h. This algorithm should satisfy the following requirements: \\nop{ \\end{Def \\nop{\\begin{figure*[htbp] \\subfigure[Edge Query]{ \\subfigure[1-hop Successor Query]{ \\subfigure[1-hop Precursor Query]{ \\caption{Theoretical Accuracy \\end{figure*",
      "For each node v in G, TCM maps it to node H(v) in G_h. For each edge e=\\overrightarrow{s,d in G, TCM maps it to edge \\overrightarrow{H(s),H(d) in G_h. The weight of an edge in G_h is an aggregation of the weights of all edges mapped to it.",
      "There has been several prior works on graph stream summarization. CM sketches and gSketches store each edge in the graph stream as an independent data item. They have O(|E|) memory usage and O(1) update time, but only support queries for the weights of edges.",
      "In graph stream summarization, updates and queries to G are all mapped to operations over the compressed graph G_h and then are conducted using the data structure DS. In the context of streaming graphs, G changes with every data item in the graph stream S, which is mapped to updating the graph sketch G_h, and conducted in data structure DS. For every new item (\\overrightarrow{s,d;t;w) in S, we map edge \\overrightarrow{s,d in G to edge \\overrightarrow{H(s),H(d) in G_h with weight w and then insert it into G_h."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 5,\n    \"Rationale\": \"Image 0 provides a visual representation of a graph stream, which helps in understanding how nodes and edges are structured. Text 5 explains the process of mapping nodes and edges from the original graph G to the graph sketch G_h, focusing on the aggregation of edge weights.\",\n    \"Answer\": \"Edges in the graph sketch G_h are aggregated by mapping each edge from the original graph G to an edge in G_h. The weight of an edge in G_h is the aggregation of the weights of all edges mapped to it.\"\n}"
  },
  "3": {
    "question": "How does the Average Relative Error (ARE) of node queries change as the width increases for different configurations of GSS and TCM?",
    "ground_truth": "The ARE of node queries generally decreases as the width increases for all configurations of GSS and TCM. However, there are some fluctuations in the ARE for some configurations.",
    "selected_figures_names": [
      "1809.01246v1-Figure11-1.png"
    ],
    "all_figures_names": [
      "1809.01246v1-Figure4-1.png",
      "1809.01246v1-Figure1-1.png",
      "1809.01246v1-Figure12-1.png",
      "1809.01246v1-Figure6-1.png",
      "1809.01246v1-Figure8-1.png",
      "1809.01246v1-TableI-1.png",
      "1809.01246v1-Figure11-1.png",
      "1809.01246v1-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 shows the Average Relative Error (ARE) of node queries across different datasets and configurations, which directly relates to the question. Text 3 explains how the ARE decreases and precision increases with the growth of the matrix width, particularly highlighting the performance of GSS compared to TCM.",
    "answer": "As the width increases, the Average Relative Error (ARE) of node queries decreases for both GSS and TCM configurations. GSS shows a more significant decrease in ARE, maintaining higher accuracy compared to TCM, which has higher errors even with increased width.",
    "text_chunks": [
      "Average Relative Error (ARE): ARE measures the accuracy of the reported weights in edge queries and node queries. Given a query q, the relative error is defined as \\[ \\] where f(q) and \\hat{f(q) are the real answer and the estimated value of q. When giving a query set, the average relative error (ARE) is measured by averaging the relative errors over all queries int it. A more accuracy data structure will have smaller ARE.",
      "We use a matrix with side length m=1000. Then the correct rate of this edge query is e^{-0.00078=0.9992. On the other hand, in TCM the accuracy analysis is the same as \\fname\\, but we have M=m. This lead to the difference on accuracy with the same size of matrix.",
      "This means the graph sketch G_h is usually much smaller than G, a lot of nodes and edges will be aggregated. As a result, the accuracy of TCM is poor. \\nop{However, TCM is not accurate enough for topology queries, like reachability query.",
      "The results tell us that \\fname\\ performs much better in supporting these query primitives than TCM, especially in the 1-hop precursor / successor query primitives. In both \\fname\\ and TCM, the ARE decreases, and the precision increases with the growth of the width of the matrix. This trend is not very significant in \\fname\\ as the accuracy is high and there are no errors in most experiments.",
      "( In \\datasete, we implement TCM with 16 times of memory rather than 256 times in the 1-hop successor/precursor query because of the limitation of the host memory). Figure , Figure , and Figure show that ARE of edge queries and average precision of 1-hop precursor / successor queries for the data sets, respectively. To reduce random error introduced by the selection of the data sample, the edge query set contains all edges in the graph stream, and the 1-hop precursor / successor query set contains all nodes in the graph stream.",
      "However, the accuracy of TCM in reachability queries is poor. It has no false negatives, but high false positives. Moreover, when the node degrees in the graph is highly skewed, the accuracy of other queries is also not good.",
      "\\nop{In TCM, an adjacency matrix is built with side length M. Each row/column in the adjacency matrix is corresponding to a hash value H(v), and stores edges emitted/received by the node H(v) in G(h). In order to achieve low compression rate and high accuracy, the value range M has to be large, leading to a matrix too large to store. On the other hand, as G is usually very sparse,most nodes have much fewer edges than m. Therefore when storing the graph sketch in \\fname, As the graph is sparse, the number of nodes is large, but each node is connected to few edges.",
      "The main reason for the inaccuracy of TCM is that there are usually millions of nodes in the graph stream, but in order to maintain memory efficiency, the side length m of the matrix we build is much smaller than the number of nodes, usually no more than 10000. when we map the nodes into addresses in range [0, m), many nodes will get the same address. While in the matrix of TCM, we can not tell the nodes mapped to the same address apart when querying."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 3,\n    \"Rationale\": \"Image 0 shows the Average Relative Error (ARE) of node queries across different datasets and configurations, which directly relates to the question. Text 3 explains how the ARE decreases and precision increases with the growth of the matrix width, particularly highlighting the performance of GSS compared to TCM.\",\n    \"Answer\": \"As the width increases, the Average Relative Error (ARE) of node queries decreases for both GSS and TCM configurations. GSS shows a more significant decrease in ARE, maintaining higher accuracy compared to TCM, which has higher errors even with increased width.\"\n}"
  },
  "4": {
    "question": "What is the relationship between the table and the graph sketch in the figure?",
    "ground_truth": "The table provides the mapping between the nodes in the original graph and their corresponding hash values, which are used to create the graph sketch.",
    "selected_figures_names": [
      "1809.01246v1-Figure2-1.png"
    ],
    "all_figures_names": [
      "1809.01246v1-Figure4-1.png",
      "1809.01246v1-Figure9-1.png",
      "1809.01246v1-Figure11-1.png",
      "1809.01246v1-Figure1-1.png",
      "1809.01246v1-Figure5-1.png",
      "1809.01246v1-Figure12-1.png",
      "1809.01246v1-TableI-1.png",
      "1809.01246v1-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 5,
    "rationale": "Image 0 shows a graph sketch with nodes and their mappings, which is directly related to the question. Text 5 explains how nodes with high degrees in the original graph are represented in the graph sketch, which helps understand the relationship between the table and the graph sketch.",
    "answer": "The table maps nodes from the original graph to nodes in the graph sketch. The graph sketch shows the connections and weights between these mapped nodes. Nodes with high degrees in the original graph will have high degrees in the graph sketch as well, as seen with node 'a' mapped to node '2'.",
    "text_chunks": [
      "Edges in the graph is maintained and queried in the local sketch their source vertexes are mapped to. As Only edges with similar weights will have hash collisions, the relative error of the gSketch is much smaller than the CM sketch. But like the CM sketch, it only stores the weights of edges but not any information about the topology of the graph.",
      "Compared to the CM sketch, gSketch is specially proposed for graph streams. It aims to improve the accuracy of the CM sketch when applies to graph streams. As in CM sketches when items with small weights collide with those with large weights, in other words, mapped to the same counter, the relative error of the smaller ones will be high.",
      "Each bucket in the matrix contains a counter. The weight of edge \\overrightarrow{H(s),H(d) in the graph sketch is added to the counter in the bucket in row H(s), column H(d). When the memory is sufficient, we can also build multiple sketches with different hash functions, and report the most accurate value in queries.",
      "A hash table that stores the hash value and the original ID pairs can be built in this map procedure to retrieve the original node IDs for some queries. Then TCM uses an adjacency matrix to represent the graph sketch. If we represent the size of the value range of H(\\cdot) with M, we need to build an M\\times M adjacency matrix.",
      "For example, in Figure , node a and node f are mapped to different nodes in the graph sketch, and they have the same fingerprint but different addresses. In the data structure the edges connected to them are stored in different areas, thus we will not mix them up. On the other hand, node a and node c have the same address but different fingerprints.",
      "For a node v with high degree in the streaming graph G, node H(v) in the graph sketch G_h will have higher degrees, and row h(v) and column h(v) will be very crowded. For example, in graph G in Fig., the out degree of node a, in other words, the number of edges with source node a, is 4, much higher than other nodes. In G_h in Fig. , node 2 which a is mapped to also has out degree 4.",
      "After computation, we find edge (a, f) is mapped to edge (2, 18) in the graph sketch G_h, and insert it into the matrix. The hash address sequence for node 2 is {1, 0, and the hash address sequence for node 18 is {3, 2. Therefore we first check bucket in row 1, column 3.",
      "Given a bucket in row R and column C and the content in it, we are able to recover the representation of the edge e in the graph sketch G_h: \\overrightarrow{H(s), H(d), where e is the edge in that bucket. This property of indexing is needed in the 1-hop successor query and the 1-hop precursor query. As in these queries, we need to check the potential buckets to see if they contain edges connected to the queried node v and retrieve the other end point in each qualified bucket."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 5,\n    \"Rationale\": \"Image 0 shows a graph sketch with nodes and their mappings, which is directly related to the question. Text 5 explains how nodes with high degrees in the original graph are represented in the graph sketch, which helps understand the relationship between the table and the graph sketch.\",\n    \"Answer\": \"The table maps nodes from the original graph to nodes in the graph sketch. The graph sketch shows the connections and weights between these mapped nodes. Nodes with high degrees in the original graph will have high degrees in the graph sketch as well, as seen with node 'a' mapped to node '2'.\"\n}"
  },
  "5": {
    "question": "Which graph shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method?",
    "ground_truth": "The graph for the Caida-networkflow dataset shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method.",
    "selected_figures_names": [
      "1809.01246v1-Figure8-1.png"
    ],
    "all_figures_names": [
      "1809.01246v1-Figure3-1.png",
      "1809.01246v1-Figure9-1.png",
      "1809.01246v1-Figure6-1.png",
      "1809.01246v1-Figure13-1.png",
      "1809.01246v1-Figure1-1.png",
      "1809.01246v1-Figure10-1.png",
      "1809.01246v1-Figure8-1.png",
      "1809.01246v1-TableI-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 provides visual data on the Average Relative Error (ARE) of edge queries for different datasets, which is directly relevant to comparing the accuracy of TCM and GSS methods. Text 3 discusses the performance comparison between TCM and \\fname\\ (GSS), highlighting that \\fname\\ achieves better performance even when TCM uses significantly more memory.",
    "answer": "The graph for 'Caida-networkflow' (e) shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method, as indicated by the larger gap in ARE values.",
    "text_chunks": [
      "From the figures we can see that the accuracy of \\fname\\ is much higher than TCM even when TCM uses much larger memory. The gap varies with the size of the graph. Along with increasing the memory and the length of the fingerprint, \\fname\\ can achieve better performance.",
      "In TCM, we need to build a {M^2=32 \\times 32 matrix to store the graph sketch G_h in Fig . While in \\fname, because it is actually a 2-dimension hash table to store the edges in the compressed graph G_h, we only need O(|E_h|) memory. Even if each bucket is larger than TCM, as O(|E_h|)<<O(|V_h|)^2 in large graphs, \\fname\\ still need much smaller memory to get the same accuracy.",
      "This enables us to apply a map function with a much larger value range, and generate a much larger graph sketch with the same size of matrix as TCM. With a 4\\times 4 matrix as in Figure , TCM can only support a map function with M=4 , and the number of nodes in the graph sketch will be no more than 4, thus the accuracy will he much poorer. \\nop{This technique makes the memory more efficiently used.",
      "\\nop{As TCM has very poor accuracy in topology queries, to make TCM comparable with \\fname, we fixed the memory of TCM 256 times as large as \\fname\\ in the first 2 datasets. In the third dataset we fixed it to 16 times because of the limitation of the memory of the server. The figure shows that although we unfairly fix the ratio of memory used by TCM and \\fname, \\fname\\ still can achieve better performance than TCM.",
      "As the value range of the fingerprints is large, the number of mixed up nodes will be much smaller than TCM. Therefore the accuracy will be much higher. Like TCM, \\fname\\ uses a hash function H(\\cdot) to compress the streaming graph G into a smaller graph G_h which is named a graph sketch.",
      "The number of memory accesses are nearly constant. On the other hand, the matrix we build is much smaller than the adjacency matrix, usually no greater than 1500 \\times 1500. By introducing fingerprints, \\fname{ achieves much higher accuracy than TCM, especially in topology queries like reachability query.",
      "( In \\datasete, we implement TCM with 16 times of memory rather than 256 times in the 1-hop successor/precursor query because of the limitation of the host memory). Figure , Figure , and Figure show that ARE of edge queries and average precision of 1-hop precursor / successor queries for the data sets, respectively. To reduce random error introduced by the selection of the data sample, the edge query set contains all edges in the graph stream, and the 1-hop precursor / successor query set contains all nodes in the graph stream.",
      "The compression rate can be controlled by the size of the value range of H(\\cdot), which we represent with M. The higher the compression rate is, the lower the accuracy is, as more nodes and edges will be combined. Different from TCM which uses an adjacency matrix to store the graph sketch G_h, \\fname\\ uses a novel data structure to store it. This data structure is specially designed for sparse graphs and can store a much bigger graph sketch with the same space."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 3,\n    \"Rationale\": \"Image 0 provides visual data on the Average Relative Error (ARE) of edge queries for different datasets, which is directly relevant to comparing the accuracy of TCM and GSS methods. Text 3 discusses the performance comparison between TCM and \\\\fname\\\\ (GSS), highlighting that \\\\fname\\\\ achieves better performance even when TCM uses significantly more memory.\",\n    \"Answer\": \"The graph for 'Caida-networkflow' (e) shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method, as indicated by the larger gap in ARE values.\"\n}"
  },
  "6": {
    "question": "Which data structure is the fastest for updating on the email-EuAll dataset?",
    "ground_truth": "GSS (no sampling)",
    "selected_figures_names": [
      "1809.01246v1-TableI-1.png"
    ],
    "all_figures_names": [
      "1809.01246v1-TableI-1.png",
      "1809.01246v1-Figure4-1.png",
      "1809.01246v1-Figure6-1.png",
      "1809.01246v1-Figure12-1.png",
      "1809.01246v1-Figure5-1.png",
      "1809.01246v1-Figure9-1.png",
      "1809.01246v1-Figure11-1.png",
      "1809.01246v1-Figure3-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 7,
    "rationale": "Image 0 provides a table comparing update speeds for different data structures on the email-EuAll dataset. Text 7 discusses the update speed of GSS, which is relevant to determining the fastest data structure.",
    "answer": "The fastest data structure for updating on the email-EuAll dataset is TCM, with an update speed of 2.10417 MIPS.",
    "text_chunks": [
      "It is a collection of communication records in the network of the Linux kernel mailing list. It contains 63399 email addresses (nodes) and 1096440 communication records(edges). Each edge is weighted by its frequency in the data set, and has a timestamp indicating the communication time.",
      ".This data set is communication network data generated using email data from a large European research institution for a period of 18 months. Each node in the directed graph corresponds to an email address. Each edge between node src and dst represents src sent at least one email to dst.",
      "The data set contains 325729 nodes and 1497134 edges. We use the Zipfian distribution to generate weights for the edges in the data set, and insert the edges into the data structure one by one to simulate the procedure of real-world incremental updating. 4)\\datasetd.",
      "The high compression rate makes the accuracy poor. Details about these data structures will be shown in Section \\presub Our Solution \\postsub In this paper we propose \\fname, which is an approximate query data structure for graph streams with linear memory usage, high update speed, high accuracy and supports all kinds of graph queries and algorithms like .",
      "In the matrix, the update need constant time. As very few (less than 0.01\\%), even none edges are stored in the buffer, we do not need to access the buffer in most cases. Even when we need to check it, the speed is still high as there are few edges in it.",
      "The update time cost is O(k+\\frac{|B|{|E_h||B|), where k is the number of sampled buckets and is a small constant. When an edge is stored in the matrix, we only need to check at most k candidate buckets, which takes O(k) time. Each edge has probability \\frac{|B|{|E_h| to be stored in the buffer.",
      "In this paper we propose \\fname, which has small memory usage, fast and constant update speed. It supports many kinds of queries, like edge queries, node queries and reachability queries. Moreover, in all these queries it has much higher accuracy than prior art.",
      "We can see that \\fname\\ has nearly 100\\% correct rate. Both TRIEST an SJ-tree have throughput less than 2\\times 10^5 edges per second, much lower than the update speed of GSS, and high update speed is important in high speed streams. \\nop{The average throughput of SJ-tree is less than 0.187M edges per second and falls greatly as window size enlarges."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 7,\n    \"Rationale\": \"Image 0 provides a table comparing update speeds for different data structures on the email-EuAll dataset. Text 7 discusses the update speed of GSS, which is relevant to determining the fastest data structure.\",\n    \"Answer\": \"The fastest data structure for updating on the email-EuAll dataset is TCM, with an update speed of 2.10417 MIPS.\"\n}"
  },
  "7": {
    "question": "How does the average precision of TCM(256*memory) compare to the other two algorithms in the email-EuAll dataset?",
    "ground_truth": "The average precision of TCM(256*memory) is lower than the other two algorithms in the email-EuAll dataset.",
    "selected_figures_names": [
      "1809.01246v1-Figure9-1.png"
    ],
    "all_figures_names": [
      "1809.01246v1-Figure6-1.png",
      "1809.01246v1-Figure8-1.png",
      "1809.01246v1-Figure4-1.png",
      "1809.01246v1-Figure1-1.png",
      "1809.01246v1-Figure9-1.png",
      "1809.01246v1-Figure5-1.png",
      "1809.01246v1-Figure13-1.png",
      "1809.01246v1-Figure12-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 5,
    "rationale": "Image 0 provides a visual comparison of average precision for different algorithms, including TCM(256*memory), specifically for the email-EuAll dataset. Text 5 discusses the accuracy of \fname compared to TCM, which is relevant to understanding the performance differences.",
    "answer": "In the email-EuAll dataset, the average precision of TCM(256*memory) is significantly lower compared to the other two algorithms (GSS with sizes 12 and 16).",
    "text_chunks": [
      "\\nop{As TCM has very poor accuracy in topology queries, to make TCM comparable with \\fname, we fixed the memory of TCM 256 times as large as \\fname\\ in the first 2 datasets. In the third dataset we fixed it to 16 times because of the limitation of the memory of the server. The figure shows that although we unfairly fix the ratio of memory used by TCM and \\fname, \\fname\\ still can achieve better performance than TCM.",
      "All experiments are performed on a server with dual 6-core CPUs (Intel Xeon CPU E5-2620 @2.0 GHz, 24 threads) and 62 GB DRAM memory, running Ubuntu. All algorithms including \\fname\\ and TCM are implemented in C++. \\presub Data Sets \\postsub",
      "In edge query primitives, we allow TCM to use 8 times memory, and in other queries we implement it with 256 times memory, as its accuracy is too poor in these queries (in \\datasetc, we implement it with 16 times memory because of the limitation of the memory of the server). This ratio is the memory used by all the 4 sketches in TCM divided by the memory used by \\fname\\ with 16 bit fingerprint. When the size of \\fname\\ varies, the size of matrix in TCM also varies correspondingly to keep the ratio unchanged.",
      "Each bucket in the matrix contains l=2 rooms. The length of the address sequences is r=16, and the number of candidate buckets for each edge is k=16 (r=8,k=8 for the small data set \\dataseta and \\datasetb). As for TCM, we apply 4 graph sketches to improve its accuracy, and allow it to use larger memory, because otherwise the gap between it and \\fname\\ will be so huge that we can hardly compare them in one figure.",
      "Because the update speed changes little with the matrix size, we only show the average speed here. The fingerprint size is 16-bit. TCM is still implemented with the same settings as above experiments.",
      "The number of memory accesses are nearly constant. On the other hand, the matrix we build is much smaller than the adjacency matrix, usually no greater than 1500 \\times 1500. By introducing fingerprints, \\fname{ achieves much higher accuracy than TCM, especially in topology queries like reachability query.",
      "( In \\datasete, we implement TCM with 16 times of memory rather than 256 times in the 1-hop successor/precursor query because of the limitation of the host memory). Figure , Figure , and Figure show that ARE of edge queries and average precision of 1-hop precursor / successor queries for the data sets, respectively. To reduce random error introduced by the selection of the data sample, the edge query set contains all edges in the graph stream, and the 1-hop precursor / successor query set contains all nodes in the graph stream.",
      "In each data set we insert all the edges into the data structure, repeat this procedure 100 times and calculate the average speed. The unit we use is Million Insertions per Second (Mips). From the figure we can see that the speed of \\fname\\ is similar to TCM, because though more memory accesses are needed, \\fname\\ computes less hash functions."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 5,\n    \"Rationale\": \"Image 0 provides a visual comparison of average precision for different algorithms, including TCM(256*memory), specifically for the email-EuAll dataset. Text 5 discusses the accuracy of \\fname compared to TCM, which is relevant to understanding the performance differences.\",\n    \"Answer\": \"In the email-EuAll dataset, the average precision of TCM(256*memory) is significantly lower compared to the other two algorithms (GSS with sizes 12 and 16).\"\n}"
  },
  "8": {
    "question": "Which query type has the highest accuracy when M/|V| is small?",
    "ground_truth": "Edge query.",
    "selected_figures_names": [
      "1809.01246v1-Figure3-1.png"
    ],
    "all_figures_names": [
      "1809.01246v1-Figure3-1.png",
      "1809.01246v1-Figure4-1.png",
      "1809.01246v1-Figure5-1.png",
      "1809.01246v1-Figure10-1.png",
      "1809.01246v1-TableI-1.png",
      "1809.01246v1-Figure2-1.png",
      "1809.01246v1-Figure1-1.png",
      "1809.01246v1-Figure13-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 and Text 2 both discuss the influence of M on the accuracy of different query types, specifically focusing on 1-hop successor and precursor queries. This directly relates to the question about accuracy when M/|V| is small.",
    "answer": "The Edge Query has the highest accuracy when M/|V| is small.",
    "text_chunks": [
      "The figure shows that we have to use a large M to achieve high accuracy in the query primitives, which is not possible in the prior works. According to Figure 3, only when \\frac{M{|V|>200, the accuracy ratio is larger than {80\\% in 1-hop successor / precursor queries. When \\frac{M{|V| \\leq 1, the accuracy ratio falls down to nearly {0, which is totally unacceptable.",
      "To achieve high accuracy, we set M \\gg |V|. This can be achieved by setting large F, in other words, using long fingerprints. When the memory is not sufficient, we can also set smaller M with smaller m and F, but this will decrease the accuracy.",
      "This needs O|V| additional memory, as |V|\\leqslant |E|, the overall memory requirement is still within O(|E|). \\prefig \\postfig \\subfigure[1-hop Successor Query]{ \\subfigure[1-hop Precursor Query]{ \\caption{Influence of M on Accuracy \\end{figure* Obviously, the size of the value range of the map function H(\\cdot), which we represent with M, will significantly influence the accuracy of the summarization, especially in the 1-hop successor / precursor query primitives.",
      "In this paper we propose \\fname, which has small memory usage, fast and constant update speed. It supports many kinds of queries, like edge queries, node queries and reachability queries. Moreover, in all these queries it has much higher accuracy than prior art.",
      "The accuracy is calculated according to the accuracy analysis in Section . For the simplicity of the presentation, we fix the graph size |E|=421578 and |V|=34546 in the figure. m is the size of matrix in these data structures.",
      "\\nop{ Fortunately, it is not always necessary to give an exact answer in the graph stream based queries. Therefore approximate query data structures with small and controllable errors are good choices for graph stream storage. However, exiting approximate query data structures either support limited query types or have poor accuracy.",
      "However, existing solutions either support limited query types or have poor query accuracy. For example, CM sketches and gSketches fail to answer queries involved with topology like reachability queries, successor queries and so on. Though TCM and gMatrix can support these queries, they have poor accuracy.",
      "The results tell us that \\fname\\ performs much better in supporting these query primitives than TCM, especially in the 1-hop precursor / successor query primitives. In both \\fname\\ and TCM, the ARE decreases, and the precision increases with the growth of the width of the matrix. This trend is not very significant in \\fname\\ as the accuracy is high and there are no errors in most experiments."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 and Text 2 both discuss the influence of M on the accuracy of different query types, specifically focusing on 1-hop successor and precursor queries. This directly relates to the question about accuracy when M/|V| is small.\",\n    \"Answer\": \"The Edge Query has the highest accuracy when M/|V| is small.\"\n}"
  }
}