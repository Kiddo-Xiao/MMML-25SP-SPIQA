{
  "0": {
    "question": "Which model performed best on average across all tasks?",
    "ground_truth": "The Linear model performed best on average with a score of 70.0.",
    "selected_figures_names": [
      "1809.02731v3-Table4-1.png"
    ],
    "all_figures_names": [
      "1809.02731v3-Table3-1.png",
      "1809.02731v3-Table4-1.png",
      "1809.02731v3-Table5-1.png",
      "1809.02731v3-Table1-1.png",
      "1809.02731v3-Table2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 6,
    "rationale": "Image 0 provides a table with average performance scores for different models across tasks, which directly answers the question. Text 6 discusses the performance of the learnt system compared to FastText, supporting the information in the table.",
    "answer": "The Linear model performed best on average across all tasks with a score of 70.0.",
    "text_chunks": [
      "As presented in Table , on unsupervised evaluation tasks (STS12-16 and SICK14), the ensemble of two encoding functions is averaging, which benefits from aligning representations from f_\\text{en and f_\\text{de^{-1 by enforcing the invertible constraint. While in the learning system without the invertible constraint, the ensemble of two encoding functions provides worse performance than f_\\text{de^{-1. On supervised evaluation tasks, as the ensemble method is concatenation and a linear model is applied on top of the concatenated representations, as long as the two encoding functions process sentences distinctively, the linear classifier is capable of picking relevant feature dimensions from both encoding functions to make good predictions, thus there is no significant difference between our model with and without invertible constraint.",
      "Table also provides evidence for the effectiveness of learning on the unsupervised evaluation tasks. \\vskip -0.1in \\end{center \\caption{Results on supervised evaluation tasks. Bold numbers are the best results among unsupervised transfer models with ordered sentences, and \\underline{underlined numbers are the best ones among all models.",
      "Therefore, it is important to check the necessity of the invertible constraint on the decoders. A model with the same hyperparameter settings but without the invertible constraint is trained as the baseline model, and macro-averaged results that summarise the same type of tasks are presented in Table . As noted in the prior work , there exists significant inconsistency between the group of unsupervised tasks and the group of supervised ones, it is possible for a model to excel on one group of tasks but fail on the other one.",
      "As presented in our table, the inverse of the decoder tends to perform better than the encoder on unsupervised tasks, and the situation reverses when it comes to the supervised ones. In our model, the invertible constraint helps the RNN encoder f_\\text{en to perform better on the unsupervised evaluation tasks, and helps the inverse of the decoder f_\\text{de^{-1 to provide better results on single sentence classification tasks. An interesting observation is that, by enforcing the invertible constraint, the model learns to sacrifice the performance of f_\\text{de^{-1 and improve the performance of f_\\text{en on unsupervised tasks to mitigate the gap between the two encoding functions, which leads to more aligned vector representations between f_\\text{en and f_\\text{de^{-1.",
      "\\end{table* On supervised evaluation tasks, we agree that higher dimensional vector representations give better results on the downstream tasks. Compared to random projections with 4096\\times 6 output dimensions, learning from unlabelled corpora leverages the distributional similarity at the sentence-level into the learnt representations and potentially helps capture the meaning of a sentence.",
      "As mentioned as one of the take-away messages , to demonstrate the effectiveness of the invertible constraint, the comparison of our model with the constraint and its own variants use the same word embeddings from FastText and have the same dimensionaility of sentence representations during learning, and use the same classifier on top of the produced representations with the same hyperparameter settings. Overall, given the performance of the inverse of each decoder presented in Table and , it is reasonable to state that the inverse of the decoder provides high-quality sentence representations as well as the encoder does. However, there is no significant difference between the two decoders in terms of the performance on the downstream tasks.",
      "As shown in the Table , the performance of our learnt system is better than FastText at the same dimensionality. It is worth mentioning that, in our system, the final representation is an average of postprocessed word vectors and the learnt representations \\vx, and the invertible constraint guarantees that the ensemble of both gives better performance. Otherwise, as discussed in the previous section, an ensemble of postprocessed word vectors and some random encoders won't necessarily lead to stronger results.",
      "Effect of Learning Recent research showed that the improvement on the supervised evaluation tasks led by learning from labelled or unlabelled corpora is rather insignificant compared to random initialised projections on top of pretrained word vectors. Another interesting direction of research that utilises probabilistic random walk models on the unit sphere derived several simple yet effective post-processing methods that operate on pretrained word vectors and are able to boost the performance of the averaged word vectors as the sentence representation on unsupervised tasks. While these papers reveal interesting aspects of the downstream tasks and question the need for optimising a learning objective, our results show that learning on unlabelled corpora helps."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 6,\n    \"Rationale\": \"Image 0 provides a table with average performance scores for different models across tasks, which directly answers the question. Text 6 discusses the performance of the learnt system compared to FastText, supporting the information in the table.\",\n    \"Answer\": \"The Linear model performed best on average across all tasks with a score of 70.0.\"\n}"
  },
  "1": {
    "question": "Which corpus has more sentences, and by how much?",
    "ground_truth": "The UMBC News corpus has more sentences, by approximately 60.5 million.",
    "selected_figures_names": [
      "1809.02731v3-Table1-1.png"
    ],
    "all_figures_names": [
      "1809.02731v3-Table3-1.png",
      "1809.02731v3-Table4-1.png",
      "1809.02731v3-Table5-1.png",
      "1809.02731v3-Table1-1.png",
      "1809.02731v3-Table2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides a direct comparison of the number of sentences in the two corpora, BookCorpus and UMBC News. Text 0 references the same table, confirming the data source.",
    "answer": "The UMBC News corpus has more sentences, with 134.5 million compared to BookCorpus's 74 million, a difference of 60.5 million sentences.",
    "text_chunks": [
      "These corpora are referred as B, and U in Table and . The UMBC News Corpus is roughly twice as large as the BookCorpus, and the details are shown in Table . Evaluation Tasks Unsupervised Evaluation",
      "\\end{table* On supervised evaluation tasks, we agree that higher dimensional vector representations give better results on the downstream tasks. Compared to random projections with 4096\\times 6 output dimensions, learning from unlabelled corpora leverages the distributional similarity at the sentence-level into the learnt representations and potentially helps capture the meaning of a sentence.",
      "In our system, the raw representations are in 2400-dimensional space, and the use of various pooling functions expands it to 2048\\times 6 dimensions, which is half as large as the random projection dimension and still yields better performance. Both our models and random projections with no training are presented in Table . The evidence from both sets of downstream tasks support our argument that learning from unlabelled corpora helps the representations capture meaning of sentences.",
      "In this case, we enforce the output of the decoder \\vx_i to have the same dimensionality as the pretrained word vectors \\vv_{w_j. The loss function is summed over all contiguous sentence pairs in the training corpus. For simplicity, we omit the subscription for indexing the sentences in the following sections.",
      "On unsupervised evaluation tasks, in order to show that learning from an unlabelled corpus helps, the performance of our learnt representations should be directly compared with the pretrained word vectors, FastText in our system, at the same dimensionality with the same post-processing . The word vectors are scattered in the 300-dimensional space, and our model has a decoder that is learnt to project a sentence representation \\vz\\in\\sR^{d_\\vz to \\vx=f_\\text{de(\\vz;\\boldsymbol{\\phi)\\in\\sR^{300. The results of our learnt representations and averaged word vectors with the same postprocessing are presented in Table .",
      "In one of the two classes of models of this type, an encoder-decoder model is learnt using a corpus of contiguous sentences to make predictions of the words in the next sentence given the words in the current one. After training, the decoder is usually discarded as it is only needed during training and is not designed to produce sentence representations. In the other class of models of this type, a large language model is learnt on unlabelled corpora, which could be an autoregressive model or a masked language model, which gives extremely powerful language encoders but requires massive computing resources and training time.",
      "Analyses show that the invertible constraint enforced on the decoder encourages each one to learn from the other one during learning, and provides improved encoding functions after learning. Ensemble of the encoder and the inverse of the decoder gives even better performance when the invertible constraint is applied on the decoder side. Furthermore, by comparing with prior work, we argue that learning from unlabelled corpora indeed helps to improve the sentence representations, although the current way of utilising corpora might not be optimal.",
      "The reason for applying different representation pooling strategies for two categories of tasks is: (1) cosine similarity of two vector representations is directly calculated in unsupervised evaluation tasks to determine the textual similarity of two sentences, and it suffers from the curse-of-dimensionality , which leads to more equidistantly distributed representations for higher dimensional vector representations decreasing the difference among similarity scores. (2) given Cover's theorem and the blessings-of-dimensionality property, it is more likely for the data points to be linearly separable when they are presented in high dimensional space, and in the supervised evaluation tasks, high dimensional vector representations are preferred as a linear classifier will be learnt to evaluate how likely the produced sentence representations are linearly separable; (3) in our case, both the encoder and the inverse of the decoder are capable of producing a vector representation per time step in a given sentence, although during training, only the last one is regarded as the sentence representation for the fast training speed, it is more reasonable to make use of all representations at all time steps with various pooling functions to compute a vector representations to produce high-quality sentence representations that excel the downstream tasks. Discussion It is worth discussing the motivation of the model design and the observations in our experiments."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides a direct comparison of the number of sentences in the two corpora, BookCorpus and UMBC News. Text 0 references the same table, confirming the data source.\",\n    \"Answer\": \"The UMBC News corpus has more sentences, with 134.5 million compared to BookCorpus's 74 million, a difference of 60.5 million sentences.\"\n}"
  },
  "2": {
    "question": "Which model performs the best on the STS16 task with unsupervised training?",
    "ground_truth": "The Bijective model performs the best on the STS16 task with unsupervised training.",
    "selected_figures_names": [
      "1809.02731v3-Table3-1.png",
      "1809.02731v3-Table4-1.png",
      "1809.02731v3-Table5-1.png",
      "1809.02731v3-Table1-1.png"
    ],
    "all_figures_names": [
      "1809.02731v3-Table3-1.png",
      "1809.02731v3-Table4-1.png",
      "1809.02731v3-Table5-1.png",
      "1809.02731v3-Table1-1.png",
      "1809.02731v3-Table2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides detailed results on unsupervised evaluation tasks, including STS16, with clear indication of the best-performing models. Text 0 discusses the performance of models with and without the invertible constraint, which is relevant to understanding the results.",
    "answer": "The Bijective model performs the best on the STS16 task with unsupervised training, achieving a score of 76.9.",
    "text_chunks": [
      "As presented in Table , on unsupervised evaluation tasks (STS12-16 and SICK14), the ensemble of two encoding functions is averaging, which benefits from aligning representations from f_\\text{en and f_\\text{de^{-1 by enforcing the invertible constraint. While in the learning system without the invertible constraint, the ensemble of two encoding functions provides worse performance than f_\\text{de^{-1. On supervised evaluation tasks, as the ensemble method is concatenation and a linear model is applied on top of the concatenated representations, as long as the two encoding functions process sentences distinctively, the linear classifier is capable of picking relevant feature dimensions from both encoding functions to make good predictions, thus there is no significant difference between our model with and without invertible constraint.",
      "Table also provides evidence for the effectiveness of learning on the unsupervised evaluation tasks. \\vskip -0.1in \\end{center \\caption{Results on supervised evaluation tasks. Bold numbers are the best results among unsupervised transfer models with ordered sentences, and \\underline{underlined numbers are the best ones among all models.",
      "As presented in our table, the inverse of the decoder tends to perform better than the encoder on unsupervised tasks, and the situation reverses when it comes to the supervised ones. In our model, the invertible constraint helps the RNN encoder f_\\text{en to perform better on the unsupervised evaluation tasks, and helps the inverse of the decoder f_\\text{de^{-1 to provide better results on single sentence classification tasks. An interesting observation is that, by enforcing the invertible constraint, the model learns to sacrifice the performance of f_\\text{de^{-1 and improve the performance of f_\\text{en on unsupervised tasks to mitigate the gap between the two encoding functions, which leads to more aligned vector representations between f_\\text{en and f_\\text{de^{-1.",
      "\\end{table* On supervised evaluation tasks, we agree that higher dimensional vector representations give better results on the downstream tasks. Compared to random projections with 4096\\times 6 output dimensions, learning from unlabelled corpora leverages the distributional similarity at the sentence-level into the learnt representations and potentially helps capture the meaning of a sentence.",
      "Effect of Learning Recent research showed that the improvement on the supervised evaluation tasks led by learning from labelled or unlabelled corpora is rather insignificant compared to random initialised projections on top of pretrained word vectors. Another interesting direction of research that utilises probabilistic random walk models on the unit sphere derived several simple yet effective post-processing methods that operate on pretrained word vectors and are able to boost the performance of the averaged word vectors as the sentence representation on unsupervised tasks. While these papers reveal interesting aspects of the downstream tasks and question the need for optimising a learning objective, our results show that learning on unlabelled corpora helps.",
      "In our system, the raw representations are in 2400-dimensional space, and the use of various pooling functions expands it to 2048\\times 6 dimensions, which is half as large as the random projection dimension and still yields better performance. Both our models and random projections with no training are presented in Table . The evidence from both sets of downstream tasks support our argument that learning from unlabelled corpora helps the representations capture meaning of sentences.",
      "Therefore, it is important to check the necessity of the invertible constraint on the decoders. A model with the same hyperparameter settings but without the invertible constraint is trained as the baseline model, and macro-averaged results that summarise the same type of tasks are presented in Table . As noted in the prior work , there exists significant inconsistency between the group of unsupervised tasks and the group of supervised ones, it is possible for a model to excel on one group of tasks but fail on the other one.",
      "\\end{table* Hyperparameter Tuning The hyperparameters are tuned on the averaged scores on STS14 of the model trained on BookCorpus, thus it is marked with a ^\\star in tables to indicate potential overfitting. The hyperparameter setting for our model is summarised as follows: the batch size N=512, the dimension of sentence vectors d_\\vz=2048, the dimension of word vectors d_{\\vv_{w_j=300, the number of negative samples K=5, and the initial learning rate is 5\\times 10^{-4 which is kept fixed during learning."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides detailed results on unsupervised evaluation tasks, including STS16, with clear indication of the best-performing models. Text 0 discusses the performance of models with and without the invertible constraint, which is relevant to understanding the results.\",\n    \"Answer\": \"The Bijective model performs the best on the STS16 task with unsupervised training, achieving a score of 76.9.\"\n}"
  }
}