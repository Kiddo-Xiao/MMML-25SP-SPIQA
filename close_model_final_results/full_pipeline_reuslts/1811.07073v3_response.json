{
  "0": {
    "question": "How does the performance of the model with convolutional self-correction compare to the model with no self-correction as the number of images in set $\\mathcal{F}$ increases?",
    "ground_truth": "The model with convolutional self-correction consistently outperforms the model with no self-correction as the number of images in set $\\mathcal{F}$ increases.",
    "selected_figures_names": [
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Table2-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Figure3-1.png"
    ],
    "all_figures_names": [
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Table2-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Figure3-1.png",
      "1811.07073v3-Figure2-1.png",
      "1811.07073v3-Figure5-1.png",
      "1811.07073v3-Table1-1.png",
      "1811.07073v3-Figure4-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 7,
    "rationale": "Image 0 provides a clear comparison of model performance with and without convolutional self-correction across different sizes of set F. Text 7 discusses the performance of different model variants based on the size of the fully supervised set F, which is directly relevant to the question.",
    "answer": "As the number of images in set F increases, the performance of the model with convolutional self-correction improves more significantly compared to the model with no self-correction. For example, with 200 images, the mIOU for convolutional self-correction is 69.38, while for no self-correction it is 73.69. With 914 images, the mIOU for convolutional self-correction increases to 79.46, compared to 75.44 for no self-correction.",
    "text_chunks": [
      "iii) The convolutional self-correction model performs comparably or better than the linear self-correction while eliminating the need for defining an \\alpha schedule. Fig.~ shows the output of these models.",
      "\\setlength{\\tabcolsep{1pt Table~ compares the performance of our models against different baselines and published results. In this experiment, we use 1464 images as \\F and 9118 images originally from the auxiliary dataset as \\W. Both self-correction models achieve similar results and outperform other models. Surprisingly, our semi-supervised models outperform the fully supervised model.",
      "However, if we train the convolutional self-correction model on the same training set and allow the model to refine the ground truth labels using self-correction, we get mAP as high as 76.88\\% (the convolutional self correction on top of bounding boxes yields 75.97\\% mAP). This indicates that modeling noise with robust loss functions and allowing for self-correction may significantly improve the performance of segmentation models. This is consonant with self-correction approaches that have been shown to be effective for edge detection~, and is in contrast to common segmentation objectives which train models using cross-entropy with one-hot annotation masks.",
      "Convolutional Self-Correction One disadvantage of linear self-correction is the hyperparameter search required for tuning the \\alpha schedule during training. In this section, we present an approach that overcomes this difficulty by replacing the linear function with a convolutional network that learns the self-correction mechanism. As a result, the network automatically tunes the mechanism dynamically as the primary model is trained.",
      "This shows that this model can also provide a good training signal for the weak set that lacks segmentation labels. ii) The linear self-correction model typically performs better than no self-correction model supporting our idea that combining the primary and ancillary model for inferring segmentation labels results in better training of the primary model.",
      "The convolutional self-correction subnetwork consists of two convolution layers. Both layers use a 3\\times3 kernel and ReLU activations. The first layer has 128 output feature maps and the second has feature maps based on the number of classes in the dataset.",
      "Note that the ancillary model parameterized by \\btheta is fixed. We call this approach the no self-correction model as it relies directly on the ancillary model for training the primary model for examples in \\W. Linear Self-Correction Eq.~ relies on the ancillary model to predict label distribution on the weak set. However, this model is trained using only instances of \\F without benefit of the data in \\W.",
      "In Table~, we show the performance of different variants of our model for different sizes of the fully supervised set \\F. The remaining examples in the training set are used as \\W. We make several observations from Table~: i) The ancillary model that predicts segmentation labels given an image and its object bounding boxes performs well even when it is trained with a training set as small as 200 images."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 7,\n    \"Rationale\": \"Image 0 provides a clear comparison of model performance with and without convolutional self-correction across different sizes of set F. Text 7 discusses the performance of different model variants based on the size of the fully supervised set F, which is directly relevant to the question.\",\n    \"Answer\": \"As the number of images in set F increases, the performance of the model with convolutional self-correction improves more significantly compared to the model with no self-correction. For example, with 200 images, the mIOU for convolutional self-correction is 69.38, while for no self-correction it is 73.69. With 914 images, the mIOU for convolutional self-correction increases to 79.46, compared to 75.44 for no self-correction.\"\n}"
  },
  "1": {
    "question": "Which method performed the best on the PASCAL VOC 2012 test set and how does it compare to the baseline model without self-correction?",
    "ground_truth": "The Conv. Self-Corr. method achieved the highest performance on the PASCAL VOC 2012 test set with a score of 82.72. This is approximately 1.11 points higher than the baseline model (\"No Self-Corr.\") which achieved a score of 81.61.",
    "selected_figures_names": [
      "1811.07073v3-Figure4-1.png",
      "1811.07073v3-Figure5-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Figure2-1.png"
    ],
    "all_figures_names": [
      "1811.07073v3-Figure4-1.png",
      "1811.07073v3-Figure5-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Figure2-1.png",
      "1811.07073v3-Table2-1.png",
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Figure3-1.png",
      "1811.07073v3-Table1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 shows qualitative results comparing different models, including Conv. Self-correction, which is relevant to the question. Text 3 discusses the performance of the proposed self-correction framework on the PASCAL VOC 2012 dataset, indicating it outperforms previous models.",
    "answer": "The Conv. Self-correction model performed the best on the PASCAL VOC 2012 test set. It typically segments objects better than the baseline model without self-correction.",
    "text_chunks": [
      "Secondly, the performance gain may also be due to explicit modeling of label uncertainty and self-correction. To test this hypothesis, we train vanilla DeepLabv3 + on only 1.4K instances in the original PASCAL VOC 2012 training set and obtain 68.8\\% mAP on the validation set.",
      "Finally, comparing Table~ and , we see that with F=200 and W=10382, our linear~self-correction model performs similarly to DeepLabv3+ trained with the whole dataset. Using the labeling cost reported in , this theoretically translates to a \\sim7x reduction in annotation cost. \\setlength{\\tabcolsep{1pt \\caption{Qualitative results on the PASCAL VOC 2012 validation set.",
      "The last four columns represent the models in column 1464 of Table~. The Conv.~Self-correction model typically segments objects better than other models. \\end{figure* \\caption{Qualitative results on the PASCAL VOC 2012 auxiliary (the weak set). The heatmap of a single class for the ancillary model is shown for several examples.",
      "We introduced two mechanisms that enable the underlying primary model to correct the weak labels provided by an ancillary model. The proposed self-correction mechanisms combine the predictions made by the primary and ancillary model either using a linear function or trainable CNN. The experiments show that our proposed framework outperforms previous semi-supervised models on both the PASCAL VOC 2012 and Cityscapes datasets.",
      "In this section, we evaluate all models on the PASCAL VOC 2012 segmentation benchmark~. This dataset consists of 1464 training, 1449 validation, and 1456 test images covering 20 foreground object classes and one background class for segmentation. An auxiliary dataset of 9118 training images is provided by~.",
      "We find that a starting value of \\alpha=30 and the final value of \\alpha=0.5 performs well for both datasets. This parameter setting is robust as moderate changes of these values have little effect. % PASCAL VOC Dataset",
      "We suspect, however, that the segmentation labels of ~ contain a small amount of noise. In this section, we refer to the union of the original PASCAL VOC training dataset and the auxiliary set as the training set. We evaluate the models mainly on the validation set and the best model is evaluated only once on the test set using the online evaluation server.",
      "To overcome this issue, we propose the following pretraining procedure: The rationale behind using half of \\F in stage 1 is that if we used all \\F for training the \\pest model, it would train to predict the segmentation mask almost perfectly on this set, therefore, the subsequent training of the convolutional self-correcting network would just learn to rely on \\pest. To overcome this training issue, the second half of \\F is held out to help the self-correcting network to learn how to combine \\pest and p(\\y|\\x). \\vspace{-0.2cm Experiments In this section, we evaluate our models on the PASCAL VOC 2012 and Cityscapes datasets."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 3,\n    \"Rationale\": \"Image 0 shows qualitative results comparing different models, including Conv. Self-correction, which is relevant to the question. Text 3 discusses the performance of the proposed self-correction framework on the PASCAL VOC 2012 dataset, indicating it outperforms previous models.\",\n    \"Answer\": \"The Conv. Self-correction model performed the best on the PASCAL VOC 2012 test set. It typically segments objects better than the baseline model without self-correction.\"\n}"
  },
  "2": {
    "question": "What is the input to the convolutional self-correction model?",
    "ground_truth": "The input to the convolutional self-correction model is the logits generated by the primary and ancillary models.",
    "selected_figures_names": [
      "1811.07073v3-Figure3-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Figure4-1.png"
    ],
    "all_figures_names": [
      "1811.07073v3-Figure3-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Figure4-1.png",
      "1811.07073v3-Figure5-1.png",
      "1811.07073v3-Table1-1.png",
      "1811.07073v3-Table2-1.png",
      "1811.07073v3-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 4,
    "rationale": "Image 0 visually represents the architecture of the convolutional self-correction model, showing that it takes logits from both the primary and ancillary models as input. Text 4 describes the input to the model, confirming that it accepts logits from the primary and ancillary models to generate refined segmentation labels.",
    "answer": "The input to the convolutional self-correction model is the logits from the primary and ancillary models.",
    "text_chunks": [
      "convolutional layer applied to the depth-wise concatenation of \\ll and \\ll^{anc with a fixed averaging kernel. This originally motivated us to develop the convolutional self-correction model in Sec.~ using trainable kernels. As the primary model's parameters are optimized during training, \\alpha biases the self-correction mechanism towards the primary model.",
      "Convolutional Self-Correction One disadvantage of linear self-correction is the hyperparameter search required for tuning the \\alpha schedule during training. In this section, we present an approach that overcomes this difficulty by replacing the linear function with a convolutional network that learns the self-correction mechanism. As a result, the network automatically tunes the mechanism dynamically as the primary model is trained.",
      "The convolutional self-correction subnetwork consists of two convolution layers. Both layers use a 3\\times3 kernel and ReLU activations. The first layer has 128 output feature maps and the second has feature maps based on the number of classes in the dataset.",
      "However, if we train the convolutional self-correction model on the same training set and allow the model to refine the ground truth labels using self-correction, we get mAP as high as 76.88\\% (the convolutional self correction on top of bounding boxes yields 75.97\\% mAP). This indicates that modeling noise with robust loss functions and allowing for self-correction may significantly improve the performance of segmentation models. This is consonant with self-correction approaches that have been shown to be effective for edge detection~, and is in contrast to common segmentation objectives which train models using cross-entropy with one-hot annotation masks.",
      "If the primary model predicts labels accurately, this network can shift its predictions towards the primary model. Fig.~ shows the architecture of the convolutional self-correcting model. This small network accepts the logits generated by p_{anc(\\y|\\x, \\bb) and p(\\y|\\x) models and generates the factorial distribution q_{conv(\\y|\\x, \\bb; \\blambda) over segmentation labels where \\blambda represents the parameters of the subnetwork.",
      "iii) The convolutional self-correction model performs comparably or better than the linear self-correction while eliminating the need for defining an \\alpha schedule. Fig.~ shows the output of these models.",
      "This model generates an initial segmentation for the weak set, which will aid training the primary model. iii) Self-correction module refines segmentations generated by the ancillary model and the current primary model for the weak set. The primary model is trained using the cross-entropy loss that matches its output to either ground-truth segmentation labels for the fully supervised examples or soft refined labels generated by the self-correction module for the weak set.",
      "Note that the ancillary model parameterized by \\btheta is fixed. We call this approach the no self-correction model as it relies directly on the ancillary model for training the primary model for examples in \\W. Linear Self-Correction Eq.~ relies on the ancillary model to predict label distribution on the weak set. However, this model is trained using only instances of \\F without benefit of the data in \\W."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 4,\n    \"Rationale\": \"Image 0 visually represents the architecture of the convolutional self-correction model, showing that it takes logits from both the primary and ancillary models as input. Text 4 describes the input to the model, confirming that it accepts logits from the primary and ancillary models to generate refined segmentation labels.\",\n    \"Answer\": \"The input to the convolutional self-correction model is the logits from the primary and ancillary models.\"\n}"
  },
  "3": {
    "question": "How does the performance of the \"Conv. Self-Correction\" method compare to other methods when using 30% of the training examples as $\\F$ and the remaining as $\\W$ on the Cityscapes validation set?",
    "ground_truth": "The \"Conv. Self-Correction\" method achieves the highest mIOU score of 79.46 compared to other methods listed in the table under the same data split condition.",
    "selected_figures_names": [
      "1811.07073v3-Figure1-1.png",
      "1811.07073v3-Table1-1.png",
      "1811.07073v3-Figure2-1.png",
      "1811.07073v3-Table3-1.png"
    ],
    "all_figures_names": [
      "1811.07073v3-Figure1-1.png",
      "1811.07073v3-Table1-1.png",
      "1811.07073v3-Figure2-1.png",
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Table2-1.png",
      "1811.07073v3-Figure4-1.png",
      "1811.07073v3-Figure5-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 6,
    "rationale": "Image 3 provides a table with performance metrics for different methods, including 'Conv. Self-Correction', on the Cityscapes validation set. Text 6 discusses the performance of models against baselines, which is relevant to understanding the comparative performance.",
    "answer": "When using 30% of the training examples as F, the 'Conv. Self-Correction' method achieves a mIOU of 77.16, which is higher than 'No Self-correction' (75.10) and 'Lin. Self-correction' (75.24), indicating better performance.",
    "text_chunks": [
      "To overcome this issue, we propose the following pretraining procedure: The rationale behind using half of \\F in stage 1 is that if we used all \\F for training the \\pest model, it would train to predict the segmentation mask almost perfectly on this set, therefore, the subsequent training of the convolutional self-correcting network would just learn to rely on \\pest. To overcome this training issue, the second half of \\F is held out to help the self-correcting network to learn how to combine \\pest and p(\\y|\\x). \\vspace{-0.2cm Experiments In this section, we evaluate our models on the PASCAL VOC 2012 and Cityscapes datasets.",
      "Table~ reports the performance of our model for an increasing number of images as \\F, and Table~ compares our models with several baselines similar to the previous dataset. The same conclusion and insights observed on the PASCAL dataset hold for the Cityscapes dataset indicating the efficacy of our self-corrective framework. Conclusion In this paper, we have proposed a semi-supervised framework for training deep CNN segmentation models using a small set of fully labeled and a set of weakly labeled images (boxes annotations only).",
      "However, if we train the convolutional self-correction model on the same training set and allow the model to refine the ground truth labels using self-correction, we get mAP as high as 76.88\\% (the convolutional self correction on top of bounding boxes yields 75.97\\% mAP). This indicates that modeling noise with robust loss functions and allowing for self-correction may significantly improve the performance of segmentation models. This is consonant with self-correction approaches that have been shown to be effective for edge detection~, and is in contrast to common segmentation objectives which train models using cross-entropy with one-hot annotation masks.",
      "The challenge here is to train this subnetwork such that it predicts the segmentation labels more accurately than either \\pest or p(\\y|\\x). To this end, we introduce an additional term in the objective function, which trains the subnetwork using training examples in \\F while the primary model is being trained on the whole dataset: where the first and second terms train the primary model on \\F and \\W (we do not backpropagate through q in the second term) and the last term trains the convolutional self-correcting network. Because the q_{conv subnetwork is initialized randomly, it is not able to accurately predict segmentation labels on \\W early on during training.",
      "Finally, comparing Table~ and , we see that with F=200 and W=10382, our linear~self-correction model performs similarly to DeepLabv3+ trained with the whole dataset. Using the labeling cost reported in , this theoretically translates to a \\sim7x reduction in annotation cost. \\setlength{\\tabcolsep{1pt \\caption{Qualitative results on the PASCAL VOC 2012 validation set.",
      "Experiments on the PASCAL VOC and Cityscapes datasets show that our models trained with a small portion of fully supervised set achieve a performance comparable to (and in some cases better than) the models trained with all the fully supervised images. Related Work \\vspace{-0.1cm \\paragraph{Semantic Segmentation: Fully convolutional networks (FCNs)~ have become indispensable models for semantic image segmentation.",
      "\\setlength{\\tabcolsep{1pt Table~ compares the performance of our models against different baselines and published results. In this experiment, we use 1464 images as \\F and 9118 images originally from the auxiliary dataset as \\W. Both self-correction models achieve similar results and outperform other models. Surprisingly, our semi-supervised models outperform the fully supervised model.",
      "Secondly, the performance gain may also be due to explicit modeling of label uncertainty and self-correction. To test this hypothesis, we train vanilla DeepLabv3 + on only 1.4K instances in the original PASCAL VOC 2012 training set and obtain 68.8\\% mAP on the validation set."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 6,\n    \"Rationale\": \"Image 3 provides a table with performance metrics for different methods, including 'Conv. Self-Correction', on the Cityscapes validation set. Text 6 discusses the performance of models against baselines, which is relevant to understanding the comparative performance.\",\n    \"Answer\": \"When using 30% of the training examples as F, the 'Conv. Self-Correction' method achieves a mIOU of 77.16, which is higher than 'No Self-correction' (75.10) and 'Lin. Self-correction' (75.24), indicating better performance.\"\n}"
  },
  "4": {
    "question": "How does the bounding box encoder network influence the segmentation process?",
    "ground_truth": "The bounding box encoder network embeds bounding box information at different scales and outputs attention maps that are used to fuse with feature maps from the encoder before being passed to the decoder.",
    "selected_figures_names": [
      "1811.07073v3-Figure2-1.png"
    ],
    "all_figures_names": [
      "1811.07073v3-Table1-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Figure2-1.png",
      "1811.07073v3-Figure1-1.png",
      "1811.07073v3-Figure5-1.png",
      "1811.07073v3-Figure4-1.png",
      "1811.07073v3-Figure3-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 provides a visual representation of the segmentation model with the bounding box encoder, showing how it integrates into the process. Text 1 explains the technical details of how the bounding box encoder works, including the use of a 3D tensor and convolution layer to create an attention map that influences the segmentation.",
    "answer": "The bounding box encoder network influences the segmentation process by creating an attention map from a binarized mask of the bounding boxes. This attention map is element-wise multiplied with the feature maps generated by the segmentation encoder, effectively guiding the segmentation process by emphasizing relevant areas within the bounding boxes.",
    "text_chunks": [
      "Our key observation in designing the ancillary model is that encoder-decoder-based segmentation networks typically rely on encoders initialized from an image-classification model (e.g., ImageNet pretrained models). This usually improves the segmentation performance by transferring knowledge from large image-classification datasets. To maintain the same advantage, we augment an encoder-decoder-based segmentation model with a parallel bounding box encoder network that embeds bounding box information at different scales (See Fig.~).",
      "The input to the bounding box encoder is a 3D tensor representing a binarized mask of the bounding boxes and a 3D shape representing the target dimensions for the encoder output. The input mask tensor is resized to the target shape then passed through a 3\\times3 convolution layer with sigmoid activations. The resulting tensor can be interpreted as an attention map which is element-wise multiplied to the feature maps generated by the segmentation encoder.",
      "The main issues with such procedures are i) bounding box information is not directly used to extract the segmentation mask, ii) the procedure may be suboptimal as it is hand-designed, and iii) the segmentation becomes ambiguous when multiple boxes overlap. In this paper, we take a different approach by designing an ancillary segmentation model that forms a per-pixel label distribution given an image and bounding box annotation. This model is easily trained using the fully supervised set (\\F) and can be used as a training signal for images in \\W. At inference time, both the image and its bounding box are fed to the network to obtain p_{anc(\\y|\\xw, \\bbw), the segmentation labels distribution.",
      "The key challenge in semi-supervised training of segmentation models with bounding box annotations is to infer the segmentation of the object inside a box. Existing approaches to this problem mainly rely on hand-crafted rule-based procedures such as GrabCut~ or an iterative label refinement~ mechanism. This latter procedure typically iterates between segmentation extraction from the image and label refinement using the bounding box information (for example, by zeroing-out the mask outside of boxes).",
      "For example, drawing a segmentation annotation on an object is on average \\sim8x slower than drawing a bounding box and \\sim78x slower than labeling the presence of objects in images~. As a result, most image segmentation datasets are orders of magnitude smaller than image-classification datasets. In this paper, we mitigate the data demands of semantic segmentation with a semi-supervised method that leverages cheap object bounding box labels in training.",
      "All these models have been restricted to image-classification problems and have not yet been applied to image segmentation. \\vspace{-0.3cm \\paragraph{Semi-Supervised Semantic Segmentation: The focus of this paper is to train deep segmentation CNNs using bounding box annotations. Papandreou \\etal~ propose an Expectation-Maximization-based (EM) algorithm on top of DeepLabv1~ to estimate segmentation labels for the weak set of images (only with box information).",
      "This approach reduces the data annotation requirements at the cost of requiring inference of the mask label for an object within a bounding box. Current state-of-the-art semi-supervised methods typically rely on hand-crafted heuristics to infer an object mask inside a bounding box~. In contrast, we propose a principled framework that trains semantic segmentation models in a semi-supervised setting using a small set of fully supervised images (with semantic object masks and bounding boxes) and a weak set of images (with only bounding box annotations).",
      "\\caption{An overview of our segmentation framework consisting of three models: i) Primary segmentation model generates a semantic segmentation of objects given an image. This is the main model that is subject to the training and is used at test time. ii) Ancillary segmentation model outputs a segmentation given an image and bounding box."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 provides a visual representation of the segmentation model with the bounding box encoder, showing how it integrates into the process. Text 1 explains the technical details of how the bounding box encoder works, including the use of a 3D tensor and convolution layer to create an attention map that influences the segmentation.\",\n    \"Answer\": \"The bounding box encoder network influences the segmentation process by creating an attention map from a binarized mask of the bounding boxes. This attention map is element-wise multiplied with the feature maps generated by the segmentation encoder, effectively guiding the segmentation process by emphasizing relevant areas within the bounding boxes.\"\n}"
  },
  "5": {
    "question": "What is the role of the self-correction module in the segmentation framework?",
    "ground_truth": "The self-correction module refines the segmentations generated by the ancillary and current primary model for the weak set.",
    "selected_figures_names": [
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Table1-1.png",
      "1811.07073v3-Figure3-1.png",
      "1811.07073v3-Figure1-1.png"
    ],
    "all_figures_names": [
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Table1-1.png",
      "1811.07073v3-Figure3-1.png",
      "1811.07073v3-Figure1-1.png",
      "1811.07073v3-Figure2-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Table2-1.png",
      "1811.07073v3-Figure4-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 2,
    "rationale": "Image 3 provides a visual overview of the segmentation framework, showing how the self-correction module fits into the process. Text 2 explains the role of the self-correction module in refining segmentations generated by the ancillary and primary models, which aligns with the question.",
    "answer": "The self-correction module refines segmentations generated by the ancillary and primary models for the weak set. It improves the segmentation labels by using a convolutional network to refine the input label distributions, enhancing the accuracy of the primary model's predictions.",
    "text_chunks": [
      "iii) The Self-correction module refines the segmentations generated by the ancillary and current primary model for the weak set. Both the ancillary and the primary models are based on DeepLabv3+~. However, our framework is general and can use any existing segmentation model.",
      "Training is formulated so that the labels supplied to the primary model are refined during training from the initial ancillary mask labels to more accurate labels obtained from the primary model itself as it improves. Hence, we call our framework a self-correcting segmentation model as it improves the weakly supervised labels based on its current probabilistic model of object masks. We propose two approaches to the self-correction mechanism.",
      "This model generates an initial segmentation for the weak set, which will aid training the primary model. iii) Self-correction module refines segmentations generated by the ancillary model and the current primary model for the weak set. The primary model is trained using the cross-entropy loss that matches its output to either ground-truth segmentation labels for the fully supervised examples or soft refined labels generated by the self-correction module for the weak set.",
      "At the beginning of training, p_{anc(\\y|\\x, \\bb) cannot predict the segmentation label distribution accurately. Therefore, we define a schedule for \\alpha where \\alpha is decreased from a large value to a small value during training of the primary model. This corrective model is called the linear self-correction model as it uses the solution to a linear combination of KL divergences (Eq.~) to infer a distribution over latent segmentation labels.",
      "The challenge here is to train this subnetwork such that it predicts the segmentation labels more accurately than either \\pest or p(\\y|\\x). To this end, we introduce an additional term in the objective function, which trains the subnetwork using training examples in \\F while the primary model is being trained on the whole dataset: where the first and second terms train the primary model on \\F and \\W (we do not backpropagate through q in the second term) and the last term trains the convolutional self-correcting network. Because the q_{conv subnetwork is initialized randomly, it is not able to accurately predict segmentation labels on \\W early on during training.",
      "\\caption{An overview of our segmentation framework consisting of three models: i) Primary segmentation model generates a semantic segmentation of objects given an image. This is the main model that is subject to the training and is used at test time. ii) Ancillary segmentation model outputs a segmentation given an image and bounding box.",
      "\\end{figure* \\vspace{-0.1cm Methods Our goal is to train a semantic segmentation network in a semi-supervised setting using two training sets: i) a small fully supervised set (containing images, segmentation ground-truth and object bounding boxes) and ii) a weak set (containing images and object bounding boxes only). An overview of our framework is shown in Fig.~. There are three models: i) The Primary segmentation model generates a semantic segmentation of objects given an image.",
      "The simplest approach to training the primary model is to train it to predict using ground-truth labels on the fully supervised set \\F and the labels generated by the ancillary model on the weak set \\W. For this ``no-self-correction'' model the Self-correction module in Fig.~ merely copies the predictions made by the ancillary segmentation model. Training is guided by optimizing: where the first term is the cross-entropy loss with one-hot ground-truth labels as target and the second term is the cross-entropy with soft probabilistic labels generated by p_{anc as target."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 2,\n    \"Rationale\": \"Image 3 provides a visual overview of the segmentation framework, showing how the self-correction module fits into the process. Text 2 explains the role of the self-correction module in refining segmentations generated by the ancillary and primary models, which aligns with the question.\",\n    \"Answer\": \"The self-correction module refines segmentations generated by the ancillary and primary models for the weak set. It improves the segmentation labels by using a convolutional network to refine the input label distributions, enhancing the accuracy of the primary model's predictions.\"\n}"
  },
  "6": {
    "question": "What is the purpose of the ancillary heatmap shown in this paper?",
    "ground_truth": "The ancillary heatmap is used to correct the labels for missing or oversegmented objects in the images.",
    "selected_figures_names": [
      "1811.07073v3-Figure4-1.png",
      "1811.07073v3-Figure2-1.png",
      "1811.07073v3-Table1-1.png",
      "1811.07073v3-Table4-1.png"
    ],
    "all_figures_names": [
      "1811.07073v3-Figure4-1.png",
      "1811.07073v3-Figure2-1.png",
      "1811.07073v3-Table1-1.png",
      "1811.07073v3-Table4-1.png",
      "1811.07073v3-Table2-1.png",
      "1811.07073v3-Figure5-1.png",
      "1811.07073v3-Table3-1.png",
      "1811.07073v3-Figure3-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 shows the qualitative results of different models, including the ancillary model, which is relevant to understanding the purpose of the ancillary heatmap. Text 2 explains the role of the ancillary model and its self-correction mechanisms, which are crucial for understanding the purpose of the heatmap.",
    "answer": "The purpose of the ancillary heatmap in this paper is to provide probabilistic segmentation labels that help correct weak labels. The ancillary model, combined with self-correction mechanisms, improves segmentation performance by refining the predictions of the primary model.",
    "text_chunks": [
      "The ancillary model can successfully correct the labels for missing or over-segmented objects in these images (marked by ellipses). \\end{figure* \\setlength{\\tabcolsep{1pt \\setlength{\\tabcolsep{1pt Cityscapes Dataset In this section we evaluate performance on the Cityscapes dataset~ which contains images collected from cars driving in cities during different seasons.",
      "The last four columns represent the models in column 1464 of Table~. The Conv.~Self-correction model typically segments objects better than other models. \\end{figure* \\caption{Qualitative results on the PASCAL VOC 2012 auxiliary (the weak set). The heatmap of a single class for the ancillary model is shown for several examples.",
      "We introduced two mechanisms that enable the underlying primary model to correct the weak labels provided by an ancillary model. The proposed self-correction mechanisms combine the predictions made by the primary and ancillary model either using a linear function or trainable CNN. The experiments show that our proposed framework outperforms previous semi-supervised models on both the PASCAL VOC 2012 and Cityscapes datasets.",
      "We hypothesize two possible explanations for this observation. Firstly, this may be due to label noise in the 9k auxiliary set~ that negatively affects performance of Vanilla DeepLapv3+. As evidence, Fig.~ compares the output of the ancillary model with ground-truth annotations and highlights some of improperly labeled instances.",
      "In Sec.~, we present the ancillary model, and in Sec.~, we show a simple way to use this model to train the primary model. In Sec.~ and Sec.~, we present two variants of self-correcting model. Notation: \\x represents an image, \\bb represents object bounding boxes in an image, and \\y =",
      "In Table~, we show the performance of different variants of our model for different sizes of the fully supervised set \\F. The remaining examples in the training set are used as \\W. We make several observations from Table~: i) The ancillary model that predicts segmentation labels given an image and its object bounding boxes performs well even when it is trained with a training set as small as 200 images.",
      "The main issues with such procedures are i) bounding box information is not directly used to extract the segmentation mask, ii) the procedure may be suboptimal as it is hand-designed, and iii) the segmentation becomes ambiguous when multiple boxes overlap. In this paper, we take a different approach by designing an ancillary segmentation model that forms a per-pixel label distribution given an image and bounding box annotation. This model is easily trained using the fully supervised set (\\F) and can be used as a training signal for images in \\W. At inference time, both the image and its bounding box are fed to the network to obtain p_{anc(\\y|\\xw, \\bbw), the segmentation labels distribution.",
      "Our work differs from these previous methods in two significant aspects: i) We replace hand-crafted rules with an ancillary CNN for extracting probabilistic segmentation labels for an object within a box for the weak set. ii) We use a self-correcting model to correct for the mismatch between the output of the ancillary CNN and the primary segmentation model during training."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 shows the qualitative results of different models, including the ancillary model, which is relevant to understanding the purpose of the ancillary heatmap. Text 2 explains the role of the ancillary model and its self-correction mechanisms, which are crucial for understanding the purpose of the heatmap.\",\n    \"Answer\": \"The purpose of the ancillary heatmap in this paper is to provide probabilistic segmentation labels that help correct weak labels. The ancillary model, combined with self-correction mechanisms, improves segmentation performance by refining the predictions of the primary model.\"\n}"
  }
}