{
  "0": {
    "question": "How is convolution represented in the frequency domain?",
    "ground_truth": "In the frequency domain, convolution is represented by element-wise multiplication.",
    "selected_figures_names": [
      "1811.08257v1-Figure3-1.png"
    ],
    "all_figures_names": [
      "1811.08257v1-Figure4-1.png",
      "1811.08257v1-Figure2-1.png",
      "1811.08257v1-Table3-1.png",
      "1811.08257v1-Figure3-1.png",
      "1811.08257v1-Figure1-1.png",
      "1811.08257v1-Figure5-1.png",
      "1811.08257v1-Table5-1.png",
      "1811.08257v1-Table4-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 4,
    "rationale": "Image 0 visually represents the process of convolution in the frequency domain using FFT and iFFT, which aligns with the question. Text 4 explains the transformation of convolution operations to the frequency domain using FFT, which directly answers the question.",
    "answer": "Convolution in the frequency domain is represented by converting the input and filters to the frequency domain using FFT, performing pointwise multiplication, and then applying the inverse FFT (iFFT) to obtain the result.",
    "text_chunks": [
      "From the formula we can see that the representations of an image in different domains have the same shape. For simplicity, we denote the FFT of input x as \\mathcal{F(x). An important property of the FFT used in this paper is linearity, i.e. for two inputs x and y, we have the following equation: To reduce the number of required operations when evaluating convolutional layers, we use the Convolution Theorem: the convolutions in the space domain are equivalent to pointwise products in the frequency domain.",
      "According to the Convolution Theorem (), the pointwise product of \\mathcal{F(x) and \\mathcal{F(f_i) corresponds to the convolution of x and f_i. Assume that the convolution result is y and the corresponding frequency domain values are \\mathcal{F(y)_R and \\mathcal{F(y)_I, we have where ``\\otimes'' represents SIMDMul, and ``\\oplus'' represents SIMDAdd. The server S then generates a random vector r of size w \\times h, and encrypts its FFT values as [-\\mathcal{F(r)_R] and [-\\mathcal{F(r)_I].",
      "In order to present our idea clearly, we first explain how to calculate the convolution in the plaintext domain for the filter f_i (i\\in\\left[1, k\\right]) that contains c channels f_{i1,f_{i2,\\cdots,f_{ic, and the input x that contains c channels x_{1,x_{2,\\cdots,x_{c. As shown in Fig. , c",
      "Then we describe a more general case where c>1 and k>1. \\subsubsection{Simple Case. In this case, the inputs are two ciphertexts, namely [\\mathcal{F(x)_R] and [\\mathcal{F(x)_I], both plaintexts of which have size w \\times h. To apply the convolution operation on frequency domain, server S performs the FFT on the filter of size f_w",
      "Denoting \\mathcal{F^{-1 as the inverse FFT, the convolutions between inputs x and y can be computed as follows: This transformation can be used to accelerate training and prediction of CNN models. The basic idea is to convert the input and filters of a convolutional layer to the frequency domain, and then perform the pointwise product instead of the convolution.",
      "Also, the selected n, i.e. the number of plaintext slots, satisfied n > w\\times h. The server S has access to k plaintext filters and needs to compute the convolution results of the input and these filters. Without FFT, previous works transform the convolution operations to matrix multiplications first, and then calculate it on the ciphertext domain.",
      "The convolutional layer has k filters (or kernels) of size f_w \\times f_h \\times c.",
      "In particular, the number of SIMDAdd and SIMDMul is respectively fixed to 2 and 4 for a given input image. At a high level, client C and server S transform their raw data, i.e. the input and the filters, into the frequency domain with the FFT, and then S evaluates the convolutional layer in the frequency domain with homomorphic encryption. In this part, we first introduce a simple case where the input has single channel (c=1) and the layer has only one filter (k=1) to present our key idea."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 4,\n    \"Rationale\": \"Image 0 visually represents the process of convolution in the frequency domain using FFT and iFFT, which aligns with the question. Text 4 explains the transformation of convolution operations to the frequency domain using FFT, which directly answers the question.\",\n    \"Answer\": \"Convolution in the frequency domain is represented by converting the input and filters to the frequency domain using FFT, performing pointwise multiplication, and then applying the inverse FFT (iFFT) to obtain the result.\"\n}"
  },
  "1": {
    "question": "How does the number of classes affect the setup and online time for the Softmax?",
    "ground_truth": "The setup and online time for the Softmax increases as the number of classes increases.",
    "selected_figures_names": [
      "1811.08257v1-Table4-1.png"
    ],
    "all_figures_names": [
      "1811.08257v1-Table5-1.png",
      "1811.08257v1-Figure3-1.png",
      "1811.08257v1-Table4-1.png",
      "1811.08257v1-Table6-1.png",
      "1811.08257v1-Table2-1.png",
      "1811.08257v1-Figure5-1.png",
      "1811.08257v1-Figure1-1.png",
      "1811.08257v1-Figure4-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides a table showing the setup and online time for different numbers of classes, which directly answers the question. Text 0 discusses the runtime cost and communication overhead related to the number of classes, making it relevant to the question.",
    "answer": "The setup and online time for the Softmax increases with the number of classes. For example, as the number of classes increases from 10 to 1000, both setup and online times increase significantly.",
    "text_chunks": [
      "As shown in Table~, the runtime cost and communication overhead of setup and online phases grow with the accuracy and the number of classes. The runtime and communication overhead of our proposed softmax evaluation method is relatively small compared with other layers of a CNN. Evaluations on Real-World Models We evaluated the performance of FALCON on two real-world models, MNIST and CIFAR-10.",
      "We show the time cost and the communication overhead in both setup and online phases. The time cost includes the computation cost of client C and server S and the transmission latency between them. To be noted, for the fairness of comparison, the softmax layer is excluded in both models.",
      "Because MiniONN and GAZELLE outperform all previous works, we only compare FALCON with them to show our performance superiority. In all previous works, only the SecureML mentioned that the softmax function can be evaluated with secure two-party computation. But it provided no details and we will show that the softmax max is too complex to implement with secure two-party computation directly.",
      "In all the classification CNNs, the last fully connected layer is always followed by a softmax layer to generate the probability distribution over K different possible classes. However, to our best knowledge, in all previous work, the authors presented that the server can return the logits to the client, who could obtain the probabilities by performing softmax function locally, e.g. GAZELLE, or the client runs argmax using secure two-party computation to only obtain the classification result without knowing logits and probabilities, e.g. MiniONN. There are two main reasons why these schemes bypass the encrypted computation of softmax layer: The heavy computation complexity is due to the division and exponentiation operations in the softmax function when using homomorphic encryption or secure two-party computation, and we thus propose a division and exponentiation free protocol to avoid performing these operations with secure two-party computation.",
      "Recall that the softmax function is given by f(x)_i=\\frac{e^{x_i{\\sum_{k=1^{K{e^{x_k, \\quad\\mbox{for i=1,2,\\cdots,K, where f(x)_i is the probability that the input belongs to the class i. Letting the target class be t, our aim is to calculate p_t=\\frac{e^{x_t{\\sum_{k=1^{K{e^{x_k. For the numerator, we have the client C calculate argmax and max functions with the server S using Yao's Garbled Circuits to learn t and share x_t. For the denominator, we let S and C multiplicatively share e^{x, thus translating highly complex calculations into simple vector inner product.",
      "We notice that in a client-server scenario, by only accessing prediction results, C is able to extract an equivalent or near-equivalent model , infer the training set , and generate adversarial examples . To tackle this issue, S can only return the necessary result, i.e. the class to which the input belongs and its corresponding probability, to C. In MiniONN, the target class will be returned without probability. While in FALCON, the probability can also be obtained by C with our secure softmax protocol.",
      "Those functions are chosen for their popularity in CNN. For the privacy-preserving evaluation of softmax layer, which has not yet been addressed by any previous work, we present an efficient and secure two-party computation protocol, which is free of using expensive division and exponentiation computations in boolean circuits. Our contributions are summarized as follows: Related Work CryptoNets scheme inspired us to process neural network models securely with leveled homomorphic encryption (LHE).",
      "Note that, the setup phases involve performing FFT on filters and encrypting random values for masking, and the online phases take only the server's computation into account. As one can see from Table~, FALCON outperforms GAZELLE in both setup and online phases. Especially for online phases, our efficient Conv and FC implementations offer us over 10\\times less runtime."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides a table showing the setup and online time for different numbers of classes, which directly answers the question. Text 0 discusses the runtime cost and communication overhead related to the number of classes, making it relevant to the question.\",\n    \"Answer\": \"The setup and online time for the Softmax increases with the number of classes. For example, as the number of classes increases from 10 to 1000, both setup and online times increase significantly.\"\n}"
  },
  "2": {
    "question": "Which framework has the lowest total communication cost for MNIST?",
    "ground_truth": "FALCON",
    "selected_figures_names": [
      "1811.08257v1-Table5-1.png",
      "1811.08257v1-Figure4-1.png",
      "1811.08257v1-Figure2-1.png",
      "1811.08257v1-Figure1-1.png"
    ],
    "all_figures_names": [
      "1811.08257v1-Table5-1.png",
      "1811.08257v1-Figure4-1.png",
      "1811.08257v1-Figure2-1.png",
      "1811.08257v1-Figure1-1.png",
      "1811.08257v1-Figure3-1.png",
      "1811.08257v1-Table2-1.png",
      "1811.08257v1-Table3-1.png",
      "1811.08257v1-Table6-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides a performance comparison table for different frameworks on MNIST, including communication costs. Text 0 discusses the performance of FALCON on MNIST, which is relevant to the question.",
    "answer": "GAZELLE has the lowest total communication cost for MNIST.",
    "text_chunks": [
      "The input to this CIFAR-10 model is a three channel image of size 32\\times 32\\times 3, and the output is the class it belongs to. This model contains 7 convolutional, 1 fully-connected, 7 ReLU and 2 mean pooling layers. As shown in Table , when evaluating the online overhead on MNIST model, FALCON is running over 27\\times faster than MiniONN while reducing communication overhead by over 97\\%.",
      "As shown in Table~, the runtime cost and communication overhead of setup and online phases grow with the accuracy and the number of classes. The runtime and communication overhead of our proposed softmax evaluation method is relatively small compared with other layers of a CNN. Evaluations on Real-World Models We evaluated the performance of FALCON on two real-world models, MNIST and CIFAR-10.",
      "\\smallskip \\noindentPerformance. The total cost is 1\\cdotSubsetGate+(k-1)\\cdotSIMD(N/k). \\subsubsection{The Optimized ReLU and max pooling layers.",
      "For the online overhead in CIFAR-10, FALCON is about 25\\times faster than MiniONN and reduce communication overhead by over 97\\%. The significant improvement in running time is due to the repeatedly use of FFT and lattice-based homomorphic encryption, which saves many multiplications over ciphertexts. When comparing with GAZELLE, FALCON also shows improvements on online overhead.",
      "\\smallskip \\noindentPerformance. The total cost is 2\\cdotSIMD(N). \\subsubsection{Secure pooling layer.",
      "MNIST is a dataset for handwritten digits and is widely used for image classification and recognition tasks. We use the CNN model for MNIST from . This CNN model takes a gray scale image with size 28\\times 28 as input, and outputs the classification result, which is either 0 to 9.",
      "We first apply max pooling operations to obtain max values of each region, and ReLU operations follow to filter out all negative values. \\smallskip \\noindentPerformance. The total cost is 1\\cdotSubsetGate+(k-1)\\cdotSIMD(N/k)+2\\cdotSIMD(N/k)=1\\cdotSubsetGate+(k+1)\\cdotSIMD(N/k).",
      "In Listing , all the yshr variables are vectors of length N, which equals to the number of input elements. \\smallskip \\noindentPerformance. As reported in ABY paper , the computation and the communication costs of ADDGate, SUBGate, GTGate and MUXGate are almost the same when using Yao sharing."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides a performance comparison table for different frameworks on MNIST, including communication costs. Text 0 discusses the performance of FALCON on MNIST, which is relevant to the question.\",\n    \"Answer\": \"GAZELLE has the lowest total communication cost for MNIST.\"\n}"
  },
  "3": {
    "question": "What is the function of the DataPreprocessing function?",
    "ground_truth": "The DataPreprocessing function performs Yao Sharing, which is a cryptographic technique for securely sharing data between multiple parties.",
    "selected_figures_names": [
      "1811.08257v1-Figure4-1.png"
    ],
    "all_figures_names": [
      "1811.08257v1-Figure3-1.png",
      "1811.08257v1-Figure4-1.png",
      "1811.08257v1-Figure1-1.png",
      "1811.08257v1-Figure5-1.png",
      "1811.08257v1-Table6-1.png",
      "1811.08257v1-Table3-1.png",
      "1811.08257v1-Table2-1.png",
      "1811.08257v1-Table5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "The image provides a function description of data preprocessing, which is directly relevant to understanding the function of the DataPreprocessing function. Text chunk 0 explains the concept of additive shares, which is crucial for understanding the context of the function.",
    "answer": "The DataPreprocessing function returns the result of (x_C + x_S) % p in Yao Sharing. It involves operations like addition, comparison, subtraction, and multiplexing to process the additive shares held by the client and server.",
    "text_chunks": [
      "\\subsubsection{Data preprocessing. Assume that x^C=\\{x^C_1,x^C_2,\\cdots, x^C_N\\ and x^S=\\{x^S_1,x^S_2,\\cdots,x^S_N\\ are the additive shares held by client C and server S, respectively. Since both x^C_i and x^S_i belong to \\left[0,p\\right), we have x^C_i + x^S_i belongs to \\left[0,2p\\right).",
      "Finally, we describe the method to translate Yao sharing back to additive shares. Before introduing the technical details, we declare that the data preprocessing only happens when shifting from convolutional layers to activation or pooling layers. That is, for consecutive activation or pooling layers, the data prepocessing will be performed only once.",
      "Thus, we denote the cost of one operation by SIMD(N) , where N is the number of values in one share, and the total cost of data preprocessing is 4\\cdotSIMD(N). \\subsubsection{Secure ReLU layer. Typically, a convolutional layer and a non-last fully connected layer are followed by a ReLU activation layer, which decides whether an input value should be continued to pass down, or in other words, whether the neuron is activated by the input value, as shown in Fig.~. Recall that the ReLU function is f(x)=\\mbox{max(x, 0), and in our setting, the input x=\\{x_1,x_2,\\cdots,x_N\\ is additively shared by client C and server S, i.e. x^C+x^S=x. Our aim is to enable that C and S additively share \\mbox{max(x, 0).",
      "Other works used a compromise way to deal with it, i.e., using argmax to replace softmax function. However, often there are cases that clients need to learn the probabilities of different output labels. Preliminaries Convolutional Neural Network",
      "The data preprocessing also guarantees that the Yao sharing ranges from \\left[0,p\\right). Then we present the boolean circuits for the ReLU and max pooling functions. This boolean circuits ever was used in GAZELLE and we present it here for the completeness of the paper.",
      "We can find that this process is equivalent to the following one: \\max\\left(\\max\\left(x_1,x_2,\\cdots,x_k\\right),0\\right), where the inside \\max() can be considered as the max pooling function while the outside as the ReLU function. Based on this observation, reversing the position of the ReLU layer and the max pooling layer in the processing pipeline will reduce the number of \\max() operations. An example is shown in Fig.~. In fact, this trick has been proposed in the study of deep learning .",
      "Because the softmax function involves division and exponentiation that are very difficult to evaluate in a privacy-preserving way, the existing works, e.g., CryptoNets, MiniONN and GAZELLE, used argmax function instead of softmax to obtain only the top one label. We, however, propose a novel efficient protocol for the softmax layer. In FALCON, we utilize the linear property of fast Fourier Transform (FFT) to design simple but efficient protocols for linear layers, i.e., convolutional and fully-connected layers.",
      "Also, the selected n, i.e. the number of plaintext slots, satisfied n > w\\times h. The server S has access to k plaintext filters and needs to compute the convolution results of the input and these filters. Without FFT, previous works transform the convolution operations to matrix multiplications first, and then calculate it on the ciphertext domain."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"The image provides a function description of data preprocessing, which is directly relevant to understanding the function of the DataPreprocessing function. Text chunk 0 explains the concept of additive shares, which is crucial for understanding the context of the function.\",\n    \"Answer\": \"The DataPreprocessing function returns the result of (x_C + x_S) % p in Yao Sharing. It involves operations like addition, comparison, subtraction, and multiplexing to process the additive shares held by the client and server.\"\n}"
  },
  "4": {
    "question": "Which framework is faster for setting up and running the FC layer?",
    "ground_truth": "FALCON is faster for both setting up and running the FC layer.",
    "selected_figures_names": [
      "1811.08257v1-Table2-1.png"
    ],
    "all_figures_names": [
      "1811.08257v1-Figure5-1.png",
      "1811.08257v1-Table2-1.png",
      "1811.08257v1-Figure4-1.png",
      "1811.08257v1-Table5-1.png",
      "1811.08257v1-Figure1-1.png",
      "1811.08257v1-Table6-1.png",
      "1811.08257v1-Figure3-1.png",
      "1811.08257v1-Figure2-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 0,
    "rationale": "Image 0 provides a direct comparison of setup and online times for the FC layer between GAZELLE and FALCON. Text 0 discusses the performance of FALCON compared to GAZELLE, specifically mentioning the efficiency in setup and online phases.",
    "answer": "FALCON is faster for setting up and running the FC layer.",
    "text_chunks": [
      "Note that, the setup phases involve performing FFT on filters and encrypting random values for masking, and the online phases take only the server's computation into account. As one can see from Table~, FALCON outperforms GAZELLE in both setup and online phases. Especially for online phases, our efficient Conv and FC implementations offer us over 10\\times less runtime.",
      "As shown in Table~, the runtime cost and communication overhead of setup and online phases grow with the accuracy and the number of classes. The runtime and communication overhead of our proposed softmax evaluation method is relatively small compared with other layers of a CNN. Evaluations on Real-World Models We evaluated the performance of FALCON on two real-world models, MNIST and CIFAR-10.",
      "For benchmarking, all input data to each layers are randomly sampled from \\left[0, p\\right). For convolutional and fully connected layers, their parameters are chosen from the CIFAR-10 model stated in Section . Our first contribution is accelerating convolutional and fully-connected layers by the integrated use of FFT and lattice-based homomorphic encryption.",
      "Secure Fully Connected Layer Normally, the fully connected layer can be treated as the multiplication of a weight matrix and an input vector, and this can be executed very fast in the plaintext domain. However, in the ciphertext domain, especially using SIMD techniques in homomorphic encryption, this kind of multiplication is expensive. Inspired by the observation that fully connected layer can be viewed as convolutional layers with filters that cover the entire input regions , we propose an efficient solution by transforming the FC layer to the convolutional layer first, and then utilizing the acceleration method in Section to evaluate the fully-connected layer.",
      "In this paper, we use the similar idea to speed up the convolution operation in the ciphertext domain, and we will show that FFT operations have little effect on the performance of a CNN model. Cryptographic Building Blocks \\subsubsection{Lattice-based Homomorphic Encryption. To implement privacy-preserving convolutions in FALCON, we require two kinds of homomorphic operations: SIMDAdd and SIMDMul.",
      "For non-linear layers including ReLU and max pooling, we provided a detailed implementation including the usage of optimized processing pipeline. For the softmax layer that has not been studied in previous works, we introduced the first efficient and privacy-preserving protocol for it. Finally, our evaluation results show that FALCON has less computation and communication overhead in each layer and achieves best known performance thus far on CNN models.",
      "Because the softmax function involves division and exponentiation that are very difficult to evaluate in a privacy-preserving way, the existing works, e.g., CryptoNets, MiniONN and GAZELLE, used argmax function instead of softmax to obtain only the top one label. We, however, propose a novel efficient protocol for the softmax layer. In FALCON, we utilize the linear property of fast Fourier Transform (FFT) to design simple but efficient protocols for linear layers, i.e., convolutional and fully-connected layers.",
      "As the authors in GAZELLE and MiniONN haven't released their codes, here we use the results from their papers for comparison. Benchmarks for Layers Here we introduce the performance of different layers, i.e. convolutional, fully-connected, ReLU, max pooling, and softmax layers. Since GAZELLE is the best known one among existing works, we only compare FALCON with it in all layers except the softmax layer, which is not implemented by GAZELLE."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 0,\n    \"Rationale\": \"Image 0 provides a direct comparison of setup and online times for the FC layer between GAZELLE and FALCON. Text 0 discusses the performance of FALCON compared to GAZELLE, specifically mentioning the efficiency in setup and online phases.\",\n    \"Answer\": \"FALCON is faster for setting up and running the FC layer.\"\n}"
  },
  "5": {
    "question": "What is the purpose of the `SubsetGate` function in the MaxPooling function?",
    "ground_truth": "The `SubsetGate` function is used to split the input `x` into `k` pieces.",
    "selected_figures_names": [
      "1811.08257v1-Figure4-1.png",
      "1811.08257v1-Table5-1.png",
      "1811.08257v1-Table3-1.png",
      "1811.08257v1-Table6-1.png"
    ],
    "all_figures_names": [
      "1811.08257v1-Figure4-1.png",
      "1811.08257v1-Table5-1.png",
      "1811.08257v1-Table3-1.png",
      "1811.08257v1-Table6-1.png",
      "1811.08257v1-Figure5-1.png",
      "1811.08257v1-Table4-1.png",
      "1811.08257v1-Table2-1.png",
      "1811.08257v1-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 4,
    "rationale": "Text chunk 4 explains the use of the SubsetGate in the context of max pooling, describing how it is used to pack elements into vectors for comparison. Image 0 provides a visual representation of boolean circuits, which is relevant to understanding the function of gates like SubsetGate.",
    "answer": "The SubsetGate function is used to pack elements into vectors for comparison within the max pooling function, facilitating the process of finding the maximum element in each region.",
    "text_chunks": [
      "\\smallskip \\noindentPooling Layer. Pooling functions reduce the dimensionality of the input by calculating the mean or the maximum value. A pooling layer will partition the input into a set of non-overlapping rectangle sub-areas and perform the mean or the maximum function on each sub-area.",
      "We can find that this process is equivalent to the following one: \\max\\left(\\max\\left(x_1,x_2,\\cdots,x_k\\right),0\\right), where the inside \\max() can be considered as the max pooling function while the outside as the ReLU function. Based on this observation, reversing the position of the ReLU layer and the max pooling layer in the processing pipeline will reduce the number of \\max() operations. An example is shown in Fig.~. In fact, this trick has been proposed in the study of deep learning .",
      "We first apply max pooling operations to obtain max values of each region, and ReLU operations follow to filter out all negative values. \\smallskip \\noindentPerformance. The total cost is 1\\cdotSubsetGate+(k-1)\\cdotSIMD(N/k)+2\\cdotSIMD(N/k)=1\\cdotSubsetGate+(k+1)\\cdotSIMD(N/k).",
      "The second GTGate judges if the sum exceeds p, and the result is sent to the MUXGate, which is a multiplexer that decides whether to subtract p or not. The subtraction is done by the SUBGate. These circuits guarantee that the output is \\left(x^C + x^S\\right) \\mbox{mod~p. Note that all the operations on the shares are in SIMD programming style, which is supported by the ABY framework, thereby reducing the overhead greatly.",
      "The pseudocode of the designed boolean circuits is shown in Listing . Since the input has been limited from 0 to p/2 by the ReLU layer, we can iteratively compare two elements to obtain the max element with GT and MUX circuits without considering the existence of negative elements. Because comparisons are performed inside each region, we pack N elements into k vectors of size N/k through SubsetGate.",
      "That is, C holds \\mbox{max(x, 0)-r while S holds r, where r is randomly generated by S. MiniONN considers \\mbox{max(x, 0) = x\\cdot \\mbox{compare(x, 0), and implements the ReLU function by GTGate and MULGate (Multiplication Gate) provided by ABY. Due to the relatively high overhead of MULGate, the MULGate is replaced by MUXGate, which has a much less overhead as stated in ABY paper.",
      "The pseudocode is shown in Listing . Recall that, for a given x, x > p/2 implies x is negative, and positive otherwise. The first GTGate performs a great-than operation (>), and the output is used by the MUXGate to select the positive x or 0 as the result.",
      "\\smallskip \\noindentPerformance. The total cost is 1\\cdotSubsetGate+(k-1)\\cdotSIMD(N/k). \\subsubsection{The Optimized ReLU and max pooling layers."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 4,\n    \"Rationale\": \"Text chunk 4 explains the use of the SubsetGate in the context of max pooling, describing how it is used to pack elements into vectors for comparison. Image 0 provides a visual representation of boolean circuits, which is relevant to understanding the function of gates like SubsetGate.\",\n    \"Answer\": \"The SubsetGate function is used to pack elements into vectors for comparison within the max pooling function, facilitating the process of finding the maximum element in each region.\"\n}"
  },
  "6": {
    "question": "What is the purpose of the activation layer in a convolutional neural network?",
    "ground_truth": "The activation layer applies a non-linear function to the output of the convolution layer. This allows the network to learn more complex features from the data.",
    "selected_figures_names": [
      "1811.08257v1-Figure1-1.png"
    ],
    "all_figures_names": [
      "1811.08257v1-Figure4-1.png",
      "1811.08257v1-Figure5-1.png",
      "1811.08257v1-Figure1-1.png",
      "1811.08257v1-Figure3-1.png",
      "1811.08257v1-Table2-1.png",
      "1811.08257v1-Table6-1.png",
      "1811.08257v1-Figure2-1.png",
      "1811.08257v1-Table4-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 1,
    "rationale": "Image 0 visually represents the structure of a CNN, including the activation layer, while Text 1 explains the role of activation layers in CNNs, highlighting their non-linear properties.",
    "answer": "The purpose of the activation layer in a convolutional neural network is to introduce non-linearity into the model, allowing it to learn complex patterns. Activation functions like ReLU are applied to each pixel, enabling the network to make more sophisticated predictions.",
    "text_chunks": [
      "Neural networks use non-linear activation functions to improve non-linearity of models. Since ReLU (rectified linear unit) is the default activation function and has the best performance in popular CNNs , we use it in our FALCON. The ReLU function is shown as follows: The output of a ReLU activation layer has the same shape as the input, which means f(x) is applied to each pixel in the input images.",
      "For a CNN model, it usually includes convolutional, activation, pooling, and fully-connected layers, and often follows by a softmax layer. Convolutional and fully-connected layers have linear property, while activation and pooling are nonlinear layers. The softmax layer is used to normalize the output of previous layers into the form of probability representation, usually used in the last layer of a CNN to output a human-friendly prediction result.",
      "A convolutional neural network (CNN) consists of four types of layers, namely convolutional, activation, pooling, and fully connected. An example CNN is shown in Fig. . The first layer of a neural network takes the original data (e.g. an X-Ray image) as the input, and each subsequent layer computes a function over the previous output.",
      "Thus, we denote the cost of one operation by SIMD(N) , where N is the number of values in one share, and the total cost of data preprocessing is 4\\cdotSIMD(N). \\subsubsection{Secure ReLU layer. Typically, a convolutional layer and a non-last fully connected layer are followed by a ReLU activation layer, which decides whether an input value should be continued to pass down, or in other words, whether the neuron is activated by the input value, as shown in Fig.~. Recall that the ReLU function is f(x)=\\mbox{max(x, 0), and in our setting, the input x=\\{x_1,x_2,\\cdots,x_N\\ is additively shared by client C and server S, i.e. x^C+x^S=x. Our aim is to enable that C and S additively share \\mbox{max(x, 0).",
      "In this paper, we focus on the fast and secure solution for Convolutional Neural Networks (CNNs), one of the most important neural networks in deep learning. This kind of neural network is characterized by the spatial input data, such as images and speeches. Typically, a deep neural network consists of many layers, each of which has its specific functionality.",
      "It is non-interactive and allows a client to obtain a result in one round of communication. Since only LHE is used, CryptoNets only supports linear activation and pooling functions, i.e. square activation and mean pooling. However, in real-world scenarios, the square activation functions are barely used for training or prediction.",
      "Below we describe the clean abstractions of each layer. \\smallskip \\noindentConvolutional Layer (Conv). The input to a convolutional layer is a w \\times h \\times c image where w and h are the width and height of the image and c is the number of channels, e.g. an RGB image has c=3.",
      "In a convolutional layer, an input image is convolved in turn with k filters, and thus producing k output images. To better understand this operation, we consider a f_w \\times f_h"
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 1,\n    \"Rationale\": \"Image 0 visually represents the structure of a CNN, including the activation layer, while Text 1 explains the role of activation layers in CNNs, highlighting their non-linear properties.\",\n    \"Answer\": \"The purpose of the activation layer in a convolutional neural network is to introduce non-linearity into the model, allowing it to learn complex patterns. Activation functions like ReLU are applied to each pixel, enabling the network to make more sophisticated predictions.\"\n}"
  },
  "7": {
    "question": "What is the role of the filter in the convolution operation?",
    "ground_truth": "The filter is used to extract features from the input image. It is a small matrix that is applied to each pixel in the image, and the result is a new pixel value.",
    "selected_figures_names": [
      "1811.08257v1-Figure2-1.png"
    ],
    "all_figures_names": [
      "1811.08257v1-Table2-1.png",
      "1811.08257v1-Figure5-1.png",
      "1811.08257v1-Figure2-1.png",
      "1811.08257v1-Table6-1.png",
      "1811.08257v1-Table5-1.png",
      "1811.08257v1-Table4-1.png",
      "1811.08257v1-Table3-1.png",
      "1811.08257v1-Figure1-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 3,
    "rationale": "Image 0 visually represents the convolution operation, showing how filters interact with input images. Text 3 explains the role of filters in a convolutional layer, detailing how each filter produces an output image by convolving with the input.",
    "answer": "In a convolution operation, the filter (or kernel) is used to extract features from the input image. Each filter is convolved with the input to produce an output image, capturing specific patterns or features. Multiple filters are used to generate multiple feature maps, each highlighting different aspects of the input.",
    "text_chunks": [
      "Also, the selected n, i.e. the number of plaintext slots, satisfied n > w\\times h. The server S has access to k plaintext filters and needs to compute the convolution results of the input and these filters. Without FFT, previous works transform the convolution operations to matrix multiplications first, and then calculate it on the ciphertext domain.",
      "In order to present our idea clearly, we first explain how to calculate the convolution in the plaintext domain for the filter f_i (i\\in\\left[1, k\\right]) that contains c channels f_{i1,f_{i2,\\cdots,f_{ic, and the input x that contains c channels x_{1,x_{2,\\cdots,x_{c. As shown in Fig. , c",
      "The convolutional layer has k filters (or kernels) of size f_w \\times f_h \\times c.",
      "In a convolutional layer, an input image is convolved in turn with k filters, and thus producing k output images. To better understand this operation, we consider a f_w \\times f_h",
      "The final output is the result of summing up these images. Denoting the input images as x_i,i\\in [1,c], the filters as w_{i,j,i\\in [1,c],j\\in [1,k], and the output images as y_j, j\\in[1,k], we can express the above computation as follows: where ``*'' is convolution operation. \\smallskip \\noindentActivation Layer.",
      "Denoting \\mathcal{F^{-1 as the inverse FFT, the convolutions between inputs x and y can be computed as follows: This transformation can be used to accelerate training and prediction of CNN models. The basic idea is to convert the input and filters of a convolutional layer to the frequency domain, and then perform the pointwise product instead of the convolution.",
      "From the formula we can see that the representations of an image in different domains have the same shape. For simplicity, we denote the FFT of input x as \\mathcal{F(x). An important property of the FFT used in this paper is linearity, i.e. for two inputs x and y, we have the following equation: To reduce the number of required operations when evaluating convolutional layers, we use the Convolution Theorem: the convolutions in the space domain are equivalent to pointwise products in the frequency domain.",
      "Then, server S sets r to its share x^S. At this point, the convolutional layer has been evaluated, and the result y is additively shared by S and C. \\subsubsection{General Case. In the convolutional layer, all k filters are independently convolved with the input to get k outputs, as shown in Fig. ."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 3,\n    \"Rationale\": \"Image 0 visually represents the convolution operation, showing how filters interact with input images. Text 3 explains the role of filters in a convolutional layer, detailing how each filter produces an output image by convolving with the input.\",\n    \"Answer\": \"In a convolution operation, the filter (or kernel) is used to extract features from the input image. Each filter is convolved with the input to produce an output image, capturing specific patterns or features. Multiple filters are used to generate multiple feature maps, each highlighting different aspects of the input.\"\n}"
  },
  "8": {
    "question": "Which operation has the lowest online time?",
    "ground_truth": "ReLU",
    "selected_figures_names": [
      "1811.08257v1-Table3-1.png"
    ],
    "all_figures_names": [
      "1811.08257v1-Table2-1.png",
      "1811.08257v1-Figure4-1.png",
      "1811.08257v1-Table3-1.png",
      "1811.08257v1-Figure2-1.png",
      "1811.08257v1-Figure1-1.png",
      "1811.08257v1-Table5-1.png",
      "1811.08257v1-Table4-1.png",
      "1811.08257v1-Figure5-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 5,
    "rationale": "Image 0 provides a table with detailed benchmarks for various operations, including their online times. Text 5 discusses the online communication overhead of ReLU and Max Pooling, which is relevant to understanding the performance metrics shown in the image.",
    "answer": "MaxPooling has the lowest online time.",
    "text_chunks": [
      "The online running time of ReLU and MaxPooling in is about 10\\times faster than that in while all algorithms and experimental settings stay no change. Furthermore, we also found inconsistent data in , the most recent version. In Table 11 of , the offline running time of GAZELLE on network topology D is 0.481 seconds.",
      "For the online overhead in CIFAR-10, FALCON is about 25\\times faster than MiniONN and reduce communication overhead by over 97\\%. The significant improvement in running time is due to the repeatedly use of FFT and lattice-based homomorphic encryption, which saves many multiplications over ciphertexts. When comparing with GAZELLE, FALCON also shows improvements on online overhead.",
      "Note that, the setup phases involve performing FFT on filters and encrypting random values for masking, and the online phases take only the server's computation into account. As one can see from Table~, FALCON outperforms GAZELLE in both setup and online phases. Especially for online phases, our efficient Conv and FC implementations offer us over 10\\times less runtime.",
      "The input to this CIFAR-10 model is a three channel image of size 32\\times 32\\times 3, and the output is the class it belongs to. This model contains 7 convolutional, 1 fully-connected, 7 ReLU and 2 mean pooling layers. As shown in Table , when evaluating the online overhead on MNIST model, FALCON is running over 27\\times faster than MiniONN while reducing communication overhead by over 97\\%.",
      "As shown in Table~, the runtime cost and communication overhead of setup and online phases grow with the accuracy and the number of classes. The runtime and communication overhead of our proposed softmax evaluation method is relatively small compared with other layers of a CNN. Evaluations on Real-World Models We evaluated the performance of FALCON on two real-world models, MNIST and CIFAR-10.",
      "In Table , we report the running time and communication overhead of setup and online phases for ReLU and Max Pooling layers. Note that, since we utilize ABY to implement our scheme, these experimental results succeed its optimization for secure two-party computation. In addition to the data preprocessing, the online communication overhead of ReLU and max pooling operations is almost negligible.",
      "In this paper, we use the similar idea to speed up the convolution operation in the ciphertext domain, and we will show that FFT operations have little effect on the performance of a CNN model. Cryptographic Building Blocks \\subsubsection{Lattice-based Homomorphic Encryption. To implement privacy-preserving convolutions in FALCON, we require two kinds of homomorphic operations: SIMDAdd and SIMDMul.",
      "We show the time cost and the communication overhead in both setup and online phases. The time cost includes the computation cost of client C and server S and the transmission latency between them. To be noted, for the fairness of comparison, the softmax layer is excluded in both models."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 5,\n    \"Rationale\": \"Image 0 provides a table with detailed benchmarks for various operations, including their online times. Text 5 discusses the online communication overhead of ReLU and Max Pooling, which is relevant to understanding the performance metrics shown in the image.\",\n    \"Answer\": \"MaxPooling has the lowest online time.\"\n}"
  }
}