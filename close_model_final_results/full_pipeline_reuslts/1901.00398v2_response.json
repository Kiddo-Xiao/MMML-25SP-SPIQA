{
  "0": {
    "question": " Which type of review was more accurately identified by the human evaluators, human-written or machine-generated? ",
    "ground_truth": "The human evaluators were more accurate at identifying human-written reviews than machine-generated reviews.",
    "selected_figures_names": [
      "1901.00398v2-Table5-1.png",
      "1901.00398v2-Figure9-1.png",
      "1901.00398v2-Figure2-1.png",
      "1901.00398v2-Figure3-1.png"
    ],
    "all_figures_names": [
      "1901.00398v2-Table5-1.png",
      "1901.00398v2-Figure9-1.png",
      "1901.00398v2-Figure2-1.png",
      "1901.00398v2-Figure3-1.png",
      "1901.00398v2-Table8-1.png",
      "1901.00398v2-Figure11-1.png",
      "1901.00398v2-Figure1-1.png",
      "1901.00398v2-Table6-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 0,
    "rationale": "Image 2 shows the accuracy of human evaluators on individual reviews, highlighting their performance on human-written versus machine-generated reviews. Text 0 provides specific accuracy rates for human evaluators, indicating they are better at identifying human-written reviews.",
    "answer": "Human evaluators more accurately identified human-written reviews.",
    "text_chunks": [
      "% Comparing to the ground-truth (of whether a review is machine-generated or collected from Amazon), individual human decisions are 66.61\\% accurate, while their majority votes can reach 72.63\\%. Neither of them is close to perfect. We observe that human evaluators generally do better at correctly labelling human-written reviews as real (true positive rate of 78.96\\% for H1 and 88.31\\% for H2), and they are confused by machine-generated reviews in close to half of the cases (true negative rate of 54.26\\% for H1 and 56.95\\% for H2).",
      "After annotating a batch of reviews, workers are asked to explain their decisions by filling in an optional free-text comment. This enables us to have a better understanding of what differentiates machine-generated from human-written reviews from human's perspective. Analyzing their comments, we identify the main reasons why human evaluators annotate a review as machine-written.",
      "These two observations are intriguing, which indicates that when identifying fake reviews, humans might focus more on word usage rather than trying to construct a ``decision boundary'' mentally. In summary, we find that 1) human evaluators cannot distinguish machine-generated reviews from real reviews perfectly, with significant bias between the two classes; 2) meta-adversarial evaluators can better distinguish individual fake reviews, but their rankings at the generator level tend to be negatively correlated with human evaluators; and 3) text-overlap evaluators are highly correlated with human evaluators in ranking generators. % Discussion We carried a systematic experiment that evaluates the evaluators for NLG.",
      "In Figure we include a screen-shot of the user interface when annotating reviews. In what follows we present samples generated by the review generators on which human annotators disagree most on whether these are human-written or machine-generated. \\item Word LSTM temp 0.7 \\item Word LSTM temp 0.5 \\item Scheduled Sampling \\item Google LM \\item Attention Attribute to Sequence \\item Contexts to Sequences \\item Gated Contexts to",
      "We observe a substantial variance in the accuracy of both human evaluators on different generators, which suggests that human evaluators are able to distinguish between generators. % The generator ranked the highest by both human evaluators is Gated Contexts to Sequences. Google LM is ranked on the lower side, which makes sense as the model is not trained to generate reviews.",
      "% Different from human evaluators that are applied to the 3,600 annotated reviews, the discriminative evaluators are applied to all reviews in D-test. Meta-adversarial Evaluators. % On individual reviews, the three deep learning based and the one SVM based evaluators achieve higher accuracy than the two human evaluators, indicating that adversarial evaluators can distinguish a single machine-generated review from human-written better than humans (Figure and Table in Appendix ).",
      "Finally and most importantly, using automated evaluation metrics is sensible only if they correlate with results of human-based evaluations and if they are accurate predictors of text quality, which is never formally verified at scale. % We present a large-scale, systematic experiment that evaluates the evaluators for NLG. We compare three types of evaluators including human evaluators, automated adversarial evaluators trained to distinguish human-written from machine-generated product reviews, and word overlap metrics (such as BLEU and ROUGE) in a particular scenario, generating online product reviews.",
      "We decided not to include them in this experiment for two reasons: 1) as the first study, we are not sure which aspects human raters would consider when they judge for the realism of a review; 2) we wanted to keep the experiment design simple, and many of these aspects are harder to define. In the post-experiment survey, the raters commented on the reasons why they considered reviews as fake. The low inter-rater agreement (0.27) reflects the difficulty/ subjectivity of the task: identifying individual reviews as human-written or machine-generated."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 0,\n    \"Rationale\": \"Image 2 shows the accuracy of human evaluators on individual reviews, highlighting their performance on human-written versus machine-generated reviews. Text 0 provides specific accuracy rates for human evaluators, indicating they are better at identifying human-written reviews.\",\n    \"Answer\": \"Human evaluators more accurately identified human-written reviews.\"\n}"
  },
  "1": {
    "question": "Which type of generator generally produced reviews that were most easily identified as machine-generated by the meta-adversarial evaluators?",
    "ground_truth": "MLE SeqGAN and Word LSTM with temperature 1.0.",
    "selected_figures_names": [
      "1901.00398v2-Figure11-1.png",
      "1901.00398v2-Table5-1.png",
      "1901.00398v2-Table2-1.png",
      "1901.00398v2-Figure3-1.png"
    ],
    "all_figures_names": [
      "1901.00398v2-Figure11-1.png",
      "1901.00398v2-Table5-1.png",
      "1901.00398v2-Table2-1.png",
      "1901.00398v2-Figure3-1.png",
      "1901.00398v2-Table1-1.png",
      "1901.00398v2-Figure1-1.png",
      "1901.00398v2-Table4-1.png",
      "1901.00398v2-Figure12-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 3,
    "rationale": "Image 3 shows the accuracy of human and meta-adversarial evaluators on reviews generated by individual generators, which directly relates to how easily the reviews were identified as machine-generated. Text 3 discusses the performance of meta-adversarial evaluators compared to human evaluators, indicating that they achieve higher accuracy in distinguishing machine-generated reviews.",
    "answer": "GAN-based generators generally produced reviews that were most easily identified as machine-generated by the meta-adversarial evaluators.",
    "text_chunks": [
      "We observe a substantial variance in the accuracy of both human evaluators on different generators, which suggests that human evaluators are able to distinguish between generators. % The generator ranked the highest by both human evaluators is Gated Contexts to Sequences. Google LM is ranked on the lower side, which makes sense as the model is not trained to generate reviews.",
      "Interestingly, humans tend not to be fooled by reviews generated by the GAN-based models (MLE SeqGAN, SeqGAN, RankGAN and LeakGAN), even though their objective is to confuse fake from real. GAN-generated reviews tend to be easily distinguishable from the real reviews by human judges. \\subsubsection{Discriminative evaluators We then analyze the 7 meta-adversarial evaluators.",
      "\\subsubsection{Discriminative evaluators The inclusion of multiple generators provides the opportunity of creating meta-adversarial evaluators, trained using a pool of generated reviews by many generators, mixed with a larger number of ``real'' reviews (D-train and D-valid datasets). Such a ``pooling'' strategy is similar to the standard practice used by the TREC conferences to evaluate different information retrieval systems . Comparing to individual adversarial evaluators, a meta-evaluator is supposed to be more robust and fair, and it can be applied to evaluate new generators without being retrained.",
      "% Different from human evaluators that are applied to the 3,600 annotated reviews, the discriminative evaluators are applied to all reviews in D-test. Meta-adversarial Evaluators. % On individual reviews, the three deep learning based and the one SVM based evaluators achieve higher accuracy than the two human evaluators, indicating that adversarial evaluators can distinguish a single machine-generated review from human-written better than humans (Figure and Table in Appendix ).",
      "Each human judge only annotates 20 reviews, and they do not know which reviews are generated by the same generator. % Comparing to an adversarial discriminator, a human judge has not seen many ``training'' examples of fake reviews or generators. That explains why the meta-adversarial evaluators are better at identifying fake reviews.",
      "These two observations are intriguing, which indicates that when identifying fake reviews, humans might focus more on word usage rather than trying to construct a ``decision boundary'' mentally. In summary, we find that 1) human evaluators cannot distinguish machine-generated reviews from real reviews perfectly, with significant bias between the two classes; 2) meta-adversarial evaluators can better distinguish individual fake reviews, but their rankings at the generator level tend to be negatively correlated with human evaluators; and 3) text-overlap evaluators are highly correlated with human evaluators in ranking generators. % Discussion We carried a systematic experiment that evaluates the evaluators for NLG.",
      "% For every review in D-test (either annotated or not), a meta-adversarial evaluator makes a judgment call. A generator is considered high quality if the meta-adversarial evaluator makes more mistakes on reviews it generated. \\subsubsection{Word-overlap evaluators We include a set of 4 text-overlap metrics used for NLG evaluation: BLEU and METEOR (specific to machine translation), ROUGE (used in text summarization), and CIDEr (used in image description evaluation).",
      "% Their true positive rates and true negative rates are more balanced than human evaluators. Meta-discriminators commonly rank GAN-based generators the highest. % This makes sense as the objective of GAN is consistent to the (reversed) evaluator accuracy."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 3,\n    \"Rationale\": \"Image 3 shows the accuracy of human and meta-adversarial evaluators on reviews generated by individual generators, which directly relates to how easily the reviews were identified as machine-generated. Text 3 discusses the performance of meta-adversarial evaluators compared to human evaluators, indicating that they achieve higher accuracy in distinguishing machine-generated reviews.\",\n    \"Answer\": \"GAN-based generators generally produced reviews that were most easily identified as machine-generated by the meta-adversarial evaluators.\"\n}"
  },
  "2": {
    "question": "What is the task that the AMT workers are being asked to do?",
    "ground_truth": "The AMT workers are being asked to decide whether each of twenty one paragraphs extracted from product reviews is real (written by a person) or fake (written by a computer algorithm).",
    "selected_figures_names": [
      "1901.00398v2-Figure8-1.png"
    ],
    "all_figures_names": [
      "1901.00398v2-Table5-1.png",
      "1901.00398v2-Figure9-1.png",
      "1901.00398v2-Figure4-1.png",
      "1901.00398v2-Figure2-1.png",
      "1901.00398v2-Table1-1.png",
      "1901.00398v2-Figure5-1.png",
      "1901.00398v2-Figure3-1.png",
      "1901.00398v2-Figure8-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 2,
    "rationale": "Image 0 provides a screenshot of the instructions given to AMT workers, which directly relates to their task. Text 2 mentions the recruitment of AMT workers and their role in annotating reviews, which complements the image.",
    "answer": "The AMT workers are being asked to decide if each review is real (written by a person) or fake (written by a computer algorithm).",
    "text_chunks": [
      "In LeakGAN, in order to address mode collapse, the authors propose an interleaved training scheme, which combines supervised training using maximum likelihood estimation with GAN adversarial training (instead of carrying only GAN adversarial training after the pretraining stage). Blending two training schemes is considered useful by the authors as it helps LeakGAN overcome local minimums, alleviates mode collapse and acts as an implicit regularizer on the generative model. Samples produced by the review generators Figure shows the instructions given to the AMT workers who participated in this study.",
      "The worker module takes the current word x_t as input and outputs matrix O_t; this matrix is then combined through a softmax with the goal vector embedding w_t: \\end{equation At training time, the manager and the worker modules are trained separately -- the manager is trained to predict which are the most rewarding positions in the discriminative feature space, while the worker is rewarded to follow these directions. The gradient for the manager module is defined as: Q_{\\mathcal{F(s_t, g_t) defines the expected reward under the current policy and can be approximated using Monte Carlo search.",
      "We markup out-of-vocabulary words in both human-written and machine-generated reviews to control for confounds of using certain rare words. There is no significant difference in proportion of the markup token between the two classes (2.5\\%-real vs. 2.2\\%-fake). We recruit 900 human annotators through the Amazon Mechanical Turk (AMT) platform.",
      "Long Short Term Memory networks (LSTMs) effectively address these limitations by relying on a memory state and gating functions to control the flow of the information throughout the network -- and in particular what information is written to the memory state, what information is read from the memory state, and what information is removed (or forgotten) from the memory state. The mathematical formulation of LSTM units can be expressed as follows: \\end{equation In the above set of equations, the input word x^{(t) and the past hidden state h^{(t-1) are used to generate new memory \\widetilde{c^{(t) which includes features of the new word x^{(t) without prior determination of whether x^{(t) is important and worth keeping. The role of the input gate is to check whether it is sensible to store the new input word given the word x^{(t) itself and the past hidden state h^{(t-1); the input gate produces i^{(t) as output, which encapsulates the worthiness decision of preserving the input information.",
      "Given a dataset of human written sequences, train a generative model G_{\\theta parameterized by \\theta to output sequence Y_{1:T = (y_1, \\ldots, y_t, \\ldots, y_T), y_t \\in Y, where Y is the word vocabulary. The current state is the sequence of tokens (y_1, \\ldots, y_{t-1) generated until timestep t, and the action a taken from this state is the selection of next token y_t.",
      "Before the adversarial training takes place, the manager undergoes a pre-training stage with a separate training scheme which mimics the transition of real text samples in the feature space: The worker uses the REINFORCE algorithm during training to maximize the reward when taking action x_t given the previous state is s_{t-1: \\end{equation During the adversarial training process, the generator G_\\theta and the discriminator D_\\phi are trained in alternative stages. When the generator G_\\theta is trained, the worker \\mathcal{W(.;\\theta_w) and the manager \\mathcal{M(.;\\theta_m) modules are trained alternatively fixing each other. Mode collapse is a common problem when training GAN models, when the generator learns to produce samples with extremely low variety, limiting the usefulness of the leant GAN model.",
      "Therefore, the training objective is slowly changed from an easy task where the previous token is known, to a realistic task where the previous token is provided by the model itself. The scheduled sampling training scheme is meant to make the model more robust and forces it to deal with its own mistakes at training time, in a similar way to inference time. However, as the model generates several consecutive tokens y_t-s, it is not clear whether the correct target distribution remains the same as in the ground truth sequence.",
      "After annotating a batch of reviews, workers are asked to explain their decisions by filling in an optional free-text comment. This enables us to have a better understanding of what differentiates machine-generated from human-written reviews from human's perspective. Analyzing their comments, we identify the main reasons why human evaluators annotate a review as machine-written."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 2,\n    \"Rationale\": \"Image 0 provides a screenshot of the instructions given to AMT workers, which directly relates to their task. Text 2 mentions the recruitment of AMT workers and their role in annotating reviews, which complements the image.\",\n    \"Answer\": \"The AMT workers are being asked to decide if each review is real (written by a person) or fake (written by a computer algorithm).\"\n}"
  }
}