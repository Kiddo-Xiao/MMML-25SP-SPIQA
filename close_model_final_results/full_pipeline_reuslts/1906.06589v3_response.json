{
  "0": {
    "question": "Which model performed the best on the test data?",
    "ground_truth": "P-FC",
    "selected_figures_names": [
      "1906.06589v3-Table10-1.png",
      "1906.06589v3-Table12-1.png",
      "1906.06589v3-Table4-1.png",
      "1906.06589v3-Table6-1.png"
    ],
    "all_figures_names": [
      "1906.06589v3-Table10-1.png",
      "1906.06589v3-Table12-1.png",
      "1906.06589v3-Table4-1.png",
      "1906.06589v3-Table6-1.png",
      "1906.06589v3-Table2-1.png",
      "1906.06589v3-Figure7-1.png",
      "1906.06589v3-Table7-1.png",
      "1906.06589v3-Table11-1.png"
    ],
    "chosen_image_idx": 2,
    "chosen_text_idx": 0,
    "rationale": "Image 2 provides a comparison of test accuracies (Atest) for different models and regularizers, highlighting the performance improvements due to DMP. Text 0 discusses the superior membership privacy-model utility tradeoffs of DMP over PATE, indicating high test accuracy.",
    "answer": "The DMP model performed the best on the test data with a test accuracy of 76.79%.",
    "text_chunks": [
      "\\caption{Evaluation of PATE using the discriminator architecture in trained on CIFAR10. The corresponding DMP-trained model has 77.98\\% and 76.79\\% accuracies on the training and test data, and 50.8\\% membership inference accuracy. Comparison of results clearly show the superior membership privacy-model utility tradeoffs of DMP over PATE.",
      "As expected, these models also have similar E_\\mathsf{gen's. However, A_\\mathsf{test of DMP models is significantly higher than AdvReg models; A^+_\\mathsf{test column shows the \\% increase in A_\\mathsf{test due to DMP over AdvReg: Accuracy improvements due to DMP over AdvReg are close to 100\\% for CIFAR-100, and 20\\% to 45\\% for other datasets. AdvReg uses accuracy of an MIA model to regularize and train its target models to fool the MIA model.",
      "At higher \\epsilon, A_\\mathsf{test of DP-SGD increases, but at the cost of very high generalization error, which facilitates stronger MIAs. Note that, further increase in privacy budget, \\epsilon, does not improve tradeoff of DP-SGD. More importantly, {for low MIA risk of \\sim 51.3\\%, DMP models have 12.8\\% higher A_\\mathsf{test (i.e., 24.5\\% improvement) than DP-SGD models, which shows the superior tradeoffs due to DMP.",
      "Missing experimental details \\subsubsection{Best tradeoffs due to adversarial regularization Table~ gives the results for best tradeoffs due to adversarial regularization that we obtain by tuning its \\lambda parameter~. \\subsubsection{Best tradeoffs due to other regularizations We see from the `Equivalent A_\\textsf{test' column in Table~ that all regularization techniques improve the classification performance over the corresponding accuracies of baseline models from the Table 2 of main paper. However, they reduce overfitting negligibly: the maximum reduction in E_\\text{gen due to the regularizations is 1.8\\% for Purchase, 10.2\\% for Texas, 3.8\\% for CIFAR100, and 2.6\\% for CIFAR10. This is because these techniques aim to produce models that generalize better to test data, but they do not necessarily reduce the memorization of the private training data by the models.",
      "We present a subset of results in Table and defer comprehensive comparison to Appendix. At low \\epsilon's (<10), GNMax hardly produces any labels, hence, the final target model has very low A_\\mathsf{test, but at higher \\epsilon's (>1000), PATE target model has acceptable A_\\mathsf{test. However, PATE cannot achieve performances close to DMP, as it divides D_\\mathsf{tr among its teachers.",
      "Although this noise marginalizes the increase in the test performance of \\theta_\\textsf{p, it also prevents \\theta_\\textsf{p from learning more about D_\\textsf{tr and prevents further inference risk. This is shown by the train accuracy and membership inference risk curves in Figure . Therefore, size of reference data should be selected based on the desired tradeoffs of the final model.",
      "Therefore, DMP-trained models have significantly higher accuracy than PATE-trained model, for similar membership inference risk, i.e., DMP achieves significantly better membership privacy-model utility tradeoffs. Missing Discussion Details In the last section of main paper, we provide various insights in to our DMP defense based on our extensive evaluation. We provide the missing details of the discussions below.",
      "Next, to avoid repetitive training, we simplify the term for each sample in \\eqref{eq:ref_obj2 using the results of~. More specifically, they propose a linear approximation to the difference in cross-entropy losses of a pair of models trained with and without a specific sample in their training data. We note that this is the exact setting of our problem. If \\theta and \\theta_{-z are two models trained with and without a member z, then the difference in cross-entropy losses of the two models on some test sample z_\\mathsf{test=(\\mathbf{x_\\mathsf{test,y_\\mathsf{test) is approximated as: \\noindent where H_\\theta is the Hessian matrix that is defined as H_\\theta=\\frac{1{n\\sum_{z\\in D_\\textsf{tr\\nabla^2_\\theta(z,\\theta)."
    ],
    "raw_gpt": "{\n    \"Image\": 2,\n    \"Text\": 0,\n    \"Rationale\": \"Image 2 provides a comparison of test accuracies (Atest) for different models and regularizers, highlighting the performance improvements due to DMP. Text 0 discusses the superior membership privacy-model utility tradeoffs of DMP over PATE, indicating high test accuracy.\",\n    \"Answer\": \"The DMP model performed the best on the test data with a test accuracy of 76.79%.\"\n}"
  },
  "1": {
    "question": "How does the size of the reference set ($X_\\textsf{ref}$) used for DMP training differ between the Purchase/Texas datasets and the CIFAR datasets? Explain the rationale behind this difference.",
    "ground_truth": "For Purchase and Texas datasets, $X_\\textsf{ref}$ is specifically selected and contains 10,000 data points. In contrast, for CIFAR datasets, the entire remaining data (25,000 points) after selecting $D_\\textsf{tr}$ is used as $X_\\textsf{ref}$. This difference is due to the smaller size of the CIFAR datasets. Using the entire remaining data as $X_\\textsf{ref}$ ensures sufficient data for effective DMP training in these cases.",
    "selected_figures_names": [
      "1906.06589v3-Table9-1.png",
      "1906.06589v3-Table8-1.png",
      "1906.06589v3-Table11-1.png",
      "1906.06589v3-Table1-1.png"
    ],
    "all_figures_names": [
      "1906.06589v3-Table9-1.png",
      "1906.06589v3-Table8-1.png",
      "1906.06589v3-Table11-1.png",
      "1906.06589v3-Table1-1.png",
      "1906.06589v3-Table2-1.png",
      "1906.06589v3-Table7-1.png",
      "1906.06589v3-Figure3-1.png",
      "1906.06589v3-Figure5-1.png"
    ],
    "chosen_image_idx": 3,
    "chosen_text_idx": 0,
    "rationale": "Image 3 provides a clear comparison of the dataset splits for DMP training, showing the sizes of the reference sets for Purchase, Texas, and CIFAR datasets. Text 0 explains the rationale behind the choice of reference set sizes, highlighting the use of entropy-based criteria and dataset size considerations.",
    "answer": "The reference set ($X_\\textsf{ref}$) for DMP training is 10,000 for the Purchase and Texas datasets, while it is 25,000 for the CIFAR datasets. This difference is due to the larger size of the CIFAR datasets, allowing the entire remaining data to be used as the reference set, whereas an entropy-based criterion is used to select the reference set for the smaller Purchase and Texas datasets.",
    "text_chunks": [
      "The dataset splits are given in Table~. For Purchase and Texas tasks, we use D_\\mathsf{ref of size 10k and select X_\\mathsf{ref of size 10k from the remaining data using our entropy-based criterion. For CIFAR datasets, we use D_\\mathsf{ref of size 25k and due to small sizes of these datasets, use the entire remaining 25k data as X_\\mathsf{ref. The `Attack training' (described shortly) column shows the MIA adversary's knowledge of members and non-members of D_\\mathsf{tr.",
      "Hence, we show that the assumption can be relaxed by using X_\\mathsf{ref {synthesized from private D_\\mathsf{tr to train DMP models. For CIFAR10, we use DC-GAN to generate synthetic X_\\mathsf{ref of sizes 12.5k, 25k, and 37.5k from D_\\mathsf{tr of size 25k. We then train three DMP models and evaluate their MIA risk using whitebox NSH attack.",
      "Similarly, reducing the size of X_\\mathsf{ref reduces MIA risk due to DMP, but comes at the cost of reduction in A_\\mathsf{test. \\subsubsection{Privacy risk to reference data (X_\\mathsf{ref). We evaluate the privacy risk to X_\\mathsf{ref, as it can be of sensitive nature, e.g., in case of Texas medical records dataset.",
      "For both PATE and DMP, we use the same 25,000 data of CIFAR10 as the private training and the rest of 25,000 data the unlabeled reference data. The accuracy of the discriminator trained on the entire private training data is 97.65\\% and 79.6\\% on training and test data, respectively. We use the 25,000 training data to train three ensembles of sizes 5, 10 and 25 teachers.",
      "For Purchase-100 and Texas-100, the fully connected network has hidden layers of sizes \\{1024, 512, 256, 128\\. For CIFAR-100, we choose two DenseNet models to assess the efficacy of DMP for two models with equivalent performance, but significantly different capacities. In Table , DenseNet12 corresponds to DenseNet-BC (L=100, k=12) and DenseNet19 corresponds to DenseNet-BC (L=190, k=40). For the comparison with PATE using CIFAR-10, we use the generator and discriminator architectures used in .",
      "We use \\delta of 10^{-4 as the order of the size of the reference data is 10^{4 . Note that, the DMP-trained discriminator has training, test, and attack accuracies of 77.98\\%, 76.79\\%, and 50.8\\%, respectively. Table shows results for PATE with teacher ensembles of different sizes: At low \\epsilon values, GNMax cannot provide many labels, and therefore, PATE suffers significant accuracy degradations.",
      "Observe that, \\mathcal{R is an extension of \\eqref{eq:prob_ratio to the setting of DMP, where \\theta_\\mathsf{p is trained via the knowledge transferred using (X_\\textsf{ref,\\theta^{X_\\textsf{ref_\\textsf{up), instead of directly training on D_\\textsf{tr. argue to reduce this ratio to improve membership privacy. Hence, we want to obtain X_\\mathsf{ref which reduces \\mathcal{R when D_\\textsf{tr, D'_\\textsf{tr, and \\theta_\\mathsf{p are kept constant.",
      "Specifically, we randomly sample D_\\mathsf{tr and X_\\mathsf{ref from Purchase100 dataset, and compute \\theta_\\mathsf{up and \\theta_\\mathsf{p using DMP. Next, for some z\\in D_\\mathsf{tr, we train \\theta'_\\mathsf{up on D'_\\mathsf{tr. Then, for each \\mathbf{x\\in X_\\mathsf{ref, we compute \\Delta\\mathcal{L_{\\scaleto{\\textsf{KL{4pt as in~\\eqref{eq:ref_obj and use the available ground truth label of \\mathbf{x to compute \\Delta\\mathcal{L_{\\scaleto{\\textsf{CE{4pt."
    ],
    "raw_gpt": "{\n    \"Image\": 3,\n    \"Text\": 0,\n    \"Rationale\": \"Image 3 provides a clear comparison of the dataset splits for DMP training, showing the sizes of the reference sets for Purchase, Texas, and CIFAR datasets. Text 0 explains the rationale behind the choice of reference set sizes, highlighting the use of entropy-based criteria and dataset size considerations.\",\n    \"Answer\": \"The reference set ($X_\\\\textsf{ref}$) for DMP training is 10,000 for the Purchase and Texas datasets, while it is 25,000 for the CIFAR datasets. This difference is due to the larger size of the CIFAR datasets, allowing the entire remaining data to be used as the reference set, whereas an entropy-based criterion is used to select the reference set for the smaller Purchase and Texas datasets.\"\n}"
  },
  "2": {
    "question": "What is the relationship between the average X_ref entropy and the generalization gap?",
    "ground_truth": "The generalization gap increases as the average X_ref entropy increases.",
    "selected_figures_names": [
      "1906.06589v3-Figure2-1.png",
      "1906.06589v3-Table12-1.png",
      "1906.06589v3-Table2-1.png",
      "1906.06589v3-Table8-1.png"
    ],
    "all_figures_names": [
      "1906.06589v3-Figure2-1.png",
      "1906.06589v3-Table12-1.png",
      "1906.06589v3-Table2-1.png",
      "1906.06589v3-Table8-1.png",
      "1906.06589v3-Figure5-1.png",
      "1906.06589v3-Figure7-1.png",
      "1906.06589v3-Table9-1.png",
      "1906.06589v3-Table6-1.png"
    ],
    "chosen_image_idx": 0,
    "chosen_text_idx": 6,
    "rationale": "Image 0 shows the relationship between average X_ref entropy and generalization gap, while Text 6 discusses the tradeoff between entropy and membership inference risk, which is related to generalization. Both provide insights into how entropy affects model performance and privacy.",
    "answer": "As the average X_ref entropy increases, the generalization gap also increases. This indicates that higher entropy predictions lead to a larger difference between training and test accuracy.",
    "text_chunks": [
      "However, unlike existing knowledge transfer, DMP proposes an entropy-based criterion to select/generate X_\\mathsf{ref. Simply put, soft labels of the unprotected model \\theta_\\mathsf{up on X_\\mathsf{ref should have low entropy and the X_\\mathsf{ref should be far from decision boundaries of \\theta_\\mathsf{up, i.e., far from D_\\mathsf{tr, in the input feature space. Intuitively, such samples are easy to classify and none of the members of D_\\textsf{tr significantly affects their predictions, and therefore, these predictions do not leak membership information of any particular member.",
      "Note that, to select/generate a reference sample, we do not need the exact difference between cross-entropy or KL-divergence losses for the sample, but only the difference for the sample relative to the other samples. Hence, although the difference between cross-entropy losses is not exactly the same as difference between KL-divergence losses, their strong positive correlation is sufficient to make the reduction \\eqref{eq:ref_obj \\rightarrow \\eqref{eq:ref_obj2 useful in our task. \\\\ \\vspace{.5em \\paragraphe{Deriving the final objective to select/generate X_\\mathsf{ref.",
      "In this section, we show that X_\\mathsf{ref with low entropy predictions of unprotected model \\theta_\\mathsf{up strengthens membership privacy and derive an entropy-based criterion to select/generate X_\\mathsf{ref. \\paragraphe{Sketch of proof of Proposition~. Due to space limitations, we defer the detailed proof to Appendix and provide its sketch here. Consider two training datasets D_\\textsf{tr and D'_\\textsf{tr such that D'_\\textsf{tr\\leftarrow D_\\textsf{tr-z, and X_\\textsf{ref. Then, the log of the ratio of the posterior probabilities of learning the exact same parameters \\theta_\\textsf{p using DMP is given by \\eqref{eq:obj_ratio.",
      "In DMP, the reference data, X_\\mathsf{ref, is selected such that the predictions of DMP's unprotected model \\theta_\\mathsf{up on X_\\mathsf{ref have low entropies. Due to memorization, predictions of \\theta_\\mathsf{up on D_\\mathsf{tr also have low entropies. Hence, an adaptive adversary may exploit this peculiar X_\\mathsf{ref selection in DMP.",
      "Because, we cannot compute the loss without the ground truth labels of X_\\textsf{ref; recall that X_\\textsf{ref is unlabeled. However, as the loss involved here is the cross-entropy loss, minimizing the loss is equivalent to minimizing the entropy of prediction \\theta_\\textsf{up(\\mathbf{x'). This gives us the final objective as: where, \\mathcal{H(\\mathbf{v)\\triangleq \\sum_{i-\\mathbf{v_i\\text{log(\\mathbf{v_i) is the entropy of \\mathbf{v.",
      "For clarity of presentation, here onward, we denote \\mathcal{L_{\\scaleto{\\textsf{CE{4pt by \\mathcal{L. Next, we assume that ground truth labels Y_\\textsf{ref of X_\\textsf{ref are available. Note that X_\\textsf{ref is unlabeled dataset, but only to empirically demonstrate the validity of the simplification of \\eqref{eq:ref_obj to \\eqref{eq:ref_obj2, we assume that ground truth labels of X_\\textsf{ref are available. We validate the simplification in Figure : for any given reference sample, the lower the difference between cross-entropy losses, \\Delta\\mathcal{L, the lower the corresponding difference between KL-divergence losses; and vice-versa.",
      "Finally we train multiple protected models, \\theta_\\mathsf{p's, using each of the X_\\mathsf{ref's. Figure (left) shows the increase in the MIA risk and Figure (right) shows the increase in the classification performance of \\theta_\\textsf{p with the increase in average entropy of the X_\\mathsf{ref used. This tradeoff is because, although the higher entropy predictions contain more useful information and lead to high accuracy of \\theta_\\mathsf{p, they also contain higher membership information about D_\\mathsf{tr and lead to higher MIA risk.",
      "This provides the result of Proposition~. Proposition~ states that, using the reference data with low entropy predictions of \\theta_\\textsf{up strengthens the membership resistance of \\theta_\\textsf{p, and vice versa. In Figure~, we empirically validate the reductions \\eqref{eq:ref_obj \\rightarrow \\eqref{eq:sim_ref_obj1\\rightarrow \\eqref{eq:final_ref_obj. Specifically, we show that, for a given \\theta_\\mathsf{up, the lower the cross-entropy loss of reference data sample, the lower the entropy of prediction of \\theta_\\mathsf{up on the sample, i.e., \\eqref{eq:sim_ref_obj1\\rightarrow \\eqref{eq:final_ref_obj."
    ],
    "raw_gpt": "{\n    \"Image\": 0,\n    \"Text\": 6,\n    \"Rationale\": \"Image 0 shows the relationship between average X_ref entropy and generalization gap, while Text 6 discusses the tradeoff between entropy and membership inference risk, which is related to generalization. Both provide insights into how entropy affects model performance and privacy.\",\n    \"Answer\": \"As the average X_ref entropy increases, the generalization gap also increases. This indicates that higher entropy predictions lead to a larger difference between training and test accuracy.\"\n}"
  }
}