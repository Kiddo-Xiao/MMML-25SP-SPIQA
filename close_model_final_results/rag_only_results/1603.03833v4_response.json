{"0": {"question": "What is the role of the LSTM-MDN network in the training phase?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 5, 'Rationale': 'Image 0 and Text 5 both describe the LSTM-MDN network architecture and its role during training, illustrating the unrolling of the network and the use of mixture density parameters for error calculation.', 'Answer': 'The role of the LSTM-MDN network in the training phase is to use LSTM layers to process sequences of input data, which include the gripper pose and object poses, to predict future states. The network is trained by unrolling for multiple time steps and utilizing mixture density parameters to form Gaussian mixtures. These are used to calculate the predicted outputs and backpropagate errors to update network weights.'}", "all_figures_names": ["1603.03833v4-Figure3-1.png", "1603.03833v4-Table1-1.png", "1603.03833v4-Figure2-1.png", "1603.03833v4-Figure1-1.png", "1603.03833v4-Figure4-1.png", "1603.03833v4-Figure5-1.png"], "referred_figures_names": ["1603.03833v4-Figure3-1.png"], "answer": "The LSTM-MDN network is used to learn the relationship between the gripper pose and status, the pose of relevant objects, and the joint angles of the robot arm.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["The presence of either LSTM or MSE in the network provided enough improvement in the controller to allow the finishing of the task in the majority of situations, while the presence of both lead to 100\\% success rate. On the other hand, the harder push to pose task requires both components to have a reasonable success rate of 95\\%. We conclude from these experiments that both adding LSTM layers, and using an MDN output provide significant benefits individually to the robot controller, and that these two techniques can be combined for more individual benefits.", "The LSTM-MDN network is described in Figure~, while the architectures of the other approaches are shown in Figure~. Each network had been separately trained for the pick and place and the push to desired pose respectively, in effect creating 8 different controllers. The resulting controllers had been tested in the virtual environment by requiring the robot to perform randomly generated tasks 20 times. If it can not complete the task in a limited time (1 minute for the first task and 2 minutes for the second one), we count the try as a failure and reset the position of the box.", "During the deployment of the trained network, the prediction represents the desired pose of the end actuator which the robot needs to achieve through its inverse kinematics calculations. The controllers for the ``pick and place'' and the ``push to desired pose'' tasks have the same network architecture but were trained on the specific tasks. Our architecture uses an LSTM recurrent neural network and relies on mixture density networks (MDNs) to predict the probability density of the output.", "{\\em LSTM-MDN: Mixture density network containing 3 layers of LSTM with 50 memory states in each layer. The mixture contains 20 Gaussian kernels.", "If yes, it commands the robot to go to the next waypoint, otherwise it waits in a loop until the end-effector reaches the desired pose or timeout occurs which means that the end-effector cannot reach that pose (either because the inverse kinematic failed or a collision occurred). Experimental study In the following we describe a series of experiments and observations justifying the three claims we made in the abstract of this paper. The benefits of LSTM and MDN Our network architecture uses LSTM layers and an MDN-based error signal.", "These virtual demonstrations are used to teach a deep neural network based robot controller. Then, the controller is transferred to the physical robot. We found that the best performance was obtained using a network with LSTM layers and a mixture density network based error signal.", "train a CNN+LSTM controller to perform a multi-step task of picking up a cube and dropping it to a table. The training is based on demonstrations acquired from a programmed optimal controller, with domain randomization in the form of variation of environmental characteristics, lighting and textures. Yet another possible approach is to learn an invariant feature space that can transfer information between the domains through a sort of analogy making~.", "The resulting neural network will predict the next joint configuration of the robot. A similar neural network architecture is designed to address the grasping problem~. In~, LSTMs are used for a robot to learn to autonomously tie knots after a pre-processing step to reduce the noise from the training data. Our approach Collecting demonstrations in a virtual environment To allow users to demonstrate their ADLs, we designed in the Unity3D game engine a virtual environment modeling a table with an attached shelf that can hold various objects, and a simple two-finger gripper that can be opened and closed to grasp and carry an object."], "referred_text_chunks": ["The presence of either LSTM or MSE in the network provided enough improvement in the controller to allow the finishing of the task in the majority of situations, while the presence of both lead to 100\\% success rate. On the other hand, the harder push to pose task requires both components to have a reasonable success rate of 95\\%. We conclude from these experiments that both adding LSTM layers, and using an MDN output provide significant benefits individually to the robot controller, and that these two techniques can be combined for more individual benefits."]}, "1": {"question": "Why is the number of demonstrations after the shift not available for the \"Push to Pose\" task?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 1, 'Rationale': 'Image 1 provides direct data on the demonstrations for both tasks, showing the missing number for demonstrations after shift for push to pose. Text 1 explains why the push-to-pose task might have discrepancies between virtual and physical environments, emphasizing challenges including friction and movement dynamics.', 'Answer': 'The number of demonstrations after the shift is not available for the \"Push to Pose\" task because the task is highly dependent on physical interactions such as friction, which causes differences between virtual and physical environments. These complexities make it challenging to calculate the shifted demonstrations accurately.'}", "all_figures_names": ["1603.03833v4-Figure3-1.png", "1603.03833v4-Table1-1.png", "1603.03833v4-Figure2-1.png", "1603.03833v4-Figure1-1.png", "1603.03833v4-Figure4-1.png", "1603.03833v4-Figure5-1.png"], "referred_figures_names": ["1603.03833v4-Table1-1.png"], "answer": "The passage mentions that additional trajectories were generated for the \"Pick and Place\" task by reducing the frequency of the recorded demonstrations. This process was not applied to the \"Push to Pose\" task, therefore no \"Demonstrations after shift\" are listed for it.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["As the pushing to desired pose task requires a specific coordinate and pose to succeed, this approach is not possible for the second task. The second observation was that by recording the demonstration at 33Hz but presenting the training trajectories at only 4Hz, we have extra trajectory points. These trajectory points can be used to generate multiple independent trajectories at a lower temporal resolution.", "As we discussed when introducing the problems, the push-to-pose tasks is more dependent of the physics (such as the friction between the object and the table determines the way the object moves when pushed). This creates a bigger difference between the virtual and the physical environment compared to the pick and place task, where after a successful grasp the robot is essentially in control of the environment. Thus, the push to pose task shows a stronger decrease in success rate when moving to the physical world.", "It is unclear how many times does the box needs to be pushed. Furthermore, the completion of the task depends on the physics: the weight of the box and the friction between the box and table impacts the way the box moves in response to pushes. The demonstrations were collected from a single user, in the course of multiple sessions.", "The {\\em pushing to desired pose task involves moving and rotating a box of size 10 \\times 7 \\times 7cm to a desired area only by pushing it on the tabletop. In this task, the robot is not allowed to grasp the object. The box is initially positioned in a way that needs to be rotated by 90^{\\circ to fit inside the desired area which is 3cm wider than the box in each direction.", "In each session, the user performed a series of demonstrations for each task. The quality of demonstrations varied: in some of them, the user could finish the task only after several tries. For instance, sometimes the grasp was unsuccessful, or the user dropped the object in an incorrect position and had to pick it up again.", "The numerical success rates are shown in the following table: {\\footnotesize The results allow us to derive several conclusions. Clearly, the Feedforward-MSE combination does not work for this particular set of problems and training data, failing to complete either task even once. Another conclusion is that the pick-and-place task is clearly the easier from the two, even when tested in the virtual environment where physical modeling errors don't play a role.", "Thus a full demonstration can be recorded as a list of pairs d = \\{ (q_1,e_1) \\ldots (q_T,e_T)\\. For our experiments we considered two manipulation tasks that are regularly found as components of ADLs: pick and place and pushing to a desired pose. The {\\em pick and place task involves picking up a small box located on top of the table, and placing it into a shelf above the table.", "We also found that having imperfect demonstrations, where users occasionally make mistakes but correct them, allows the controller to correct the inevitable mistakes it makes when transferred to a physical environment. Our team is working to improve these results along several directions. Multi-task learning might reduce the necessary number of demonstrations as many features are likely shared between tasks."], "referred_text_chunks": ["As the pushing to desired pose task requires a specific coordinate and pose to succeed, this approach is not possible for the second task. The second observation was that by recording the demonstration at 33Hz but presenting the training trajectories at only 4Hz, we have extra trajectory points. These trajectory points can be used to generate multiple independent trajectories at a lower temporal resolution."]}, "2": {"question": "How does the frequency reduction process create multiple trajectories from a single demonstration?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 2, 'Rationale': 'Image 2 visually explains how multiple trajectories are derived from a single demonstration by showing waypoints and trajectory generation. Text chunk 2 discusses the method of reducing recording frequency to create additional trajectories, providing context on how these trajectories are established.', 'Answer': 'The frequency reduction process creates multiple trajectories by recording demonstrations at a higher frequency and then using some of the extra trajectory points to generate several independent trajectories at a lower frequency, as illustrated by the generation of new waypoints and trajectories.'}", "all_figures_names": ["1603.03833v4-Figure3-1.png", "1603.03833v4-Table1-1.png", "1603.03833v4-Figure2-1.png", "1603.03833v4-Figure1-1.png", "1603.03833v4-Figure4-1.png", "1603.03833v4-Figure5-1.png"], "referred_figures_names": ["1603.03833v4-Figure2-1.png"], "answer": " The frequency reduction process takes a high-frequency trajectory and samples it at a lower frequency, resulting in multiple trajectories with different starting and ending points. ", "figure_type": "** Schematic", "content_type": "figure", "text_chunks": ["After finishing a demonstration, the user was immediately presented with a new instance of the problem, with randomly generated initial conditions. All the experiments had been recorded in the trajectory representation format presented above, at a recording frequency of 33Hz. However, we found that the neural network controller can be trained more efficiently if the trajectories are sampled at a lower rate, with a rate of 4Hz to giving the best results.", "The process of the trajectory generation by frequency reduction is shown in Figure~. Table~ describes the size of the final dataset. The neural network based robot controller The robot controller takes as input the pose of the objects involved and the pose and open/close status of the gripper at time t and outputs a prediction of the pose and the open/closed status of the gripper at time t+1. During training, this prediction is used to generate the error signal.", "As the pushing to desired pose task requires a specific coordinate and pose to succeed, this approach is not possible for the second task. The second observation was that by recording the demonstration at 33Hz but presenting the training trajectories at only 4Hz, we have extra trajectory points. These trajectory points can be used to generate multiple independent trajectories at a lower temporal resolution.", "We also found that having imperfect demonstrations, where users occasionally make mistakes but correct them, allows the controller to correct the inevitable mistakes it makes when transferred to a physical environment. Our team is working to improve these results along several directions. Multi-task learning might reduce the necessary number of demonstrations as many features are likely shared between tasks.", "One obvious way to mitigate this problem is by acquiring a large number of demonstrations covering as many situations as possible. Some researchers proposed cloud-based and crowdsourced data collection techniques~, and some others proposed to use simulation environments~. Another direction is to use smaller number of demonstrations, but change the learning model to generalize better.", "To improve learning, we extended our training data by exploiting both the properties of the individual tasks and trajectory recording technique. First, we noticed that in the pick and place task the user can put the object to any location on the shelf. Thus we were able to generate new synthetic training data by shifting the existing demonstration trajectories parallel with the shelf.", "In each session, the user performed a series of demonstrations for each task. The quality of demonstrations varied: in some of them, the user could finish the task only after several tries. For instance, sometimes the grasp was unsuccessful, or the user dropped the object in an incorrect position and had to pick it up again.", "The error signal, in this case, is based on the negative logarithm likelihood of the next target waypoint given the probability density implied by the MDN. Let us now describe the intuitions that led to these choices. The solution to both manipulation tasks contain a series of individual movements which need to be executed in a specific sequence."], "referred_text_chunks": ["After finishing a demonstration, the user was immediately presented with a new instance of the problem, with randomly generated initial conditions. All the experiments had been recorded in the trajectory representation format presented above, at a recording frequency of 33Hz. However, we found that the neural network controller can be trained more efficiently if the trajectories are sampled at a lower rate, with a rate of 4Hz to giving the best results."]}, "3": {"question": "What is the role of the virtual environment in the proposed approach?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 and Text 0 both describe the use of a virtual environment for demonstrating tasks. Image 3 visually represents the process of using virtual demonstrations to train the neural network, and Text 0 explains how virtual environments allow safe collection of data for training a robot controller.', 'Answer': 'The virtual environment allows users to demonstrate tasks safely and thoroughly, collecting data to train a neural network-based robot controller, which is then transferred to a physical robot.'}", "all_figures_names": ["1603.03833v4-Figure3-1.png", "1603.03833v4-Table1-1.png", "1603.03833v4-Figure2-1.png", "1603.03833v4-Figure1-1.png", "1603.03833v4-Figure4-1.png", "1603.03833v4-Figure5-1.png"], "referred_figures_names": ["1603.03833v4-Figure1-1.png"], "answer": "The virtual environment is used to collect demonstrations of the task from the user. This allows for safe and efficient data collection.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["In this paper we propose an approach where the users demonstrate the tasks to be performed in a virtual environment. This allows the safe collection of sufficient demonstrations to train a deep neural network based robot controller. The trained controller is then transferred to the physical robot.", "This approach had been taken by Grounded Simulation Learning~ and improved by Grounded Action Transformation~ on the task of teaching a Nao robot to walk faster. Another approach for improving the virtual to physical transfer is to increase the generality of the policy learned in simulation through domain randomization~. note that the sequence of states learned by the controller in simulation might be reasonable even if the exact controls in the physical world are different. Their approach computes what the next state would be, and relies on learned deep inverse dynamics model to decide on the actions to achieve the equivalent real world state.", "One obvious way to mitigate this problem is by acquiring a large number of demonstrations covering as many situations as possible. Some researchers proposed cloud-based and crowdsourced data collection techniques~, and some others proposed to use simulation environments~. Another direction is to use smaller number of demonstrations, but change the learning model to generalize better.", "Conclusions and future work In this paper we have developed a technique through which a robotic arm can be taught to perform certain manipulation tasks. We focused on two tasks that are frequently required of robots that assist disabled users in activities of daily living: pick and place and push to desired pose. As disabled users can not generate large numbers of demonstrations on physical robots, we designed an approach where the user demonstrates the task in a virtual environment.", "The resulting neural network will predict the next joint configuration of the robot. A similar neural network architecture is designed to address the grasping problem~. In~, LSTMs are used for a robot to learn to autonomously tie knots after a pre-processing step to reduce the noise from the training data. Our approach Collecting demonstrations in a virtual environment To allow users to demonstrate their ADLs, we designed in the Unity3D game engine a virtual environment modeling a table with an attached shelf that can hold various objects, and a simple two-finger gripper that can be opened and closed to grasp and carry an object.", "The general flow is illustrated in Figure~. In the remainder of this paper we discuss related work, describe the approach in detail, and through an experimental study validate the three contributions outlined in the abstract. Related work \\noindent{\\em Virtual training to physical execution. The desirability of transferring learning from a simulated robot to a physical one had been recognized by many researchers.", "Another challenge is that while in the virtual world we had perfect knowledge of the pose of the effector and all the objects in the environment, we needed to acquire this information through sensing. As our controller architecture only performs robot arm control, in order to supplant the missing vision component, we relied on a Microsoft Kinect sensor and objects annotated with markers to track their pose. One of the problems with this approach is that the robot arm might occlude the view of the sensor.", "In the work described in this paper, we took a different approach. We had to accept that the simulation is nowhere close to the physical environment, thus the robot {\\em will make mistakes. Instead of domain randomization, we rely on the natural imperfections of demonstrations done by (possibly disabled) humans, but also on the ability of humans to correct the errors they made."], "referred_text_chunks": ["In this paper we propose an approach where the users demonstrate the tasks to be performed in a virtual environment. This allows the safe collection of sufficient demonstrations to train a deep neural network based robot controller. The trained controller is then transferred to the physical robot."]}, "4": {"question": "What is the difference between the pick and place task in simulation and the real world?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 3, 'Rationale': 'Image 4 provides a direct visual comparison of the pick and place task in both the simulation and real-world, which helps in understanding practical differences. Text 3 discusses challenges like sensor inaccuracies and physical differences, which are crucial for understanding task variations.', 'Answer': 'The difference between the pick and place task in simulation and the real world involves factors like sensor inaccuracies, occlusions, and physical discrepancies such as friction and size variations, leading to a lower success rate in the physical environment than in the simulation.'}", "all_figures_names": ["1603.03833v4-Figure3-1.png", "1603.03833v4-Table1-1.png", "1603.03833v4-Figure2-1.png", "1603.03833v4-Figure1-1.png", "1603.03833v4-Figure4-1.png", "1603.03833v4-Figure5-1.png"], "referred_figures_names": ["1603.03833v4-Figure4-1.png"], "answer": "In the simulation, the robot is able to pick up the object and place it in the desired location without any errors. However, in the real world, the robot makes some errors, such as dropping the object or placing it in the wrong location.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["As we discussed when introducing the problems, the push-to-pose tasks is more dependent of the physics (such as the friction between the object and the table determines the way the object moves when pushed). This creates a bigger difference between the virtual and the physical environment compared to the pick and place task, where after a successful grasp the robot is essentially in control of the environment. Thus, the push to pose task shows a stronger decrease in success rate when moving to the physical world.", "This is not because the virtual and physical worlds are highly similar. The size of the gripper of the Baxter robot is different from the one in the virtual world. The friction coefficients are very different, and the physics simulation in the virtual world is also of limited accuracy.", "The success rates in the virtual and physical worlds are compared as follows: {\\footnotesize The first conclusion we can draw from these values is that the approach successfully demonstrated the ability to transfer an unchanged controller trained in the virtual world to a physical robot. As expected, the success rate was lower in the physical world for both tasks. Some of the reasons behind the lower success rate is obvious: for instance, in the physical world there is an inevitable noise in the position of the objects and the end effector.", "Some of the noise is a consequence of limited sensor accuracy (such as the calibration of the Kinect sensor) and effector performance. Another source of inaccuracy is due to the way in which we acquired the positional information through a Kinect sensor: if during the manipulation the robot arm occluded the view of the object to the Kinect sensor, we temporarily lost the ability to track the object. Another reason for the lower performance in the physical world is due to the differences in the size, shape, physical attributes such as friction, etc. of the gripper and objects between the simulation and real world.", "Evaluating the transfer to the physical robot To verify the ability of the controller trained in the virtual world to perform in the physical one, we subjected both the virtual and the physical robots to the same tasks. The sequence of images in the Figure~ shows the controller acting autonomously for the pick and place and pushing to pose tasks in the virtual and physical environments respectively. We have found that indeed, in most cases, the physical robot was successful in executing both tasks.", "The numerical success rates are shown in the following table: {\\footnotesize The results allow us to derive several conclusions. Clearly, the Feedforward-MSE combination does not work for this particular set of problems and training data, failing to complete either task even once. Another conclusion is that the pick-and-place task is clearly the easier from the two, even when tested in the virtual environment where physical modeling errors don't play a role.", "Conclusions and future work In this paper we have developed a technique through which a robotic arm can be taught to perform certain manipulation tasks. We focused on two tasks that are frequently required of robots that assist disabled users in activities of daily living: pick and place and push to desired pose. As disabled users can not generate large numbers of demonstrations on physical robots, we designed an approach where the user demonstrates the task in a virtual environment.", "This would be acceptable if the policy corrects the error, keeping the difference bounded. In practice, it was found that if a policy was learned from demonstrations, initially small differences between the simulation and the real world tend to grow larger and larger as the state diverges farther and farther from the settings in which the demonstrations took place -- an aggravated version of the problem that led to the development of algorithms such as DAgger~. A number of different approaches had been developed to deal with this problem. One approach is to try to bring the simulation closer to reality through learning."], "referred_text_chunks": ["As we discussed when introducing the problems, the push-to-pose tasks is more dependent of the physics (such as the friction between the object and the table determines the way the object moves when pushed). This creates a bigger difference between the virtual and the physical environment compared to the pick and place task, where after a successful grasp the robot is essentially in control of the environment. Thus, the push to pose task shows a stronger decrease in success rate when moving to the physical world."]}, "5": {"question": "What are the three different network architectures used in the comparison study?", "referred_figures_indices": [5], "response": "```json\n{\n  \"Image\": 5,\n  \"Text\": 0,\n  \"Rationale\": \"Image 5 shows the three network architectures clearly: Feedforward-MSE, LSTM-MSE, and Feedforward-MDN. Text chunk 0 mentions these architectures while discussing their comparison, fitting the question's focus.\",\n  \"Answer\": \"The three network architectures used in the comparison study are Feedforward-MSE, LSTM-MSE, and Feedforward-MDN.\"\n}\n```", "all_figures_names": ["1603.03833v4-Figure3-1.png", "1603.03833v4-Table1-1.png", "1603.03833v4-Figure2-1.png", "1603.03833v4-Figure1-1.png", "1603.03833v4-Figure4-1.png", "1603.03833v4-Figure5-1.png"], "referred_figures_names": ["1603.03833v4-Figure5-1.png"], "answer": "Feedforward-MSE, LSTM-MSE, and Feedforward-MDN.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["In order to be able to perform a rough comparison, we implemented a feedforward network that closely matches that controller, but replaces the convolutional layers with a direct position input. With this, we have four choices for the controller structure: {\\em FeedForward-MSE: 3 layers of fully connected feedforward network with 100 neurons in each layer and mean squared error as the cost function. {\\em LSTM-MSE: 3 layers of LSTM with 50 memory states in each layer and mean squared error as the cost function.", "During the deployment of the trained network, the prediction represents the desired pose of the end actuator which the robot needs to achieve through its inverse kinematics calculations. The controllers for the ``pick and place'' and the ``push to desired pose'' tasks have the same network architecture but were trained on the specific tasks. Our architecture uses an LSTM recurrent neural network and relies on mixture density networks (MDNs) to predict the probability density of the output.", "The LSTM-MDN network is described in Figure~, while the architectures of the other approaches are shown in Figure~. Each network had been separately trained for the pick and place and the push to desired pose respectively, in effect creating 8 different controllers. The resulting controllers had been tested in the virtual environment by requiring the robot to perform randomly generated tasks 20 times. If it can not complete the task in a limited time (1 minute for the first task and 2 minutes for the second one), we count the try as a failure and reset the position of the box.", "The requirement of a controller with a memory leads us to the choice of recurrent neural networks, in particular, one of the most widely used models, the LSTM~. We are using three LSTM layers with 50 nodes each as shown in Figure~. The second intuition applies to the choice of the output layer and error signal. Both tasks allow multiple solutions.", "{\\em LSTM-MDN: Mixture density network containing 3 layers of LSTM with 50 memory states in each layer. The mixture contains 20 Gaussian kernels.", "The resulting neural network will predict the next joint configuration of the robot. A similar neural network architecture is designed to address the grasping problem~. In~, LSTMs are used for a robot to learn to autonomously tie knots after a pre-processing step to reduce the noise from the training data. Our approach Collecting demonstrations in a virtual environment To allow users to demonstrate their ADLs, we designed in the Unity3D game engine a virtual environment modeling a table with an attached shelf that can hold various objects, and a simple two-finger gripper that can be opened and closed to grasp and carry an object.", "{\\em FeedForward-MDN: Mixture density network containing 3 fully connected feedforward layers with 100 neurons in each layer. The mixture contains 20 Gaussian kernels.", "The presence of either LSTM or MSE in the network provided enough improvement in the controller to allow the finishing of the task in the majority of situations, while the presence of both lead to 100\\% success rate. On the other hand, the harder push to pose task requires both components to have a reasonable success rate of 95\\%. We conclude from these experiments that both adding LSTM layers, and using an MDN output provide significant benefits individually to the robot controller, and that these two techniques can be combined for more individual benefits."], "referred_text_chunks": ["In order to be able to perform a rough comparison, we implemented a feedforward network that closely matches that controller, but replaces the convolutional layers with a direct position input. With this, we have four choices for the controller structure: {\\em FeedForward-MSE: 3 layers of fully connected feedforward network with 100 neurons in each layer and mean squared error as the cost function. {\\em LSTM-MSE: 3 layers of LSTM with 50 memory states in each layer and mean squared error as the cost function."]}}