{"0": {"question": "Which policy resulted in the highest average cost?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 0, 'Rationale': 'Image 2 directly compares the average cost of different policies (ALOQ, MAP, RQ-ALOQ) and indicates specific cost ranges. Text chunk 0 discusses the cost comparison of ALOQ, MAP, and RQ-ALOQ, noting ALOQ\\'s effectiveness in reducing costs.', 'Answer': 'The MAP policy resulted in the highest average cost.'}", "all_figures_names": ["1605.07496v3-Table2-1.png", "1605.07496v3-Figure6-1.png", "1605.07496v3-Table1-1.png", "1605.07496v3-Figure2-1.png", "1605.07496v3-Figure7-1.png", "1605.07496v3-Figure9-1.png", "1605.07496v3-Figure11-1.png", "1605.07496v3-Figure4-1.png"], "referred_figures_names": ["1605.07496v3-Table1-1.png"], "answer": "MAP Policy", "figure_type": "table", "content_type": "table", "text_chunks": ["For evaluation, we drew 1000 samples of \\theta from the more granular posterior distribution and measured the returns of the three policies for each of the samples. The average cost incurred by the ALOQ policy (presented in Table ) was 31\\% lower than that incurred by the MAP policy and 23.6\\% lower than the RQ-ALOQ policy. This is because ALOQ finds a policy that slightly underperforms the MAP policy in some of cases but avoids over 95\\% of the SREs (cost \\geq70 in Table ) experienced by the MAP and RQ-ALOQ policies.", "The final learned policy, i.e., \\hat{\\pi^*, of each algorithm is shown as a vertical line, along with \\pi^* (the true maximum). These lines illustrate that properly accounting for significant rare events can lead to learning qualitatively different policies. Figures , which plots the performance of all methods the two functions, shows that ALOQ substantially outperforms all the other algorithms except for one-step ALOQ (note that both WSN and the na\\\"{ive approach fail completely in these settings).", "Figures and show the expected cost (lower is better) of the arm configurations after each timestep for each method. ALOQ, unwarped ALOQ, and RQ-ALOQ greatly outperform the other baselines. Reinforce and TRPO, being relatively sample inefficient, exhibit a very slow rate of improvement in performance, while WSN fails to converge at all.", "= f(\\pi_i, \\theta_i), we assume we have a dataset \\mathcal{D_{1:l = \\{(\\pi_1, \\theta_1, f_1), (\\pi_2, \\theta_2, f_2), \\ldots , (\\pi_l, \\theta_l, f_l)\\. Our objective is to find an optimal policy \\pi^*: First, consider a na\\\"{ive approach consisting of a standard application of BO that disregards \\theta, performs BO on \\tilde{f(\\pi) = f(\\pi, \\theta) with only one input \\pi, and attempts to estimate \\pi^*.", "We applied ALOQ to learn the optimal policy given these damage probabilities, but restricted the search to the policies in the archive created by . Figure shows that ALOQ finds a policy with much higher expected reward than RQ-ALOQ. It also shows the policy that generates the maximum reward when none of the legs are damaged or broken (`opt undamaged policy').", "To show that active selection of \\theta is advantageous, we also compare against the policy learned by RQ-ALOQ. Since we are approximating the unknown p(\\theta) with a set of samples, it makes sense to keep the sample size relatively low for computational efficiency when finding the ALOQ policy (50 samples in this instance). However, to show that ALOQ is robust to this approximation, when comparing the performance of the ALOQ and MAP policies, we used a much larger sample size of 400 for the posterior distribution.", "Again for readability we only present the arm configurations for the most competitive algorithms. \\subsubsection{Performance of Reinforce and TRPO Both these baselines are relatively sample inefficient. However, one question that arises is whether these methods eventually find the optimal policy.", "Fortunately, policies can often be tested in a simulator that exposes key environment variables -- state features that are unobserved and randomly determined by the environment in a physical setting but are controllable in the simulator. This paper considers how to use environment variables to help learn robust policies. Although running trials in a simulator is cheaper and safer than running physical trials, the computational cost of each simulated trial can still be quite high."], "referred_text_chunks": ["For evaluation, we drew 1000 samples of \\theta from the more granular posterior distribution and measured the returns of the three policies for each of the samples. The average cost incurred by the ALOQ policy (presented in Table ) was 31\\% lower than that incurred by the MAP policy and 23.6\\% lower than the RQ-ALOQ policy. This is because ALOQ finds a policy that slightly underperforms the MAP policy in some of cases but avoids over 95\\% of the SREs (cost \\geq70 in Table ) experienced by the MAP and RQ-ALOQ policies."]}, "1": {"question": "Which of the algorithms performs the best on the robotic arm joint breakage task?", "referred_figures_indices": [3], "response": "{'Image': 5, 'Text': 6, 'Rationale': 'Image 5 and Text 6 provide detailed comparisons of algorithm performances specifically for the robotic arm joint breakage task, listing expected costs and quartiles. This directly relates to identifying the best performing algorithm.', 'Answer': 'The ALOQ algorithm performs the best on the robotic arm joint breakage task, with the lowest quartiles of expected cost.'}", "all_figures_names": ["1605.07496v3-Table1-1.png", "1605.07496v3-Figure4-1.png", "1605.07496v3-Table3-1.png", "1605.07496v3-Figure3-1.png", "1605.07496v3-Figure10-1.png", "1605.07496v3-Table2-1.png", "1605.07496v3-Figure7-1.png", "1605.07496v3-Figure1-1.png"], "referred_figures_names": ["1605.07496v3-Figure3-1.png"], "answer": "ALOQ.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["\\end{figure* \\caption{Performance and learned configurations on the robotic arm joint breakage task. \\end{figure* Experimental Results To evaluate ALOQ we applied it to 1) a simulated robot arm control task, including a variation where p(\\theta) is not known a priori but must be inferred from data, and 2) a hexapod locomotion task .", "Breakage The target for the arm breakage experiment was set to the final position of the end effector for \\pi' =[0.4, 0.2, 0.6]. Angles between [0.3, 0.7] for the first joint have an associated 5\\% probability of breakage. \\subsubsection*{Comparison of runtimes A comparison of the per-step runtimes for the GP based methods are presented in Figure .", "Optimising the BO/BQ acquisition functions: We used DIRECT to maximise the BO acquisition function \\alpha_{ALOQ. To minimise the BQ acquisition function, we exhaustively computed \\mathbb{V(\\bar{f(\\pi_{t+1)|\\mathcal{D_{1:t, \\pi_{t+1, \\theta) for each \\theta since this was computationally very cheap. \\section*{Robotic Arm Simulator The configuration of the robot arm is determined by three joint angles, each of which is normalised to lie in [0,1].", "Figure shows the learned arm configurations together with the policy that would be learned if there were no SREs (`No break'). The shaded region represents the joint angles that can lead to failure. This figure illustrates that ALOQ learns a qualitatively different policy than the other algorithms, one which avoids the joint angles that might lead to a breakage while still getting close to the target faster than the other methods.", "Furthermore, while all the BO based algorithms learn to avoid the wall, active selection of \\theta allows ALOQ to do so more quickly: smart quadrature allows it to more efficiently observe rare events and accurately estimate their boundary. For readability we have only presented the arm configurations for algorithms which have performance comparable to ALOQ. \\subsubsection{Joint", "In this experiment, we evaluate ALOQ's performance on a robot control problem implemented in a kinematic simulator. The goal is to configure each of the three controllable joints of a robot arm such that the tip of the arm gets as close as possible to a predefined target point. \\subsubsection{Collision Avoidance In the first setting, we assume that the robotic arm is part of a mobile robot that has localised itself near the target.", "Breakage Next we consider a variation in which instead of uncertainty introduced by localisation, some settings of the first joint carry a 5\\% probability of it breaking, which consequently incurs a large cost. Minimising cost thus entails getting as close to the target as possible, while minimising the probability of the joint breaking. Figures and shows the expected cost (lower is better) of the arm configurations after each timestep for each method.", "Figures and show the expected cost (lower is better) of the arm configurations after each timestep for each method. ALOQ, unwarped ALOQ, and RQ-ALOQ greatly outperform the other baselines. Reinforce and TRPO, being relatively sample inefficient, exhibit a very slow rate of improvement in performance, while WSN fails to converge at all."], "referred_text_chunks": ["\\end{figure* \\caption{Performance and learned configurations on the robotic arm joint breakage task. \\end{figure* Experimental Results To evaluate ALOQ we applied it to 1) a simulated robot arm control task, including a variation where p(\\theta) is not known a priori but must be inferred from data, and 2) a hexapod locomotion task ."]}, "2": {"question": "Which algorithm performs the best in the Joint Breakage experiment?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 1, 'Rationale': \"Image 2 shows the quartiles of the expected cost for each algorithm specifically for the Joint Breakage experiment. Text 1 discusses the learning policy related to arm configurations in the presence of breakage, noting that ALOQ performs best by avoiding joint angles that could lead to breakage.\", 'Answer': 'ALOQ performs the best in the Joint Breakage experiment.'}", "all_figures_names": ["1605.07496v3-Figure8-1.png", "1605.07496v3-Table3-1.png", "1605.07496v3-Table2-1.png", "1605.07496v3-Figure2-1.png", "1605.07496v3-Table1-1.png", "1605.07496v3-Figure7-1.png", "1605.07496v3-Figure9-1.png", "1605.07496v3-Figure3-1.png"], "referred_figures_names": ["1605.07496v3-Table2-1.png"], "answer": "ALOQ", "figure_type": "table", "content_type": "table", "text_chunks": ["Breakage The target for the arm breakage experiment was set to the final position of the end effector for \\pi' =[0.4, 0.2, 0.6]. Angles between [0.3, 0.7] for the first joint have an associated 5\\% probability of breakage. \\subsubsection*{Comparison of runtimes A comparison of the per-step runtimes for the GP based methods are presented in Figure .", "Figure shows the learned arm configurations together with the policy that would be learned if there were no SREs (`No break'). The shaded region represents the joint angles that can lead to failure. This figure illustrates that ALOQ learns a qualitatively different policy than the other algorithms, one which avoids the joint angles that might lead to a breakage while still getting close to the target faster than the other methods.", "Furthermore, while all the BO based algorithms learn to avoid the wall, active selection of \\theta allows ALOQ to do so more quickly: smart quadrature allows it to more efficiently observe rare events and accurately estimate their boundary. For readability we have only presented the arm configurations for algorithms which have performance comparable to ALOQ. \\subsubsection{Joint", "See for the mathematical formulation of these test functions. The performance of the algorithms on the two functions is presented in Figure . In the Branin function, ALOQ, RQ-ALOQ, unwarped ALOQ, and one-step ALOQ all substantially outperform WSN.", "As expected, ALOQ is once again much faster than WSN. % \\subsubsection*{Variation in performance The quartiles of the expected cost of the final \\hat{\\pi^* by each algorithm across the 20 independent runs are presented in Table . \\subsubsection*{Setting with unknown p(\\theta) As described in the paper, in this setting we assume that \\pi \\in [0,1]^3 is the torque applied to the joints, and", "\\end{figure* \\caption{Performance and learned configurations on the robotic arm joint breakage task. \\end{figure* Experimental Results To evaluate ALOQ we applied it to 1) a simulated robot arm control task, including a variation where p(\\theta) is not known a priori but must be inferred from data, and 2) a hexapod locomotion task .", "The final learned policy, i.e., \\hat{\\pi^*, of each algorithm is shown as a vertical line, along with \\pi^* (the true maximum). These lines illustrate that properly accounting for significant rare events can lead to learning qualitatively different policies. Figures , which plots the performance of all methods the two functions, shows that ALOQ substantially outperforms all the other algorithms except for one-step ALOQ (note that both WSN and the na\\\"{ive approach fail completely in these settings).", "In such cases, the na\\\"{ive approach will not see such rare events often enough to learn an appropriate response. Instead, we propose a new approach called alternating optimisation and quadrature (ALOQ) specifically aimed towards learning policies that are robust to these rare events while being as sample efficient as possible. The main idea is to actively select the environment variables (instead of sampling them) in a simulator thanks to a Gaussian Process (GP) that models returns as a function of both the policy and the environment variables and then, at each time-step, to use BO and Bayesian Quadrature (BQ) in turn to select a policy and environment setting, respectively, to evaluate."], "referred_text_chunks": ["Breakage The target for the arm breakage experiment was set to the final position of the end effector for \\pi' =[0.4, 0.2, 0.6]. Angles between [0.3, 0.7] for the first joint have an associated 5\\% probability of breakage. \\subsubsection*{Comparison of runtimes A comparison of the per-step runtimes for the GP based methods are presented in Figure ."]}, "3": {"question": "Which algorithm performed better on the arm breakage task?", "referred_figures_indices": [7], "response": "{'Image': 0, 'Text': 4, 'Rationale': 'Image 0 shows expected costs related to the arm breakage task across different algorithms, highlighting ALOQ and its variations. Text chunk 4 compares expected costs and highlights ALOQ\u2019s superior performance over other baselines on the arm configuration task.', 'Answer': 'ALOQ performed better on the arm breakage task.'}", "all_figures_names": ["1605.07496v3-Figure3-1.png", "1605.07496v3-Figure7-1.png", "1605.07496v3-Figure10-1.png", "1605.07496v3-Table2-1.png", "1605.07496v3-Table3-1.png", "1605.07496v3-Figure2-1.png", "1605.07496v3-Table1-1.png", "1605.07496v3-Figure4-1.png"], "referred_figures_names": ["1605.07496v3-Figure4-1.png"], "answer": "Both TRPO and Reinforce performed similarly on the arm breakage task.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["\\end{figure* \\caption{Performance and learned configurations on the robotic arm joint breakage task. \\end{figure* Experimental Results To evaluate ALOQ we applied it to 1) a simulated robot arm control task, including a variation where p(\\theta) is not known a priori but must be inferred from data, and 2) a hexapod locomotion task .", "Figure shows the learned arm configurations together with the policy that would be learned if there were no SREs (`No break'). The shaded region represents the joint angles that can lead to failure. This figure illustrates that ALOQ learns a qualitatively different policy than the other algorithms, one which avoids the joint angles that might lead to a breakage while still getting close to the target faster than the other methods.", "Breakage The target for the arm breakage experiment was set to the final position of the end effector for \\pi' =[0.4, 0.2, 0.6]. Angles between [0.3, 0.7] for the first joint have an associated 5\\% probability of breakage. \\subsubsection*{Comparison of runtimes A comparison of the per-step runtimes for the GP based methods are presented in Figure .", "Furthermore, while all the BO based algorithms learn to avoid the wall, active selection of \\theta allows ALOQ to do so more quickly: smart quadrature allows it to more efficiently observe rare events and accurately estimate their boundary. For readability we have only presented the arm configurations for algorithms which have performance comparable to ALOQ. \\subsubsection{Joint", "Figures and show the expected cost (lower is better) of the arm configurations after each timestep for each method. ALOQ, unwarped ALOQ, and RQ-ALOQ greatly outperform the other baselines. Reinforce and TRPO, being relatively sample inefficient, exhibit a very slow rate of improvement in performance, while WSN fails to converge at all.", "Again for readability we only present the arm configurations for the most competitive algorithms. \\subsubsection{Performance of Reinforce and TRPO Both these baselines are relatively sample inefficient. However, one question that arises is whether these methods eventually find the optimal policy.", "Optimising the BO/BQ acquisition functions: We used DIRECT to maximise the BO acquisition function \\alpha_{ALOQ. To minimise the BQ acquisition function, we exhaustively computed \\mathbb{V(\\bar{f(\\pi_{t+1)|\\mathcal{D_{1:t, \\pi_{t+1, \\theta) for each \\theta since this was computationally very cheap. \\section*{Robotic Arm Simulator The configuration of the robot arm is determined by three joint angles, each of which is normalised to lie in [0,1].", "To check this, we ran them for 2000 iterations with a batch size of 5 trajectories (thus a total of 10000 simulator calls). We repeated this for both the Collision Avoidance and Joint Breakage settings. The expected cost of the arm configurations after each iteration are presented in Figure (we only present the results up to 1000 simulator calls for readability - there is no improvement beyond what can be seen in the plot)."], "referred_text_chunks": ["\\end{figure* \\caption{Performance and learned configurations on the robotic arm joint breakage task. \\end{figure* Experimental Results To evaluate ALOQ we applied it to 1) a simulated robot arm control task, including a variation where p(\\theta) is not known a priori but must be inferred from data, and 2) a hexapod locomotion task ."]}, "4": {"question": "Between WSN and ALOQ, which method is the most efficient in terms of runtime for both F-SRE1 and F-SRE2?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 directly compares the runtime of ALOQ and WSN on F-SRE1 and F-SRE2, showing ALOQ as more efficient. Text chunk 0 discusses runtime, emphasizing WSN is slower than ALOQ.', 'Answer': 'ALOQ is more efficient in terms of runtime for both F-SRE1 and F-SRE2.'}", "all_figures_names": ["1605.07496v3-Figure3-1.png", "1605.07496v3-Table1-1.png", "1605.07496v3-Figure2-1.png", "1605.07496v3-Figure11-1.png", "1605.07496v3-Figure7-1.png", "1605.07496v3-Figure1-1.png", "1605.07496v3-Figure4-1.png", "1605.07496v3-Figure8-1.png"], "referred_figures_names": ["1605.07496v3-Figure11-1.png"], "answer": "ALOQ is significantly more efficient than WSN.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["This is not surprising: since the problem does not have SREs, the intensification procedure used by ALOQ does not yield any significant benefit. Figure plots in log scale the per-step runtime of each algorithm, i.e., the time taken to process one data point on the two test functions. WSN takes significantly longer than ALOQ or the other baselines, and shows a clear increasing trend.", "Again WSN is significantly slower than all other methods. In fact, it was not computationally feasible to run WSN beyond 100 data points for F-SRE2. To provide a sense of the variance in the performance of each algorithm across the 20 independent runs, Table presents the quartiles of the expected function value of the final \\hat{\\pi^* for all four artificial test functions.", "See for the mathematical formulation of these test functions. The performance of the algorithms on the two functions is presented in Figure . In the Branin function, ALOQ, RQ-ALOQ, unwarped ALOQ, and one-step ALOQ all substantially outperform WSN.", "Figures and show the expected cost (lower is better) of the arm configurations after each timestep for each method. ALOQ, unwarped ALOQ, and RQ-ALOQ greatly outperform the other baselines. Reinforce and TRPO, being relatively sample inefficient, exhibit a very slow rate of improvement in performance, while WSN fails to converge at all.", "WSN performs better in the Hartmann 6 function as it does not get stuck in a local maximum. However, it still cannot outperform one-step ALOQ. Note that ALOQ slightly underperforms one-step ALOQ.", "This approach will almost surely fail due to not sampling SREs often enough to learn a suitable response. By contrast, our method ALOQ (see Alg.~) models f(\\pi, \\theta) as a GP: f \\sim GP(m, k), acknowledging both its inputs. The main idea behind ALOQ is, given \\mathcal{D_{1:l, to use a BO acquisition function to select \\pi_{l+1 for evaluation and then use a BQ acquisition function to select \\theta_{l+1, conditioning on \\pi_{l+1.", "As expected, ALOQ is once again much faster than WSN. % \\subsubsection*{Variation in performance The quartiles of the expected cost of the final \\hat{\\pi^* by each algorithm across the 20 independent runs are presented in Table . \\subsubsection*{Setting with unknown p(\\theta) As described in the paper, in this setting we assume that \\pi \\in [0,1]^3 is the torque applied to the joints, and", "Our results presented in Section show that, compared to ALOQ, their method is unsuitable for settings with SREs. Further, their method is far more computationally expensive and fails even to outperform a baseline that randomly samples the environment variable at each step. \\citeauthor{krause2011contextual (\\citeyear{krause2011contextual) also address optimising performance in the presence of environment variables."], "referred_text_chunks": ["This is not surprising: since the problem does not have SREs, the intensification procedure used by ALOQ does not yield any significant benefit. Figure plots in log scale the per-step runtime of each algorithm, i.e., the time taken to process one data point on the two test functions. WSN takes significantly longer than ALOQ or the other baselines, and shows a clear increasing trend."]}, "5": {"question": "Which method performs the best on the F-SRE1 test function?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 6, 'Rationale': 'Image 1 shows the expected function value over simulator calls for F-SRE1, directly indicating performance. Text chunk 6 discusses the performance comparison of ALOQ with baselines for test functions like F-SRE1.', 'Answer': 'ALOQ performs the best on the F-SRE1 test function.'}", "all_figures_names": ["1605.07496v3-Table2-1.png", "1605.07496v3-Figure10-1.png", "1605.07496v3-Figure8-1.png", "1605.07496v3-Figure9-1.png", "1605.07496v3-Figure6-1.png", "1605.07496v3-Figure11-1.png", "1605.07496v3-Table3-1.png", "1605.07496v3-Figure2-1.png"], "referred_figures_names": ["1605.07496v3-Figure10-1.png"], "answer": "ALOQ", "figure_type": "plot", "content_type": "figure", "text_chunks": ["These results underscore the fact that a meaningful evaluation must include a problem with SREs, as such problems do demand more robust methods. To create such an evaluation, we formulated two test functions, F-SRE1 and F-SRE2, that are characterised by significant rare events. For \\pi \\in [-2,2], F-SRE1 is defined as: \\end{split \\end{align And for \\pi \\in [-2,2], F-SRE2 is defined as: \\end{split \\end{align Figure shows the contour plots of these two functions.", "Again WSN is significantly slower than all other methods. In fact, it was not computationally feasible to run WSN beyond 100 data points for F-SRE2. To provide a sense of the variance in the performance of each algorithm across the 20 independent runs, Table presents the quartiles of the expected function value of the final \\hat{\\pi^* for all four artificial test functions.", "Both functions have a narrow band of \\theta which corresponds to the SRE regions, i.e. the scale of the rewards is much larger in these regions. In F-SRE1 this is -1<\\theta<0 while in F-SRE2 this is -0.2<\\theta<0.2. We downscaled the region corresponding to the SRE by a factor of 10 to make the plots more readable.", "This approach will almost surely fail due to not sampling SREs often enough to learn a suitable response. By contrast, our method ALOQ (see Alg.~) models f(\\pi, \\theta) as a GP: f \\sim GP(m, k), acknowledging both its inputs. The main idea behind ALOQ is, given \\mathcal{D_{1:l, to use a BO acquisition function to select \\pi_{l+1 for evaluation and then use a BQ acquisition function to select \\theta_{l+1, conditioning on \\pi_{l+1.", "Our results presented in Section show that, compared to ALOQ, their method is unsuitable for settings with SREs. Further, their method is far more computationally expensive and fails even to outperform a baseline that randomly samples the environment variable at each step. \\citeauthor{krause2011contextual (\\citeyear{krause2011contextual) also address optimising performance in the presence of environment variables.", "See for the mathematical formulation of these test functions. The performance of the algorithms on the two functions is presented in Figure . In the Branin function, ALOQ, RQ-ALOQ, unwarped ALOQ, and one-step ALOQ all substantially outperform WSN.", "Further experiments on test functions to clearly show the how each element of ALOQ is necessary for settings with SREs is presented in the supplementary material. We compare ALOQ to several baselines: 1) the na\\\"{ive method described in the previous section; 2) the method of \\citeauthor{williams_santner (\\citeyear{williams_santner), which we refer to as WSN; 3) the simple policy gradient method Reinforce , and 4) the state-of-the-art policy gradient method TRPO . To show the importance of each component of ALOQ, we also perform experiments with ablated versions of ALOQ, namely: 1) Random Quadrature ALOQ (RQ-ALOQ), in which \\theta is sampled randomly from p(\\theta) instead of being chosen actively; 2) unwarped ALOQ, which does not perform Beta warping of the inputs; and 3) one-step ALOQ, which does not use intensification.", "This is not surprising: since the problem does not have SREs, the intensification procedure used by ALOQ does not yield any significant benefit. Figure plots in log scale the per-step runtime of each algorithm, i.e., the time taken to process one data point on the two test functions. WSN takes significantly longer than ALOQ or the other baselines, and shows a clear increasing trend."], "referred_text_chunks": ["These results underscore the fact that a meaningful evaluation must include a problem with SREs, as such problems do demand more robust methods. To create such an evaluation, we formulated two test functions, F-SRE1 and F-SRE2, that are characterised by significant rare events. For \\pi \\in [-2,2], F-SRE1 is defined as: \\end{split \\end{align And for \\pi \\in [-2,2], F-SRE2 is defined as: \\end{split \\end{align Figure shows the contour plots of these two functions."]}, "6": {"question": "How does the predicted return change as a function of \u03b8 for a fixed value of \u03c0 = 1.5?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 0, 'Rationale': 'Image 5 contains a plot (b) explicitly showing the predicted return of \u03c0 = 1.5 as a function of \u03b8, which aligns directly with the question. Text chunk 0 describes how the method models the return as a function of (\u03c0, \u03b8) and outlines the role of \u03b8, which is relevant to understanding the changes based on \u03b8.', 'Answer': 'The predicted return for \u03c0 = 1.5 varies with different \u03b8 values, with uncertainty shown in the image indicating variations in expected outcomes.'}", "all_figures_names": ["1605.07496v3-Figure2-1.png", "1605.07496v3-Figure3-1.png", "1605.07496v3-Figure4-1.png", "1605.07496v3-Figure10-1.png", "1605.07496v3-Figure9-1.png", "1605.07496v3-Figure1-1.png", "1605.07496v3-Table2-1.png", "1605.07496v3-Figure6-1.png"], "referred_figures_names": ["1605.07496v3-Figure1-1.png"], "answer": "The predicted return decreases as \u03b8 increases, with a minimum at around \u03b8 = 0.5.", "figure_type": "Plot", "content_type": "figure", "text_chunks": ["This method requires significant prior knowledge to be able to set the simulator settings such that it provides just the right amount of challenge to the policy. Furthermore, it does not consider any settings with SREs. \\caption{ALOQ models the return f as a function of (\\pi,\\theta); (a) the predicted mean based on some observed data; (b) the predicted return of \\pi=1.5 for different \\theta, together with the uncertainty associated with them, given p(\\theta); (c) ALOQ marginalises out \\theta and computes \\bar{f(\\pi) and its associated uncertainty, which is used to actively select \\pi.", "Formally, this approach models \\tilde{f as a GP with a zero mean function and a suitable covariance function k(\\pi, \\pi'). For any given \\pi, the variation in f due to different settings of \\theta is treated as noise. To estimate \\pi^*, the na\\\"{ive approach applies BO, while sampling \\theta from p(\\theta) at each timestep.", "Once \\pi_{l+1 is chosen, ALOQ selects \\theta_{l+1 by minimising a BQ acquisition function \\eqref{eq:bq_acq quantifying the uncertainty about \\bar{f(\\pi_{l+1). After (\\pi_{t+1, \\theta_{l+1) is selected, ALOQ evaluates it on the simulator and updates the GP with the new datapoint (\\pi_{l+1, \\theta_{l+1, f_{l+1). Our estimate of \\pi^* is thus: Although the approach described so far actively selects \\pi and \\theta through BO and BQ, it is unlikely to perform well in practice.", "The prior can be combined with observed values to update the belief about the function in a Bayesian way to generate a posterior distribution. The prior mean function of the GP is often assumed to be 0 for convenience. A popular choice for the covariance function is the class of stationary functions of the form k(\\mathbf{x, \\mathbf{x') = k(\\mathbf{x - \\mathbf{x'), which implies that the correlation between the function values of any two points depends only on the distance between them.", "In GP regression, it is assumed that the observed function values \\{f(\\mathbf{x_i)\\_{i=1^{N is a sample from a multivariate Gaussian distribution. The prediction for a new point \\mathbf{x^* is connected with the observations through the mean and covariance functions. By conditioning on the observed data, this can be computed analytically as a Gaussian \\mathcal{N\\bigl(\\mu\\bigl(f(\\mathbf{x^*)\\bigr), \\sigma^2\\bigl(f(\\mathbf{x^*)\\bigr)\\bigr): \\end{subequations where \\mathbf{X denotes the vector of observed inputs, f(\\mathbf{X) the vector of corresponding function values, and \\mathbf{K is the matrix with entries k(\\mathbf{x_i, \\mathbf{x_j).", "For discrete \\theta with support \\{\\theta_1, \\theta_2, \\ldots, \\theta_{N_\\theta\\, the estimate of the mean \\mu and variance \\sigma^2 for \\bar{f(\\pi) \\mid \\mathcal{D_{1:l is straightforward: \\end{subequations where f(\\pi,\\theta) is the prediction from the GP with mean and covariance computed using \\eqref{eq:GP_predictions. For continuous \\theta, we apply Monte Carlo quadrature. Although this requires sampling a large number of \\theta and evaluating the corresponding f(\\pi,\\theta) \\mid \\mathcal{D_{1:l, it is feasible since we evaluate f(\\pi,\\theta) \\mid \\mathcal{D_{1:l, not from the expensive simulator, but from the computationally cheaper GP.", "In the first evaluation, (\\pi_{l+1, \\theta_{l+1) is selected via the BO/BQ scheme described earlier. In the second stage, (\\hat{\\pi^*, \\theta^*) is evaluated, where \\hat{\\pi^* \\in \\pi_{1:l+1 is selected using \\eqref{eq:obj_GP and \\theta^*|\\hat{\\pi^* using the BQ acquisition function \\eqref{eq:bq_acq. Computing \\bar{f(\\pi):", "To simulate a set of n physical trials with a baseline policy \\pi_b, we sample \\theta from U(0.5,1) and observe the return f(\\pi_b, \\theta) and add iid Gaussian noise to them. The posterior can then be computed as p(\\theta|\\mathcal{D_{1:n^b, \\pi_b) \\propto p(\\theta)p(\\mathcal{D_{1:n^b|\\pi_b, \\theta), where \\mathcal{D_{1:n^b = \\{(\\pi_b, f_1), (\\pi_b, f_2), ..., (\\pi_b, f_n) \\. We can approximate this using slice sampling since both the prior and the likelihood are analytical."], "referred_text_chunks": ["This method requires significant prior knowledge to be able to set the simulator settings such that it provides just the right amount of challenge to the policy. Furthermore, it does not consider any settings with SREs. \\caption{ALOQ models the return f as a function of (\\pi,\\theta); (a) the predicted mean based on some observed data; (b) the predicted return of \\pi=1.5 for different \\theta, together with the uncertainty associated with them, given p(\\theta); (c) ALOQ marginalises out \\theta and computes \\bar{f(\\pi) and its associated uncertainty, which is used to actively select \\pi."]}, "7": {"question": "What is the difference between the \"True max\" and the \"ALOQ\" curves?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 shows the curves for True max and ALOQ in different settings directly, allowing a visual comparison. Text chunk 0 discusses the performance of ALOQ algorithms compared to max values, relevant for understanding the difference.', 'Answer': 'The \"True max\" curve represents the optimal policy value, while the \"ALOQ\" curve indicates the learned policy value by ALOQ. They differ in how they account for significant rare events, with ALOQ learning qualitatively different policies than the true maximum.'}", "all_figures_names": ["1605.07496v3-Figure9-1.png", "1605.07496v3-Figure3-1.png", "1605.07496v3-Figure4-1.png", "1605.07496v3-Figure6-1.png", "1605.07496v3-Figure2-1.png", "1605.07496v3-Figure7-1.png", "1605.07496v3-Figure1-1.png", "1605.07496v3-Table1-1.png"], "referred_figures_names": ["1605.07496v3-Figure9-1.png"], "answer": "The \"True max\" curve is the true maximum of the function, while the \"ALOQ\" curve is an approximation of the maximum. The \"ALOQ\" curve is lower than the \"True max\" curve, indicating that it underestimates the maximum value of the function.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["The final learned policy, i.e., \\hat{\\pi^*, of each algorithm is shown as a vertical line, along with \\pi^* (the true maximum). These lines illustrate that properly accounting for significant rare events can lead to learning qualitatively different policies. Figures , which plots the performance of all methods the two functions, shows that ALOQ substantially outperforms all the other algorithms except for one-step ALOQ (note that both WSN and the na\\\"{ive approach fail completely in these settings).", "We applied ALOQ to learn the optimal policy given these damage probabilities, but restricted the search to the policies in the archive created by . Figure shows that ALOQ finds a policy with much higher expected reward than RQ-ALOQ. It also shows the policy that generates the maximum reward when none of the legs are damaged or broken (`opt undamaged policy').", "Figure shows the learned arm configurations together with the policy that would be learned if there were no SREs (`No break'). The shaded region represents the joint angles that can lead to failure. This figure illustrates that ALOQ learns a qualitatively different policy than the other algorithms, one which avoids the joint angles that might lead to a breakage while still getting close to the target faster than the other methods.", "ALOQ transforms each dimension of \\pi and \\theta independently, and treats the corresponding (\\alpha,\\beta) as hyperparameters. We assume that we are working with the transformed inputs for the rest of the paper. While the resulting algorithm should be able to cope with SREs, the \\hat{\\pi^* that it returns at each iteration may still be poor, since our BQ evaluation of \\bar{f(\\pi) leads to a noisy approximation of the true expected return.", "This is not surprising: since the problem does not have SREs, the intensification procedure used by ALOQ does not yield any significant benefit. Figure plots in log scale the per-step runtime of each algorithm, i.e., the time taken to process one data point on the two test functions. WSN takes significantly longer than ALOQ or the other baselines, and shows a clear increasing trend.", "Conclusions This paper proposed ALOQ, a novel approach to using BO and BQ to perform sample-efficient RL in a way that is robust to the presence of significant rare events. We empirically evaluated ALOQ on different simulated tasks involving a robotic arm simulator, and a hexapod locomotion task and showed how it can be also be applied to settings where the distribution of the environment variable is unknown a priori, and that it successfully transfers to a real robot. Our results demonstrated that ALOQ outperforms multiple baselines, including related methods proposed in the literature.", "This method requires significant prior knowledge to be able to set the simulator settings such that it provides just the right amount of challenge to the policy. Furthermore, it does not consider any settings with SREs. \\caption{ALOQ models the return f as a function of (\\pi,\\theta); (a) the predicted mean based on some observed data; (b) the predicted return of \\pi=1.5 for different \\theta, together with the uncertainty associated with them, given p(\\theta); (c) ALOQ marginalises out \\theta and computes \\bar{f(\\pi) and its associated uncertainty, which is used to actively select \\pi.", "Once \\pi_{l+1 is chosen, ALOQ selects \\theta_{l+1 by minimising a BQ acquisition function \\eqref{eq:bq_acq quantifying the uncertainty about \\bar{f(\\pi_{l+1). After (\\pi_{t+1, \\theta_{l+1) is selected, ALOQ evaluates it on the simulator and updates the GP with the new datapoint (\\pi_{l+1, \\theta_{l+1, f_{l+1). Our estimate of \\pi^* is thus: Although the approach described so far actively selects \\pi and \\theta through BO and BQ, it is unlikely to perform well in practice."], "referred_text_chunks": ["The final learned policy, i.e., \\hat{\\pi^*, of each algorithm is shown as a vertical line, along with \\pi^* (the true maximum). These lines illustrate that properly accounting for significant rare events can lead to learning qualitatively different policies. Figures , which plots the performance of all methods the two functions, shows that ALOQ substantially outperforms all the other algorithms except for one-step ALOQ (note that both WSN and the na\\\"{ive approach fail completely in these settings)."]}, "8": {"question": "Which method performs the best on the Branin function?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 0, 'Rationale': 'Image 2 specifically shows the performance of different methods on the Branin function, and Text 0 discusses the algorithms outperforming others on this specific function.', 'Answer': 'ALOQ, RQ-ALOQ, unwarped ALOQ, and one-step ALOQ all perform the best on the Branin function, substantially outperforming WSN.'}", "all_figures_names": ["1605.07496v3-Figure11-1.png", "1605.07496v3-Figure4-1.png", "1605.07496v3-Figure7-1.png", "1605.07496v3-Figure8-1.png", "1605.07496v3-Figure9-1.png", "1605.07496v3-Figure6-1.png", "1605.07496v3-Figure3-1.png", "1605.07496v3-Figure1-1.png"], "referred_figures_names": ["1605.07496v3-Figure7-1.png"], "answer": "One Step ALOQ", "figure_type": "plot", "content_type": "figure", "text_chunks": ["See for the mathematical formulation of these test functions. The performance of the algorithms on the two functions is presented in Figure . In the Branin function, ALOQ, RQ-ALOQ, unwarped ALOQ, and one-step ALOQ all substantially outperform WSN.", "For our experiment, we set the reward such that failure to cross the finish line within 5 seconds yields zero reward, while crossing the finish line gives a reward of 100+50v where v is the average velocity in m/s. \\section*{Further experiments In this section, we present the results of further experiments performed on test functions to demonstrate that each element of ALOQ is necessary for settings with SREs. We begin with modified versions of the Branin and Hartmann 6 test functions used by \\citeauthor{williams_santner. The modified Branin test function is a four-dimensional problem, with two dimensions treated as discrete environment variables with a total of 12 support points, while the modified Hartmann 6 test function is six-dimensional with two dimensions treated as environment variables with a total of 49 support points.", "\\kappa \\sigma(\\mathbf{x), where \\kappa controls the exploration-exploitation tradeoff. BQ is a sample-efficient technique for computing integrals of the form \\bar{f = \\int f(\\mathbf{x) p(\\mathbf{x) d\\mathbf{x, where p(\\mathbf{x) is a probability distribution. Using GP regression to compute the prediction for any f(\\mathbf{x) given some observed data, \\bar{f is a Gaussian whose mean and variance can be computed analytically for particular choices of the covariance function and p(\\mathbf{x) .", "The challenge then is to develop algorithms that are sample efficient, i.e., that minimise the number of such trials. In such settings, Bayesian Optimisation (BO) is a sample-efficient approach that has been successfully applied to RL in multiple domains . A na\\\"{ive approach would be to randomly sample values for the environment variables in each trial, so as to estimate expected performance.", "If no analytical solution exists, we can approximate the mean and variance via Monte Carlo quadrature by sampling the predictions of various f(\\mathbf{x). Given some observed data \\mathcal{D, we can also devise acquisition functions for BQ to actively select the next point \\mathbf{x^* for evaluation. A natural objective here is to select \\mathbf{x that minimises the uncertainty of \\bar{f, i.e., \\mathbf{x^* = \\argmin_\\mathbf{x \\mathbb{V(\\bar{f|\\mathcal{D, \\mathbf{x) .", "Monte Carlo (MC) quadrature simply samples (\\mathbf{x_1, \\mathbf{x_2, ... , \\mathbf{x_N) from p(\\mathbf{x) and estimates the integral as \\bar{f \\approx \\frac{1{N\\sum_{i=1^N f(\\mathbf{x_i). This typically requires a large N and so is less sample efficient than BQ: it should only be used if f is cheap to evaluate. The many merits of BQ over MC, both philosophically and practically, are brought out by \\citet{oHagan1987 and \\citet{HenOsbGirRSPA2015.", "Due to the nature of GPs, \\mathbb{V(\\bar{f|\\mathcal{D, \\mathbf{x) does not depend on f(\\mathbf{x) and is thus computationally feasible to evaluate. Uncertainty sampling is an alternative acquisition function that chooses the \\mathbf{x^* with the maximum posterior variance: \\mathbf{x^* = \\argmax_\\mathbf{x \\mathbb{V(f(\\mathbf{x)|\\mathcal{D). Although simple and computationally cheap, it is not the same as reducing uncertainty about \\bar{f since evaluating the point with the highest prediction uncertainty does not necessarily lead to the maximum reduction in the uncertainty of the estimate of the integral.", "Figures and show the expected cost (lower is better) of the arm configurations after each timestep for each method. ALOQ, unwarped ALOQ, and RQ-ALOQ greatly outperform the other baselines. Reinforce and TRPO, being relatively sample inefficient, exhibit a very slow rate of improvement in performance, while WSN fails to converge at all."], "referred_text_chunks": ["See for the mathematical formulation of these test functions. The performance of the algorithms on the two functions is presented in Figure . In the Branin function, ALOQ, RQ-ALOQ, unwarped ALOQ, and one-step ALOQ all substantially outperform WSN."]}}