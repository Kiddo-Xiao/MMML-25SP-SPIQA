{"0": {"question": "Which system from Ortiz et al. achieved the highest BLEU and METEOR scores, and how does it compare to the CCA inference algorithm in terms of performance?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 1, 'Rationale': 'Image 3 provides the BLEU and METEOR scores for multiple systems, allowing direct comparison. Text 1 discusses the evaluation based on these scores, relevant for understanding performance differences.', 'Answer': 'The SMT system by Ortiz et al. achieved the highest BLEU score of 43.7 and METEOR score of 35.6. Compared to the CCA inference algorithm with BLEU 26.1 and METEOR 25.6, the SMT system performs better according to these metrics.'}", "all_figures_names": ["1608.02784v2-Figure3-1.png", "1608.02784v2-Figure4-1.png", "1608.02784v2-Figure6-1.png", "1608.02784v2-Table2-1.png", "1608.02784v2-Table3-1.png", "1608.02784v2-Figure2-1.png", "1608.02784v2-Figure5-1.png", "1608.02784v2-Table1-1.png"], "referred_figures_names": ["1608.02784v2-Table2-1.png"], "answer": "The SMT system from Ortiz et al. achieved the highest BLEU score (43.7) and METEOR score (35.6) among their tested systems. While the SMT system outperforms the CCA inference algorithm in terms of BLEU score, the CCA algorithm achieves a slightly higher METEOR score (25.6 vs 25.5).", "figure_type": "N/A", "content_type": "table", "text_chunks": ["This experiment is similar to the one done by Ortiz et al. The ranking results are given in Table~. The results show that our system tends to score higher for images which are highly ranked (by both the SMT system and CCA), but tends to score lower for images which are lower ranked. In addition, we checked the MT evaluation scores for highly ranked captions both for SMT and CCA (ranking larger than 4).", "\\newcite{ortiz2015learning partially measure the success of their system by comparing BLEU and METEOR scores of their different systems while using the descriptions given in the dataset as a reference set. The scores for their different systems are given in Table~. They compare their system (SMT, based on phrase-based machine translation) against several baselines: model), but it performed badly, giving a BLEU score of 10.20 and a METEOR score of 15.20 with inappropriate captions. It seems like \\textsc{Seq2Seq models is unfit for this dataset, perhaps because of its size.", "The average reference BLEU score is 24.1 and the average METEOR score is 20.0, a significant drop compared to the machine translation system of \\newcite{ortiz2015learning. We concluded from this result that the SMT system is not ``creatively'' mapping the images to their corresponding descriptions. It relies heavily on the training set captions, and learns how to map images to sentences in a manner which does not generalize very well outside of the training set.", "For SMT, the BLEU scores are 49.70 (METEOR 40.10) and for CCA it is 41.80 (METEOR 33.10). This is not the result of images in SMT being ranked higher, as the average ranking among these images is 4.18 for the SMT system and 4.25 for CCA. The lower CCA score again indicates that our system gives captions which are not necessarily aligned with the references, but still correct.", "It also highlights the flaw with using MT evaluation metrics for this dataset. Figure~ also demonstrates that the correlation between BLEU scores and human ranking is not high. More specifically, in that plot, the correlation between the x-axis (ranking) and y-axis (BLEU scores) for CCA is 0.3 and for the SMT system 0.31.", "Another indication that our system creates a more diverse set of captions is that the number of {\\em unique captions it generates for the test set is significantly larger than that of the SMT system by Ortiz et al. The SMT system generates 359 unique captions (out of 2,004 instances in the test set), while CCA generates 496 captions, an increase of 38.1\\%. To test this hypothesis about caption diversity, we conducted the following human experiment.", "See also \\newcite{rastogi2016weighting for similar results. Our system does not score as high as their machine translation system. \\caption{Examples of outputs from the machine translation system and from CCA inference.", "In our CCA learning algorithm, we also need to decide on the value of m. We varied m between 30 and 300 (in steps of 10) and tuned its value on the development set by maximizing BLEU score against the set of references. package from \\url{https://github.com/jhclark/multeval. Interestingly enough, the BLEU scores did not change that much (they usually were within one point of each other for sufficiently large m), pointing to a stability of the algorithm with respect to the number of dimensions used."], "referred_text_chunks": ["This experiment is similar to the one done by Ortiz et al. The ranking results are given in Table~. The results show that our system tends to score higher for images which are highly ranked (by both the SMT system and CCA), but tends to score lower for images which are lower ranked. In addition, we checked the MT evaluation scores for highly ranked captions both for SMT and CCA (ranking larger than 4)."]}, "1": {"question": "What is the difference between the outputs of the machine translation system (SMT) and the CCA inference?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 2, 'Rationale': 'Image 7 shows examples of outputs from both SMT and CCA systems with different evaluations, illustrating the differences in their handling of image captions. Text 2 provides BLEU and METEOR scores, offering a clear comparison in performance metrics between the systems.', 'Answer': 'SMT outputs are more aligned with references, resulting in higher BLEU and METEOR scores, whereas CCA produces more diverse and creative captions, which might not align directly with references but can still be correct.'}", "all_figures_names": ["1608.02784v2-Table3-1.png", "1608.02784v2-Figure3-1.png", "1608.02784v2-Figure5-1.png", "1608.02784v2-Table2-1.png", "1608.02784v2-Table1-1.png", "1608.02784v2-Figure2-1.png", "1608.02784v2-Figure1-1.png", "1608.02784v2-Figure6-1.png"], "referred_figures_names": ["1608.02784v2-Figure6-1.png"], "answer": "The SMT outputs are literal translations of the images, while the CCA outputs take into account the context of the images and generate more natural-sounding descriptions.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["See also \\newcite{rastogi2016weighting for similar results. Our system does not score as high as their machine translation system. \\caption{Examples of outputs from the machine translation system and from CCA inference.", "The top three images give examples where the CCA inference outputs were rated highly by human evaluations (4 or 5), and the SMT ones were rated poorly (1 or 2). The bottom three pictures give the reverse case. \\end{figure*", "For SMT, the BLEU scores are 49.70 (METEOR 40.10) and for CCA it is 41.80 (METEOR 33.10). This is not the result of images in SMT being ranked higher, as the average ranking among these images is 4.18 for the SMT system and 4.25 for CCA. The lower CCA score again indicates that our system gives captions which are not necessarily aligned with the references, but still correct.", "Figure~ describes six examples in which the human raters rated the SMT system highly and CCA poorly and vice-versa. Conclusion We described a technique to predict structures from complex input spaces to complex output spaces based on canonical correlation analysis. Our approach projects the input space into a low-dimensional representation, and then converts it back into an instance in the output space.", "Another indication that our system creates a more diverse set of captions is that the number of {\\em unique captions it generates for the test set is significantly larger than that of the SMT system by Ortiz et al. The SMT system generates 359 unique captions (out of 2,004 instances in the test set), while CCA generates 496 captions, an increase of 38.1\\%. To test this hypothesis about caption diversity, we conducted the following human experiment.", "This experiment is similar to the one done by Ortiz et al. The ranking results are given in Table~. The results show that our system tends to score higher for images which are highly ranked (by both the SMT system and CCA), but tends to score lower for images which are lower ranked. In addition, we checked the MT evaluation scores for highly ranked captions both for SMT and CCA (ranking larger than 4).", "It also highlights the flaw with using MT evaluation metrics for this dataset. Figure~ also demonstrates that the correlation between BLEU scores and human ranking is not high. More specifically, in that plot, the correlation between the x-axis (ranking) and y-axis (BLEU scores) for CCA is 0.3 and for the SMT system 0.31.", "The average reference BLEU score is 24.1 and the average METEOR score is 20.0, a significant drop compared to the machine translation system of \\newcite{ortiz2015learning. We concluded from this result that the SMT system is not ``creatively'' mapping the images to their corresponding descriptions. It relies heavily on the training set captions, and learns how to map images to sentences in a manner which does not generalize very well outside of the training set."], "referred_text_chunks": ["See also \\newcite{rastogi2016weighting for similar results. Our system does not score as high as their machine translation system. \\caption{Examples of outputs from the machine translation system and from CCA inference."]}, "2": {"question": "Which system, CCA or SMT, generally performs better when the caption is of low quality (average rank less than 3)?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 4, 'Rationale': 'Image 4 directly compares CCA and SMT performance regarding rankings and gives specific data for when ratings are less than 3. Text 4 discusses detailed human rating outcomes comparing SMT and CCA, which is crucial for answering the question on low-quality captions.', 'Answer': 'When the caption is of low quality (average rank less than 3), SMT generally performs better with an average rank of 1.77 compared to CCA which has an average rank of 1.64.'}", "all_figures_names": ["1608.02784v2-Figure6-1.png", "1608.02784v2-Figure1-1.png", "1608.02784v2-Figure2-1.png", "1608.02784v2-Figure5-1.png", "1608.02784v2-Table3-1.png", "1608.02784v2-Figure3-1.png", "1608.02784v2-Figure4-1.png", "1608.02784v2-Table2-1.png"], "referred_figures_names": ["1608.02784v2-Table3-1.png"], "answer": "It is difficult to definitively say which system performs better for low-quality captions based solely on the provided data. However, we can observe some trends:\n\n1. When the SMT average rank is below 3, the CCA average rank is also lower (1.64 vs. 1.77).\n2. When the SMT average rank is 3 or higher, the CCA average rank is significantly higher (3.54 vs. 3.46).\n\nThis suggests that CCA might perform relatively better for low-quality captions, but more data and analysis are needed for a conclusive answer.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["For SMT, the BLEU scores are 49.70 (METEOR 40.10) and for CCA it is 41.80 (METEOR 33.10). This is not the result of images in SMT being ranked higher, as the average ranking among these images is 4.18 for the SMT system and 4.25 for CCA. The lower CCA score again indicates that our system gives captions which are not necessarily aligned with the references, but still correct.", "This experiment is similar to the one done by Ortiz et al. The ranking results are given in Table~. The results show that our system tends to score higher for images which are highly ranked (by both the SMT system and CCA), but tends to score lower for images which are lower ranked. In addition, we checked the MT evaluation scores for highly ranked captions both for SMT and CCA (ranking larger than 4).", "The top three images give examples where the CCA inference outputs were rated highly by human evaluations (4 or 5), and the SMT ones were rated poorly (1 or 2). The bottom three pictures give the reverse case. \\end{figure*", "Another indication that our system creates a more diverse set of captions is that the number of {\\em unique captions it generates for the test set is significantly larger than that of the SMT system by Ortiz et al. The SMT system generates 359 unique captions (out of 2,004 instances in the test set), while CCA generates 496 captions, an increase of 38.1\\%. To test this hypothesis about caption diversity, we conducted the following human experiment.", "Figure~ describes six examples in which the human raters rated the SMT system highly and CCA poorly and vice-versa. Conclusion We described a technique to predict structures from complex input spaces to complex output spaces based on canonical correlation analysis. Our approach projects the input space into a low-dimensional representation, and then converts it back into an instance in the output space.", "The average reference BLEU score is 24.1 and the average METEOR score is 20.0, a significant drop compared to the machine translation system of \\newcite{ortiz2015learning. We concluded from this result that the SMT system is not ``creatively'' mapping the images to their corresponding descriptions. It relies heavily on the training set captions, and learns how to map images to sentences in a manner which does not generalize very well outside of the training set.", "A score of 1 or 2 means that the caption likely does not adequately describe the scene. A score of 3 usually means that the caption describes some salient component in the scene, but perhaps not the most important one. Scores of 4 and 5 usually denote good captions that adequately describe the corresponding scenes.", "It also highlights the flaw with using MT evaluation metrics for this dataset. Figure~ also demonstrates that the correlation between BLEU scores and human ranking is not high. More specifically, in that plot, the correlation between the x-axis (ranking) and y-axis (BLEU scores) for CCA is 0.3 and for the SMT system 0.31."], "referred_text_chunks": ["For SMT, the BLEU scores are 49.70 (METEOR 40.10) and for CCA it is 41.80 (METEOR 33.10). This is not the result of images in SMT being ranked higher, as the average ranking among these images is 4.18 for the SMT system and 4.25 for CCA. The lower CCA score again indicates that our system gives captions which are not necessarily aligned with the references, but still correct."]}, "3": {"question": "What is the purpose of the singular value decomposition step in the CCA algorithm?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 1, 'Rationale': \"Image 0 shows the CCA learning algorithm steps, including singular value decomposition (SVD), and Text 1 explains the role of SVD in the CCA algorithm. Together, they detail how SVD is used to get linear projections maximizing the correlation between vectors.\", 'Answer': 'The singular value decomposition step in the CCA algorithm is used to apply linear projections to random vectors, achieving maximal correlation between them.'}", "all_figures_names": ["1608.02784v2-Figure1-1.png", "1608.02784v2-Figure3-1.png", "1608.02784v2-Figure2-1.png", "1608.02784v2-Table2-1.png", "1608.02784v2-Figure5-1.png", "1608.02784v2-Table1-1.png", "1608.02784v2-Figure4-1.png", "1608.02784v2-Figure6-1.png"], "referred_figures_names": ["1608.02784v2-Figure1-1.png"], "answer": "The singular value decomposition step is used to find the projection matrices U and V.", "figure_type": "Schematic", "content_type": "figure", "text_chunks": ["The problem of CCA can be solved by applying singular value decomposition (SVD) on a cross-covariance matrix between the two random vectors X and Y, normalized by the covariance matrices of X and Y. More specifically, CCA is solved by applying thin singular value decomposition (SVD) on the empirical version of the following matrix: \\noindent where E[\\cdot] is the expectation operator and \\Sigma is a diagonal matrix of size m \\times m for some small m. Since this version of CCA requires inverting matrices of potentially large dimension (d \\times d and d'", "Introduction Canonical correlation analysis (CCA) is a method to reduce the dimensionality of multiview data, introduced by \\newcite{hotelling1935canonical. It takes two random vectors and computes their corresponding empirical cross-covariance matrix. It then applies singular value decomposition (SVD) on this matrix to get linear projections of the random vectors that have maximal correlation.", "\\times d'), it is often the case that only the diagonal elements from these matrices are used, as we see in Section~. CCA and its variants have been used in various contexts in NLP. They were used to derive word embeddings , derive multilingual embeddings , build bilingual lexicons , encode prior knowledge into embeddings , semantically analyze text and reduce the dimensions of text with many views . CCA is also an important sub-routine in the family of spectral algorithms for estimating structured models such as latent-variable PCFGs and HMMs or finding word clusters .", "This dataset has been used for description generation , sentence-to-scene generation and object dynamics prediction so far. Learning and Decoding We now describe the learning algorithm, based on CCA, and the corresponding decoding algorithm when the output space is text. Learning Based on Canonical Correlation Analysis We assume two structured spaces, an input space \\inx and an output space \\outy.", "In our CCA learning algorithm, we also need to decide on the value of m. We varied m between 30 and 300 (in steps of 10) and tuned its value on the development set by maximizing BLEU score against the set of references. package from \\url{https://github.com/jhclark/multeval. Interestingly enough, the BLEU scores did not change that much (they usually were within one point of each other for sufficiently large m), pointing to a stability of the algorithm with respect to the number of dimensions used.", "The value of m should be relatively small compared to d and d'. We then choose u and v to be: \\end{center \\caption{Demonstration of CCA inference. An object from the input space \\inx (the image on the left x) is mapped to a unit vector.", "Figure~ gives a description of our CCA algorithm in the context of this problem. The Abstract Scenes Dataset consists of 10,020 scenes, each represented as a set of clipart objects placed in different positions and sizes in a background image (consisting of a grassy area and sky). Cliparts can appear in different ways, for example, the boy and the girl (cliparts 18 and 19), can be depicted sad, angry, sitting or running.", "The procedure that CCA follows finds a projection of the two views in a shared space of dimension m, such that the correlation between the two views is maximized at each coordinate, and there is minimal redundancy between the coordinates of each view. This means that CCA solves the following sequence of optimization problems for j \\in \\{1 ,\\ldots, m \\ where a_j \\in \\mathbb{R^{1 \\times d and b_j \\in \\mathbb{R^{1 \\times d': \\noindent where \\mathrm{corr is a function that accepts two vectors and returns the Pearson correlation between the pairwise elements of the two vectors."], "referred_text_chunks": ["The problem of CCA can be solved by applying singular value decomposition (SVD) on a cross-covariance matrix between the two random vectors X and Y, normalized by the covariance matrices of X and Y. More specifically, CCA is solved by applying thin singular value decomposition (SVD) on the empirical version of the following matrix: \\noindent where E[\\cdot] is the expectation operator and \\Sigma is a diagonal matrix of size m \\times m for some small m. Since this version of CCA requires inverting matrices of potentially large dimension (d \\times d and d'"]}, "4": {"question": "What is the relationship between the input space and the output space in CCA inference?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 3, 'Rationale': 'Image 3 visually demonstrates the mapping process in CCA inference from an input space (image) to an output space (text), aligning with the key concept of projecting inputs and outputs in low-dimensional space. Text 3 describes using CCA for structured prediction by projecting inputs and outputs to a low-dimensional space, reinforcing this relationship.', 'Answer': 'In CCA inference, the input space is projected onto a low-dimensional space where it is correlated with a corresponding projection of the output space. The outputs that correspond to each other are projected to close points, establishing a relationship between them.'}", "all_figures_names": ["1608.02784v2-Table1-1.png", "1608.02784v2-Figure3-1.png", "1608.02784v2-Figure4-1.png", "1608.02784v2-Figure2-1.png", "1608.02784v2-Figure5-1.png", "1608.02784v2-Table2-1.png", "1608.02784v2-Table3-1.png", "1608.02784v2-Figure6-1.png"], "referred_figures_names": ["1608.02784v2-Figure2-1.png"], "answer": "The input space and the output space are related by a cosine similarity measure.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Figure~ describes six examples in which the human raters rated the SMT system highly and CCA poorly and vice-versa. Conclusion We described a technique to predict structures from complex input spaces to complex output spaces based on canonical correlation analysis. Our approach projects the input space into a low-dimensional representation, and then converts it back into an instance in the output space.", "The value of m should be relatively small compared to d and d'. We then choose u and v to be: \\end{center \\caption{Demonstration of CCA inference. An object from the input space \\inx (the image on the left x) is mapped to a unit vector.", "This dataset has been used for description generation , sentence-to-scene generation and object dynamics prediction so far. Learning and Decoding We now describe the learning algorithm, based on CCA, and the corresponding decoding algorithm when the output space is text. Learning Based on Canonical Correlation Analysis We assume two structured spaces, an input space \\inx and an output space \\outy.", "In this paper, we investigate the idea of using CCA for a full-fledged structured prediction problem. More specifically, we suggest a method in which we take a structured prediction problem training set, and then project both the inputs and the outputs to low-dimensional space. The projection ensures that inputs and outputs that correspond to each other are projected to close points in low-dimensional space.", "This is indeed the recipe that the cosine similarity metric follows. In Section~ we give an additional interpretation to the use of cosine similarity, as finding the maximum aposteriori solution for a re-normalized von Mises-Fisher distribution. When the Output Space is Language While our approach to mapping from an input space to an output space through CCA is rather abstract and general, in this paper we focus in cases where the output space \\outy", "The top three images give examples where the CCA inference outputs were rated highly by human evaluations (4 or 5), and the SMT ones were rated poorly (1 or 2). The bottom three pictures give the reverse case. \\end{figure*", "Introduction Canonical correlation analysis (CCA) is a method to reduce the dimensionality of multiview data, introduced by \\newcite{hotelling1935canonical. It takes two random vectors and computes their corresponding empirical cross-covariance matrix. It then applies singular value decomposition (SVD) on this matrix to get linear projections of the random vectors that have maximal correlation.", "The procedure that CCA follows finds a projection of the two views in a shared space of dimension m, such that the correlation between the two views is maximized at each coordinate, and there is minimal redundancy between the coordinates of each view. This means that CCA solves the following sequence of optimization problems for j \\in \\{1 ,\\ldots, m \\ where a_j \\in \\mathbb{R^{1 \\times d and b_j \\in \\mathbb{R^{1 \\times d': \\noindent where \\mathrm{corr is a function that accepts two vectors and returns the Pearson correlation between the pairwise elements of the two vectors."], "referred_text_chunks": ["Figure~ describes six examples in which the human raters rated the SMT system highly and CCA poorly and vice-versa. Conclusion We described a technique to predict structures from complex input spaces to complex output spaces based on canonical correlation analysis. Our approach projects the input space into a low-dimensional representation, and then converts it back into an instance in the output space."]}, "5": {"question": "What is the role of the temperature variable t in the CCA decoding algorithm?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 6, 'Rationale': 'Image 3 shows the CCA decoding algorithm where the role of temperature is clearly outlined in the algorithm steps. Text 6 describes the input and algorithm for CCA, providing relevant context.', 'Answer': 'The temperature variable \\( t \\) in the CCA decoding algorithm controls the annealing process, gradually reducing randomness in selecting phrases and guiding the algorithm towards convergence.'}", "all_figures_names": ["1608.02784v2-Table1-1.png", "1608.02784v2-Table2-1.png", "1608.02784v2-Figure6-1.png", "1608.02784v2-Figure3-1.png", "1608.02784v2-Figure5-1.png", "1608.02784v2-Figure2-1.png", "1608.02784v2-Figure1-1.png", "1608.02784v2-Figure4-1.png"], "referred_figures_names": ["1608.02784v2-Figure3-1.png"], "answer": "The temperature variable t controls the probability of accepting a new candidate solution y. As t decreases, the probability of accepting a worse solution decreases.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["The problem of CCA can be solved by applying singular value decomposition (SVD) on a cross-covariance matrix between the two random vectors X and Y, normalized by the covariance matrices of X and Y. More specifically, CCA is solved by applying thin singular value decomposition (SVD) on the empirical version of the following matrix: \\noindent where E[\\cdot] is the expectation operator and \\Sigma is a diagonal matrix of size m \\times m for some small m. Since this version of CCA requires inverting matrices of potentially large dimension (d \\times d and d'", "Introduction Canonical correlation analysis (CCA) is a method to reduce the dimensionality of multiview data, introduced by \\newcite{hotelling1935canonical. It takes two random vectors and computes their corresponding empirical cross-covariance matrix. It then applies singular value decomposition (SVD) on this matrix to get linear projections of the random vectors that have maximal correlation.", "This dataset has been used for description generation , sentence-to-scene generation and object dynamics prediction so far. Learning and Decoding We now describe the learning algorithm, based on CCA, and the corresponding decoding algorithm when the output space is text. Learning Based on Canonical Correlation Analysis We assume two structured spaces, an input space \\inx and an output space \\outy.", "\\times d'), it is often the case that only the diagonal elements from these matrices are used, as we see in Section~. CCA and its variants have been used in various contexts in NLP. They were used to derive word embeddings , derive multilingual embeddings , build bilingual lexicons , encode prior knowledge into embeddings , semantically analyze text and reduce the dimensions of text with many views . CCA is also an important sub-routine in the family of spectral algorithms for estimating structured models such as latent-variable PCFGs and HMMs or finding word clusters .", "In our CCA learning algorithm, we also need to decide on the value of m. We varied m between 30 and 300 (in steps of 10) and tuned its value on the development set by maximizing BLEU score against the set of references. package from \\url{https://github.com/jhclark/multeval. Interestingly enough, the BLEU scores did not change that much (they usually were within one point of each other for sufficiently large m), pointing to a stability of the algorithm with respect to the number of dimensions used.", "= \\cos \\theta. \\end{figure* For the similarity metric, we use the cosine similarity: Figure~ describes a sketch of our CCA inference algorithm.", "The value of m should be relatively small compared to d and d'. We then choose u and v to be: \\end{center \\caption{Demonstration of CCA inference. An object from the input space \\inx (the image on the left x) is mapped to a unit vector.", "For SMT, the BLEU scores are 49.70 (METEOR 40.10) and for CCA it is 41.80 (METEOR 33.10). This is not the result of images in SMT being ranked higher, as the average ranking among these images is 4.18 for the SMT system and 4.25 for CCA. The lower CCA score again indicates that our system gives captions which are not necessarily aligned with the references, but still correct."], "referred_text_chunks": ["The problem of CCA can be solved by applying singular value decomposition (SVD) on a cross-covariance matrix between the two random vectors X and Y, normalized by the covariance matrices of X and Y. More specifically, CCA is solved by applying thin singular value decomposition (SVD) on the empirical version of the following matrix: \\noindent where E[\\cdot] is the expectation operator and \\Sigma is a diagonal matrix of size m \\times m for some small m. Since this version of CCA requires inverting matrices of potentially large dimension (d \\times d and d'"]}, "6": {"question": "What is the relationship between BLEU score and human ranking for CCA and SMT systems?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 0, 'Rationale': 'Image 5 visually shows the correlation between BLEU scores and human ratings for CCA and SMT systems, which directly pertains to the relationship inquired. Text 0 mentions the correlation values between BLEU scores and human rankings for both systems, directly addressing the question.', 'Answer': 'The correlation between BLEU scores and human ranking is low for both CCA (0.3) and SMT (0.31) systems, indicating that BLEU scores are not strongly aligned with human evaluations.'}", "all_figures_names": ["1608.02784v2-Figure5-1.png", "1608.02784v2-Figure2-1.png", "1608.02784v2-Table3-1.png", "1608.02784v2-Table1-1.png", "1608.02784v2-Figure1-1.png", "1608.02784v2-Figure4-1.png", "1608.02784v2-Figure6-1.png", "1608.02784v2-Figure3-1.png"], "referred_figures_names": ["1608.02784v2-Figure4-1.png"], "answer": "The correlation between BLEU scores and human ranking is not high for either CCA or SMT systems.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["It also highlights the flaw with using MT evaluation metrics for this dataset. Figure~ also demonstrates that the correlation between BLEU scores and human ranking is not high. More specifically, in that plot, the correlation between the x-axis (ranking) and y-axis (BLEU scores) for CCA is 0.3 and for the SMT system 0.31.", "For SMT, the BLEU scores are 49.70 (METEOR 40.10) and for CCA it is 41.80 (METEOR 33.10). This is not the result of images in SMT being ranked higher, as the average ranking among these images is 4.18 for the SMT system and 4.25 for CCA. The lower CCA score again indicates that our system gives captions which are not necessarily aligned with the references, but still correct.", "The average reference BLEU score is 24.1 and the average METEOR score is 20.0, a significant drop compared to the machine translation system of \\newcite{ortiz2015learning. We concluded from this result that the SMT system is not ``creatively'' mapping the images to their corresponding descriptions. It relies heavily on the training set captions, and learns how to map images to sentences in a manner which does not generalize very well outside of the training set.", "As such, the calculation of SMT evaluation scores with respect to a reference set is not necessarily the best mechanism to identify the correctness of a textual description. To demonstrate this point, we measure BLEU scores of one of the reference sentences while comparing it to the other references in the set. We did that for each of the eight batches of references available in the training set.", "\\newcite{ortiz2015learning partially measure the success of their system by comparing BLEU and METEOR scores of their different systems while using the descriptions given in the dataset as a reference set. The scores for their different systems are given in Table~. They compare their system (SMT, based on phrase-based machine translation) against several baselines: model), but it performed badly, giving a BLEU score of 10.20 and a METEOR score of 15.20 with inappropriate captions. It seems like \\textsc{Seq2Seq models is unfit for this dataset, perhaps because of its size.", "This experiment is similar to the one done by Ortiz et al. The ranking results are given in Table~. The results show that our system tends to score higher for images which are highly ranked (by both the SMT system and CCA), but tends to score lower for images which are lower ranked. In addition, we checked the MT evaluation scores for highly ranked captions both for SMT and CCA (ranking larger than 4).", "In our CCA learning algorithm, we also need to decide on the value of m. We varied m between 30 and 300 (in steps of 10) and tuned its value on the development set by maximizing BLEU score against the set of references. package from \\url{https://github.com/jhclark/multeval. Interestingly enough, the BLEU scores did not change that much (they usually were within one point of each other for sufficiently large m), pointing to a stability of the algorithm with respect to the number of dimensions used.", "The top three images give examples where the CCA inference outputs were rated highly by human evaluations (4 or 5), and the SMT ones were rated poorly (1 or 2). The bottom three pictures give the reverse case. \\end{figure*"], "referred_text_chunks": ["It also highlights the flaw with using MT evaluation metrics for this dataset. Figure~ also demonstrates that the correlation between BLEU scores and human ranking is not high. More specifically, in that plot, the correlation between the x-axis (ranking) and y-axis (BLEU scores) for CCA is 0.3 and for the SMT system 0.31."]}}