{"0": {"question": "What is the function of the ReOrth Layer in the Projection Block of the Grassmann Network architecture?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 4, 'Rationale': 'Image 0 directly illustrates the structure of the Grassmann Network, showing where the ReOrth Layer is placed within the Projection Block. Text 4 specifically explains the function of the ReOrth Layer, detailing its role in normalizing the matrices.', 'Answer': 'The ReOrth Layer normalizes the output matrices of the FRMap layers to maintain orthogonality, ensuring they become orthonormal matrices on the Stiefel manifold.'}", "all_figures_names": ["1611.05742v3-Figure1-1.png", "1611.05742v3-Figure2-1.png", "1611.05742v3-Table1-1.png"], "referred_figures_names": ["1611.05742v3-Figure1-1.png"], "answer": "The ReOrth Layer re-orthogonalizes the output of the FRMap Layer.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Grassmann Network Architecture In analogy to convolutional networks (ConvNets), the proposed Grassmann network (GrNet) also devises a Projection block containing fully connected convolution-like layers and normalization layers, named full rank mapping (FRMap) layers and re-orthonormalization (ReOrth) layers respectively. Inspired by the geometry-aware manifold learning idea , the FRMap layers are proposed to firstly perform transformations on input orthonormal matrices of subspaces to generate new matrices by adopting a full rank mapping scheme.", "The GrNets design brand new ReOrth, ProjMap and ProjPooling layers. Particularly, for the ReOrth layers, the exploration of the QR decomposition in backpropagation is a very important theoretical contribution. Besides, it devises pooling layer across and within projection matrices (A-ProjPooling, W-ProjPooling), both of which work on the top of the M-FRMap layers.", "\\caption{Conceptual illustration of the proposed Grassmann Network (GrNet) architecture. The rectangles in blue represent three basic blocks, i.e., Projection, Pooling and Output blocks, respectively. \\end{figure*", "ProjMap Layer The ProjMap layer is designed to perform Grassmannian computation on the resulting orthonormal matrices. As well-studied in , the projection metric is one of the most popular Grassmannian metrics, and is able to endow the specific Riemannian manifold with an inner product structure so that the manifold is reduced to a flat space.", "Then, the ReOrth layers are developed to normalize the output matrices of the FRMap layers so that they can keep the basic orthogonality. In other words, the normalized data become orthonormal matrices that reside on Stiefel manifold,d_{k-1) is the set of d_{k-dimensional orthogonal matrices of the \\mathbb{R^{d_{k-1.. As well-studied in , the projection metric performing on orthonormal matrices can represent linear subspaces and respect the geometry of Grassmann manifolds, which is actually a quotient manifold of the Stiefel manifold.", "For PaSC, although the used softmax output layer in the GrNet does not suit the verification task well, we find that it still reaches the highest performances in the case of 2 blocks, which learns more favorable Grassmannian representation. As studied in existing state-of-the-art Grassmann learning methods, the GrNet without Riemannian computing (i.e., ProjMap) and without geometry-aware learning (i.e., ReOrth) perform very badly (17.62\\% and 26.15\\% respectively for AFEW). Besides, the consistent improvement of stacking more GrNet blocks verifies it can learn more discriminative Grassmannian representations and finally improve the classification performances.", "Additionally, since traditional pooling layers can reduce the network complexity, we also study projection pooling (ProjPooling) layers on the projection matrix form of the resulting orthonormal matrices. As it is non-trivial to perform pooling on non-Euclidean data directly, we develop a Pooling block to combine ProMap, ProjPooling and orthonormal mapping (OrthMap) layers, which respectively achieves Euclidean representation, performs pooling on resulting Euclidean data and transforms the results back to orthonormal data. The proposed GrNet structure is illustrated in Fig.. FRMap Layer To learn compact and discriminative Grassmannian representation for better classification, we design the FRMap layers to first transform the input orthonormal matrices of subspaces to new matrices by a linear mapping function f_{fr as where \\bm{X_{k-1 \\in Gr(q,d_{k-1) is the input of the k-th layer, \\bm{W_k \\in \\mathbb{R_{*^{d_{k \\times d_{k-1, (d_{k < d_{k-1) is the transformation matrix (connection weights) that is basically required to be a \\mbox{row full rank matrix, \\bm{X_k \\in \\mathbb{R^{d_{k \\times q is the resulting matrix.", "Introduction This paper introduces a deep network architecture on Grassmannians, which are manifolds of linear subspaces. In many computer vision applications, linear subspaces have become a core representation. For example, for face verification , emotion estimation and activity recognition , the image sets of a single person are often modeled by low dimensional subspaces that are then compared on Grassmannians."], "referred_text_chunks": ["Grassmann Network Architecture In analogy to convolutional networks (ConvNets), the proposed Grassmann network (GrNet) also devises a Projection block containing fully connected convolution-like layers and normalization layers, named full rank mapping (FRMap) layers and re-orthonormalization (ReOrth) layers respectively. Inspired by the geometry-aware manifold learning idea , the FRMap layers are proposed to firstly perform transformations on input orthonormal matrices of subspaces to generate new matrices by adopting a full rank mapping scheme."]}, "1": {"question": "Which pooling method is the most accurate for the AFEW database?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 6, 'Rationale': \"Image 1 directly compares different pooling methods including A-ProjPooling and W-ProjPooling and their accuracies on the AFEW dataset. Text 6 discusses the performance of W-ProjPooling relative to A-ProjPooling, indicating W-ProjPooling typically outperforms A-ProjPooling.\", 'Answer': 'W-ProjPooling is the most accurate for the AFEW database.'}", "all_figures_names": ["1611.05742v3-Figure1-1.png", "1611.05742v3-Figure2-1.png", "1611.05742v3-Table1-1.png"], "referred_figures_names": ["1611.05742v3-Figure2-1.png"], "answer": "W-ProjPooling", "figure_type": "plot", "content_type": "figure", "text_chunks": ["\\noindentEmotion Recognition: We utilize the popular Acted Facial Expression in Wild (AFEW) dataset. The dataset contains 1,345 sequences of facial expressions acted by 330 actors in close to real world setting.", "The learning rate \\lambda and the batch size are set to 0.01 and 30 respectively. The FRMap matrices are all initialized as random full rank matrices, and the number of them per layer is set to 16. For all the ProjPooling layers, the number n of the instances for pooling are fixed as 4.", "Additionally, since traditional pooling layers can reduce the network complexity, we also study projection pooling (ProjPooling) layers on the projection matrix form of the resulting orthonormal matrices. As it is non-trivial to perform pooling on non-Euclidean data directly, we develop a Pooling block to combine ProMap, ProjPooling and orthonormal mapping (OrthMap) layers, which respectively achieves Euclidean representation, performs pooling on resulting Euclidean data and transforms the results back to orthonormal data. The proposed GrNet structure is illustrated in Fig.. FRMap Layer To learn compact and discriminative Grassmannian representation for better classification, we design the FRMap layers to first transform the input orthonormal matrices of subspaces to new matrices by a linear mapping function f_{fr as where \\bm{X_{k-1 \\in Gr(q,d_{k-1) is the input of the k-th layer, \\bm{W_k \\in \\mathbb{R_{*^{d_{k \\times d_{k-1, (d_{k < d_{k-1) is the transformation matrix (connection weights) that is basically required to be a \\mbox{row full rank matrix, \\bm{X_k \\in \\mathbb{R^{d_{k \\times q is the resulting matrix.", "It is known that classical pooling layers with max, min and mean pooling functions reduce the sizes of the representations to lower the model complexity, and therefore improve the regular ConvNets. Motivated by this, we also design pooling layers for the feature maps of Grassmannian data. Without loss of generality, we here study mean pooling for the Grassmannian points.", "The instances can be either n projection matrices or n entries within an square patch of size \\sqrt{n \\times \\sqrt{n located in one projection matrix. In other words, the first type of pooling is performed across the projection matrices (A-ProjPooling), while the second one is executed by sliding the mean filter over each square patch within one projection matrix (W-ProjPooling). As a result, A-ProjPooling with OrthMap finally yields \\frac{m{n orthonormal matrices of size d_{k-1 \\times q, while W-ProjPooling with OrthMap outputs m orthonormal matrices of size \\frac{d_{k-1{\\sqrt{n \\times q. Output Layers As shown in Fig., after applying the ProjMap layer, the outputs (i.e., the projection matrices) lie in Euclidean space, and thus can be converted into vector forms.", "(b)(c) show the GrNet can use much less epochs (than the SPDNet) to converge on AFEW, and the validation gets near 12\\% improvement after training. For larger datasets like HDM05, the improvement is even up to 40\\%. Conclusion This paper introduced the first network architecture to perform deep learning over Grassmann manifolds.", "As studied in Fig. (a), W-ProjPooling typically outperforms A-ProjPooling. The unfavorable performance of A-ProjPoolings would be caused by the extrinsic mean calulation on the Grassmannian and the weak relationship across multiple projection matrices.", "For PaSC, although the used softmax output layer in the GrNet does not suit the verification task well, we find that it still reaches the highest performances in the case of 2 blocks, which learns more favorable Grassmannian representation. As studied in existing state-of-the-art Grassmann learning methods, the GrNet without Riemannian computing (i.e., ProjMap) and without geometry-aware learning (i.e., ReOrth) perform very badly (17.62\\% and 26.15\\% respectively for AFEW). Besides, the consistent improvement of stacking more GrNet blocks verifies it can learn more discriminative Grassmannian representations and finally improve the classification performances."], "referred_text_chunks": ["\\noindentEmotion Recognition: We utilize the popular Acted Facial Expression in Wild (AFEW) dataset. The dataset contains 1,345 sequences of facial expressions acted by 330 actors in close to real world setting."]}, "2": {"question": "Which method performs best on the PaSC dataset for the handheld testing scenario (PaSC2), and how does its performance compare to other methods?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 1, 'Rationale': 'Image 2 directly presents the performance data for PaSC in handheld testing (PaSC2). Text 1 explains the superior performance of the GrNet with 2 blocks, highlighting its effectiveness in improving classification performances.', 'Answer': 'The GrNet with 1 block performs best on PaSC2 with an accuracy of 72.51%, outperforming other methods like SPDNet (72.83%) and VGGDeepFace (68.76%).'}", "all_figures_names": ["1611.05742v3-Figure1-1.png", "1611.05742v3-Figure2-1.png", "1611.05742v3-Table1-1.png"], "referred_figures_names": ["1611.05742v3-Table1-1.png"], "answer": "The method that performs best on the PaSC dataset for the handheld testing scenario (PaSC2) is SPDNet, with an accuracy of 72.83%. This performance is slightly higher than GrNet-2Blocks (72.76%) and significantly higher than other methods like VGGDeepFace (68.24%) and DeepO2P (60.14%).", "figure_type": "N/A", "content_type": "table", "text_chunks": ["\\noindentFace Verification: We employ one standard dataset named Point-and-Shoot Challenge (PaSC) . For 256 subjects, it owns 1,401 videos taken by control cameras, and 1,401 videos from handheld cameras. For the dataset, designs control and handheld face verification experiments.", "For PaSC, although the used softmax output layer in the GrNet does not suit the verification task well, we find that it still reaches the highest performances in the case of 2 blocks, which learns more favorable Grassmannian representation. As studied in existing state-of-the-art Grassmann learning methods, the GrNet without Riemannian computing (i.e., ProjMap) and without geometry-aware learning (i.e., ReOrth) perform very badly (17.62\\% and 26.15\\% respectively for AFEW). Besides, the consistent improvement of stacking more GrNet blocks verifies it can learn more discriminative Grassmannian representations and finally improve the classification performances.", "\\noindentComparing methods: 1). General manifold learning methods: Expressionlets on Spatio-Temporal Manifold (STM-ExpLet) and Riemannian Sparse Representaion combining with Manifold Learning on the manifold of SPD matrices (RSR-SPDML) ; 2). Grassmann learning methods: Discriminative Canonical Correlations (DCC) , Grassmann Discriminant Analysis (GDA) , Grassmannian Graph-Embedding Discriminant Analysis (GGDA) and Projection Metric Learning (PML) ; 3).", "The dataset consists of 2,337 sequences of 130 action classes, and provides 3D locations of 31 joints of the subjects. Following the protocol designed in , we conduct 10 random evaluations, each of which randomly selected half of sequences for training and the rest for testing. For data augmentation, the training sequences are divided into around 18,000 small subsequences in each random evaluation.", "The results in Fig. (a) show the benefit of using M-FRMap. Furthermore, it alternatively studies a new connection weight update rule on PSD manifolds rather than the one on Stiefel manifolds, whose performances (i.e., 32.13\\%, 57.25\\%, 80.15\\%, 71.28\\%) on the datasets are often worse. 2).", "The results show our GrNet with 2 blocks can outperform the existing general manifold learning, Grassmann learning methods and standard ConvNets (i.e., VGGDeepFace and DeepO2P). Particularly, on HDM05, we can observe that our GrNet outperforms the state-of-the-art Grassmann learning methods by a large margin (more than 11\\%). This verifies that the proposed GrNet yields great improvements when the training data is large enough.", "We set the sizes of GrNet-1Block weights to 401 \\times 100, while setting those to 401 \\times 300 and 150 \\times 100 for GrNet-2Blocks. \\noindent{Experimental Analysis Table. presents the performances of the comparing methods for the three used datasets.", "The standard protocol designed by splits the dataset into three data sets, i.e., training, validation and test data sets. In the training and validation data sets, each video is classified into one of seven expressions, while the ground truth of the test set has not been released. As a result, we follow to present the results on the validation set."], "referred_text_chunks": ["\\noindentFace Verification: We employ one standard dataset named Point-and-Shoot Challenge (PaSC) . For 256 subjects, it owns 1,401 videos taken by control cameras, and 1,401 videos from handheld cameras. For the dataset, designs control and handheld face verification experiments."]}}