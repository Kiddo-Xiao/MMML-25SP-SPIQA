{"0": {"question": "Which network architecture has the highest accuracy on the CIFAR-10 dataset?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 0, 'Rationale': 'Image 6 provides direct comparison of classification errors across various architectures on CIFAR-10, showing DMRNet-Wide achieving the lowest error. Text 0 supports this by stating that their networks achieve competitive or superior results compared to state-of-the-arts with specific error rates.', 'Answer': 'DMRNet-Wide with an error rate of 3.57% has the highest accuracy on the CIFAR-10 dataset.'}", "all_figures_names": ["1611.07718v2-Table5-1.png", "1611.07718v2-Figure1-1.png", "1611.07718v2-Figure6-1.png", "1611.07718v2-Figure5-1.png", "1611.07718v2-Table4-1.png", "1611.07718v2-Figure7-1.png", "1611.07718v2-Table3-1.png", "1611.07718v2-Table2-1.png"], "referred_figures_names": ["1611.07718v2-Table3-1.png"], "answer": "DMRNet-Wide", "figure_type": "table", "content_type": "table", "text_chunks": ["The empirical results demonstrate that the performances of our networks are superior to the corresponding ResNets with comparable setup on CIFAR-10, CIFAR-100, SVHN and ImageNet. Our networks achieve competitive results compared to state-of-the-arts (e.g., 3.57\\% testing error on CIFAR-10, 19.00\\% on CIFAR-100, 1.51\\% on SVHN). Related Works There have been rapid and great progress of deep neural networks in various aspects, such as optimization techniques~, initialization schemes~, regularization strategies~, activation and pooling functions~, network architecture~, and applications.", "They are formed with the same number of layers, and each block in a DILNet and a DMRNet corresponds to two residual blocks in a ResNet. Table~ depicts the network structures. The comparison on CIFAR-10 is given in Table~. One can see that compared with ResNets, DILNets and DMRNets consistently perform better, and DMRNets perform the best.", "The two datasets are both subsets~ drawn from the 80-million tiny image database~. CIFAR-10 consists of 60000 32\\times 32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. CIFAR-100 is like CIFAR-10, except that it has 100 classes each containing 600 images.", "Figure~ shows the convergence curves of ResNets and DMRNets over CIFAR-10, CIFAR-100, and SVHN. We show training losses instead of training errors because the training errors in CIFAR-10 almost reach zero at the convergence and are not distinguishable. One can see that the testing errors of DMRNets are smaller than ResNets and that the training losses are also smaller during the optimization process, suggesting that our gains are not from regularization but from richer representation.", "We mark the results that outperform existing state-of-the-arts in bold and the best results in blue. One can see that the DMRNet-Wide of depth 50 outperforms existing state-of-the-art results and achieves the best results on CIFAR-10 and CIFAR-100. Compared with the second best approach DenseNet{\\footnote{ Both DMRNets and DenseNets do not use bottlenecks.", "The averages of the depths of the paths are both (L+2+1), with counting two projection layers in the shortcut connections. We vary L and record the classification errors for each kind of residual network. Figure~ shows the curves in terms of the average depth of all the paths vs. classification error over the example dataset CIFAR-10.", "One exception is that on CIFAR-100 the ResNet of depth 26 (L=12) performs better than DILNet and on SVHN the ResNet of depth 26 performs better than the DILNet and DMRNet. The reason might be that the paths in the DILNet and DMRNet are not very long and that too many short paths lower down the performance and that for networks of such a depth, the benefit from increasing the width is less than the benefit from increasing the depth. \\vspace{.5em \\noindentConvergence curves.", "Experiments We empirically show the superiority of DILNets and DMRNets, compared with ResNets. We demonstrate the effectiveness of our DMRNets on several benchmark datasets and compare it with the state-of-the-arts. Datasets \\noindentCIFAR-10 and CIFAR-100."], "referred_text_chunks": ["The empirical results demonstrate that the performances of our networks are superior to the corresponding ResNets with comparable setup on CIFAR-10, CIFAR-100, SVHN and ImageNet. Our networks achieve competitive results compared to state-of-the-arts (e.g., 3.57\\% testing error on CIFAR-10, 19.00\\% on CIFAR-100, 1.51\\% on SVHN). Related Works There have been rapid and great progress of deep neural networks in various aspects, such as optimization techniques~, initialization schemes~, regularization strategies~, activation and pooling functions~, network architecture~, and applications."]}, "1": {"question": "What is the difference between a deep residual network and a network built by stacking inception-like blocks?", "referred_figures_indices": [4], "response": "{'Image': 2, 'Text': 2, 'Rationale': 'Image 2 visually compares the architectures of residual blocks, inception-like blocks, and merge-and-run blocks, which is directly relevant to the question. Text 2 provides a detailed explanation of the differences in assembling residual branches within these blocks.', 'Answer': 'A deep residual network uses sequential assembly of residual branches with identity mappings, while a network built by stacking inception-like blocks assembles multiple branches in parallel, typically involving concatenations or sums of their outputs.'}", "all_figures_names": ["1611.07718v2-Table5-1.png", "1611.07718v2-Figure4-1.png", "1611.07718v2-Figure1-1.png", "1611.07718v2-Table3-1.png", "1611.07718v2-Figure2-1.png", "1611.07718v2-Table4-1.png", "1611.07718v2-Figure6-1.png", "1611.07718v2-Figure5-1.png"], "referred_figures_names": ["1611.07718v2-Figure2-1.png"], "answer": "Deep residual networks have skip connections that allow the gradient to flow directly from one layer to another, while networks built by stacking inception-like blocks do not.", "figure_type": "Schematic", "content_type": "figure", "text_chunks": ["Obviously, the resulting network contains shorter paths as the parallel assembly of residual branches directly reduces the network depth. We give a straightforward verification: The average length of two residual blocks is 2, while the average lengths of the corresponding inception-like block and merge-and-run block are \\frac{4{3 and \\frac{2{3, respectively. Our networks, built by stacking merge-and-run blocks, are less deep and thus easier to train.", "In comparison to ResNeXts~ that also assemble residual branches in parallel, our approach adopts parallel assembly to directly reduce the depth and does not modify residual branches, while ResNeXts~ transform a residual branch to many small residual branches. Compared to Inception~ and Inception-ResNet blocks~ that are highly customized, our approach requires less efforts to design and more flexible. Deep Merge-and-Run Neural Networks \\newcommand{\\blocka[2]{\\multirow{3{*{\\left[\\begin{array{c\\text{3\\times3, #1\\\\[-.1em] \\text{3\\times3, #1 \\end{array\\right]\\times#2 \\newcommand{\\blockb[3]{\\multirow{3{*{\\left[\\begin{array{c\\text{1\\times1, #2\\\\[-.1em] \\text{3\\times3, #2\\\\[-.1em] \\text{1\\times1, #1\\end{array\\right]\\times#3 \\renewcommand\\arraystretch{1.1 \\setlength{\\tabcolsep{3pt Architectures", "In this paper, we are interested in further reducing the training difficulty and present a simple network architecture, called deep merge and run neural networks, which assemble residual branches more effectively. The key point is a novel building block, the merge-and-run block, which assembles residual branches in parallel with a merge-and-run mapping: Average the inputs of these residual branches (Merge), and add the average to the output of each residual branch as the input of the subsequent residual branch (Run), respectively. Figure~ depicts the architectures by taking two residual branches as an example: (a) two residual blocks that assemble two residual branches sequentially, (b) an inception-like block and (c) a merge and run block both assemble the same two residual branches in parallel.", "Introduction Deep convolutional neural networks, since the breakthrough result in the ImageNet classification challenge~, have been widely studied~. Surprising performances have been achieved in many other computer vision tasks, including object detection~, semantic segmentation~, edge detection~, and so on. Residual networks (ResNets)~ have been attracting a lot of attentions since it won the ImageNet challenge and various extensions have been studied~. The basic unit is a residual block consisting of a residual branch and an identity mapping. Identify mappings introduce short paths from the input to the intermediate layers and from the intermediate layers to the output layers~, and thus reduce the training difficulty.", "Deeply-fused networks~, FractalNet~, and ensemble view~ point out that a ResNet and a GoogleNet~ are a mixture of many dependent networks. Ensemble view~ observes that ResNets behave like an exponential ensemble of relatively shallow networks, and points out that introducing short paths helps ResNets to avoid the vanishing gradient problem, which is similar to the analysis in deeply-fused networks~ and FractalNet~. The architecture of our approach is closely related to Inception~ and Inception-ResNet blocks~, multi-residual networks~, and ResNeXt~, which also contain multiple branches in each block. One notable point is that we introduce merge-and-run mappings, which are linear idempotent functions, to improve information flow for building blocks consisting of parallel residual branches.", "They are formed from the same plain network of depth 2L+2, whose structure is like the one forming ResNets given in Table~: (i) Each residual branch is of length \\frac{2{3L and corresponds to one stage. There are totally 3 residual blocks. (ii) Each residual branch is of length \\frac{1{3L. There are totally 6 residual blocks (like Figure~ (a)).", "The block is wider than an inception-like block because the outputs of two residual branches in the merge-and-run block are separated and the outputs for the inception-like block are aggregated. The two residual branches are not independent as the merge-and-run mapping adds the input of one residual branch to the output of the other residual branch. Figure~ shows that the merge-and-run block is transformed to a two-branch block.", "Numerous studies have been conducted on going deeper, learning very deep networks, even of depth 1000+. Our work can be regarded as a way to going wider and less deep, which is also discussed in~. The manner of increasing the width in our approach is different from Inception~, where the outputs of the branches are concatenated for~width increase and then a convolution/pooling layer for each branch in the subsequent Inception block is conducted but for {width decrease. Our merge-and-run mapping suggests a novel and cheap way of increasing the width."], "referred_text_chunks": ["Obviously, the resulting network contains shorter paths as the parallel assembly of residual branches directly reduces the network depth. We give a straightforward verification: The average length of two residual blocks is 2, while the average lengths of the corresponding inception-like block and merge-and-run block are \\frac{4{3 and \\frac{2{3, respectively. Our networks, built by stacking merge-and-run blocks, are less deep and thus easier to train."]}, "2": {"question": "What is the difference between the residual block and the merge-and-run block?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 2, 'Rationale': \"Image 1 shows visualizations of both residual and merge-and-run blocks, clearly depicting structural differences. Text 2 provides a description of the merge-and-run block, highlighting its novel architecture and how it differs from traditional residual blocks. Together, they offer a comprehensive comparison.\", 'Answer': \"The residual block consists of sequentially assembled residual branches, while the merge-and-run block assembles them in parallel, using an average operation to combine inputs before adding them to the outputs of each branch. This creates shorter paths and reduces network depth, which facilitates easier training.\"}", "all_figures_names": ["1611.07718v2-Figure7-1.png", "1611.07718v2-Figure1-1.png", "1611.07718v2-Figure8-1.png", "1611.07718v2-Figure4-1.png", "1611.07718v2-Figure2-1.png", "1611.07718v2-Figure5-1.png", "1611.07718v2-Table3-1.png", "1611.07718v2-Figure3-1.png"], "referred_figures_names": ["1611.07718v2-Figure1-1.png"], "answer": "The residual block assembles two residual branches sequentially, while the merge-and-run block assembles the same two residual branches in parallel.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["A {merge-and-run block is formed by assembling two residual branches in parallel with a merge-and-run mapping: Average the inputs of two residual branches (Merge), and add the average to the output of each residual branch as the input of the subsequent residual branch (Run), respectively. It is formulated as below, where \\mathbf{x_{2t and \\mathbf{x_{2t+1 (\\mathbf{x_{2(t+1) and \\mathbf{x_{2(t+1)+1) are the inputs (outputs) of two residual branches of the tth block. There is a clear difference from inception-like blocks in Equation~: the inputs of two residual branches are different, and their outputs are also separated.", "The block is wider than an inception-like block because the outputs of two residual branches in the merge-and-run block are separated and the outputs for the inception-like block are aggregated. The two residual branches are not independent as the merge-and-run mapping adds the input of one residual branch to the output of the other residual branch. Figure~ shows that the merge-and-run block is transformed to a two-branch block.", "In this paper, we are interested in further reducing the training difficulty and present a simple network architecture, called deep merge and run neural networks, which assemble residual branches more effectively. The key point is a novel building block, the merge-and-run block, which assembles residual branches in parallel with a merge-and-run mapping: Average the inputs of these residual branches (Merge), and add the average to the output of each residual branch as the input of the subsequent residual branch (Run), respectively. Figure~ depicts the architectures by taking two residual branches as an example: (a) two residual blocks that assemble two residual branches sequentially, (b) an inception-like block and (c) a merge and run block both assemble the same two residual branches in parallel.", "Obviously, the resulting network contains shorter paths as the parallel assembly of residual branches directly reduces the network depth. We give a straightforward verification: The average length of two residual blocks is 2, while the average lengths of the corresponding inception-like block and merge-and-run block are \\frac{4{3 and \\frac{2{3, respectively. Our networks, built by stacking merge-and-run blocks, are less deep and thus easier to train.", "We further show that merge-and-run blocks are wider than residual blocks. Empirical results validate that for very deep networks, as a way to increase the number of layers, increasing the width is more effective than increasing the depth. Besides, we discuss the generalizability of merge-and-run mappings to other linear idempotent transformations, and the extension to more residual branches.", "Discussions \\noindentMerge-and-run mappings for K branches. The merge-and-run mapping studied in this paper is about two residual branches. It can be easily extended to more (K) branches, and accordingly merge-and-run mappings become a linear transformation where the corresponding transformation matrix is of K \\times K blocks, with each block being \\frac{1{K\\mathbf{I. \\vspace{.5em \\noindentIdempotent mappings.", "In comparison to ResNeXts~ that also assemble residual branches in parallel, our approach adopts parallel assembly to directly reduce the depth and does not modify residual branches, while ResNeXts~ transform a residual branch to many small residual branches. Compared to Inception~ and Inception-ResNet blocks~ that are highly customized, our approach requires less efforts to design and more flexible. Deep Merge-and-Run Neural Networks \\newcommand{\\blocka[2]{\\multirow{3{*{\\left[\\begin{array{c\\text{3\\times3, #1\\\\[-.1em] \\text{3\\times3, #1 \\end{array\\right]\\times#2 \\newcommand{\\blockb[3]{\\multirow{3{*{\\left[\\begin{array{c\\text{1\\times1, #2\\\\[-.1em] \\text{3\\times3, #2\\\\[-.1em] \\text{1\\times1, #1\\end{array\\right]\\times#3 \\renewcommand\\arraystretch{1.1 \\setlength{\\tabcolsep{3pt Architectures", "The functions, corresponding to the (2t)th and (2t+1)th residual branches, are as follows, where \\mathbf{x_{2t and \\mathbf{x_{2(t+1) are the input and the output of the tth inception-like block. This structure resembles the building block in the concurrently-developed ResNeXt~, but the purposes are different: Our purpose is to reduce the depth through assembling residual branches in parallel while the purpose of ResNeXt~ is to transform a single residual branches to many small residual branches. \\vspace{.5em \\noindentMerge-and-run."], "referred_text_chunks": ["A {merge-and-run block is formed by assembling two residual branches in parallel with a merge-and-run mapping: Average the inputs of two residual branches (Merge), and add the average to the output of each residual branch as the input of the subsequent residual branch (Run), respectively. It is formulated as below, where \\mathbf{x_{2t and \\mathbf{x_{2t+1 (\\mathbf{x_{2(t+1) and \\mathbf{x_{2(t+1)+1) are the inputs (outputs) of two residual branches of the tth block. There is a clear difference from inception-like blocks in Equation~: the inputs of two residual branches are different, and their outputs are also separated."]}, "3": {"question": "How does the training loss of DMRNet compare to that of ResNet on the CIFAR-10 dataset with L = 30?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 shows the training losses and testing errors of ResNet and DMRNet on CIFAR-10, specifically highlighting the comparison for L=30. Text chunk 0 discusses the convergence curves and mentions that DMRNet has smaller training losses compared to ResNet on the same dataset.', 'Answer': 'The training loss of DMRNet is smaller than that of ResNet on the CIFAR-10 dataset with L = 30.'}", "all_figures_names": ["1611.07718v2-Figure7-1.png", "1611.07718v2-Figure5-1.png", "1611.07718v2-Table5-1.png", "1611.07718v2-Figure6-1.png", "1611.07718v2-Figure8-1.png", "1611.07718v2-Table4-1.png", "1611.07718v2-Table2-1.png", "1611.07718v2-Figure4-1.png"], "referred_figures_names": ["1611.07718v2-Figure7-1.png"], "answer": "The training loss of DMRNet is lower than that of ResNet on the CIFAR-10 dataset with L = 30.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Figure~ shows the convergence curves of ResNets and DMRNets over CIFAR-10, CIFAR-100, and SVHN. We show training losses instead of training errors because the training errors in CIFAR-10 almost reach zero at the convergence and are not distinguishable. One can see that the testing errors of DMRNets are smaller than ResNets and that the training losses are also smaller during the optimization process, suggesting that our gains are not from regularization but from richer representation.", "Experiments We empirically show the superiority of DILNets and DMRNets, compared with ResNets. We demonstrate the effectiveness of our DMRNets on several benchmark datasets and compare it with the state-of-the-arts. Datasets \\noindentCIFAR-10 and CIFAR-100.", "One exception is that on CIFAR-100 the ResNet of depth 26 (L=12) performs better than DILNet and on SVHN the ResNet of depth 26 performs better than the DILNet and DMRNet. The reason might be that the paths in the DILNet and DMRNet are not very long and that too many short paths lower down the performance and that for networks of such a depth, the benefit from increasing the width is less than the benefit from increasing the depth. \\vspace{.5em \\noindentConvergence curves.", "They are formed with the same number of layers, and each block in a DILNet and a DMRNet corresponds to two residual blocks in a ResNet. Table~ depicts the network structures. The comparison on CIFAR-10 is given in Table~. One can see that compared with ResNets, DILNets and DMRNets consistently perform better, and DMRNets perform the best.", "The empirical results demonstrate that the performances of our networks are superior to the corresponding ResNets with comparable setup on CIFAR-10, CIFAR-100, SVHN and ImageNet. Our networks achieve competitive results compared to state-of-the-arts (e.g., 3.57\\% testing error on CIFAR-10, 19.00\\% on CIFAR-100, 1.51\\% on SVHN). Related Works There have been rapid and great progress of deep neural networks in various aspects, such as optimization techniques~, initialization schemes~, regularization strategies~, activation and pooling functions~, network architecture~, and applications.", "We mark the results that outperform existing state-of-the-arts in bold and the best results in blue. One can see that the DMRNet-Wide of depth 50 outperforms existing state-of-the-art results and achieves the best results on CIFAR-10 and CIFAR-100. Compared with the second best approach DenseNet{\\footnote{ Both DMRNets and DenseNets do not use bottlenecks.", "We can see that our approach performs the best in terms of top-5 validation error: our approach gets 1.7 gain, compared with the results of ResNet-101 from our implementation, and 0.3 gain compared with the result from~. The training and validation error curves of ResNet-101 and our DMRNet are given in Figure~. It can be observed that our approach performs better for both training errors and validation errors, which also suggests that the gains are not from regularization but from richer representation. For example, the top-1 validation error of our approach is lower about 5\\% than that of the ResNet from the 30th epoch to the 55th epoch.", "The superiority of DILNets over ResNets stems from the less long paths and greater width. The additional advantages of a DMRNet are much greater width than a DILNet. The comparisons over CIFAR-100 and SVHN shown in Table~ are consistent."], "referred_text_chunks": ["Figure~ shows the convergence curves of ResNets and DMRNets over CIFAR-10, CIFAR-100, and SVHN. We show training losses instead of training errors because the training errors in CIFAR-10 almost reach zero at the convergence and are not distinguishable. One can see that the testing errors of DMRNets are smaller than ResNets and that the training losses are also smaller during the optimization process, suggesting that our gains are not from regularization but from richer representation."]}, "4": {"question": "Which network has the shortest average path length when L = 9?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 1, 'Rationale': 'Image 5 directly shows a comparison of path lengths for ResNet, DILNet, and DMRNet when L = 9. Text 1 explains the link between shorter paths and network architecture, stating that shorter paths are achieved using inception-like and merge-and-run blocks.', 'Answer': 'DMRNet has the shortest average path length when L = 9.'}", "all_figures_names": ["1611.07718v2-Figure7-1.png", "1611.07718v2-Figure6-1.png", "1611.07718v2-Table3-1.png", "1611.07718v2-Figure2-1.png", "1611.07718v2-Figure1-1.png", "1611.07718v2-Figure3-1.png", "1611.07718v2-Table5-1.png", "1611.07718v2-Figure4-1.png"], "referred_figures_names": ["1611.07718v2-Figure3-1.png"], "answer": "DMRNet", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Suppose each residual branch contains B layers (there are 2 layers for the example shown in Figure~), and the ResNet, DIRNet and DMRNet contain 2L, L, and L building blocks, the average lengths (without counting projections in short-cut connections) are BL+2, \\frac{2B{3L+2, and \\frac{B{3L+2, respectively. Figure~ shows the distributions of path lengths of the three networks. Refer to Table~ for the details of the network structures.", "Obviously, the resulting network contains shorter paths as the parallel assembly of residual branches directly reduces the network depth. We give a straightforward verification: The average length of two residual blocks is 2, while the average lengths of the corresponding inception-like block and merge-and-run block are \\frac{4{3 and \\frac{2{3, respectively. Our networks, built by stacking merge-and-run blocks, are less deep and thus easier to train.", "We have the following observations. When the network is not very deep and the average length is small (\\leqslant 15 for 3 blocks, \\leqslant 21 for 6 blocks\\footnote{There are more short paths for 6 blocks, which leads to lower testing error than 3 blocks.) , the testing error becomes smaller as the average length increases, and when the length is large, the testing error becomes larger as the length increases.", "The averages of the depths of the paths are both (L+2+1), with counting two projection layers in the shortcut connections. We vary L and record the classification errors for each kind of residual network. Figure~ shows the curves in terms of the average depth of all the paths vs. classification error over the example dataset CIFAR-10.", "The learning rate starts with 0.1 and is reduced by a factor 10 at the 1/2, 3/4 and 7/8 fractions of the number of training epochs. Similar to~, the weight decay is 0.0001, the momentum is 0.9, and the weights are initialized as in~. Our implementation is based on MXNet~. Empirical Study \\noindentShorter paths. We study how the performance changes as the average length of the paths changes, based on two kinds of residual networks.", "It is shown in~ that for very deep networks the training becomes hard and that a shorter (but still very deep) plain network performs even better than a longer plain network\\footnote{Our empirical results even show that the deepest path in ResNets hurts the training of other paths and thus deteriorates the performance.. According to Figure~ showing that the lengths of the paths in our proposed network are distributed in the range of lower lengths, the proposed deep merge-and-run network potentially performs better. \\vspace{.5em \\noindentInception-like blocks are wider.", "One exception is that on CIFAR-100 the ResNet of depth 26 (L=12) performs better than DILNet and on SVHN the ResNet of depth 26 performs better than the DILNet and DMRNet. The reason might be that the paths in the DILNet and DMRNet are not very long and that too many short paths lower down the performance and that for networks of such a depth, the benefit from increasing the width is less than the benefit from increasing the depth. \\vspace{.5em \\noindentConvergence curves.", "This indicates that shorter paths result in the higher accuracy for very deep networks. \\vspace{.5em \\noindentComparison with ResNets. We compare DILNets and DMRNets, and the baseline ResNets algorithm."], "referred_text_chunks": ["Suppose each residual branch contains B layers (there are 2 layers for the example shown in Figure~), and the ResNet, DIRNet and DMRNet contain 2L, L, and L building blocks, the average lengths (without counting projections in short-cut connections) are BL+2, \\frac{2B{3L+2, and \\frac{B{3L+2, respectively. Figure~ shows the distributions of path lengths of the three networks. Refer to Table~ for the details of the network structures."]}, "5": {"question": "Which model has the lowest Top-1 validation error on ImageNet?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 provides a direct comparison of Top-1 validation errors for ResNet-101 and DMRNet on ImageNet. Text 0 discusses the performance of the models, emphasizing the lower validation error of DMRNet over ResNet.', 'Answer': 'ResNet-101 has the lowest Top-1 validation error on ImageNet with 23.60%.'}", "all_figures_names": ["1611.07718v2-Table3-1.png", "1611.07718v2-Figure3-1.png", "1611.07718v2-Figure7-1.png", "1611.07718v2-Table4-1.png", "1611.07718v2-Figure8-1.png", "1611.07718v2-Figure2-1.png", "1611.07718v2-Figure4-1.png", "1611.07718v2-Figure5-1.png"], "referred_figures_names": ["1611.07718v2-Table4-1.png"], "answer": "ResNet-101 from the reference paper reports top-1 validation error of 23.60 which is lower than ResNet-101 reevaluated (26.41) and DMRNet (23.66)", "figure_type": "table", "content_type": "table", "text_chunks": ["We can see that our approach performs the best in terms of top-5 validation error: our approach gets 1.7 gain, compared with the results of ResNet-101 from our implementation, and 0.3 gain compared with the result from~. The training and validation error curves of ResNet-101 and our DMRNet are given in Figure~. It can be observed that our approach performs better for both training errors and validation errors, which also suggests that the gains are not from regularization but from richer representation. For example, the top-1 validation error of our approach is lower about 5\\% than that of the ResNet from the 30th epoch to the 55th epoch.", "ImageNet Classification We compare our DMRNet against the ResNet on the ImageNet 2012 classification dataset~, which consists of 1000 classes of images. The models are trained on the 1.28 million training images, and evaluated on the 50,000 validation images. \\vspace{0.1cm \\noindentNetwork architecture.", "Figure~ shows the convergence curves of ResNets and DMRNets over CIFAR-10, CIFAR-100, and SVHN. We show training losses instead of training errors because the training errors in CIFAR-10 almost reach zero at the convergence and are not distinguishable. One can see that the testing errors of DMRNets are smaller than ResNets and that the training losses are also smaller during the optimization process, suggesting that our gains are not from regularization but from richer representation.", "The empirical results demonstrate that the performances of our networks are superior to the corresponding ResNets with comparable setup on CIFAR-10, CIFAR-100, SVHN and ImageNet. Our networks achieve competitive results compared to state-of-the-arts (e.g., 3.57\\% testing error on CIFAR-10, 19.00\\% on CIFAR-100, 1.51\\% on SVHN). Related Works There have been rapid and great progress of deep neural networks in various aspects, such as optimization techniques~, initialization schemes~, regularization strategies~, activation and pooling functions~, network architecture~, and applications.", "We mark the results that outperform existing state-of-the-arts in bold and the best results in blue. One can see that the DMRNet-Wide of depth 50 outperforms existing state-of-the-art results and achieves the best results on CIFAR-10 and CIFAR-100. Compared with the second best approach DenseNet{\\footnote{ Both DMRNets and DenseNets do not use bottlenecks.", "We adopt the same data augmentation as in~. We train the models for 100 epochs, and start from a learning rate of 0.1, and then divide it by 10 every 30 epochs which are the same as the learning rate changing in~. We evaluate on the single 224\\times224 center crop from an image whose shorter side is 256. \\vspace{0.1cm \\noindentResults. Table~ shows the results of our approach and our MXNet implementation of ResNet-101, and the results of ResNet-101 from~.", "\\vspace{0.1cm \\noindentOptimization. We follow~ and use SGD to train the two models using the same hyperparameters (weight decay =0.0001, and momentum =0.9) with~. The mini-batch size is 256, and we use 8 GPUs (32 samples per GPU).", "One exception is that on CIFAR-100 the ResNet of depth 26 (L=12) performs better than DILNet and on SVHN the ResNet of depth 26 performs better than the DILNet and DMRNet. The reason might be that the paths in the DILNet and DMRNet are not very long and that too many short paths lower down the performance and that for networks of such a depth, the benefit from increasing the width is less than the benefit from increasing the depth. \\vspace{.5em \\noindentConvergence curves."], "referred_text_chunks": ["We can see that our approach performs the best in terms of top-5 validation error: our approach gets 1.7 gain, compared with the results of ResNet-101 from our implementation, and 0.3 gain compared with the result from~. The training and validation error curves of ResNet-101 and our DMRNet are given in Figure~. It can be observed that our approach performs better for both training errors and validation errors, which also suggests that the gains are not from regularization but from richer representation. For example, the top-1 validation error of our approach is lower about 5\\% than that of the ResNet from the 30th epoch to the 55th epoch."]}, "6": {"question": "Does sharing the first convolutional layer and the last fully connected layer improve the accuracy of the merge-and-run mapping?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 1, 'Rationale': 'Image 0 shows a comparison of accuracy with and without sharing the first convolutional and last fully connected layers, directly addressing the question. Text 1 discusses the effect of sharing these layers, highlighting the interaction benefits.', 'Answer': 'No, sharing the first convolutional and last FC layers decreases accuracy in merge-and-run mapping as shown in lower error rates without sharing in Image 0.'}", "all_figures_names": ["1611.07718v2-Table5-1.png", "1611.07718v2-Figure7-1.png", "1611.07718v2-Figure4-1.png", "1611.07718v2-Figure3-1.png", "1611.07718v2-Table2-1.png", "1611.07718v2-Table3-1.png", "1611.07718v2-Figure8-1.png", "1611.07718v2-Figure6-1.png"], "referred_figures_names": ["1611.07718v2-Table5-1.png"], "answer": "Yes.", "figure_type": "table", "content_type": "table", "text_chunks": ["Consequently, merge-and-run mappings improve both forward and backward information flow. \\vspace{.5em \\noindentShorter paths. All the three networks are mixtures of paths, where a path is defined as a sequence of connected residual branches, identity mappings, and possibly other layers (e.g., the first convolution layer, the FC layer) from the input to the output.", "We conducted experiments using a simple idempotent mapping, \\mathbf{I, for which there is no interaction between the two residual branches and accordingly the resulting network consists of two ResNets that are separate except only sharing the first convolution layer and the last FC layer. We also compare the performances of the two schemes without sharing those two layers. The overall superior results of our approach, from Table~, show that the interactions introduced by merge-and-run mappings are helpful.", "We further show that merge-and-run blocks are wider than residual blocks. Empirical results validate that for very deep networks, as a way to increase the number of layers, increasing the width is more effective than increasing the depth. Besides, we discuss the generalizability of merge-and-run mappings to other linear idempotent transformations, and the extension to more residual branches.", "Numerous studies have been conducted on going deeper, learning very deep networks, even of depth 1000+. Our work can be regarded as a way to going wider and less deep, which is also discussed in~. The manner of increasing the width in our approach is different from Inception~, where the outputs of the branches are concatenated for~width increase and then a convolution/pooling layer for each branch in the subsequent Inception block is conducted but for {width decrease. Our merge-and-run mapping suggests a novel and cheap way of increasing the width.", "In the experiments, we implement our approach by taking two parallel residual branches as an example. For convolutional layers with kernel size 3\\times 3, each side of the inputs is zero-padded by one pixel. At the end of the last merge-and-run block, a global average pooling is performed and then a soft-max classifier is attached.", "Conclusions In this paper, we propose deep merge-and-run neural networks, which improves residual networks by assembling residual branches in parallel with merge-and-run mappings for further reducing the training difficulty. The superior performance stems from several benefits: Information flow is improved, the paths are shorter, and the width is increased. {\\small \\bibliographystyle{ieee \\bibliography{ensemble", "In this paper, we are interested in further reducing the training difficulty and present a simple network architecture, called deep merge and run neural networks, which assemble residual branches more effectively. The key point is a novel building block, the merge-and-run block, which assembles residual branches in parallel with a merge-and-run mapping: Average the inputs of these residual branches (Merge), and add the average to the output of each residual branch as the input of the subsequent residual branch (Run), respectively. Figure~ depicts the architectures by taking two residual branches as an example: (a) two residual blocks that assemble two residual branches sequentially, (b) an inception-like block and (c) a merge and run block both assemble the same two residual branches in parallel.", "Setup \\noindentNetworks. We follow ResNets to design our layers: use three stages (conv1\\_x, conv2\\_x, conv3\\_x) of merge-and-run blocks with the number of filter channels being 16,32,64, respectively, and use a Conv-BN-ReLU as a basic layer with kernel size 3 \\times 3. The image is fed into the first convolutional layer (conv0) with 16 output channels, which then go to the subsequent merge-and-run blocks."], "referred_text_chunks": ["Consequently, merge-and-run mappings improve both forward and backward information flow. \\vspace{.5em \\noindentShorter paths. All the three networks are mixtures of paths, where a path is defined as a sequence of connected residual branches, identity mappings, and possibly other layers (e.g., the first convolution layer, the FC layer) from the input to the output."]}, "7": {"question": "How does the classification error of a residual network change as the average path length increases?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 3, 'Rationale': \"Image 3 shows how classification errors change with different average path lengths, which directly relates to the question. Text 3 discusses the relationship between path length and testing error, indicating that as the average length increases, the testing error tends to increase after a certain point.\", 'Answer': \"The classification error of a residual network initially decreases with increasing average path length but eventually increases as the path length becomes larger.\"}", "all_figures_names": ["1611.07718v2-Figure5-1.png", "1611.07718v2-Figure2-1.png", "1611.07718v2-Figure8-1.png", "1611.07718v2-Figure6-1.png", "1611.07718v2-Table3-1.png", "1611.07718v2-Table5-1.png", "1611.07718v2-Table2-1.png", "1611.07718v2-Figure3-1.png"], "referred_figures_names": ["1611.07718v2-Figure6-1.png"], "answer": "The classification error of a residual network generally increases as the average path length increases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["The averages of the depths of the paths are both (L+2+1), with counting two projection layers in the shortcut connections. We vary L and record the classification errors for each kind of residual network. Figure~ shows the curves in terms of the average depth of all the paths vs. classification error over the example dataset CIFAR-10.", "The learning rate starts with 0.1 and is reduced by a factor 10 at the 1/2, 3/4 and 7/8 fractions of the number of training epochs. Similar to~, the weight decay is 0.0001, the momentum is 0.9, and the weights are initialized as in~. Our implementation is based on MXNet~. Empirical Study \\noindentShorter paths. We study how the performance changes as the average length of the paths changes, based on two kinds of residual networks.", "Obviously, the resulting network contains shorter paths as the parallel assembly of residual branches directly reduces the network depth. We give a straightforward verification: The average length of two residual blocks is 2, while the average lengths of the corresponding inception-like block and merge-and-run block are \\frac{4{3 and \\frac{2{3, respectively. Our networks, built by stacking merge-and-run blocks, are less deep and thus easier to train.", "We have the following observations. When the network is not very deep and the average length is small (\\leqslant 15 for 3 blocks, \\leqslant 21 for 6 blocks\\footnote{There are more short paths for 6 blocks, which leads to lower testing error than 3 blocks.) , the testing error becomes smaller as the average length increases, and when the length is large, the testing error becomes larger as the length increases.", "Suppose each residual branch contains B layers (there are 2 layers for the example shown in Figure~), and the ResNet, DIRNet and DMRNet contain 2L, L, and L building blocks, the average lengths (without counting projections in short-cut connections) are BL+2, \\frac{2B{3L+2, and \\frac{B{3L+2, respectively. Figure~ shows the distributions of path lengths of the three networks. Refer to Table~ for the details of the network structures.", "Introduction Deep convolutional neural networks, since the breakthrough result in the ImageNet classification challenge~, have been widely studied~. Surprising performances have been achieved in many other computer vision tasks, including object detection~, semantic segmentation~, edge detection~, and so on. Residual networks (ResNets)~ have been attracting a lot of attentions since it won the ImageNet challenge and various extensions have been studied~. The basic unit is a residual block consisting of a residual branch and an identity mapping. Identify mappings introduce short paths from the input to the intermediate layers and from the intermediate layers to the output layers~, and thus reduce the training difficulty.", "They are formed from the same plain network of depth 2L+2, whose structure is like the one forming ResNets given in Table~: (i) Each residual branch is of length \\frac{2{3L and corresponds to one stage. There are totally 3 residual blocks. (ii) Each residual branch is of length \\frac{1{3L. There are totally 6 residual blocks (like Figure~ (a)).", "It is shown in~ that for very deep networks the training becomes hard and that a shorter (but still very deep) plain network performs even better than a longer plain network\\footnote{Our empirical results even show that the deepest path in ResNets hurts the training of other paths and thus deteriorates the performance.. According to Figure~ showing that the lengths of the paths in our proposed network are distributed in the range of lower lengths, the proposed deep merge-and-run network potentially performs better. \\vspace{.5em \\noindentInception-like blocks are wider."], "referred_text_chunks": ["The averages of the depths of the paths are both (L+2+1), with counting two projection layers in the shortcut connections. We vary L and record the classification errors for each kind of residual network. Figure~ shows the curves in terms of the average depth of all the paths vs. classification error over the example dataset CIFAR-10."]}}