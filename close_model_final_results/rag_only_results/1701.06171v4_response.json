{"0": {"question": "How many iterations did the greedy EM-type learning process take to learn the part models for the watch image?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 5, 'Rationale': \"Image 0 illustrates the EM-type learning process over iterations, directly showing the iterations on a watch image. Text chunk 5 specifically mentions that a part model converges to a stable solution after 5 iterations in the greedy scheme, providing a concrete number of iterations relevant to the question.\", 'Answer': '5 iterations.'}", "all_figures_names": ["1701.06171v4-Figure4-1.png", "1701.06171v4-Figure1-1.png", "1701.06171v4-Table1-1.png", "1701.06171v4-Table2-1.png", "1701.06171v4-Figure2-1.png", "1701.06171v4-Figure5-1.png", "1701.06171v4-Figure3-1.png"], "referred_figures_names": ["1701.06171v4-Figure4-1.png"], "answer": "22 iterations", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["At each layer of the hierarchy, the parts are learned according to a greedy EM-type learning scheme that infers the structure of each part as well as the number of part models from training images I (Algorithm , line 3). Greedy EM-type learning. The general procedure of the proposed greedy learning scheme is described in (Algorithm , line 6-19), while Figure analogously illustrates a particular learning example.", "The learning proceeds until iteration \\beta^1_2 is converged. This greedy learning scheme repeats until a newly initialized model is not able to explain training patches better than any previously learned model or the background model (Algorithm , line 19). Figure analogously illustrates the first iterations of such a greedy EM-type learning scheme.", "We first explain the algorithmic process and then comment on the visual illustration. The learning process is initialized with two part models (\\beta^1_1, \\beta^1_2) which are learned with matching pursuit from image patches that are randomly sampled from the training data (Algorithm , line 10). Subsequently an EM-type update scheme is performed (Algorithm , line 12-16) as follows: During the EM iterations we only update one part model (\\beta^1_1) while the other model (\\beta^1_2) stays fixed to its initial state and only participates in the detection phase.", "Thereby, we follow the same greedy EM-type learning process. This time, however, instead of composing single basis filter, the algorithm composes the elements of B^1 into higher order parts. We repeat the compositional learning iteratively layer by layer until the normalized model posterior does not increase anymore (Algorithm , line 5), thus generating dictionaries of hierarchical part models at each layer of the hierarchy \\{B^1,\\dots,B^L\\ (Figure , blue box).", "Note how different models have specialized to different parts of the watch. Bottom-up learning. After the parts of the first layer B^1are learned, the structure induction process continues by composing the elements of B^1 into parts of the second layer B^2.", "The Gabor filters and higher layer parts are rotated in 10 degree steps. We found empirically that in the greedy learning scheme, a part model is converged to stable solution after 5 learning iterations. The hierarchical graph structure is defined to compose two parts at each layer of the hierarchy.", "In the next section, we propose a greedy structure learning framework for estimating those parameters from data. \\caption{Bottom-Up Compositional Clustering \\end{algorithm Greedy structure learning In this section, we describe a greedy structure learning algorithm that infers the full dependency structure of the multi-layer CABM from natural images. Figure~ illustrates the two phases of this learning process: A bottom-up compositional clustering process (Figure~ blue box) and a top-down model composition phase (Figure~ green box).", "The learning of CABMs was originally proposed in , however, the number of parts per layer was assumed to be known a-priori and the number of layers was fixed to 2 (see Figure ). In the next section, we present a generalization of the CABM which will enable us to overcome this assumption via a greedy structure learning framework in Section . Proposed approach: Multi-Layer CABM We can generalize the CABM model to an arbitrary numbers of hierarchical layers L: which corresponds to the graphical model shown in Figure~. Based on this multi-layer generalization the model becomes more expressive, and therefore can represent objects with very different geometry structure, such as e.g.\\ long and thin objects as well as small but compact objects."], "referred_text_chunks": ["At each layer of the hierarchy, the parts are learned according to a greedy EM-type learning scheme that infers the structure of each part as well as the number of part models from training images I (Algorithm , line 3). Greedy EM-type learning. The general procedure of the proposed greedy learning scheme is described in (Algorithm , line 6-19), while Figure analogously illustrates a particular learning example."]}, "1": {"question": "What is the relationship between the variables in the Compositional Active Basis Model?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 0, 'Rationale': \"Image 4 visually illustrates the tree-like dependency structure of a Compositional Active Basis Model (CABM), which directly addresses the question about the relationship between the variables. Text 0 explains the same dependency structure in words, highlighting the significance of dynamic programming and hierarchical dependencies, making it the most relevant text.\", 'Answer': \"The relationship between variables in a Compositional Active Basis Model (CABM) is a tree-like hierarchical structure where the parent node influences the child nodes. This setup allows for efficient modeling and inference using dynamic programming.\"}", "all_figures_names": ["1701.06171v4-Figure4-1.png", "1701.06171v4-Figure1-1.png", "1701.06171v4-Table1-1.png", "1701.06171v4-Table2-1.png", "1701.06171v4-Figure2-1.png", "1701.06171v4-Figure5-1.png", "1701.06171v4-Figure3-1.png"], "referred_figures_names": ["1701.06171v4-Figure2-1.png"], "answer": "The variables in the Compositional Active Basis Model are hierarchically dependent. The variables at each layer are dependent on the variables at the layer above it.", "figure_type": "** Schematic", "content_type": "figure", "text_chunks": ["Compositional Active Basis Model Figure graphically illustrates the dependency structure of a two-layered Compositional Active Basis Model (CABM) as proposed by Dai et al. . Note the tree-like dependency structure between the variables which enables a fast posterior inference via dynamic programming. The probabilistic image model of a two-layered CABM is defined as: where the operator ch(\\cdot) selects the set of children nodes.", "The Active Basis Model is a deformable object model that is formulated within an elegant information-theoretic framework and, in addition to shape deformations, also models the object's appearance. In this work, we use the hierarchical compositional generalization of the Active Basis Model as object representation. Hierarchical compositional models.", "We start by introducing the Active Basis Model (ABM, Section ) and its compositional generalization (CABM, Section ). Building on this theoretical background, we introduce the proposed generalized multi-layer CABM in Section . Active Basis Model ABMs are probabilistic generative models that model an object's variability in terms of shape and appearance.", "In this context, we made the following contributions: Multi-layer Compositional Active Basis Models (CABMs). Building on related work, we proposed a generalized probabilistic formulation of CABMs with arbitrary numbers of layers and parts per layer. Our model is more flexible and enables the representation of objects with very different geometry structures.", "The core limitation of ABMs is that they assume statistical independence between individual basis filters (Eq. ). Therefore they are limited in terms of their ability to model large object deformations and strong appearance changes . In the next section, we introduce Compositional Active Basis Models which overcome this limitation by introducing hierarchical relations between the basis filters.", "Compared to the original ABM (Equation ), additional dependencies are introduced between group of the individual basis filters (ch(\\beta^1_j)). In this way, the object's global structure is partitioned into multiple conditionally independent groups of basis filters. This allows for the modeling of long-range correlations in the object's geometry which cannot be achieved with the standard ABM.", "Hierarchical compositional models have developed as a class of models which extend deformable templates into hierarchical graphs that explicitly allow for part sharing, and thus yield big gains in computational efficiency. Furthermore, they have proven to be highly robust under strong changes in the visual domain , while achieving state-of-the-art performance in several computer vision tasks . These approaches hand-specify the graph structures of the models and are restricted to learn the parameters only, whereas, in this work we propose to learn the graph structure from data.", "An ABM represents an image I as a linear combination of basis filters F_{\\beta^0_i: The image I is decomposed into a set of Gabor filters F_{\\beta^0_i with fixed frequency band, coefficients c_i and a residual image U. The variable \\beta^0_i denotes the absolute position and orientation of a basis filter in the image frame. These parameters are encoded relative to the objects center \\beta^1 such that \\beta^0_i = \\Delta\\beta^0_i+\\beta^1."], "referred_text_chunks": ["Compositional Active Basis Model Figure graphically illustrates the dependency structure of a two-layered Compositional Active Basis Model (CABM) as proposed by Dai et al. . Note the tree-like dependency structure between the variables which enables a fast posterior inference via dynamic programming. The probabilistic image model of a two-layered CABM is defined as: where the operator ch(\\cdot) selects the set of children nodes."]}, "2": {"question": "What is the difference between the hierarchical part dictionary learned with the bottom-up process and the holistic object model learned with the top-down process?", "referred_figures_indices": [5], "response": "{'Image': 6, 'Text': 1, 'Rationale': 'Image 6 clearly depicts both the bottom-up and top-down processes, allowing for a visual comparison. Text 1 directly explains the composition of the hierarchical parts into a holistic object model, which is the focus of the question.', 'Answer': 'The hierarchical part dictionary learned with the bottom-up process involves learning parts progressively from lower to higher layers, creating individual parts. In contrast, the top-down process composes these parts into a holistic object model, integrating them into a complete structure.'}", "all_figures_names": ["1701.06171v4-Figure4-1.png", "1701.06171v4-Figure1-1.png", "1701.06171v4-Table1-1.png", "1701.06171v4-Table2-1.png", "1701.06171v4-Figure2-1.png", "1701.06171v4-Figure5-1.png", "1701.06171v4-Figure3-1.png"], "referred_figures_names": ["1701.06171v4-Figure5-1.png"], "answer": "The hierarchical part dictionary learned with the bottom-up process is a set of parts that can be combined to create objects. The holistic object model learned with the top-down process is a single model that represents the entire object.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["In the bottom-up process, the parts of lower layers in the hierarchy are learned first and subsequently composed into higher-order parts. The top-down process composes the (independent) hierarchical parts into a holistic object model. The following paragraphs describe both processes in detail.", "Top-down model building After the bottom-up learning process, the learned part dictionaries \\{B^1,\\dots,B^L\\ must be composed into a holistic object model (Figure , green box). Note that different parts of an object can terminate at different layers of the hierarchy. For example, the hour markings on the dial of the watch in Figure (orange box) are represented at the second layer, whereas, the circular shape of the watch is composed of more elements and is therefore represented at a higher layer.", "We suggest a top-down model building process for learn the dependency structure of the complete object, we introduce a top-down model building process. The training images are first aligned, by detecting the part model of the highest layer B^L in all training images followed by aligning the images such that the models B^L are in a canonical orientation and position. After this alignment step, we proceed in a top-down manner (green box in Figure ), adding parts from the highest layer to the object model with matching pursuit.", "A limitation of our approach and any prior work, including HABM, is that so far it is not possible to learn HCMs of articulated objects (e.g. the windmill in Figure~). Although the individual parts of the windmill are learned by our bottom-up process (Figure ), the top-down process cannot compose the parts into a holistic object model as our deformation model assumes that the relative orientation between parts of an object stays approximately the same. Conclusion \\vspace{-.17cm In this work, we considered the challenging problem of learning a hierarchical generative model of an object from only a set of images which show examples of the object in the presence of variable background clutter.", "Note how different models have specialized to different parts of the watch. Bottom-up learning. After the parts of the first layer B^1are learned, the structure induction process continues by composing the elements of B^1 into parts of the second layer B^2.", "Bottom-up compositional clustering We propose to formulate the structure learning task as a compositional clustering process that is described in Algorithm . The dependence structure of our HCM is a tree-like Markov random field (Figure \\& Equation ). This enables us to learn the model in a bottom-up manner, i.e. we can learn the first-layer parts first, before proceeding to compose them into larger parts (Algorithm , lines 1-5).", "Learning the full hierarchical structure enables the reuse of parts within the hierarchy (e.g. the wheels of the bike in Figure ), which is not possible in the HABM approach (Figure ). Therefore, our HCMs have semantically more meaningful parts that provide additional information about the internal semantics of the object. Furthermore, our learning process is more data efficient as the part models can leverage the redundancy within objects (e.g. if the same part occurs multiple times within the same object).", "We iteratively proceed layer-wise until the bottom layer of the hierarchy. At this point, we have learned a hierarchical compositional object model from natural images (orange box in Figure ). Note that the number of layers L , the number of parts per layer N_L,\\dots,N_0 and the hierarchical dependency structure have been learned from natural images without restricting the object's geometry or requiring segmented training data."], "referred_text_chunks": ["In the bottom-up process, the parts of lower layers in the hierarchy are learned first and subsequently composed into higher-order parts. The top-down process composes the (independent) hierarchical parts into a holistic object model. The following paragraphs describe both processes in detail."]}, "3": {"question": "What is the difference between the top-down and bottom-up compositional learning schemes?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 2, 'Rationale': \"Image 6 illustrates both the bottom-up and top-down compositional learning schemes clearly, showing their joint function in model building. Text chunk 2 describes how the hierarchical parts are composed into a holistic object model during the top-down process, which complements the bottom-up details. This combination explains the integration of both schemes into a holistic model.\", 'Answer': 'The bottom-up compositional learning scheme involves learning lower layers first and composing them into higher layers. In contrast, the top-down scheme starts with a holistic object model and decomposes it into hierarchical parts, integrating these parts into a complete structure. Both processes contribute to forming a comprehensive object model.'}", "all_figures_names": ["1701.06171v4-Figure4-1.png", "1701.06171v4-Figure1-1.png", "1701.06171v4-Table1-1.png", "1701.06171v4-Table2-1.png", "1701.06171v4-Figure2-1.png", "1701.06171v4-Figure5-1.png", "1701.06171v4-Figure3-1.png"], "referred_figures_names": ["1701.06171v4-Figure3-1.png"], "answer": "The top-down compositional learning scheme starts with a holistic object model and decomposes it into smaller parts, while the bottom-up compositional learning scheme starts with basic parts and composes them into a holistic object model.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["In the bottom-up process, the parts of lower layers in the hierarchy are learned first and subsequently composed into higher-order parts. The top-down process composes the (independent) hierarchical parts into a holistic object model. The following paragraphs describe both processes in detail.", "Bottom-up compositional clustering We propose to formulate the structure learning task as a compositional clustering process that is described in Algorithm . The dependence structure of our HCM is a tree-like Markov random field (Figure \\& Equation ). This enables us to learn the model in a bottom-up manner, i.e. we can learn the first-layer parts first, before proceeding to compose them into larger parts (Algorithm , lines 1-5).", "Top-down model building After the bottom-up learning process, the learned part dictionaries \\{B^1,\\dots,B^L\\ must be composed into a holistic object model (Figure , green box). Note that different parts of an object can terminate at different layers of the hierarchy. For example, the hour markings on the dial of the watch in Figure (orange box) are represented at the second layer, whereas, the circular shape of the watch is composed of more elements and is therefore represented at a higher layer.", "We suggest a top-down model building process for learn the dependency structure of the complete object, we introduce a top-down model building process. The training images are first aligned, by detecting the part model of the highest layer B^L in all training images followed by aligning the images such that the models B^L are in a canonical orientation and position. After this alignment step, we proceed in a top-down manner (green box in Figure ), adding parts from the highest layer to the object model with matching pursuit.", "In the next section, we propose a greedy structure learning framework for estimating those parameters from data. \\caption{Bottom-Up Compositional Clustering \\end{algorithm Greedy structure learning In this section, we describe a greedy structure learning algorithm that infers the full dependency structure of the multi-layer CABM from natural images. Figure~ illustrates the two phases of this learning process: A bottom-up compositional clustering process (Figure~ blue box) and a top-down model composition phase (Figure~ green box).", "Note how different models have specialized to different parts of the watch. Bottom-up learning. After the parts of the first layer B^1are learned, the structure induction process continues by composing the elements of B^1 into parts of the second layer B^2.", "Thereby, we follow the same greedy EM-type learning process. This time, however, instead of composing single basis filter, the algorithm composes the elements of B^1 into higher order parts. We repeat the compositional learning iteratively layer by layer until the normalized model posterior does not increase anymore (Algorithm , line 5), thus generating dictionaries of hierarchical part models at each layer of the hierarchy \\{B^1,\\dots,B^L\\ (Figure , blue box).", "Learning the structure of hierarchical compositional models. It is desirable to learn the structure of HCMs from data. A number of works showed that exploiting the modularity of compositional models makes possible to learn HCMs one parent child clique at a time in a bottom-up manner by clustering contours."], "referred_text_chunks": ["In the bottom-up process, the parts of lower layers in the hierarchy are learned first and subsequently composed into higher-order parts. The top-down process composes the (independent) hierarchical parts into a holistic object model. The following paragraphs describe both processes in detail."]}}