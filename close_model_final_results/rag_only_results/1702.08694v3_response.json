{"0": {"question": "Which method, C-Tarone or Binarization, achieves higher precision when the number of features is small and the number of data points is large?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 4, 'Rationale': 'Image 3 shows precision comparisons between C-Tarone and binarization for small features and large data points, but Image 4 provides a clearer overall comparison on real datasets. Text 4 discusses C-Tarone achieving higher precision in a clear context.', 'Answer': 'C-Tarone achieves higher precision when the number of features is small and the number of data points is large.'}", "all_figures_names": ["1702.08694v3-Figure1-1.png", "1702.08694v3-Figure2-1.png", "1702.08694v3-Table1-1.png", "1702.08694v3-Figure5-1.png", "1702.08694v3-Figure3-1.png", "1702.08694v3-Figure4-1.png", "1702.08694v3-Table2-1.png"], "referred_figures_names": ["1702.08694v3-Figure5-1.png"], "answer": "C-Tarone.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["If they are not balanced in the original dataset, we randomly subsample data points from the larger class. We summarize results in Figure~. Again, C-Tarone shows higher precision in all datasets than the binarization method and better or competitive recall, resulting in higher F-measure scores in all datasets. In addition, running time is competitive with the binarization method, which means that C-Tarone can successfully prune the massive candidate space for significant feature combinations.", "This is why binarization method cannot distinguish correlated and uncorrelated combinations as we discussed in Introduction and illustrated in Figure~, resulting in including uncorrelated features into significant combinations. Although recall is competitive across various N and d, in all cases, the F-measure of C-Tarone are higher than those of the median-based binarization method. In both methods, precision drops when the sample size N becomes large: As we gain more and more statistical power for larger N, many feature combinations, even those with very small dependence to the class labels and not used in the decision tree, tend to reach statistical significance.", "Although the algorithm LCM (itemset mining) used in the binarization approach is highly optimized with respect to the efficiency, C-Tarone is competitive with it on all datasets, as can be seen in Figure~ (bottom row). To summarize, we observe that C-Tarone improves over the competing binarization approach in terms of the F-measure in detecting higher quality feature combinations in classification. Results on Real Data.", "There exists no existing method that can enumerate significant feature combinations from continuous data with multiple testing correction. Thus we compare C-Tarone to significant itemset mining method with prior binarization of a given dataset since significant itemset mining offers to enumerate all significant feature combinations while controlling the FWER from binary data. We employ median-based binarization as a preprocessing.", "See Figure~ in Appendix for r_1 = 0.2 (classes are imbalanced). In the figure, we plot results with varying N while fixing d = 20 on the left column and those with varying d while fixing N = 1,000 on the right column. In comparison with the median-based binarization method (plotted in blue), C-Tarone (plotted in red) has a clear advantage regarding the precision, which is several orders of magnitude higher than the binarization method in every case.", "These results demonstrate the effectiveness of C-Tarone. Conclusion In this paper, we have proposed a solution to the open problem of finding all multiplicative feature interactions between continuous features that are significantly associated with an output variable after rigorously controlling for multiple testing. While interaction detection with multiple testing has been studied before, our approach, called C-Tarone, is the first to overcome the problem of detecting all higher-order interactions from the enormous search space 2^d for d features.", "In each path, we use the collection of features used in the path as a positive feature combination of the ground truth, that is, a feature combination found by C-Tarone (or binarization method) is deemed to be true positive if and only if it constitutes one of full paths from the root to a leaf of the learned decision tree. Note that the FWER is always controlled under \\alpha in both of C-Tarone and binarization method, and our aim is to empirically examine the quality of the feature combinations compared to those selected by a standard decision tree. We used not forests but a single tree as the ground truth depends on the number of trees if we use forests, resulting in arbitrary results.", "Our goal in this paper is to present the first method, called C-Tarone, that can find all higher-order interactions between continuous features that are statistically significantly associated with the class variable, while controlling the FWER. Our approach is to use the rank order statistics to directly estimate the probability of joint occurrence of each feature combination from continuous data, which is known as copula support~, and apply a likelihood ratio test to assess the significance of association between feature interactions and the class label, which solves the problem~. We present the tight lower bound on p-values of association, which enables us to prune unnecessary interactions that can never be significant through the notion of testability proposed by~\\citet{Tarone90, and can solve both problems~ and~. This paper is organized as follows: We introduce our method C-Tarone in Section~. We introduce a likelihood ratio test as a statistical association test for interactions in Section~, analyze multiple testing correction using the testability in Section~, and present an algorithm in Section~."], "referred_text_chunks": ["If they are not balanced in the original dataset, we randomly subsample data points from the larger class. We summarize results in Figure~. Again, C-Tarone shows higher precision in all datasets than the binarization method and better or competitive recall, resulting in higher F-measure scores in all datasets. In addition, running time is competitive with the binarization method, which means that C-Tarone can successfully prune the massive candidate space for significant feature combinations."]}, "1": {"question": "How does the C-Tarone method compare to the binarization method in terms of precision, recall, F-measure, and running time?", "referred_figures_indices": [4], "response": "{'Image': 1, 'Text': 0, 'Rationale': \"Image 1 clearly compares precision, recall, F-measure, and running time for C-Tarone and binarization methods. Text 0 explains C-Tarone's superior precision and F-measure across datasets, consistent with the graph trends.\", 'Answer': \"C-Tarone shows higher precision and F-measure compared to binarization across datasets. It also has competitive recall and running time, maintaining efficiency while achieving better accuracy.\"}", "all_figures_names": ["1702.08694v3-Figure1-1.png", "1702.08694v3-Figure2-1.png", "1702.08694v3-Table1-1.png", "1702.08694v3-Figure5-1.png", "1702.08694v3-Figure3-1.png", "1702.08694v3-Figure4-1.png", "1702.08694v3-Table2-1.png"], "referred_figures_names": ["1702.08694v3-Figure3-1.png"], "answer": "The C-Tarone method has higher precision and F-measure than the binarization method in all datasets. The C-Tarone method has better or competitive recall than the binarization method. The running time of the C-Tarone method is competitive with the binarization method.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["If they are not balanced in the original dataset, we randomly subsample data points from the larger class. We summarize results in Figure~. Again, C-Tarone shows higher precision in all datasets than the binarization method and better or competitive recall, resulting in higher F-measure scores in all datasets. In addition, running time is competitive with the binarization method, which means that C-Tarone can successfully prune the massive candidate space for significant feature combinations.", "This is why binarization method cannot distinguish correlated and uncorrelated combinations as we discussed in Introduction and illustrated in Figure~, resulting in including uncorrelated features into significant combinations. Although recall is competitive across various N and d, in all cases, the F-measure of C-Tarone are higher than those of the median-based binarization method. In both methods, precision drops when the sample size N becomes large: As we gain more and more statistical power for larger N, many feature combinations, even those with very small dependence to the class labels and not used in the decision tree, tend to reach statistical significance.", "Although the algorithm LCM (itemset mining) used in the binarization approach is highly optimized with respect to the efficiency, C-Tarone is competitive with it on all datasets, as can be seen in Figure~ (bottom row). To summarize, we observe that C-Tarone improves over the competing binarization approach in terms of the F-measure in detecting higher quality feature combinations in classification. Results on Real Data.", "There exists no existing method that can enumerate significant feature combinations from continuous data with multiple testing correction. Thus we compare C-Tarone to significant itemset mining method with prior binarization of a given dataset since significant itemset mining offers to enumerate all significant feature combinations while controlling the FWER from binary data. We employ median-based binarization as a preprocessing.", "See Figure~ in Appendix for r_1 = 0.2 (classes are imbalanced). In the figure, we plot results with varying N while fixing d = 20 on the left column and those with varying d while fixing N = 1,000 on the right column. In comparison with the median-based binarization method (plotted in blue), C-Tarone (plotted in red) has a clear advantage regarding the precision, which is several orders of magnitude higher than the binarization method in every case.", "In each path, we use the collection of features used in the path as a positive feature combination of the ground truth, that is, a feature combination found by C-Tarone (or binarization method) is deemed to be true positive if and only if it constitutes one of full paths from the root to a leaf of the learned decision tree. Note that the FWER is always controlled under \\alpha in both of C-Tarone and binarization method, and our aim is to empirically examine the quality of the feature combinations compared to those selected by a standard decision tree. We used not forests but a single tree as the ground truth depends on the number of trees if we use forests, resulting in arbitrary results.", "Our goal in this paper is to present the first method, called C-Tarone, that can find all higher-order interactions between continuous features that are statistically significantly associated with the class variable, while controlling the FWER. Our approach is to use the rank order statistics to directly estimate the probability of joint occurrence of each feature combination from continuous data, which is known as copula support~, and apply a likelihood ratio test to assess the significance of association between feature interactions and the class label, which solves the problem~. We present the tight lower bound on p-values of association, which enables us to prune unnecessary interactions that can never be significant through the notion of testability proposed by~\\citet{Tarone90, and can solve both problems~ and~. This paper is organized as follows: We introduce our method C-Tarone in Section~. We introduce a likelihood ratio test as a statistical association test for interactions in Section~, analyze multiple testing correction using the testability in Section~, and present an algorithm in Section~.", "Moreover, C-Tarone is independent of the feature ordering and the above completeness with respect to the set of significant combinations is always satisfied. The time complexity of C-Tarone is O(|\\Tc|). Experiments We examine the effectiveness and the efficiency of C-Tarone using synthetic and real-world datasets."], "referred_text_chunks": ["If they are not balanced in the original dataset, we randomly subsample data points from the larger class. We summarize results in Figure~. Again, C-Tarone shows higher precision in all datasets than the binarization method and better or competitive recall, resulting in higher F-measure scores in all datasets. In addition, running time is competitive with the binarization method, which means that C-Tarone can successfully prune the massive candidate space for significant feature combinations."]}, "2": {"question": "For a fixed value of $b$, how does the maximum achievable KL divergence and the corresponding minimum achievable p-value change with increasing values of $a$?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 4, 'Rationale': 'Image 5 directly shows how the maximum achievable KL divergence and the corresponding p-value vary with different values of a, which is exactly what the question asks. Text 4 discusses the behavior of the bound B(a, b) as a function of a for a fixed b, aligning with the question\\'s focus.', 'Answer': 'As the value of a increases, the maximum achievable KL divergence initially increases, then decreases, showing a non-linear pattern with peaks. The corresponding minimum achievable p-value decreases, then increases, also showing a non-linear pattern with valleys.'}", "all_figures_names": ["1702.08694v3-Figure1-1.png", "1702.08694v3-Figure2-1.png", "1702.08694v3-Table1-1.png", "1702.08694v3-Figure5-1.png", "1702.08694v3-Figure3-1.png", "1702.08694v3-Figure4-1.png", "1702.08694v3-Table2-1.png"], "referred_figures_names": ["1702.08694v3-Figure4-1.png"], "answer": "The maximum achievable KL divergence initially increases with increasing values of $a$ until it reaches a peak. Then, it decreases with increasing values of $a$. The minimum achievable p-value initially decreases with increasing values of $a$ until it reaches a minimum. Then, it increases with increasing values of $a$.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["In the following, we prove that we can analytically obtain the tight upper bound of the KL divergence, which immediately leads to the tight lower bound of the p-value. for all \\vec{p \\in \\vec{\\Pc(a, b) and this is tight (see Appendix for its proof). \\end{theorem Proof.\\hspace*{.5em", "When b < a < 1/2, we have \\partial B(a, b) / \\partial a = \\log(a - b) / a < 0, thereby it monotonically decreases as a increases. We illustrate the bound B(a, b) with b = 0.3 and the corresponding minimum achievable p-value with the sample size N = 100 in Figure~ in Appendix.", "Let \\psi(J) be the lower bound of the p-value of J obtained from the upper bound of the KL-divergence proved in Theorem~. Suppose that J_1, J_2, \\dots, J_{2^d be the sorted sequence of all combinations such that is satisfied. Let m be the threshold such that \\citet{Tarone90 showed that the FWER is controlled under \\alpha with the correction factor m. The set \\Tc = \\{J_1, J_2, \\dots, J_m\\ is called testable combinations, and each J \\in \\Tc is significant if \\text{p-value(J) < \\delta_{\\Tar = \\alpha / m.", "Suppose that \\Pr(Y = l) = r_l for each class l \\in \\{0, 1\\. From two binary variables X_J and Y, we obtain a 2 \\times 2 contingency table, where each cell denotes the joint probability \\Pr(X_J = l, Y = l') with l, l' \\in \\{0, 1\\ and can be described as a four-dimensional probability vector \\vec{p: Let \\vec{p_{\\mathrm{E be the probability vector under the null hypothesis X_J \\ip Y and \\vec{p_{\\mathrm{O be the empirical vector obtained from N observations. The difference between two distributions \\vec{p_{\\mathrm{O and \\vec{p_{\\mathrm{E can be measured by the Kullback--Leibler (KL) divergence \\KL(\\vec{p_{\\mathrm{O, \\vec{p_{\\mathrm{E) = \\sum_i p_{\\mathrm{Oi \\log (p_{\\mathrm{Oi / p_{\\mathrm{Ei), and the independence X_J \\ip Y is translated into the condition \\KL(\\vec{p_{\\mathrm{O, \\vec{p_{\\mathrm{E) = 0. In the G-test, which is a special case of likelihood ratio test, the test statistic is given as \\lambda = 2N\\KL(\\vec{p_{\\mathrm{O, \\vec{p_{\\mathrm{E), which follows the \\chi^2-distribution with the degree of freedom 1.", "Let us denote by B(a, b) the upper bound provided in Equation~\\eqref{eq:KLmax. We analyze the behavior of the bound B(a, b) as a function of a with fixed b. This is a typical situation in our setting as a corresponds to the copula support \\eta(J), which varied across combinations, while b corresponds to the class ratio r_1, which is fixed in each analysis.", "Let f(x) = \\KL(\\vec{p, \\vec{p_{\\mathrm{E) with \\vec{p = (x, a - x, b - x, (1 - b) - (a - x)) and a' = 1 - a, b' = 1 - b. We have The second derivative of f(x) is given as which is always positive from the constraint 0 <", "The partial derivative of \\delta with respect to a is \\partial \\delta / \\partial a = \\log(1 - a - b) - \\log(b - a), which is always positive as (1 - a - b) - (b - a) = 1 - 2b \\ge 0 with the condition b \\le 1/2. Hence the difference \\delta takes the minimum value at a = 0 and we obtain Thus f(x) is the tight upper bound when x \\to a.\\hfill\\square Here we formally introduce how to prune unnecessary hypotheses by Tarone's testability.", "Since the lower bound of the p-value \\psi(J) takes the minimum value when \\eta(J) = r_1 (see Figure~ in Appendix by letting a = \\eta(J) and b = r_1) and is monotonically decreasing as \\eta(J) decreases, for any \\eta(J) \\le r_1, \\Cc \\supseteq \\Tc with \\Cc = \\{J \\in 2^V \\mid \\eta(J) \\ge \\sigma\\ and \\Tc = \\{J \\in 2^V \\mid \\psi(J) \\le B(\\sigma, r_1)\\ is always guaranteed, where \\sigma is a threshold for copula supports."], "referred_text_chunks": ["In the following, we prove that we can analytically obtain the tight upper bound of the KL divergence, which immediately leads to the tight lower bound of the p-value. for all \\vec{p \\in \\vec{\\Pc(a, b) and this is tight (see Appendix for its proof). \\end{theorem Proof.\\hspace*{.5em"]}, "3": {"question": "Which dataset would likely require the most computational resources for C-Tarone to analyze?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 1, 'Rationale': 'Image 6 provides the number of candidate combinations for each dataset, directly indicating computational complexity. Text 1 gives context about the datasets used for evaluation.', 'Answer': 'Faults dataset, due to 134,217,728 candidate combinations.'}", "all_figures_names": ["1702.08694v3-Figure1-1.png", "1702.08694v3-Figure2-1.png", "1702.08694v3-Table1-1.png", "1702.08694v3-Figure5-1.png", "1702.08694v3-Figure3-1.png", "1702.08694v3-Figure4-1.png", "1702.08694v3-Table2-1.png"], "referred_figures_names": ["1702.08694v3-Table2-1.png"], "answer": "The \"wdbc\" dataset would likely require the most computational resources for C-Tarone to analyze.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["If they are not balanced in the original dataset, we randomly subsample data points from the larger class. We summarize results in Figure~. Again, C-Tarone shows higher precision in all datasets than the binarization method and better or competitive recall, resulting in higher F-measure scores in all datasets. In addition, running time is competitive with the binarization method, which means that C-Tarone can successfully prune the massive candidate space for significant feature combinations.", "We also evaluate C-Tarone on real-world datasets shown in Table~ in Appendix, which are benchmark datasets for binary classification from the UCI repository~. To clarify the exponentially large search space, we also show the number 2^d of candidate combinations for d features in the table. All datasets are balanced to maximize the statistical power for comparing detected significant combinations, i.e.\\ r_1 = 0.5.", "We experimentally validate our method in Section~ and summarize our findings in Section~. The Proposed Method: C-Tarone Given a supervised dataset D = \\{\\,(\\vec{v_1, y_1), (\\vec{v_2, y_2), \\dots, (\\vec{v_N, y_N)\\,\\, where each data point is a pair of an d-dimensional vector \\vec{v_i = (v_{i^1, v_{i^2, \\dots, v_{i^d) \\in \\R^d and a binary class label y_i \\in \\{0, 1\\. We denote the set of features by V = \\{1, 2, \\dots, d\\ and the power set of features by 2^V. For each feature j \\in \\{1, 2, \\dots, d\\, we write \\vec{v^j = (v_1^j, v_2^j, \\dots, v_N^j), which is the N-dimensional vector composed of the jth feature of the dataset D. Our goal is to find every multiplicative feature interaction that is associated with class labels. To tackle the problem, first we measure the joint occurrence probability \\eta(J) \\in \\R of a feature combination J \\in 2^V in a dataset.", "There exists no existing method that can enumerate significant feature combinations from continuous data with multiple testing correction. Thus we compare C-Tarone to significant itemset mining method with prior binarization of a given dataset since significant itemset mining offers to enumerate all significant feature combinations while controlling the FWER from binary data. We employ median-based binarization as a preprocessing.", "Although the algorithm LCM (itemset mining) used in the binarization approach is highly optimized with respect to the efficiency, C-Tarone is competitive with it on all datasets, as can be seen in Figure~ (bottom row). To summarize, we observe that C-Tarone improves over the competing binarization approach in terms of the F-measure in detecting higher quality feature combinations in classification. Results on Real Data.", "Results on Synthetic Data. First we evaluate C-Tarone on synthetic data with varying the sample size N from 1,000 to 200,000, the number d of features from 20 to 100, and setting the class ratio to r_1 = 0.5 or r_1 = 0.2, i.e., the number N_1 of samples in the minor class is N/2 or N/5. In each dataset, we generate 20\\% of features that are associated with the class labels.", "Our goal in this paper is to present the first method, called C-Tarone, that can find all higher-order interactions between continuous features that are statistically significantly associated with the class variable, while controlling the FWER. Our approach is to use the rank order statistics to directly estimate the probability of joint occurrence of each feature combination from continuous data, which is known as copula support~, and apply a likelihood ratio test to assess the significance of association between feature interactions and the class label, which solves the problem~. We present the tight lower bound on p-values of association, which enables us to prune unnecessary interactions that can never be significant through the notion of testability proposed by~\\citet{Tarone90, and can solve both problems~ and~. This paper is organized as follows: We introduce our method C-Tarone in Section~. We introduce a likelihood ratio test as a statistical association test for interactions in Section~, analyze multiple testing correction using the testability in Section~, and present an algorithm in Section~.", "Algorithm We present the algorithm of C-Tarone that efficiently finds testable combinations J_1, J_2, \\dots, J_m such that \\psi(J_1) \\le \\psi(J_2) \\le \\dots \\le \\psi(J_m), where m satisfies the condition~\\eqref{eq:testability. We summarize C-Tarone in Algorithm~, which performs depth-first search to find J_1, J_2, \\dots, J_m such that \\psi(J_1) \\le \\psi(J_2) \\le \\dots \\le \\psi(J_m), where m satisfies the condition~\\eqref{eq:testability. Suppose that r_1 \\le 1/2 \\le r_0."], "referred_text_chunks": ["If they are not balanced in the original dataset, we randomly subsample data points from the larger class. We summarize results in Figure~. Again, C-Tarone shows higher precision in all datasets than the binarization method and better or competitive recall, resulting in higher F-measure scores in all datasets. In addition, running time is competitive with the binarization method, which means that C-Tarone can successfully prune the massive candidate space for significant feature combinations."]}}