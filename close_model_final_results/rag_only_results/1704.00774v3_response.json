{"0": {"question": "What is the effect of increasing K on the test PPL of the different models?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 visually demonstrates how increasing K affects the test PPL across various models. Text chunk 0 supports this by describing the relationship between model capacity and test set perplexity as K increases.', 'Answer': 'Increasing K generally decreases test PPL for the models, indicating improved performance without additional computational costs. The r-RNTN with f mapping closely approximates the larger RNTN, suggesting dedicated matrices for frequent words are beneficial.'}", "all_figures_names": ["1704.00774v3-Figure1-1.png", "1704.00774v3-Table1-1.png"], "referred_figures_names": ["1704.00774v3-Figure1-1.png"], "answer": "The test PPL of all the models decreases as K increases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Results \\eupdt{Results \\eupdt{are shown in \\cref{fig:keffectlarge and \\cref{tab:keffect. Comparing the r-RNTN to the baseline s-RNN with H = 100 \\eupdt{(\\cref{fig:keffectlarge), % as model capacity grows with K, test set perplexity drops, showing that r-RNTN is an effective way to increase model capacity with no additional computational cost. As expected, the f mapping outperforms \\eupdt{the baseline f_{mod mapping at smaller K. As K increases, we see a convergence of both mappings.", "This reinforces our hypothesis that complex transformation modeling afforded by distinct matrices is needed for frequent words, but not so much for infrequent words which can be well represented by a shared matrix and a distinct vector embedding. As shown in \\cref{tab:keffect, with an equal number of parameters, the r-RNTN with f mapping outperforms the s-RNN with a bigger hidden layer. It appears that heuristically allocating increased model capacity as done by the f based r-RNTN is a better way to increase performance than simply increasing hidden layer size, which also incurs a computational penalty.", "r-RNTNs were motivated by the need to increase RNN model capacity without increasing computational costs, while also satisfying the % ideas \\eupdt{that some words are better modeled by matrices rather than vectors . We achieved both goals by pruning the size of the recurrent neural tensor network described by \\citet{Sutskever2011 via sensible word-to-matrix mapping. Results validated our hypothesis that frequent words \\eupdt{benefit from richer, dedicated modeling \\eupdt{as reflected in large perplexity improvements for low values of K. Interestingly, results for s-RNNs and r-GRUs suggest that given the same number of parameters, it is possible to obtain higher performance by increasing K and reducing H.", "Results show that r-RNTNs improve language model performance over s-RNNs even for small K with no computational overhead, and even for small K approximate the performance of RNTNs using a fraction of the parameters. \\eupdt{We also experiment with r-RNTNs using Gated Recurrent Units (GRU) \\citep{Cho2014LearningPR and Long Short-Term Memory (LSTM) \\citep{Hochreiter1997LongSM, obtaining lower perplexity for fixed hidden layer sizes. \\eupdt{This paper discusses related work (\\S), and presents r-RNTNs (\\S) along with the evaluation method (\\S).", "Given that |V| words must be assigned K matrices, we map the most frequent K-1 words to the first K-1 matrices, and share the K-th matrix among the remaining words. \\eupdt{This mapping is driven by the statistical intuition that frequent words are more likely to appear in diverse contexts and so require richer modeling, \\eupdt{and by the greater presence of predicates and function words among the most frequent words in standard \\eupdt{corpora like COCA . \\eupdt{As a result, adding K matrices to the s-RNN both increases model capacity and \\eupdt{satisfies the idea that \\eupdt{some words are better represented by matrices.", "This \\eupdt{may be due to matrix sharing at large K between frequent and % infrequent words because of the modulus operation in \\cref{eq:fmod. As infrequent words are rarely observed, frequent words dominate the matrix updates and approximate having distinct matrices, as they would have with the f mapping. It is remarkable that even with K \\eupdt{as small as 100, the r-RNTN approaches the performance of the RNTN with a small fraction of the parameters.", "\\eupdt{Finally, we train GRUs, LSTMs, and their r-RNTN variants using the PTB corpus and parameters similar to those used by \\citet{Zaremba2014. All networks use embeddings of size 650 and a single hidden layer. Targeting K = 100, we set H = 244 for the r-GRU and compare \\eupdt{with a GRU with H", "\\citet{Sutskever2011 increased the performance of % \\eupdt{a character-level language model \\eupdt{with a multiplicative RNN (m-RNN), the factored approximation of a recurrent neural tensor network (RNTN), which maps each symbol to separate hidden layer weights (referred to as recurrence matrices from hereon). Besides increasing model capacity while keeping computation constant, this approach has another motivation: viewing the RNN's hidden state as being transformed by each new symbol in the sequence, it is intuitive that different symbols will transform \\eupdt{the network's hidden state in different ways . \\eupdt{Various studies on \\eupdt{compositionality similarly argue that some words are better modeled by matrices than by vectors \\citep{Baroni2010,Socher2012."], "referred_text_chunks": ["Results \\eupdt{Results \\eupdt{are shown in \\cref{fig:keffectlarge and \\cref{tab:keffect. Comparing the r-RNTN to the baseline s-RNN with H = 100 \\eupdt{(\\cref{fig:keffectlarge), % as model capacity grows with K, test set perplexity drops, showing that r-RNTN is an effective way to increase model capacity with no additional computational cost. As expected, the f mapping outperforms \\eupdt{the baseline f_{mod mapping at smaller K. As K increases, we see a convergence of both mappings."]}}