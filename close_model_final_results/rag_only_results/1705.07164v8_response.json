{"0": {"question": "What can you infer from the training curves for the ACGAN?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 3, 'Rationale': \"Image 0 contains the training curves for ACGAN, showing generator and discriminator losses, which directly relate to the question. Text chunk 3 discusses training metrics including generator and discriminator loss, relevant to interpreting the training stability inferred from the curves.\", 'Answer': \"The training curves for the ACGAN show an increasing generator loss and a more stable discriminator loss, suggesting potential instability in training. This may need adjustments for improved model convergence.\"}", "all_figures_names": ["1705.07164v8-Figure1-1.png", "1705.07164v8-Figure3-1.png", "1705.07164v8-Table2-1.png", "1705.07164v8-Table1-1.png"], "referred_figures_names": ["1705.07164v8-Figure1-1.png"], "answer": "The training curves for the ACGAN show that the generator and discriminator losses both decrease over time. This indicates that the ACGAN is able to learn to generate realistic images.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Figure~ shows that RWGAN and WGAN achieve the most stable training among all approaches. Compared to WGAN, RWGAN suffers from relatively high variance, which however improves the training process. As indicated by Figure~, RWGAN is the fastest to generate interpretable images.", "For an overview of the GANs theory and a bunch of relevant applications, we refer to the survey papers~. Despite the popularity of GANs, the training requires finding a Nash equilibrium of a nonconvex continuous game with high-dimensional parameters where the gradient-based algorithms fail to converge~. There have been many attempts to mitigate this curse when using GANs, whether through minibatch discrimination and batch normalization~; averaging and extragradient~ or by using the Wasserstein-1 distance~.", "Introduction Generative Adversarial Networks (GANs)~ are a class of approaches for learning generative models based on game theory. They find various applications in image processing~, wireless communications~ and signal processing~. The GANs framework can be also easily modified with other loss functions and learning dynamics, triggering numerous variants~.", "The metrics include generator loss, discriminator loss and inception score~. While the former two ones characterize the stability of training, a high inception score stands for high quality of images generated by the model. For \\textsc{mnist and \\textsc{fashion-mnist, we set the maximum epoch number as 100 and consider both generator and discriminator using a convolutional architecture (DCGAN)~ in our approach.", "For \\textsc{cifar10 and \\textsc{imagenet, besides the DCGAN architecture, we also consider the generator using ReLU-MLP~ with 4 layers and 512 units and set the maximum epoch number as 100 for \\textsc{cifar10 and 25 for \\textsc{imagenet. Figure shows that RWGAN achieves a good balance between robustness and efficiency while WGAN(g) are highly unstable. The effectiveness of RWGAN is also proven using inception score and sample images obtained by running all approaches; see Table~ and Figure~. Conclusion We propose new Relaxed Wasserstein (RW) distances by generalizing Wasserstein-1 distance with Bregman cost functions.", "We present a gradient-based algorithm for RWGANs; see Algorithm . While back propagation is used to train generator and discriminator and update the parameters once in the generator and n_{critic times in the discriminator, our framework is different from WGANs~. Indeed, we use asymmetric clipping based on \\phi to guarantee that f_w", "RW distances enjoy favorable statistical and computational properties, motivating RWGANs for learning generative models. Experiments on real images show that RWGANs with KL cost function achieves a good balance between robustness and efficiency. Future directions include a proper choice of \\phi in practice.", "Finally, we conduct experiments on real images. In the experiment, we select Kullback-Leibler (KL) cost function and the commonly-used DCGAN and MLP architectures. Experimental results demonstrate that our approach not only generates excellent samples but strikes a good balance between WGAN and WGAN with gradient penalty, denoted as WGANs(g)."], "referred_text_chunks": ["Figure~ shows that RWGAN and WGAN achieve the most stable training among all approaches. Compared to WGAN, RWGAN suffers from relatively high variance, which however improves the training process. As indicated by Figure~, RWGAN is the fastest to generate interpretable images."]}, "1": {"question": "Which model performs better based on the training curves?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 0, 'Rationale': 'Image 1 shows the training curves for WGAN(g), which directly displays training stability, and Text 0 discusses training stability for RWGAN and WGAN.', 'Answer': 'RWGAN and WGAN achieve the most stable training, with RWGAN being the fastest to generate interpretable images.'}", "all_figures_names": ["1705.07164v8-Figure1-1.png", "1705.07164v8-Figure3-1.png", "1705.07164v8-Table2-1.png", "1705.07164v8-Table1-1.png"], "referred_figures_names": ["1705.07164v8-Figure3-1.png"], "answer": "It is difficult to say definitively which model performs better based on the training curves alone. However, it appears that the WGAN(g) model may be performing better than the other models, as its generator and discriminator losses are both lower than the other models.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Figure~ shows that RWGAN and WGAN achieve the most stable training among all approaches. Compared to WGAN, RWGAN suffers from relatively high variance, which however improves the training process. As indicated by Figure~, RWGAN is the fastest to generate interpretable images.", "The metrics include generator loss, discriminator loss and inception score~. While the former two ones characterize the stability of training, a high inception score stands for high quality of images generated by the model. For \\textsc{mnist and \\textsc{fashion-mnist, we set the maximum epoch number as 100 and consider both generator and discriminator using a convolutional architecture (DCGAN)~ in our approach.", "More specifically, our approach converges faster than WGAN and is more robust than WGAN(g) which is likely to fail in practice. Our approach is thus an effective alternative to WGAN and WGAN(g) for learning generative models on large-scale datasets. Organization.", "In particular, the dual representation of Wasserstein-1 distance~ provides a theoretical foundation for reducing mode collapse and stabilizing learning process. The gradient penalty technique is also proposed for training Wasserstein GANs (WGANs)~. However, Wasserstein-1 distance is too restrictive to fit data geometry and WGAN training converges slowly even though it is more stable than other approaches. In this paper, we propose a new class of Relaxed Wasserstein (RW) distances by generalizing Wasserstein-1 distance with Bregman cost functions..", "RW distances enjoy favorable statistical and computational properties, motivating RWGANs for learning generative models. Experiments on real images show that RWGANs with KL cost function achieves a good balance between robustness and efficiency. Future directions include a proper choice of \\phi in practice.", "Finally, we conduct experiments on real images. In the experiment, we select Kullback-Leibler (KL) cost function and the commonly-used DCGAN and MLP architectures. Experimental results demonstrate that our approach not only generates excellent samples but strikes a good balance between WGAN and WGAN with gradient penalty, denoted as WGANs(g).", "Our new distances provide a unified mathematical framework for learning generative models with a good balance between model adaptability and computational tractability. Our contributions can be summarized as follows. First, we study the statistical behavior of RW distances.", "For an overview of the GANs theory and a bunch of relevant applications, we refer to the survey papers~. Despite the popularity of GANs, the training requires finding a Nash equilibrium of a nonconvex continuous game with high-dimensional parameters where the gradient-based algorithms fail to converge~. There have been many attempts to mitigate this curse when using GANs, whether through minibatch discrimination and batch normalization~; averaging and extragradient~ or by using the Wasserstein-1 distance~."], "referred_text_chunks": ["Figure~ shows that RWGAN and WGAN achieve the most stable training among all approaches. Compared to WGAN, RWGAN suffers from relatively high variance, which however improves the training process. As indicated by Figure~, RWGAN is the fastest to generate interpretable images."]}, "2": {"question": "Which method achieved the highest Inception Score (IS) at the end of training for both CIFAR10 and ImageNet datasets? Did this method also achieve the highest initial IS score?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 5, 'Rationale': 'Image 2 directly presents the Inception Scores for CIFAR10 and ImageNet, making it easy to identify which method achieved the highest scores. Text 5 discusses the effectiveness of the method used, which is relevant to understanding performance implications.', 'Answer': 'WGAN achieved the highest Inception Score at the end for both CIFAR10 and ImageNet. It did not have the highest initial IS score.'}", "all_figures_names": ["1705.07164v8-Figure1-1.png", "1705.07164v8-Figure3-1.png", "1705.07164v8-Table2-1.png", "1705.07164v8-Table1-1.png"], "referred_figures_names": ["1705.07164v8-Table2-1.png"], "answer": "For CIFAR10, WGAN achieved the highest IS at the end of training (2.42). However, RWGAN had a higher initial IS score (1.86) compared to WGAN's 1.63. \n\nFor ImageNet, WGAN again achieved the highest final IS (2.80), while RWGAN had a slightly higher initial IS (2.04 vs. 2.00). ", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Experiments In this section, we report the numerical results which evaluate our approach on real images. All baseline approaches are discussed in the introduction and their implementations are available online. The datasets include \\textsc{mnist, \\textsc{fashion-mnist, \\textsc{cifar10 and \\textsc{imagenet publicly available online: http://image-net.org/small/train{\\_64x64.tar.", "The metrics include generator loss, discriminator loss and inception score~. While the former two ones characterize the stability of training, a high inception score stands for high quality of images generated by the model. For \\textsc{mnist and \\textsc{fashion-mnist, we set the maximum epoch number as 100 and consider both generator and discriminator using a convolutional architecture (DCGAN)~ in our approach.", "For \\textsc{cifar10 and \\textsc{imagenet, besides the DCGAN architecture, we also consider the generator using ReLU-MLP~ with 4 layers and 512 units and set the maximum epoch number as 100 for \\textsc{cifar10 and 25 for \\textsc{imagenet. Figure shows that RWGAN achieves a good balance between robustness and efficiency while WGAN(g) are highly unstable. The effectiveness of RWGAN is also proven using inception score and sample images obtained by running all approaches; see Table~ and Figure~. Conclusion We propose new Relaxed Wasserstein (RW) distances by generalizing Wasserstein-1 distance with Bregman cost functions.", "Figure~ shows that RWGAN and WGAN achieve the most stable training among all approaches. Compared to WGAN, RWGAN suffers from relatively high variance, which however improves the training process. As indicated by Figure~, RWGAN is the fastest to generate interpretable images.", "\\in \\hat{\\FCal(\\XCal) while WGANs use symmetric clipping such that f_w \\in \\FCal(\\XCal). We also adopt RMSProp~ with large stepsize which works well in practice.", "More specifically, our approach converges faster than WGAN and is more robust than WGAN(g) which is likely to fail in practice. Our approach is thus an effective alternative to WGAN and WGAN(g) for learning generative models on large-scale datasets. Organization.", "Finally, we conduct experiments on real images. In the experiment, we select Kullback-Leibler (KL) cost function and the commonly-used DCGAN and MLP architectures. Experimental results demonstrate that our approach not only generates excellent samples but strikes a good balance between WGAN and WGAN with gradient penalty, denoted as WGANs(g).", "We present a gradient-based algorithm for RWGANs; see Algorithm . While back propagation is used to train generator and discriminator and update the parameters once in the generator and n_{critic times in the discriminator, our framework is different from WGANs~. Indeed, we use asymmetric clipping based on \\phi to guarantee that f_w"], "referred_text_chunks": ["Experiments In this section, we report the numerical results which evaluate our approach on real images. All baseline approaches are discussed in the introduction and their implementations are available online. The datasets include \\textsc{mnist, \\textsc{fashion-mnist, \\textsc{cifar10 and \\textsc{imagenet publicly available online: http://image-net.org/small/train{\\_64x64.tar."]}, "3": {"question": "What is the difference between the Euclidean and Mahalanobis Bregman cost functions?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 2, 'Rationale': 'Image 3 directly provides the table listing Euclidean and Mahalanobis Bregman cost functions, which is pertinent to the question. Text 2 discusses Bregman cost functions and their role in the paper, offering necessary theoretical context.', 'Answer': 'The Euclidean Bregman cost function is \\\\(||x-y||^2\\\\), while the Mahalanobis Bregman cost function is \\\\((x-y)^T A (x-y)\\\\), where A is a positive semidefinite matrix.'}", "all_figures_names": ["1705.07164v8-Figure1-1.png", "1705.07164v8-Figure3-1.png", "1705.07164v8-Table2-1.png", "1705.07164v8-Table1-1.png"], "referred_figures_names": ["1705.07164v8-Table1-1.png"], "answer": "The Euclidean Bregman cost function is simply the squared difference between two points, while the Mahalanobis Bregman cost function takes into account the covariance of the data.", "figure_type": "table", "content_type": "table", "text_chunks": ["The Wasserstein-1 distance is used in WGANs~ and has a dual representation~ as follows, where \\FCal(\\XCal) is the set of all the 1-Lipschitz functions defined on the set \\XCal. Another key ingredient in our paper is a class of cost functions based on the celebrated Bregman distances~. Specifically, let x, y \\in \\XCal, the Bregman cost function between them is defined by where \\phi: \\XCal \\rightarrow \\br is strictly convex and continuously differentiable.", "Finally, we conduct experiments on real images. In the experiment, we select Kullback-Leibler (KL) cost function and the commonly-used DCGAN and MLP architectures. Experimental results demonstrate that our approach not only generates excellent samples but strikes a good balance between WGAN and WGAN with gradient penalty, denoted as WGANs(g).", "Examples of the function \\phi and the resulting Bregman cost functions are listed in Table~. We propose our new Relaxed Wasserstein (RW) distances, which generalizes Wasserstein-1 distance with Bregman cost functions. Specifically, let \\mu, \\nu \\in \\PCal(\\XCal) and \\phi: \\XCal \\rightarrow \\br be strictly convex and continuously differentiable, the relaxed Wasserstein distance (parameterized by \\phi) between \\mu and \\nu is defined by Main Results Throughout this section, we let \\phi: \\XCal \\rightarrow \\br be strictly convex and continuously differentiable and \\XCal be compact.", "In particular, the dual representation of Wasserstein-1 distance~ provides a theoretical foundation for reducing mode collapse and stabilizing learning process. The gradient penalty technique is also proposed for training Wasserstein GANs (WGANs)~. However, Wasserstein-1 distance is too restrictive to fit data geometry and WGAN training converges slowly even though it is more stable than other approaches. In this paper, we propose a new class of Relaxed Wasserstein (RW) distances by generalizing Wasserstein-1 distance with Bregman cost functions..", "For \\textsc{cifar10 and \\textsc{imagenet, besides the DCGAN architecture, we also consider the generator using ReLU-MLP~ with 4 layers and 512 units and set the maximum epoch number as 100 for \\textsc{cifar10 and 25 for \\textsc{imagenet. Figure shows that RWGAN achieves a good balance between robustness and efficiency while WGAN(g) are highly unstable. The effectiveness of RWGAN is also proven using inception score and sample images obtained by running all approaches; see Table~ and Figure~. Conclusion We propose new Relaxed Wasserstein (RW) distances by generalizing Wasserstein-1 distance with Bregman cost functions.", "Thus, we have This implies that Eq.~\\eqref{Eq:lower_RW_bound holds true. \\noindent Proving Eq.~\\eqref{Eq:upper_RW_bound: Let \\varepsilon>0, there exists g_1, g_2 \\in \\CCal(\\XCal) with g_1 \\oplus g_2 \\leq B_\\phi such that Now we construct a function f \\in \\FCal_u(\\XCal) out of g_1 and g_2. More specifically, we first define the function f: \\XCal \\rightarrow \\br by f(x) = \\inf_{y \\in \\XCal \\{B_\\phi(x, y)-g_2(y)\\ for all x \\in \\XCal.", "The rest of this paper is organized as follows. We introduce the basic setup and define the Relaxed Wasserstein (RW) distances in Section~. We present the theoretical results as well as the gradient-based algorithm in Section~. We report the numerical results on real images from four large-scale datasets in Section~ and conclude in Section~. Notations. Throughout the paper, \\|\\cdot\\| refers to the Euclidean norm (in the corresponding vector space).", "Then one approximates \\mu_r using \\mu_\\theta by adapting \\theta to minimize RW distances between \\mu_r and \\mu_\\theta. In our experiment, we consider KL cost function where \\phi(x)=\\sum_i x_i\\log(x_i). Such choice is based on the observation that KL distance can fit different type of data geometry well in clustering tasks~."], "referred_text_chunks": ["The Wasserstein-1 distance is used in WGANs~ and has a dual representation~ as follows, where \\FCal(\\XCal) is the set of all the 1-Lipschitz functions defined on the set \\XCal. Another key ingredient in our paper is a class of cost functions based on the celebrated Bregman distances~. Specifically, let x, y \\in \\XCal, the Bregman cost function between them is defined by where \\phi: \\XCal \\rightarrow \\br is strictly convex and continuously differentiable."]}}