{"0": {"question": "Which policy learning method achieved the lowest regret in Ex. 2?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 6, 'Rationale': 'Image 0 shows regressions for different policy learning methods in Ex. 2 and their corresponding regret values. Text 6 discusses regret bounds and consistency, linking to evaluating policies with minimal regret.', 'Answer': 'The Balanced policy learner achieved the lowest regret of 0.06 in Ex. 2.'}", "all_figures_names": ["1705.07384v2-Figure2-1.png", "1705.07384v2-Table2-1.png"], "referred_figures_names": ["1705.07384v2-Figure2-1.png"], "answer": "The DR-SVM method achieved the lowest regret in Ex. 2, with a regret of 0.18.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Regret bounds follow as a corollary. If \\hat\\pi_n^\\text{bal-DR is as in \\eqref{DRbalancedlearning then: And, all the same results hold when replacing {\\mathfrak R_n(\\Pi) with \\widehat{\\mathfrak R_n(\\Pi) and/or replacing R_\\Pi with \\widehat R_\\Pi. \\end{corollary Conclusion Considering the policy evaluation and learning problems using observational or logged data, we presented a new method that is based on finding optimal balancing weights that make the data look like the target policy and that is aimed at ameliorating the shortcomings of existing methods, which included having to deal with near-zero propensities, using too few positive weights, and using an awkward two-stage procedure.", "We define the sample and population regret, respectively, as \\ts R_\\Pi(\\hat\\pi)=\\op{PAPE(\\hat\\pi)-\\min_{\\pi\\in\\Pi\\op{PAPE(\\pi),\\quad \\widehat R_\\Pi(\\hat\\pi)=\\op{SAPE(\\hat\\pi)-\\min_{\\pi\\in\\Pi\\op{SAPE(\\pi) A key requirement for these to converge is that the best-in-class policy is learnable. We quantify that using Rademacher complexity \\citep{bartlett2003rademacher and later extend our results to VC dimension. Let us define \\ts\\widehat{\\mathfrak R_n(\\mathcal F)=\\frac1{2^n\\sum_{\\rho_i\\in\\fbraces{-1,+1^n\\sup_{f\\in\\mathcal F\\frac1n\\sum_{i=1^n\\rho_if(X_i),\\quad{\\mathfrak", "Indeed, when the policy class \\Pi is very flexible relative to n and if outcomes are nonnegative, then the anti-logging policy \\pi_{T_i(X_i)=0 minimizes any of the above estimates. POEM avoids learning the anti-logging policy by regularizing overlap, reducing variance but limiting novelty of \\pi. A refinement, SNPOEM \\citep{swaminathan2015self uses a normalized and clipped IPW (NCIPW) estimator (and regularizes variance):", "Moreover, if \\pi^* is such that \\pi^*_t(x)>0\\iff t\\in\\argmin_{s\\in[m]\\mu_s(x), then \\widehat R(\\pi)=\\op{SAPE(\\pi)-\\op{SAPE(\\pi^*) is the regret of \\pi \\citep{bubeck2012regret. The policy evaluation task is closely related to causal effect estimation \\citep{imbens2015causal where, for m=2, one is interested in estimating the sample and population average treatment effects: \\op{SATE=\\frac1n\\sum_{i=1^n(\\mu_2(X_i)-\\mu_1(X_i)), \\op{PATE=\\Efb{\\op{SATE=\\Efb{Y(2)-Y(1). In policy learning, we wish to find a policy \\hat\\pi that achieves small outcomes, \\ie, small SAPE and PAPE.", "Balanced Learning Next we consider a balanced approach to policy learning. Given a policy class \\Pi\\subset[\\mathcal X\\to\\Delta^m], we let the balanced policy learner yield the policy \\pi\\in\\Pi that minimizes the balanced policy evaluation using either a vanilla or DR estimator plus a potential regularization term in the worst-case/posterior CMSE of the evaluation. We formulate this as a bilevel optimization problem: The regularization term regularizes both the balance (\\ie, worst-case/posterior bias) that is achievable for \\pi and the variance in evaluating \\pi.", "The second statement in Lemma~ suggests that, in practice, model selection of \\gamma_{1:m, \\Lambda, and kernel hyperparameters such as s or even \\hat S, can done by the marginal likelihood method \\citep[see][Ch.~5]{williams2006gaussian. Evaluation Using Optimal Balancing Weights Our policy evaluation estimates are given by either the estimator \\hat\\tau_{W^*(\\pi;\\fmagd\\cdot,\\Lambda) or \\hat\\tau_{W^*(\\pi;\\fmagd\\cdot,\\Lambda),\\hat\\mu where W^*(\\pi)=W^*(\\pi;\\fmagd\\cdot,\\Lambda) is the minimizer of \\mathfrak E^2(W,\\pi;\\fmagd\\cdot,\\Lambda) over the space of all weights W that sum to n, \\mathcal W=\\fbraces{W\\in\\R n_+:\\sum_{i=1^nW_i=n=n\\Delta^n. Specifically, \\ts W^*(\\pi;\\fmagd\\cdot,\\Lambda)\\in\\argmin_{W\\in\\mathcal W\\mathfrak E^2(W,\\pi;\\fmagd\\cdot,\\Lambda).", "Uniform Consistency and Regret Bounds Next, we establish consistency results uniformly over policy classes. This allows us to bound the regret of the balanced policy learner.", "For learning, we propose a bilevel optimization problem: minimize over \\pi\\in\\Pi, the estimated risk \\hat\\tau_{W^*(\\pi) (or a doubly robust version thereof and potentially plus a regularization term), given by the weights W^*(\\pi) that minimize the estimation error. Our empirical results show the stark benefit of this approach while our main theoretical results (Thm.~, Cor.~) establish vanishing regret bounds. Balanced Evaluation CMSE and Worst-Case CMSE We begin by presenting the approach in the context of evaluation."], "referred_text_chunks": ["Regret bounds follow as a corollary. If \\hat\\pi_n^\\text{bal-DR is as in \\eqref{DRbalancedlearning then: And, all the same results hold when replacing {\\mathfrak R_n(\\Pi) with \\widehat{\\mathfrak R_n(\\Pi) and/or replacing R_\\Pi with \\widehat R_\\Pi. \\end{corollary Conclusion Considering the policy evaluation and learning problems using observational or logged data, we presented a new method that is based on finding optimal balancing weights that make the data look like the target policy and that is aimed at ameliorating the shortcomings of existing methods, which included having to deal with near-zero propensities, using too few positive weights, and using an awkward two-stage procedure."]}}