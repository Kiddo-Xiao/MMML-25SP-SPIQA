{"0": {"question": "Which year is most associated with the terms \"sept\", \"hijackers\", and \"attacks\"?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 directly highlights the association of the terms \"sept\", \"hijackers\", and \"attacks\" with the year 2001. Text 0 mentions the dominance of these terms in the vertical direction, likely linking them to a specific year.', 'Answer': 'The year most associated with the terms \"sept\", \"hijackers\", and \"attacks\" is 2001.'}", "all_figures_names": ["1705.09296v2-Table6-1.png", "1705.09296v2-Figure1-1.png", "1705.09296v2-Table4-1.png", "1705.09296v2-Figure2-1.png", "1705.09296v2-Table5-1.png", "1705.09296v2-Table1-1.png", "1705.09296v2-Table2-1.png", "1705.09296v2-Figure3-1.png"], "referred_figures_names": ["1705.09296v2-Figure3-1.png"], "answer": "2001", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Meanwhile, the events of 9/11 dominate the vertical direction, with the words sept, hijackers, and attacks increasing in probability as we move up in the space. If we wanted to look at each year individually, we could drop the embedding of years, and learn a sparse set of topic-year interactions, similar to tone in Table . Additional Related Work The literature on topic models is vast; in addition to papers cited throughout, other efforts to incorporate metadata into topic models include Dirichlet-multinomial regression (DMR; \\citealp{mimno.2008), Labeled LDA , and MedLDA .", "We omit the topics in the interest of space, but Figure shows the learned embedding for each year, along with the top terms of the corresponding deviations. As can be seen, the model learns that adjacent years tend to produce similar deviations, {\\it even though we have not explicitly encoded this information. The left-right dimension roughly tracks a temporal trend with positive deviations shifting from the years of Clinton and INS on the left, to Obama and ICE on the right.", "Finally, we demonstrate the ability to incorporate covariates and/or labels in an exploratory data analysis (\\S). The scores we report are generalization to held-out data, measured in terms of perplexity; coherence, measured in terms of non-negative point-wise mutual information (NPMI; \\citealp{chang.2009,newman.2010), and classification accuracy on test data. For coherence we evaluate NPMI using the top 10 words of each topic, both internally (using test data), and externally, using a decade of articles from the English Gigaword dataset \\citep{graff2003english.", "For the external evaluation of NPMI, we use the co-occurrence statistics from all New York Times articles in the English Gigaword published from the start of 2000 to the end of 2009, processed in the same way as our data. For the text classification experiments, we use the \\texttt{scikit-learn implementation of logistic regression. We give it access to the same input data as our model (using the same vocabulary), and tune the strength of l_2 regularization using cross-validation.", "\\paragraph{Combined model with temporal metadata. Finally, we incorporate both the tone annotations and the year of publication of each article, treating the former as a label and the latter as a covariate. In this model, we also include an embedding matrix, \\mathbf{W_c, to project the one-hot {\\em year vectors down to a two-dimensional continuous space, with a learned deviation for each dimension.", "Next we consider using tone as a covariate, and build a model using both tone and tone-topic interactions. Table shows a set of topics learned from the immigration data, along with the most highly-weighted words in the corresponding tone-topic interaction terms. As can be seen, these interaction terms tend to capture different frames (e.g., ``criminal'' vs.~``detainees'', and ``illegals'' vs. ``newcomers'', etc).", "Table shows the equivalent results for the Yahoo answers dataset, using 250 topics, and a 5,000-word vocabulary. These results closely match those for the IMDB dataset, with our model having higher perplexity but also higher internal coherence than LDA. As with IMDB, the use of word vectors improves coherence, both internally and externally, but again at the cost of worse perplexity.", "Surprisingly, our model without a background term actually has the best external coherence on this dataset, but as described in the main paper, these tend to give high weight primarily to common words, and are more repetitive as a result."], "referred_text_chunks": ["Meanwhile, the events of 9/11 dominate the vertical direction, with the words sept, hijackers, and attacks increasing in probability as we move up in the space. If we wanted to look at each year individually, we could drop the embedding of years, and learn a sparse set of topic-year interactions, similar to tone in Table . Additional Related Work The literature on topic models is vast; in addition to papers cited throughout, other efforts to incorporate metadata into topic models include Dirichlet-multinomial regression (DMR; \\citealp{mimno.2008), Labeled LDA , and MedLDA ."]}, "1": {"question": "Which node in the generative model represents the latent variable?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 2, 'Rationale': 'Image 7 directly shows the generative model and highlights different nodes, making it most relevant for identifying the latent variable. Text 2 explains the role of the generative components and how they interact with document representation, addressing the question of latent variables.', 'Answer': 'In the generative model, the node labeled \u03b8 represents the latent variable.'}", "all_figures_names": ["1705.09296v2-Table6-1.png", "1705.09296v2-Table1-1.png", "1705.09296v2-Table3-1.png", "1705.09296v2-Table2-1.png", "1705.09296v2-Table5-1.png", "1705.09296v2-Figure2-1.png", "1705.09296v2-Table4-1.png", "1705.09296v2-Figure1-1.png"], "referred_figures_names": ["1705.09296v2-Figure1-1.png"], "answer": "The node labeled \u03b7 represents the latent variable.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["The background term in our model does not have much effect on perplexity, but plays an important role in producing coherent topics; as in SAGE, the background can account for common words, so they are mostly absent among the most heavily weighted words in the topics. For instance, words like film and movie in the IMDB corpus are relatively unimportant in the topics learned by our model, but would be much more heavily weighted without the background term, as they are in topics learned by LDA. Text Classification We next consider the utility of our model in the context of categorical labels, and consider them alternately as observed covariates and as labels generated conditional on the latent representation.", "In order to alleviate this problem, we reconstruct \\bm{\\eta as a convex combination of two copies of the output of the generator network, one passed through a batchnorm layer, and one not. During training, we then gradually anneal the model from relying entirely on the component passed through the batchnorm layer, to relying entirely on the one that is not. This ensures that the the final weights and document representations will be properly interpretable.", "This effectively solves the problem of mode collapse, but the batchnorm layer on \\bm{\\eta introduces an additional problem, not previously reported. At test time, the batchnorm layer will shift the input based on the learned population mean of the training data; this effectively encodes information about the distribution of words in this model that is not captured by the topic weights and background distribution. As such, although reconstruction error will be low, the document representation \\bm{\\theta, will not necessarily be a useful representation of the topical content of each document.", "We therefore use the same experimental setup as \\citet{srivastava.2017 (learning rate, momentum, batch size, and number of epochs) and find the same general patterns as they reported (see Table and supplementary material): our model returns more coherent topics than LDA, but at the cost of worse perplexity. SAGE, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than LDA. As expected, the NVDM produces relatively low perplexity, but very poor coherence, due to its lack of constraints on \\bm{\\theta.", "\\paragraph{Combined model with temporal metadata. Finally, we incorporate both the tone annotations and the year of publication of each article, treating the former as a label and the latter as a covariate. In this model, we also include an embedding matrix, \\mathbf{W_c, to project the one-hot {\\em year vectors down to a two-dimensional continuous space, with a learned deviation for each dimension.", "Next we consider using tone as a covariate, and build a model using both tone and tone-topic interactions. Table shows a set of topics learned from the immigration data, along with the most highly-weighted words in the corresponding tone-topic interaction terms. As can be seen, these interaction terms tend to capture different frames (e.g., ``criminal'' vs.~``detainees'', and ``illegals'' vs. ``newcomers'', etc).", "Experimental Details For all experiments we use a model with 300-dimensional embeddings of words, and we take f_e to be only the element-wise softplus nonlinearity (followed by the linear transformations for \\bm{\\mu and \\log \\bm{\\sigma^2). Similarly, f_y is a linear transformation of \\bm \\theta, followed by a softplus layer, followed by a linear transformation to the size of the output (the number of classes). During training, we set S (the number of samples of \\bm\\epsilon) to be 1; for estimating the ELBO at on test documents, we set S=20.", "We omit the topics in the interest of space, but Figure shows the learned embedding for each year, along with the top terms of the corresponding deviations. As can be seen, the model learns that adjacent years tend to produce similar deviations, {\\it even though we have not explicitly encoded this information. The left-right dimension roughly tracks a temporal trend with positive deviations shifting from the years of Clinton and INS on the left, to Obama and ICE on the right."], "referred_text_chunks": ["The background term in our model does not have much effect on perplexity, but plays an important role in producing coherent topics; as in SAGE, the background can account for common words, so they are mostly absent among the most heavily weighted words in the topics. For instance, words like film and movie in the IMDB corpus are relatively unimportant in the topics learned by our model, but would be much more heavily weighted without the background term, as they are in topics learned by LDA. Text Classification We next consider the utility of our model in the context of categorical labels, and consider them alternately as observed covariates and as labels generated conditional on the latent representation."]}, "2": {"question": "Which model achieves the best NPMI scores (both internal and external) in the unsupervised setting, and what trade-off does this model exhibit compared to other models?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 5, 'Rationale': 'Image 1 provides detailed NPMI scores both internally and externally for different models in an unsupervised setting. Text 5 discusses the model coherence compared to LDA, highlighting the trade-offs.', 'Answer': 'The SCHOLAR with word vectors (SCHOLAR + W.V.) achieves the best NPMI scores (0.18 internal, 0.17 external) in the unsupervised setting, but it has worse perplexity compared to LDA.'}", "all_figures_names": ["1705.09296v2-Table2-1.png", "1705.09296v2-Table1-1.png", "1705.09296v2-Table6-1.png", "1705.09296v2-Figure3-1.png", "1705.09296v2-Figure2-1.png", "1705.09296v2-Figure1-1.png", "1705.09296v2-Table5-1.png", "1705.09296v2-Table3-1.png"], "referred_figures_names": ["1705.09296v2-Table1-1.png"], "answer": "The Scholar + w.v. model achieves the best NPMI scores (both internal and external) in the unsupervised setting. However, this model also has the highest number of people parameters, indicating a trade-off between topic coherence and model complexity.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Surprisingly, our model without a background term actually has the best external coherence on this dataset, but as described in the main paper, these tend to give high weight primarily to common words, and are more repetitive as a result.", "For our model, we only tune the number of epochs, evaluating on development data. Our models for this task did not use regularization or word vectors. Additional Experimental Results In this section we include additional experimental results in the unsupervised setting.", "Unsupervised Evaluation Although the emphasis of this work is on incorporating observed labels and/or covariates, we briefly report on experiments in the unsupervised setting. Recall that, without metadata, \\scholar equates to ProdLDA, but with an explicit background term.", "Finally, we demonstrate the ability to incorporate covariates and/or labels in an exploratory data analysis (\\S). The scores we report are generalization to held-out data, measured in terms of perplexity; coherence, measured in terms of non-negative point-wise mutual information (NPMI; \\citealp{chang.2009,newman.2010), and classification accuracy on test data. For coherence we evaluate NPMI using the top 10 words of each topic, both internally (using test data), and externally, using a decade of articles from the English Gigaword dataset \\citep{graff2003english.", "Also, although the NVDM still has worse perplexity than LDA, the effects are not as dramatic as reported in . Regularization is also more beneficial for this data, with both SAGE and our regularized model obtaining better coherence than LDA. The topics from \\textsc{scholar for this dataset are also shown in Table .", "We therefore use the same experimental setup as \\citet{srivastava.2017 (learning rate, momentum, batch size, and number of epochs) and find the same general patterns as they reported (see Table and supplementary material): our model returns more coherent topics than LDA, but at the cost of worse perplexity. SAGE, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than LDA. As expected, the NVDM produces relatively low perplexity, but very poor coherence, due to its lack of constraints on \\bm{\\theta.", "Experiments and Results To evaluate and demonstrate the potential of this model, we present a series of experiments below. We first test \\scholar without observed metadata, and explore the effects of using regularization and/or word vector initialization, compared to LDA, SAGE, and NVDM (\\S). We then evaluate our model in terms of predictive performance, in comparison to SLDA and an l_2-regularized logistic regression baseline (\\S).", "For the unsupervised results, we use the same set up as : Adam optimizer with \\beta_1=0.99, learning rate =0.002, batch size of 200, and training for 200 epochs. The setup was the same for all datasets, except we only trained for 150 epochs on Yahoo answers because it is much larger. For LDA, we updated the hyperparameters every 10 epochs."], "referred_text_chunks": ["Surprisingly, our model without a background term actually has the best external coherence on this dataset, but as described in the main paper, these tend to give high weight primarily to common words, and are more repetitive as a result."]}, "3": {"question": "Which topic has the highest internal coherence value?", "referred_figures_indices": [1], "response": "{'Image': 3, 'Text': 2, 'Rationale': 'Image 3 shows the internal coherence values (NPMI) for different models on a dataset which is a key indicator for answering the question. Text 2 mentions the coherence of models, specifically noting the improvement with word vectors, which relates directly to internal coherence.', 'Answer': 'SCHOLAR has the highest internal coherence value of 0.35 on the 20 newsgroups dataset according to Image 3.'}", "all_figures_names": ["1705.09296v2-Table2-1.png", "1705.09296v2-Table6-1.png", "1705.09296v2-Figure2-1.png", "1705.09296v2-Table4-1.png", "1705.09296v2-Figure3-1.png", "1705.09296v2-Figure1-1.png", "1705.09296v2-Table1-1.png", "1705.09296v2-Table5-1.png"], "referred_figures_names": ["1705.09296v2-Table6-1.png"], "answer": "The topic with the highest internal coherence value is \"turks armenian armenia turkish roads escape soviet muslim mountain soul\".", "figure_type": "table", "content_type": "table", "text_chunks": ["Further experimentation revealed that the VAE framework involves a tradeoff among the scores; running for more epochs tends to result in better perplexity on held-out data, but at the cost of worse coherence. Adding regularization to encourage sparse topics has a similar effect as in SAGE, leading to worse perplexity and coherence, but it does create sparse topics. Interestingly, initializing the encoder with pretrained word2vec embeddings, and not updating them returned a model with the best internal coherence of any model we considered for IMDB and Yahoo answers, and the second-best for 20 newsgroups.", "Finally, we demonstrate the ability to incorporate covariates and/or labels in an exploratory data analysis (\\S). The scores we report are generalization to held-out data, measured in terms of perplexity; coherence, measured in terms of non-negative point-wise mutual information (NPMI; \\citealp{chang.2009,newman.2010), and classification accuracy on test data. For coherence we evaluate NPMI using the top 10 words of each topic, both internally (using test data), and externally, using a decade of articles from the English Gigaword dataset \\citep{graff2003english.", "Table shows the equivalent results for the Yahoo answers dataset, using 250 topics, and a 5,000-word vocabulary. These results closely match those for the IMDB dataset, with our model having higher perplexity but also higher internal coherence than LDA. As with IMDB, the use of word vectors improves coherence, both internally and externally, but again at the cost of worse perplexity.", "Surprisingly, our model without a background term actually has the best external coherence on this dataset, but as described in the main paper, these tend to give high weight primarily to common words, and are more repetitive as a result.", "Next we consider using tone as a covariate, and build a model using both tone and tone-topic interactions. Table shows a set of topics learned from the immigration data, along with the most highly-weighted words in the corresponding tone-topic interaction terms. As can be seen, these interaction terms tend to capture different frames (e.g., ``criminal'' vs.~``detainees'', and ``illegals'' vs. ``newcomers'', etc).", "Figure shows a set of topics learned in this way, along with the predicted probability of an article being pro-immigration conditioned on the given topic. All topics are coherent, and the predicted probabilities have strong face validity, e.g., ``arrested charged charges agents operation'' is least associated with pro-immigration. \\paragraph{Tone as a covariate.", "We therefore use the same experimental setup as \\citet{srivastava.2017 (learning rate, momentum, batch size, and number of epochs) and find the same general patterns as they reported (see Table and supplementary material): our model returns more coherent topics than LDA, but at the cost of worse perplexity. SAGE, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than LDA. As expected, the NVDM produces relatively low perplexity, but very poor coherence, due to its lack of constraints on \\bm{\\theta.", "Meanwhile, the events of 9/11 dominate the vertical direction, with the words sept, hijackers, and attacks increasing in probability as we move up in the space. If we wanted to look at each year individually, we could drop the embedding of years, and learn a sparse set of topic-year interactions, similar to tone in Table . Additional Related Work The literature on topic models is vast; in addition to papers cited throughout, other efforts to incorporate metadata into topic models include Dirichlet-multinomial regression (DMR; \\citealp{mimno.2008), Labeled LDA , and MedLDA ."], "referred_text_chunks": ["Further experimentation revealed that the VAE framework involves a tradeoff among the scores; running for more epochs tends to result in better perplexity on held-out data, but at the cost of worse coherence. Adding regularization to encourage sparse topics has a similar effect as in SAGE, leading to worse perplexity and coherence, but it does create sparse topics. Interestingly, initializing the encoder with pretrained word2vec embeddings, and not updating them returned a model with the best internal coherence of any model we considered for IMDB and Yahoo answers, and the second-best for 20 newsgroups."]}, "4": {"question": "Which model performed best on the IMDB dataset for classifying documents with categorical labels, and how much better did it perform compared to the SLDA model?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 3, 'Rationale': 'Image 5 provides accuracy results of models on the IMDB dataset, showing comparative performance, which directly answers the question. Text 3 discusses the model learning low-dimensional representations relevant to labels, suggesting relevance to classification tasks.', 'Answer': 'The SCHOLAR (covariates) model performed best on the IMDB dataset for classifying documents with categorical labels with an accuracy of 0.87, outperforming SLDA, which had an accuracy of 0.64.'}", "all_figures_names": ["1705.09296v2-Table4-1.png", "1705.09296v2-Table5-1.png", "1705.09296v2-Figure1-1.png", "1705.09296v2-Figure2-1.png", "1705.09296v2-Figure3-1.png", "1705.09296v2-Table2-1.png", "1705.09296v2-Table1-1.png", "1705.09296v2-Table3-1.png"], "referred_figures_names": ["1705.09296v2-Table2-1.png"], "answer": "Both the SCHOLAR (covariates) and Logistic Regression models achieved the highest accuracy of 0.87 on the IMDB dataset. This represents a 0.23 improvement over the SLDA model, which achieved an accuracy of 0.64.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Table shows the equivalent results for the Yahoo answers dataset, using 250 topics, and a 5,000-word vocabulary. These results closely match those for the IMDB dataset, with our model having higher perplexity but also higher internal coherence than LDA. As with IMDB, the use of word vectors improves coherence, both internally and externally, but again at the cost of worse perplexity.", "The background term in our model does not have much effect on perplexity, but plays an important role in producing coherent topics; as in SAGE, the background can account for common words, so they are mostly absent among the most heavily weighted words in the topics. For instance, words like film and movie in the IMDB corpus are relatively unimportant in the topics learned by our model, but would be much more heavily weighted without the background term, as they are in topics learned by LDA. Text Classification We next consider the utility of our model in the context of categorical labels, and consider them alternately as observed covariates and as labels generated conditional on the latent representation.", "Also, although the NVDM still has worse perplexity than LDA, the effects are not as dramatic as reported in . Regularization is also more beneficial for this data, with both SAGE and our regularized model obtaining better coherence than LDA. The topics from \\textsc{scholar for this dataset are also shown in Table .", "Rather, the high accuracies we obtain demonstrate that we are learning low-dimensional representations of documents that are relevant to the label of interest, outperforming SLDA, and have the same attractive properties as topic models. Further, any neural network that is successful for text classification could be incorporated into f_y and trained end-to-end along with topic discovery. Exploratory Study We demonstrate how our model might be used to explore an annotated corpus of articles about immigration, and adapt to different assumptions about the data.", "It is challenging to fairly evaluate the relative computational efficiency of our approach compared to past work (due to the stochastic nature of our approach to inference, choices about hyperparameters such as tolerance, and because of differences in implementation). Nevertheless, in practice, the performance of our approach is highly appealing. For all experiments in this paper, our implementation was much faster than SLDA or SAGE (implemented in C and Matlab, respectively), and competitive with Mallet.", "We take advantage of stochastic variational inference to develop a general algorithm for our framework such that variations do not require any model-specific algorithm derivations. Our model demonstrates the tradeoff between perplexity, coherence, and sparsity, and outperforms SLDA in predicting document labels. Furthermore, the flexibility of our model enables intriguing exploration of a text corpus on US immigration.", "Experiments and Results To evaluate and demonstrate the potential of this model, we present a series of experiments below. We first test \\scholar without observed metadata, and explore the effects of using regularization and/or word vector initialization, compared to LDA, SAGE, and NVDM (\\S). We then evaluate our model in terms of predictive performance, in comparison to SLDA and an l_2-regularized logistic regression baseline (\\S).", "We therefore use the same experimental setup as \\citet{srivastava.2017 (learning rate, momentum, batch size, and number of epochs) and find the same general patterns as they reported (see Table and supplementary material): our model returns more coherent topics than LDA, but at the cost of worse perplexity. SAGE, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than LDA. As expected, the NVDM produces relatively low perplexity, but very poor coherence, due to its lack of constraints on \\bm{\\theta."], "referred_text_chunks": ["Table shows the equivalent results for the Yahoo answers dataset, using 250 topics, and a 5,000-word vocabulary. These results closely match those for the IMDB dataset, with our model having higher perplexity but also higher internal coherence than LDA. As with IMDB, the use of word vectors improves coherence, both internally and externally, but again at the cost of worse perplexity."]}, "5": {"question": "Which topic is most likely to be associated with a pro-immigration stance?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 1, 'Rationale': 'Image 2 visually represents the probability of a pro-immigration stance based on topics. Text [1] discusses how tone-topic interactions can reveal frames like \"detainees\" and \"newcomers,\" which are associated with pro-immigration.', 'Answer': 'The topic with words \"english language city spanish community\" is most likely associated with a pro-immigration stance.'}", "all_figures_names": ["1705.09296v2-Table2-1.png", "1705.09296v2-Table1-1.png", "1705.09296v2-Figure2-1.png", "1705.09296v2-Table6-1.png", "1705.09296v2-Table4-1.png", "1705.09296v2-Table3-1.png", "1705.09296v2-Table5-1.png", "1705.09296v2-Figure3-1.png"], "referred_figures_names": ["1705.09296v2-Figure2-1.png"], "answer": "\"english language city spanish community\"", "figure_type": "** plot", "content_type": "figure", "text_chunks": ["Figure shows a set of topics learned in this way, along with the predicted probability of an article being pro-immigration conditioned on the given topic. All topics are coherent, and the predicted probabilities have strong face validity, e.g., ``arrested charged charges agents operation'' is least associated with pro-immigration. \\paragraph{Tone as a covariate.", "Next we consider using tone as a covariate, and build a model using both tone and tone-topic interactions. Table shows a set of topics learned from the immigration data, along with the most highly-weighted words in the corresponding tone-topic interaction terms. As can be seen, these interaction terms tend to capture different frames (e.g., ``criminal'' vs.~``detainees'', and ``illegals'' vs. ``newcomers'', etc).", "Since our model employs variational methods, the reported perplexity is an upper bound based on the ELBO. As datasets we use the familiar 20 newsgroups, the IMDB corpus of 50,000 movie reviews , and the UIUC Yahoo answers dataset with 150,000 documents in 15 categories . For further exploration, we also make use of a corpus of approximately 4,000 time-stamped news articles about US immigration, each annotated with pro- or anti-immigration tone .", "We only use a small number of topics in this part (K=8) for compact presentation. \\paragraph{Tone as a label. We first consider using the annotations as a label, and train a joint model to infer topics relevant to the tone of the article (pro- or anti-immigration).", "Meanwhile, the events of 9/11 dominate the vertical direction, with the words sept, hijackers, and attacks increasing in probability as we move up in the space. If we wanted to look at each year individually, we could drop the embedding of years, and learn a sparse set of topic-year interactions, similar to tone in Table . Additional Related Work The literature on topic models is vast; in addition to papers cited throughout, other efforts to incorporate metadata into topic models include Dirichlet-multinomial regression (DMR; \\citealp{mimno.2008), Labeled LDA , and MedLDA .", "We take advantage of stochastic variational inference to develop a general algorithm for our framework such that variations do not require any model-specific algorithm derivations. Our model demonstrates the tradeoff between perplexity, coherence, and sparsity, and outperforms SLDA in predicting document labels. Furthermore, the flexibility of our model enables intriguing exploration of a text corpus on US immigration.", "Rather, the high accuracies we obtain demonstrate that we are learning low-dimensional representations of documents that are relevant to the label of interest, outperforming SLDA, and have the same attractive properties as topic models. Further, any neural network that is successful for text classification could be incorporated into f_y and trained end-to-end along with topic discovery. Exploratory Study We demonstrate how our model might be used to explore an annotated corpus of articles about immigration, and adapt to different assumptions about the data.", "We omit the topics in the interest of space, but Figure shows the learned embedding for each year, along with the top terms of the corresponding deviations. As can be seen, the model learns that adjacent years tend to produce similar deviations, {\\it even though we have not explicitly encoded this information. The left-right dimension roughly tracks a temporal trend with positive deviations shifting from the years of Clinton and INS on the left, to Obama and ICE on the right."], "referred_text_chunks": ["Figure shows a set of topics learned in this way, along with the predicted probability of an article being pro-immigration conditioned on the given topic. All topics are coherent, and the predicted probabilities have strong face validity, e.g., ``arrested charged charges agents operation'' is least associated with pro-immigration. \\paragraph{Tone as a covariate."]}, "6": {"question": "How does the model capture different perspectives on immigration when considering tone as a covariate?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 and Text 0 directly address how the model captures different perspectives on immigration by using tone as a covariate, presenting specific words associated with anti- and pro-immigration tones.', 'Answer': 'The model captures different perspectives on immigration by using tone-topic interaction terms which highlight distinct frames, such as \"criminal\" vs. \"detainees\" and \"illegals\" vs. \"newcomers\".'}", "all_figures_names": ["1705.09296v2-Table3-1.png", "1705.09296v2-Table2-1.png", "1705.09296v2-Table5-1.png", "1705.09296v2-Figure1-1.png", "1705.09296v2-Table1-1.png", "1705.09296v2-Table6-1.png", "1705.09296v2-Figure2-1.png", "1705.09296v2-Figure3-1.png"], "referred_figures_names": ["1705.09296v2-Table3-1.png"], "answer": "The model captures different perspectives on immigration by highlighting contrasting words associated with the same topic, depending on whether the tone is anti-immigration or pro-immigration.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Next we consider using tone as a covariate, and build a model using both tone and tone-topic interactions. Table shows a set of topics learned from the immigration data, along with the most highly-weighted words in the corresponding tone-topic interaction terms. As can be seen, these interaction terms tend to capture different frames (e.g., ``criminal'' vs.~``detainees'', and ``illegals'' vs. ``newcomers'', etc).", "Figure shows a set of topics learned in this way, along with the predicted probability of an article being pro-immigration conditioned on the given topic. All topics are coherent, and the predicted probabilities have strong face validity, e.g., ``arrested charged charges agents operation'' is least associated with pro-immigration. \\paragraph{Tone as a covariate.", "We only use a small number of topics in this part (K=8) for compact presentation. \\paragraph{Tone as a label. We first consider using the annotations as a label, and train a joint model to infer topics relevant to the tone of the article (pro- or anti-immigration).", "Since our model employs variational methods, the reported perplexity is an upper bound based on the ELBO. As datasets we use the familiar 20 newsgroups, the IMDB corpus of 50,000 movie reviews , and the UIUC Yahoo answers dataset with 150,000 documents in 15 categories . For further exploration, we also make use of a corpus of approximately 4,000 time-stamped news articles about US immigration, each annotated with pro- or anti-immigration tone .", "\\paragraph{Combined model with temporal metadata. Finally, we incorporate both the tone annotations and the year of publication of each article, treating the former as a label and the latter as a covariate. In this model, we also include an embedding matrix, \\mathbf{W_c, to project the one-hot {\\em year vectors down to a two-dimensional continuous space, with a learned deviation for each dimension.", "The background term in our model does not have much effect on perplexity, but plays an important role in producing coherent topics; as in SAGE, the background can account for common words, so they are mostly absent among the most heavily weighted words in the topics. For instance, words like film and movie in the IMDB corpus are relatively unimportant in the topics learned by our model, but would be much more heavily weighted without the background term, as they are in topics learned by LDA. Text Classification We next consider the utility of our model in the context of categorical labels, and consider them alternately as observed covariates and as labels generated conditional on the latent representation.", "We take advantage of stochastic variational inference to develop a general algorithm for our framework such that variations do not require any model-specific algorithm derivations. Our model demonstrates the tradeoff between perplexity, coherence, and sparsity, and outperforms SLDA in predicting document labels. Furthermore, the flexibility of our model enables intriguing exploration of a text corpus on US immigration.", "Rather, the high accuracies we obtain demonstrate that we are learning low-dimensional representations of documents that are relevant to the label of interest, outperforming SLDA, and have the same attractive properties as topic models. Further, any neural network that is successful for text classification could be incorporated into f_y and trained end-to-end along with topic discovery. Exploratory Study We demonstrate how our model might be used to explore an annotated corpus of articles about immigration, and adapt to different assumptions about the data."], "referred_text_chunks": ["Next we consider using tone as a covariate, and build a model using both tone and tone-topic interactions. Table shows a set of topics learned from the immigration data, along with the most highly-weighted words in the corresponding tone-topic interaction terms. As can be seen, these interaction terms tend to capture different frames (e.g., ``criminal'' vs.~``detainees'', and ``illegals'' vs. ``newcomers'', etc)."]}}