{"0": {"question": "Which method achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset, and how does it compare to the best single-shot method on the same dataset?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 3, 'Rationale': 'Image 0 provides direct information about the top-1 accuracy across different methods and datasets, including BIWI. Text 3 discusses top performance in multi-shot mode and compares a specific method with others, making it relevant to the question.', 'Answer': 'The highest Top-1 Accuracy for multi-shot re-identification on the BIWI dataset is achieved by \"Our method with RTA attention\" at 50.0%. In comparison, the best single-shot method on BIWI is \"3D RAM\" with an accuracy of 30.1%.'}", "all_figures_names": ["1705.09882v2-Table1-1.png", "1705.09882v2-Figure6-1.png", "1705.09882v2-Figure2-1.png", "1705.09882v2-Table2-1.png", "1705.09882v2-Figure7-1.png", "1705.09882v2-Figure5-1.png", "1705.09882v2-Figure3-1.png", "1705.09882v2-Figure4-1.png"], "referred_figures_names": ["1705.09882v2-Table1-1.png"], "answer": "The proposed method with RTA attention achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset with a score of 50.0%. This is significantly higher than the best single-shot method on the same dataset, which is our method (CNN) with a score of 25.4%.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["However, we make the assumption that the 32 subjects that participate in the second recording are known for all competing methods. In Table~ we show that re-identification from body depth is more robust than from body RGB~, presenting 6.2\\% higher top-1 accuracy and 10.7\\% larger nAUC in single-shot mode. Next, we explore the benefit of using head information, which is less sensitive than clothes to day-by-day changes.", "In Fig.~ we visualize the CMC curves for single-shot setting. We observe that ReID from body depth scales better than its counterparts, which is validated by the nAUC scores. Conclusion In the paper, we present a novel approach for depth-based person re-identification.", "However, it is worth noting that 3D RAM performs better on BIWI. Our conjecture is that the spatial attention mechanism is important in datasets with significant variation in human pose and partial body occlusions. On the other hand, the spatial attention is evidently less critical on DPI-T, which contains views from the top and the visible region is mostly uniform across frames.", "Our algorithm is the top performer in multi-shot mode, as our RTA unit effectively learns to re-weight the most effective frames based on classification-specific reward. The split-rate RGB-to-Depth transfer enables our method to leverage on RGB data effectively and provides discriminative depth-based ReID features. This is especially reflected by the single-shot accuracy on DPI-T, where we report 19.3\\% better top-1 accuracy compared to 3D RAM.", "\\paragraph{BIWI. In order to explore sequences with varying human pose and scale, we use BIWI~, where 50 individuals appear in a living room. 28 of them are re-recorded in a different room with new clothes and walking patterns.", "Otherwise, the log-probability decreases for sequence of frames that lead to low reward. All in all, the agent jointly optimizes the accumulated reward and the classification loss, which constitute a hybrid supervised objective. Experiments Depth-based Datasets \\paragraph{DPI-T (Depth-based Person Identification from Top).", "Introduction Person re-identification is a fundamental problem in automated video surveillance and has attracted significant attention in recent years~. When a person is captured by cameras with non-overlapping views, or by the same camera but over many days, the objective is to recognize them across views among a large number of imposters. This is a difficult problem because of the visual ambiguity in a person's appearance due to large variations in illumination, human pose, camera settings and viewpoint.", "Additionally, re-identification systems have to be robust to partial occlusions and cluttered background. Multi-person association has wide applicability and utility in areas such as robotics, multimedia, forensics, autonomous driving and cashier-free shopping. Related work Existing methods of person re-identification typically focus on designing invariant and discriminant features~, which can enable identification despite nuisance factors such as scale, location, partial occlusion and changing lighting conditions."], "referred_text_chunks": ["However, we make the assumption that the 32 subjects that participate in the second recording are known for all competing methods. In Table~ we show that re-identification from body depth is more robust than from body RGB~, presenting 6.2\\% higher top-1 accuracy and 10.7\\% larger nAUC in single-shot mode. Next, we explore the benefit of using head information, which is less sensitive than clothes to day-by-day changes."]}, "1": {"question": "How does the proposed split-rate RGB-to-Depth transfer scheme differ from the R3D [90] method of Yosinski et al.?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 7, 'Rationale': 'Image 0 explicitly compares the proposed method with Yosinski et al. [90], detailing the differences in layer treatment and learning rates. Text 7 discusses the core principles of the split-rate transfer methodology, highlighting differences from Yosinski\u2019s approach.', 'Answer': 'The split-rate RGB-to-Depth transfer scheme differs from Yosinski et al. [90] by using slower (or zero) learning rate for the bottom layers and higher for the top layers, and only fine-tuning the top layers. In contrast, Yosinski et al. fine-tune all layers with a higher rate only for the classifier.'}", "all_figures_names": ["1705.09882v2-Figure4-1.png", "1705.09882v2-Figure7-1.png", "1705.09882v2-Figure2-1.png", "1705.09882v2-Table2-1.png", "1705.09882v2-Figure5-1.png", "1705.09882v2-Figure6-1.png", "1705.09882v2-Figure3-1.png", "1705.09882v2-Figure1-1.png"], "referred_figures_names": ["1705.09882v2-Figure4-1.png"], "answer": "The proposed split-rate RGB-to-Depth transfer scheme differs from the R3D [90] method in two ways. First, the proposed method uses a different learning rate for the bottom three layers of the network. Second, the proposed method uses a different initialization for the weights of the bottom three layers of the network.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Our split-rate RGB-to-depth transfer scheme is drawn in Fig.~. We show in Fig.~ that our method outperforms a popular fine-tuning method by more effectively utilizing pre-trained models from RGB data.\\\\ \\indent iii) Extensive experiments in Sec.~ not only show the superiority of our method compared to the state of the art in depth-based person re-identification from video, but also tackle a challenging application scenario where the persons wear clothes that were unseen during training. In Table~ we demonstrate the robustness of our method compared to its RGB-based counterpart and the mutual gains when jointly using the person's head information. Our Method Input Representation", "As it is unclear whether and which subset of RGB parameters is beneficial for depth embedding, we first gain insight from work by Yosinski et al.~ in CNN feature transferability. They showed that between two almost equal-sized splits from Imagenet~, the most effective model adaptation is to transfer and slowly fine-tune the weights of the bottom convolutional layers, while re-training the top layers. Other works that tackle model transfer from a large to a small-sized dataset (e.g.~) copy and slowly fine-tune the weights of the whole hierarchy except for the classifier which is re-trained using a higher learning rate.", "Our algorithm is the top performer in multi-shot mode, as our RTA unit effectively learns to re-weight the most effective frames based on classification-specific reward. The split-rate RGB-to-Depth transfer enables our method to leverage on RGB data effectively and provides discriminative depth-based ReID features. This is especially reflected by the single-shot accuracy on DPI-T, where we report 19.3\\% better top-1 accuracy compared to 3D RAM.", "Inspired by both approaches, we investigate the model transferability between RGB and depth. Our method has three differences compared to~. First, we found that even though RGB and depth are quite different modalities (cf.~Fig.~), the bottom layers of the RGB models can be shared with the depth data (without fine-tuning). Second, fine-tuning parameters transferred from RGB works better than training from scratch for the top layers.", "To address the data scarcity problem, we propose split-rate RGB-depth transfer to effectively leverage pre-trained models from large RGB data and learn strong frame-level features. To enhance re-identification from video sequences, we propose the Reinforced Temporal Attention unit, which lies on top of the frame-level features and is not dependent on the network architecture. Extensive experiments show that our approach outperforms the state of the art in depth-based person re-identification, and it is more effective than its RGB-based counterpart in a scenario where the persons change clothes.", "Our algorithm has several key differences from this work. First, observing that there are large amount of RGB data available for training frame-level person ReID models, we transfer parameters from pre-trained RGB models with an improved transfer scheme. Second, since the input to our frame-level model is the entire body region, we do not have any visibility constraints at a cost of resolution.", "For end-to-end training, we use momentum \\mu=0.9, batch size 50 and learning rate that linearly decreases from 0.01 to 0.0001 in 200 epochs up to 250 epochs maximum duration. The LSTM history consists of \\rho=3 frames. Evaluation of the Split-Rate RGB-to-Depth Transfer In Fig.~ we show results of our split-rate RGB-to-Depth transfer (which is described in Sec.~) compared to~.", "Third, using slower (or zero) learning rate for the bottom layers and higher for the top layers is more effective than using uniform rate across the hierarchy. Thus, we term our method as split-rate transfer. This first and third remarks also consist key differences with~, as firstly they fine-tune all layers and secondly they deploy higher learning rate only for the classifier."], "referred_text_chunks": ["Our split-rate RGB-to-depth transfer scheme is drawn in Fig.~. We show in Fig.~ that our method outperforms a popular fine-tuning method by more effectively utilizing pre-trained models from RGB data.\\\\ \\indent iii) Extensive experiments in Sec.~ not only show the superiority of our method compared to the state of the art in depth-based person re-identification from video, but also tackle a challenging application scenario where the persons wear clothes that were unseen during training. In Table~ we demonstrate the robustness of our method compared to its RGB-based counterpart and the mutual gains when jointly using the person's head information. Our Method Input Representation"]}, "2": {"question": "What is the difference between the grayscale depth representation and the result after background subtraction?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': \"Image 7 shows the grayscale depth representation and the result after background subtraction, aligning directly with the question. Text 0 discusses the process of background subtraction, providing details on how it's achieved with the depth region and mask application.\", 'Answer': \"The grayscale depth representation shows the depth information without processing, while the result after background subtraction removes the background using body index information, isolating the subject.\"}", "all_figures_names": ["1705.09882v2-Figure7-1.png", "1705.09882v2-Figure3-1.png", "1705.09882v2-Figure4-1.png", "1705.09882v2-Table1-1.png", "1705.09882v2-Figure6-1.png", "1705.09882v2-Table2-1.png", "1705.09882v2-Figure5-1.png", "1705.09882v2-Figure2-1.png"], "referred_figures_names": ["1705.09882v2-Figure2-1.png"], "answer": " The grayscale depth representation shows the depth of each pixel in the image, with darker pixels representing closer objects and lighter pixels representing further objects. The result after background subtraction shows only the foreground object, with the background removed.", "figure_type": "** photograph(s)", "content_type": "figure", "text_chunks": ["This results in the ``grayscale\" person representation {\\bf D_p^g. When the body index is available, we deploy {\\bf B_p \\subset{\\bf B as mask on the depth region {\\bf D_p in order to achieve background subtraction before applying range normalization (see Fig.~). Model Structure", "Inspired by both approaches, we investigate the model transferability between RGB and depth. Our method has three differences compared to~. First, we found that even though RGB and depth are quite different modalities (cf.~Fig.~), the bottom layers of the RGB models can be shared with the depth data (without fine-tuning). Second, fine-tuning parameters transferred from RGB works better than training from scratch for the top layers.", "They reported that the bottom layers cannot be shared between RGB and depth models and it is better to retrain them from scratch. Our conjecture is that this behavior is in part specific to the HHA depth encoding~, which is not used in our representation. Some recent works in natural language processing~ explore temporal attention in order to keep track of long-range structural dependencies.", "Haque et al.~ designed a glimpse layer and used a 4D convolutional autoencoder in order to compress the 4D spatiotemporal input video representation, while the next spatial location (glimpse) is inferred within a recurrent attention framework using reinforcement learning~. However, for small patches (at the glimpse location), the model loses sight of the overall body shape, while for large patches, it loses the depth resolution. Achieving a good trade-off between visibility and resolution within the objective of compressing the input space to tractable levels is hard with limited data.", "Our algorithm has several key differences from this work. First, observing that there are large amount of RGB data available for training frame-level person ReID models, we transfer parameters from pre-trained RGB models with an improved transfer scheme. Second, since the input to our frame-level model is the entire body region, we do not have any visibility constraints at a cost of resolution.", "As it is unclear whether and which subset of RGB parameters is beneficial for depth embedding, we first gain insight from work by Yosinski et al.~ in CNN feature transferability. They showed that between two almost equal-sized splits from Imagenet~, the most effective model adaptation is to transfer and slowly fine-tune the weights of the bottom convolutional layers, while re-training the top layers. Other works that tackle model transfer from a large to a small-sized dataset (e.g.~) copy and slowly fine-tune the weights of the whole hierarchy except for the classifier which is re-trained using a higher learning rate.", "\\subsubsection{Frame-level Feature Embedding f_{CNN(\\theta_g): Given that there is little depth data but a large amount of RGB data available for person re-identification, we would like to leverage the RGB data to train depth models for frame-level feature extraction. We discovered that the parameters at the bottom convolutional layers of a deep neural network can be directly shared between RGB and depth data (cf.~Sec.~) through a simple depth encoding, that is, each pixel with depth D is replicated to three channels and encoded as (D,D,D), which corresponds to the three RGB channels.", "We show the top-1 re-identification accuracy on DPI-T when the bottom CNN layers are frozen (left) and slowly fine-tuned (right). The top layers are transferred from RGB and rapidly fine-tuned in our approach, while they were re-trained in~. Given that the CNN architecture has 7 main layers before the classifier, the x axis is the number of layers that are frozen or fine-tuned counting from the bottom. Evidently, transferring and freezing the three bottom layers, while rapidly fine-tuning the subsequent ``inception\" and fully-connected layers, brings in the best performance on DPI-T. Attempting to freeze too many layers leads to performance drop for both approaches, which can been attributed to feature specificity."], "referred_text_chunks": ["This results in the ``grayscale\" person representation {\\bf D_p^g. When the body index is available, we deploy {\\bf B_p \\subset{\\bf B as mask on the depth region {\\bf D_p in order to achieve background subtraction before applying range normalization (see Fig.~). Model Structure"]}, "3": {"question": "Which modality achieved the highest top-1 accuracy in the multi-shot evaluation on TUM-GAID?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': \"Image 3 provides the top-1 accuracy for different modalities on the TUM-GAID dataset in multi-shot mode. Text chunk 0 discusses the RTA unit's effectiveness and higher accuracy using RGB-to-Depth transfer, relevant to the question's context about modality performance. Image 3 directly includes the relevant accuracy data, while Text 0 explains the technique and performance improvement.\", 'Answer': 'Body Depth & Head RGB (ms: LSTM & RTA) achieved the highest top-1 accuracy with 75.0%.'}", "all_figures_names": ["1705.09882v2-Figure6-1.png", "1705.09882v2-Table1-1.png", "1705.09882v2-Figure4-1.png", "1705.09882v2-Table2-1.png", "1705.09882v2-Figure3-1.png", "1705.09882v2-Figure1-1.png", "1705.09882v2-Figure7-1.png", "1705.09882v2-Figure5-1.png"], "referred_figures_names": ["1705.09882v2-Table2-1.png"], "answer": "Body Depth & Head RGB (ms: LSTM & RTA)", "figure_type": "table", "content_type": "table", "text_chunks": ["Our algorithm is the top performer in multi-shot mode, as our RTA unit effectively learns to re-weight the most effective frames based on classification-specific reward. The split-rate RGB-to-Depth transfer enables our method to leverage on RGB data effectively and provides discriminative depth-based ReID features. This is especially reflected by the single-shot accuracy on DPI-T, where we report 19.3\\% better top-1 accuracy compared to 3D RAM.", "To evaluate on a large pool of identities, we use the TUM-GAID database~, which contains RGB and depth video for 305 people in three variations. A subset of 32 people is recorded a second time after three months with different clothes, which makes it ideal for our application scenario in Sec.~. In our experiments we use the ``normal\" sequences (n) from each recording. Evaluation Metrics Top-k accuracy equals the percentage of test images or sequences for which the ground-truth label is contained within the first k model predictions.", "To evaluate our method when shorter video sequences are available, we use IIT PAVIS~. This dataset includes 79 persons that are recorded in 5-frame walking sequences twice. We use Walking1 and Walking2 sequences as the training and testing set, respectively. \\paragraph{TUM-GAID.", "Application in Scenario with Unseen Clothes Towards tackling our key motivation, we compare our system compared to a state-of-the-art RGB method in scenario where the individuals change clothes between the recordings for training and test set. We use the TUM-GAID database at which 305 persons appear in sequences n01--n06 from session 1, and 32 among them appear with new clothes in sequences n07--n12 from session 2. Following the official protocol, we use the Training IDs to perform RGB-to-Depth transfer for our CNN embedding.", "However, we make the assumption that the 32 subjects that participate in the second recording are known for all competing methods. In Table~ we show that re-identification from body depth is more robust than from body RGB~, presenting 6.2\\% higher top-1 accuracy and 10.7\\% larger nAUC in single-shot mode. Next, we explore the benefit of using head information, which is less sensitive than clothes to day-by-day changes.", "To that end, we transfer the RGB-based pre-trained model from~ and fine-tune on the upper body part, which we call ``Head RGB\". This results in increased accuracy, individually and jointly with body depth. Finally, we show the mutual benefits in multi-shot performance for both body depth, head RGB and their linear combination in class posterior.", "In Fig.~ we visualize the CMC curves for single-shot setting. We observe that ReID from body depth scales better than its counterparts, which is validated by the nAUC scores. Conclusion In the paper, we present a novel approach for depth-based person re-identification.", "The parametric layer maps the 256\\times1 hidden vector h_t to the N\\times1 class posterior vector c_t, which has length equal to the number of classes N. The multi-shot prediction with RTA attention is the weighted sum of frame-level predictions c_t, as they are weighted by the normalized, RTA weights w_t' = \\frac{f_w(g_t;\\theta_w){\\sum_{t=1^T f_w(g_t;\\theta_w) . The Bernoulli-Sigmoid unit is stochastic during training and therefore we resort to the REINFORCE algorithm in order to obtain the gradient for the backward pass."], "referred_text_chunks": ["Our algorithm is the top performer in multi-shot mode, as our RTA unit effectively learns to re-weight the most effective frames based on classification-specific reward. The split-rate RGB-to-Depth transfer enables our method to leverage on RGB data effectively and provides discriminative depth-based ReID features. This is especially reflected by the single-shot accuracy on DPI-T, where we report 19.3\\% better top-1 accuracy compared to 3D RAM."]}, "4": {"question": "What is the relationship between the Bernoulli parameter and the image?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 0, 'Rationale': 'Image 2 directly illustrates Bernoulli parameters on example frames, showing their variations. Text 0 discusses the variations of Bernoulli parameters in frames, specifying their relation to noisy frames and poses.', 'Answer': 'The Bernoulli parameter relates to the image by indicating the reliability or noisiness of each frame. Lower values often correspond to noisy or occluded frames.'}", "all_figures_names": ["1705.09882v2-Table2-1.png", "1705.09882v2-Figure3-1.png", "1705.09882v2-Figure6-1.png", "1705.09882v2-Figure1-1.png", "1705.09882v2-Figure4-1.png", "1705.09882v2-Figure2-1.png", "1705.09882v2-Table1-1.png", "1705.09882v2-Figure7-1.png"], "referred_figures_names": ["1705.09882v2-Figure6-1.png"], "answer": "The Bernoulli parameter is a measure of the probability of a pixel being foreground or background. The higher the Bernoulli parameter, the more likely the pixel is to be foreground. This is reflected in the images, where the pixels with higher Bernoulli parameters are more likely to be part of the person's silhouette.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Next in Fig.~ we show a testing sequence with the predicted Bernoulli parameter f_w(g_t;\\theta_w) printed. After inspecting the Bernoulli parameter value on testing sequences, we observe large variations even among neighboring frames. Smaller values are typically associated with noisy frames, or frames with unusual pose (e.g. person turning) and partial occlusions.", "This module consists of a linear layer which maps the 256\\times1 vector g_t to one scalar, followed by Sigmoid non-linearity which squashes real-valued inputs to a [0,1] range. Next, the output w_t is defined by a Bernoulli random variable with probability mass function: \\end{equation \\noindent The Bernoulli parameter is conditioned on the Sigmoid output f_w(g_t;\\theta_w), shaping a Bernoulli-Sigmoid unit~.", "= \\mathbb{E_{\\pi[R_t], as the mean square error between R_t^i and b_t is also minimized by backpropagation. At each step t, the agent makes a prediction w_t and the reward signal R_t^i evaluates the effectiveness of the agent for the classification task. The REINFORCE update increases the log-probability of an action that results in higher than the expected accumulated reward (i.e. by increasing the Bernoulli parameter f_w(g_t;\\theta_w)).", "During training, the output w_t is sampled stochastically to be a binary value in \\{0,1\\. During evaluation, instead of sampling from the distribution, the output is deterministically decided to be equal to the Bernoulli parameter and, therefore, w_t=f_w(g_t;\\theta_w). \\subsubsection{Classifier f_c(\\theta_c) and Reward: The classifier consists of a sequence of a rectified linear unit, dropout with rate r=0.4, a fully connected layer and Softmax.", "This involves calculating the expectation J(\\theta_g, \\theta_w)=\\mathbb{E_{p(s_{1:T; \\theta_g, \\theta_w)[R] over the distribution of all possible sequences p(s_{1:T; \\theta_g, \\theta_w), which is intractable. Thus, a sample approximation, known as the REINFORCE rule~, can be applied on the Bernoulli-Sigmoid unit~, which models the sub-policy \\pi_1(w_t | f_w(g_t;\\theta_w)). Given probability mass function \\log \\pi_1(w_t; p_t)=w_t\\log p_t+(1-w_t)\\log(1-p_t) with Bernoulli parameter p_t=f_w(g_t;\\theta_w), the gradient approximation is: where sequences i, i\\in \\{1,\\ldots,M\\, are obtained while running the agent for M episodes and R_t^i=\\sum_{\\tau=1^t r_{\\tau^i is the cumulative reward at episode i acquired after collecting the sample w_t^i.", "Being inspired by the work of Likas~ in online clustering and Kontoravdis et al.~ in exploration of binary domains, we model the weight of each frame prediction as a Bernoulli-sigmoid unit. We review our model in detail in Sec.~. Depth-based methods that use measurements from 3D skeleton data have emerged in order to infer anthropometric and human gait criteria . In an effort to leverage the full power of depth data, recent methods use 3D point clouds to estimate motion trajectories and the length of specific body parts~.", "The parametric layer maps the 256\\times1 hidden vector h_t to the N\\times1 class posterior vector c_t, which has length equal to the number of classes N. The multi-shot prediction with RTA attention is the weighted sum of frame-level predictions c_t, as they are weighted by the normalized, RTA weights w_t' = \\frac{f_w(g_t;\\theta_w){\\sum_{t=1^T f_w(g_t;\\theta_w) . The Bernoulli-Sigmoid unit is stochastic during training and therefore we resort to the REINFORCE algorithm in order to obtain the gradient for the backward pass.", "However, their training sets are in the order of 20,000 positive samples, i.e. two orders of magnitude smaller than Imagenet~, which has been successfully used for object recognition~. The small sample size problem is especially acute in person re-identification from temporal sequences , as the feature dimensionality increases linearly in the number of frames that are accumulated compared to the single-shot representations. On the other hand, explicitly modeling temporal dynamics and using multiple frames help algorithms to deal with noisy measurements, occlusions, adverse poses and lighting."], "referred_text_chunks": ["Next in Fig.~ we show a testing sequence with the predicted Bernoulli parameter f_w(g_t;\\theta_w) printed. After inspecting the Bernoulli parameter value on testing sequences, we observe large variations even among neighboring frames. Smaller values are typically associated with noisy frames, or frames with unusual pose (e.g. person turning) and partial occlusions."]}, "5": {"question": "Which part of the model is responsible for deciding which frames are most important for the re-identification task?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 4, 'Rationale': 'Image 6 clearly highlights the Reinforced Temporal Attention (RTA) unit, which is specifically responsible for deciding the importance of frames. Text chunk 4 discusses this RTA unit and its role in adaptively weighting frame predictions.', 'Answer': 'The Reinforced Temporal Attention (RTA) unit is responsible for deciding the importance of frames for the re-identification task.'}", "all_figures_names": ["1705.09882v2-Figure1-1.png", "1705.09882v2-Figure2-1.png", "1705.09882v2-Figure4-1.png", "1705.09882v2-Figure7-1.png", "1705.09882v2-Figure6-1.png", "1705.09882v2-Table1-1.png", "1705.09882v2-Figure3-1.png", "1705.09882v2-Table2-1.png"], "referred_figures_names": ["1705.09882v2-Figure3-1.png"], "answer": "The Reinforced Temporal Attention (RTA) unit.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["The problem is formulated as sequential decision process of an agent that performs human recognition from a partially observed environment via video sequences. At each time step, the agent observes the environment via depth camera, calculates a feature vector based on a deep Convolutional Neural Network (CNN) and actively infers the importance of the current frame for the re-identification task using novel Reinforced Temporal Attention (RTA). On top of the CNN features, a Long Short-Term Memory (LSTM) unit models short-range temporal dynamics.", "However, their training sets are in the order of 20,000 positive samples, i.e. two orders of magnitude smaller than Imagenet~, which has been successfully used for object recognition~. The small sample size problem is especially acute in person re-identification from temporal sequences , as the feature dimensionality increases linearly in the number of frames that are accumulated compared to the single-shot representations. On the other hand, explicitly modeling temporal dynamics and using multiple frames help algorithms to deal with noisy measurements, occlusions, adverse poses and lighting.", "Introduction Person re-identification is a fundamental problem in automated video surveillance and has attracted significant attention in recent years~. When a person is captured by cameras with non-overlapping views, or by the same camera but over many days, the objective is to recognize them across views among a large number of imposters. This is a difficult problem because of the visual ambiguity in a person's appearance due to large variations in illumination, human pose, camera settings and viewpoint.", "Additionally, re-identification systems have to be robust to partial occlusions and cluttered background. Multi-person association has wide applicability and utility in areas such as robotics, multimedia, forensics, autonomous driving and cashier-free shopping. Related work Existing methods of person re-identification typically focus on designing invariant and discriminant features~, which can enable identification despite nuisance factors such as scale, location, partial occlusion and changing lighting conditions.", "We propose novel reinforced temporal attention on top of the frame-level features to better leverage the temporal information from video sequences by learning to adaptively weight the predictions of individual frames based on a task-based reward. In Sec.~ we define the model, its end-to-end training is described in Sec.~, and comparisons with baselines are shown in Sec.~.\\\\ \\indent ii) We tackle the data scarcity problem in depth-based person re-identification by leveraging the large amount of RGB data to obtain stronger frame-level features.", "Our algorithm has several key differences from this work. First, observing that there are large amount of RGB data available for training frame-level person ReID models, we transfer parameters from pre-trained RGB models with an improved transfer scheme. Second, since the input to our frame-level model is the entire body region, we do not have any visibility constraints at a cost of resolution.", "Third, in order to better utilize the temporal information from video, we propose a novel reinforced temporal attention unit on top of the frame-level features which is guided by the task in order to predict the weights of individual frames into the final prediction. Our method for transferring a RGB Person ReID model to the depth domain is based on the key observation that the model parameters at the bottom layers of a deep convolutional neural network can be directly shared between RGB and depth data while the remaining upper layers need to be fine-tuned. At first glance, our observation is inconsistent with what was reported in the RGB-D object recognition approach by Song et al.~.", "The parametric layer maps the 256\\times1 hidden vector h_t to the N\\times1 class posterior vector c_t, which has length equal to the number of classes N. The multi-shot prediction with RTA attention is the weighted sum of frame-level predictions c_t, as they are weighted by the normalized, RTA weights w_t' = \\frac{f_w(g_t;\\theta_w){\\sum_{t=1^T f_w(g_t;\\theta_w) . The Bernoulli-Sigmoid unit is stochastic during training and therefore we resort to the REINFORCE algorithm in order to obtain the gradient for the backward pass."], "referred_text_chunks": ["The problem is formulated as sequential decision process of an agent that performs human recognition from a partially observed environment via video sequences. At each time step, the agent observes the environment via depth camera, calculates a feature vector based on a deep Convolutional Neural Network (CNN) and actively infers the importance of the current frame for the re-identification task using novel Reinforced Temporal Attention (RTA). On top of the CNN features, a Long Short-Term Memory (LSTM) unit models short-range temporal dynamics."]}, "6": {"question": "How does the performance of our RGB-to-Depth transfer compare to Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 1, 'Rationale': 'Image 4 and Text 1 provide detailed comparison and ablation study of the RGB-to-Depth transfer methods, specifically comparing our method with Yosinski et al. in terms of top-1 accuracy.', 'Answer': 'Our RGB-to-Depth transfer method achieves better top-1 accuracy on DPI-T compared to Yosinski et al. when all layers are fine-tuned.'}", "all_figures_names": ["1705.09882v2-Figure4-1.png", "1705.09882v2-Figure7-1.png", "1705.09882v2-Figure6-1.png", "1705.09882v2-Table2-1.png", "1705.09882v2-Figure5-1.png", "1705.09882v2-Figure3-1.png", "1705.09882v2-Figure1-1.png", "1705.09882v2-Figure2-1.png"], "referred_figures_names": ["1705.09882v2-Figure5-1.png"], "answer": "The proposed RGB-to-Depth transfer performs slightly better than Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["As it is unclear whether and which subset of RGB parameters is beneficial for depth embedding, we first gain insight from work by Yosinski et al.~ in CNN feature transferability. They showed that between two almost equal-sized splits from Imagenet~, the most effective model adaptation is to transfer and slowly fine-tune the weights of the bottom convolutional layers, while re-training the top layers. Other works that tackle model transfer from a large to a small-sized dataset (e.g.~) copy and slowly fine-tune the weights of the whole hierarchy except for the classifier which is re-trained using a higher learning rate.", "We show the top-1 re-identification accuracy on DPI-T when the bottom CNN layers are frozen (left) and slowly fine-tuned (right). The top layers are transferred from RGB and rapidly fine-tuned in our approach, while they were re-trained in~. Given that the CNN architecture has 7 main layers before the classifier, the x axis is the number of layers that are frozen or fine-tuned counting from the bottom. Evidently, transferring and freezing the three bottom layers, while rapidly fine-tuning the subsequent ``inception\" and fully-connected layers, brings in the best performance on DPI-T. Attempting to freeze too many layers leads to performance drop for both approaches, which can been attributed to feature specificity.", "Our algorithm is the top performer in multi-shot mode, as our RTA unit effectively learns to re-weight the most effective frames based on classification-specific reward. The split-rate RGB-to-Depth transfer enables our method to leverage on RGB data effectively and provides discriminative depth-based ReID features. This is especially reflected by the single-shot accuracy on DPI-T, where we report 19.3\\% better top-1 accuracy compared to 3D RAM.", "Inspired by both approaches, we investigate the model transferability between RGB and depth. Our method has three differences compared to~. First, we found that even though RGB and depth are quite different modalities (cf.~Fig.~), the bottom layers of the RGB models can be shared with the depth data (without fine-tuning). Second, fine-tuning parameters transferred from RGB works better than training from scratch for the top layers.", "Our split-rate RGB-to-depth transfer scheme is drawn in Fig.~. We show in Fig.~ that our method outperforms a popular fine-tuning method by more effectively utilizing pre-trained models from RGB data.\\\\ \\indent iii) Extensive experiments in Sec.~ not only show the superiority of our method compared to the state of the art in depth-based person re-identification from video, but also tackle a challenging application scenario where the persons wear clothes that were unseen during training. In Table~ we demonstrate the robustness of our method compared to its RGB-based counterpart and the mutual gains when jointly using the person's head information. Our Method Input Representation", "First, we replace the top classification layer with a 256\\times N fully connected layer, where N is the number of subjects at the target dataset and its weights are initialized at random from a zero-mean Gaussian distribution with standard deviation 0.01. Second, we add dropout regularization between the fully-connected layers. In Sec.~ we demonstrate an effective way to transfer the model parameters from RGB to Depth.", "To that end, we transfer the RGB-based pre-trained model from~ and fine-tune on the upper body part, which we call ``Head RGB\". This results in increased accuracy, individually and jointly with body depth. Finally, we show the mutual benefits in multi-shot performance for both body depth, head RGB and their linear combination in class posterior.", "Our algorithm has several key differences from this work. First, observing that there are large amount of RGB data available for training frame-level person ReID models, we transfer parameters from pre-trained RGB models with an improved transfer scheme. Second, since the input to our frame-level model is the entire body region, we do not have any visibility constraints at a cost of resolution."], "referred_text_chunks": ["As it is unclear whether and which subset of RGB parameters is beneficial for depth embedding, we first gain insight from work by Yosinski et al.~ in CNN feature transferability. They showed that between two almost equal-sized splits from Imagenet~, the most effective model adaptation is to transfer and slowly fine-tune the weights of the bottom convolutional layers, while re-training the top layers. Other works that tackle model transfer from a large to a small-sized dataset (e.g.~) copy and slowly fine-tune the weights of the whole hierarchy except for the classifier which is re-trained using a higher learning rate."]}, "7": {"question": "What is the difference between the filter responses from the \u201cconv1\u201d, \u201cconv2\u201d and \u201cconv3\u201d layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB and the feature embedding fCNN of a framework that utilizes depth data?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 directly shows the filter responses in conv layers for RGB and depth frames, which is critical to understanding the differences between the two modalities. Text 0 discusses the use of RGB-to-depth transfers and how the network layers interact, which aligns with the question context.', 'Answer': 'The filter responses in conv1, conv2, and conv3 layers differ between RGB and depth data. Image 7 demonstrates these differences visually, where the depth data uses a different encoding in the conv layers to accommodate the lack of color information inherent in RGB data.'}", "all_figures_names": ["1705.09882v2-Table2-1.png", "1705.09882v2-Figure3-1.png", "1705.09882v2-Figure6-1.png", "1705.09882v2-Figure7-1.png", "1705.09882v2-Figure2-1.png", "1705.09882v2-Table1-1.png", "1705.09882v2-Figure5-1.png", "1705.09882v2-Figure1-1.png"], "referred_figures_names": ["1705.09882v2-Figure1-1.png"], "answer": "The filter responses from the \u201cconv1\u201d, \u201cconv2\u201d and \u201cconv3\u201d layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB are more detailed and contain more information than the filter responses from the fCNN of a framework that utilizes depth data. This is because RGB images contain more information than depth images.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["\\subsubsection{Frame-level Feature Embedding f_{CNN(\\theta_g): Given that there is little depth data but a large amount of RGB data available for person re-identification, we would like to leverage the RGB data to train depth models for frame-level feature extraction. We discovered that the parameters at the bottom convolutional layers of a deep neural network can be directly shared between RGB and depth data (cf.~Sec.~) through a simple depth encoding, that is, each pixel with depth D is replicated to three channels and encoded as (D,D,D), which corresponds to the three RGB channels.", "Application in Scenario with Unseen Clothes Towards tackling our key motivation, we compare our system compared to a state-of-the-art RGB method in scenario where the individuals change clothes between the recordings for training and test set. We use the TUM-GAID database at which 305 persons appear in sequences n01--n06 from session 1, and 32 among them appear with new clothes in sequences n07--n12 from session 2. Following the official protocol, we use the Training IDs to perform RGB-to-Depth transfer for our CNN embedding.", "Our algorithm has several key differences from this work. First, observing that there are large amount of RGB data available for training frame-level person ReID models, we transfer parameters from pre-trained RGB models with an improved transfer scheme. Second, since the input to our frame-level model is the entire body region, we do not have any visibility constraints at a cost of resolution.", "Third, in order to better utilize the temporal information from video, we propose a novel reinforced temporal attention unit on top of the frame-level features which is guided by the task in order to predict the weights of individual frames into the final prediction. Our method for transferring a RGB Person ReID model to the depth domain is based on the key observation that the model parameters at the bottom layers of a deep convolutional neural network can be directly shared between RGB and depth data while the remaining upper layers need to be fine-tuned. At first glance, our observation is inconsistent with what was reported in the RGB-D object recognition approach by Song et al.~.", "To evaluate on a large pool of identities, we use the TUM-GAID database~, which contains RGB and depth video for 305 people in three variations. A subset of 32 people is recorded a second time after three months with different clothes, which makes it ideal for our application scenario in Sec.~. In our experiments we use the ``normal\" sequences (n) from each recording. Evaluation Metrics Top-k accuracy equals the percentage of test images or sequences for which the ground-truth label is contained within the first k model predictions.", "To address the data scarcity problem, we propose split-rate RGB-depth transfer to effectively leverage pre-trained models from large RGB data and learn strong frame-level features. To enhance re-identification from video sequences, we propose the Reinforced Temporal Attention unit, which lies on top of the frame-level features and is not dependent on the network architecture. Extensive experiments show that our approach outperforms the state of the art in depth-based person re-identification, and it is more effective than its RGB-based counterpart in a scenario where the persons change clothes.", "Inspired by both approaches, we investigate the model transferability between RGB and depth. Our method has three differences compared to~. First, we found that even though RGB and depth are quite different modalities (cf.~Fig.~), the bottom layers of the RGB models can be shared with the depth data (without fine-tuning). Second, fine-tuning parameters transferred from RGB works better than training from scratch for the top layers.", "Next, we first describe our transfer learning for the frame-level embedding and following the hybrid supervised training algorithm for the recursive model with temporal attention. \\subsubsection{Split-rate Transfer Learning for Feature Embedding f_{CNN(\\theta_g): In order to leverage on vast RGB data, our approach relies on transferring parameters \\theta_g from a RGB pre-trained model for initialization."], "referred_text_chunks": ["\\subsubsection{Frame-level Feature Embedding f_{CNN(\\theta_g): Given that there is little depth data but a large amount of RGB data available for person re-identification, we would like to leverage the RGB data to train depth models for frame-level feature extraction. We discovered that the parameters at the bottom convolutional layers of a deep neural network can be directly shared between RGB and depth data (cf.~Sec.~) through a simple depth encoding, that is, each pixel with depth D is replicated to three channels and encoded as (D,D,D), which corresponds to the three RGB channels."]}}