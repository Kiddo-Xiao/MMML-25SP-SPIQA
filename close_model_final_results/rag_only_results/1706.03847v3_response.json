{"0": {"question": "How does the alpha parameter affect recommendation accuracy for different sample sizes on the CLASS dataset?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 2, 'Rationale': 'Image 6 directly shows the impact of the alpha parameter on recommendation accuracy for different sample sizes on the CLASS dataset with various losses. Text 2 explains the effect of the alpha parameter in relation to sample sizes and its impact on sampling methods.', 'Answer': 'The alpha parameter affects recommendation accuracy by favoring higher values at low sample sizes, which helps initially, but lower values are beneficial as sample sizes increase, providing more balanced sampling.'}", "all_figures_names": ["1706.03847v3-Figure4-1.png", "1706.03847v3-Table2-1.png", "1706.03847v3-Figure3-1.png", "1706.03847v3-Figure6-1.png", "1706.03847v3-Figure1-1.png", "1706.03847v3-Figure2-1.png", "1706.03847v3-Figure5-1.png", "1706.03847v3-Table3-1.png"], "referred_figures_names": ["1706.03847v3-Figure5-1.png"], "answer": "The alpha parameter generally increases recommendation accuracy as the sample size increases. However, the effect of alpha varies depending on the loss function used. For cross-entropy loss, alpha has a relatively small effect on accuracy. For TOP1-max loss, alpha has a larger effect on accuracy, especially for smaller sample sizes. For BPR-max loss, alpha has a very large effect on accuracy, with higher values of alpha leading to much higher accuracy.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["\\caption{The effect of the alpha parameter on recommendation accuracy at different sample sizes on the CLASS dataset. Left: cross-entropy loss; Middle: TOP1-max loss; Right: BPR-max loss.", "Code is available publicly on GitHub for reproducibility. Using additional samples The first set of experiments examines the effect of additional negative samples on recommendation accuracy. Experiments were performed on the CLASS and the VIDEO datasets.", "Cross-entropy favors higher \\alpha values with low sample sizes and low \\alpha values for large samples. This is inline with our discussion in Section~: popular samples are useful when the sample size is very limited and at the beginning of the training, but might be exhausted quickly, thus switching to a more balanced sampling can be beneficial if we have the means to (e.g.\\ large enough sample size). Also, the uniform sampling in this case is supplemented by the few popularity based samples of the mini-batch sampling.", "These additional samples are used along with the N_B-1 samples coming from the mini-batch based sampling (described above). The additional samples can be sampled in any way. We chose to sample proportional to \\mathrm{supp_i^\\alpha, where \\mathrm{supp_i is the support of the item and \\alpha is the parameter of the sampling (0\\le\\alpha\\le1). \\alpha=0 and \\alpha=1 gives uniform and popularity-based sampling respectively.", "Since results are quite similar we excluded the VIDEO results to save some space. Figure~ depicts the performance of the network with TOP1, cross-entropy, TOP1-max and BPR-max losses. Recommendation accuracy was measured with different number of additional samples, as well as in the case when all scores are computed and there is no sampling.", "If the number of items is large, the small sample size further hinders the chance of including all of the high scoring negative examples. (2) Mini-batch size has a direct effect on the training. E.g.\\ we found that training with smaller mini-batch sizes (30-100) produces more accurate models, but training with larger ones is faster on the GPU due to parallelization.", "\\end{figure* In the next experiment we perform a parameter sensitivity analysis of the \\alpha parameter that controls the sampling. Figure~ depicts the performance over different \\alpha values for the cross-entropy, TOP1-max and BPR-max losses.", "Mini-batch sampling is basically a form of popularity-based sampling, since the training iterates through all events, thus the probability of an item acting as a negative sample is proportional to its support. The problem with popularity-based sampling is that learning can slow down after the algorithm learns to (generally) rank target items above popular ones, and thus can still be inaccurate for ranking long tail high scoring items. On the other hand, uniform sampling slows down learning, due to the high number of low scoring negative samples, but might produce an overall more accurate model if trained indefinitely."], "referred_text_chunks": ["\\caption{The effect of the alpha parameter on recommendation accuracy at different sample sizes on the CLASS dataset. Left: cross-entropy loss; Middle: TOP1-max loss; Right: BPR-max loss."]}, "1": {"question": "Which combination of method and dataset achieved the highest Recall@20 score, and how much higher was it compared to the original GRU4Rec model?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 provides detailed Recall@20 scores with corresponding improvements over the original GRU4Rec, allowing us to identify the highest score and its comparison. Text 0 explains improvements in recall with data augmentation.', 'Answer': 'The highest Recall@20 score was achieved on the VIDXL dataset using BPR-max with additional samples, achieving 0.8058, which is a 28.66% improvement over the original GRU4Rec model.'}", "all_figures_names": ["1706.03847v3-Figure3-1.png", "1706.03847v3-Figure5-1.png", "1706.03847v3-Table3-1.png", "1706.03847v3-Table1-1.png", "1706.03847v3-Figure6-1.png", "1706.03847v3-Figure1-1.png", "1706.03847v3-Figure2-1.png", "1706.03847v3-Table2-1.png"], "referred_figures_names": ["1706.03847v3-Table2-1.png"], "answer": "The highest Recall@20 score was achieved by the GRU4Rec with additional samples and BPR-max loss function on the RSC15 dataset. This score was 42.37% higher than the Recall@20 score of the original GRU4Rec model on the same dataset.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["The increase with sampling and the proper loss function is stunning as the best results exceed the accuracy of the original GRU4Rec by 18-37.5\\% and that of item-kNN by up to 55\\%. BPR-max performs similarly (2 of 4) or better (2 of 4; +2-6\\% improvement) than cross-entropy when extra samples are used for both method. On RSC15, reported \\sim0.685 and \\sim0.29 in recall@20 and MRR@20 respectively using data augmentation.", "Unlike our solutions, data augmentation greatly increases training times. Data augmentation and our improvements are not mutually exclusive, thus it is possible that by combining the two methods, even better results can be achieved. A recent paper proposes the Bayesian version of GRU4Rec and reports \\sim0.61 and \\sim0.25 in recall@20 and MRR@20 when using 100 units.", "We essentially devise a new class of loss functions that combines learnings from the deep learning and the learning to rank literature. Experimental results on several datasets coming from industry validate these improvements by showing significant increase in recommendation accuracy measured by Mean Reciprocal Rank (MRR) and Recall@20. With these improvements the difference between RNNs and conventional memory-based collaborative filtering jumps to 53\\% in terms of MRR and Recall@20, demonstrating the potential that deep learning methods bring to the area of Recommender Systems.", "Results: Figure~ shows the relative performance gain of GRU4Rec over the complex logic. The error bars represent the confidence interval at p=0.05. GRU4Rec outperforms the baseline in both prediction modes.", "The trend is similar on the VIDEO dataset, with training times starting around 20 minutes, starting to increase at 2048 extra samples (to 24 minutes) and quickly above thereafter. This means that the proposed method can be used with zero to little additional cost in practice, unlike data augmentation methods. It is also clear that GRU4Rec can work just as well with a few thousands of negative examples as with the whole itemset, thus it can be kept scalable.", "Preliminary experiments (Table~) show additional slight improvements in recall@20 and slight decrease in MRR@20 for most of the datasets. However, for the CLASS dataset both recall and MRR are increased significantly when unified embeddings are used (+18.74\\% and +29.44\\% in recall and MRR respectively, compared to the model trained without embeddings). Unified embeddings have the additional benefit of reducing the overall memory footprint and the model size by a factor of \\sim4.", "We extend this sampling with an option for additional samples and argue that this is crucial for the increased recommendation accuracy we achieve (up to 53\\% improvement). In each training step, GRU4Rec takes the item of the current event in the session -- represented by a one-hot vector -- as an input. The output of the network is a set of scores over the items, corresponding to their likelihood of being the next item in the session.", "Therefore GRU4Rec uses a sampling mechanism and computes the scores for only a subset of the items during training. Instead of making a forward and backward pass with one training example only and then moving to the next, the network is fed with a bundle of examples and is trained on the mean gradient. This common practice is called mini-batch training and has several benefits, e.g.\\ utilizing the parallelization capabilities of current hardware better, thus training faster, and producing more stable gradients than stochastic gradient training and thus converging faster."], "referred_text_chunks": ["The increase with sampling and the proper loss function is stunning as the best results exceed the accuracy of the original GRU4Rec by 18-37.5\\% and that of item-kNN by up to 55\\%. BPR-max performs similarly (2 of 4) or better (2 of 4; +2-6\\% improvement) than cross-entropy when extra samples are used for both method. On RSC15, reported \\sim0.685 and \\sim0.29 in recall@20 and MRR@20 respectively using data augmentation."]}, "2": {"question": "What is the performance of GRU4Rec relative to the baseline in terms of watch time?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 directly compares the relative performance of GRU4Rec to the baseline in terms of watch time along with other KPIs. Text chunk 0 indicates relative performance gain, which provides context for understanding the improvements of GRU4Rec over a baseline.', 'Answer': 'GRU4Rec outperforms the baseline in terms of watch time, as shown in the graph with performance metrics above 100%.'}", "all_figures_names": ["1706.03847v3-Table3-1.png", "1706.03847v3-Figure4-1.png", "1706.03847v3-Table2-1.png", "1706.03847v3-Figure6-1.png", "1706.03847v3-Figure3-1.png", "1706.03847v3-Figure2-1.png", "1706.03847v3-Table1-1.png", "1706.03847v3-Figure1-1.png"], "referred_figures_names": ["1706.03847v3-Figure6-1.png"], "answer": "GRU4Rec has a slightly higher performance than the baseline in terms of watch time.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Results: Figure~ shows the relative performance gain of GRU4Rec over the complex logic. The error bars represent the confidence interval at p=0.05. GRU4Rec outperforms the baseline in both prediction modes.", "The trend is similar on the VIDEO dataset, with training times starting around 20 minutes, starting to increase at 2048 extra samples (to 24 minutes) and quickly above thereafter. This means that the proposed method can be used with zero to little additional cost in practice, unlike data augmentation methods. It is also clear that GRU4Rec can work just as well with a few thousands of negative examples as with the whole itemset, thus it can be kept scalable.", "Therefore GRU4Rec uses a sampling mechanism and computes the scores for only a subset of the items during training. Instead of making a forward and backward pass with one training example only and then moving to the next, the network is fed with a bundle of examples and is trained on the mean gradient. This common practice is called mini-batch training and has several benefits, e.g.\\ utilizing the parallelization capabilities of current hardware better, thus training faster, and producing more stable gradients than stochastic gradient training and thus converging faster.", "The third group is served by GRU4Rec in ``sequence mode'' where GRU4Rec generates a sequence of items based on the session so far. The sequence is generated greedily, i.e.\\ the algorithm assumes that its next best guesses were correct so far. Bots and power users are filtered from the measurement as they can skew the results.", "The big accuracy improvement comes from the combination of additional samples and the loss functions (fixed cross-entropy, TOP1-max and BPR-max). Table~ showcases our most important results. Besides the original version of GRU4Rec and the item-kNN, we included results with cross-entropy (XE) loss without additional sampling to confirm that the fixed cross-entropy loss still performs just slightly better than TOP1.", "The former introduces some noise, while the latter does not allow the separated use of the transformation and the loss, but both methods stabilize the loss. We did not observe any differences in the results of the two variants. Ranking losses: TOP1 \\& BPR GRU4Rec offers two loss functions based on pairwise losses.", "Users are divided into three groups. The first group is served by the baseline logic. The second group is served by GRU4Rec in ``next best'' mode, meaning that the algorithm shows recommendations that are very likely to be the next in the user's session.", "next time (e.g.\\ if r_j was negative, moving it towards zero decreases the difference with r_i). The score regularization in TOP1 is very beneficial to the overall learning process, so even though the loss might not be theoretically optimal, it can achieve good results. GRU4Rec support two forms of regularization with every loss: dropout and \\ell_2 regularization of the model parameters."], "referred_text_chunks": ["Results: Figure~ shows the relative performance gain of GRU4Rec over the complex logic. The error bars represent the confidence interval at p=0.05. GRU4Rec outperforms the baseline in both prediction modes."]}, "3": {"question": "Which dataset contains the most interactions (events) in the training set and how much larger is it compared to the dataset with the least interactions in the training set?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 3, 'Rationale': 'Image 4 provides the number of events in the training set for each dataset, which directly addresses the question about dataset size. Text 3 offers background on how the datasets were processed and split.',\n'Answer': 'The VIDXL dataset contains the most interactions with 69,312,698 events. It is 7.69 times larger than the CLASS dataset, which has 9,011,321 events, the least among the datasets.'}", "all_figures_names": ["1706.03847v3-Table3-1.png", "1706.03847v3-Figure1-1.png", "1706.03847v3-Figure6-1.png", "1706.03847v3-Figure4-1.png", "1706.03847v3-Table1-1.png", "1706.03847v3-Figure5-1.png", "1706.03847v3-Table2-1.png", "1706.03847v3-Figure2-1.png"], "referred_figures_names": ["1706.03847v3-Table1-1.png"], "answer": "The VIDXL dataset contains the most interactions (events) in the training set with 69,312,698 events. This is roughly 7.7 times larger than the RSC15 dataset, which has the least interactions (9,011,321) in the training set. ", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Please note the logarithmic scale. The actual training time depends on not just the dataset, but on the model parameters as well (especially mini-batch size) and how certain operators used for computing the loss are supported by the framework. The trend, however, is similar for all losses.", "Preliminary experiments (Table~) show additional slight improvements in recall@20 and slight decrease in MRR@20 for most of the datasets. However, for the CLASS dataset both recall and MRR are increased significantly when unified embeddings are used (+18.74\\% and +29.44\\% in recall and MRR respectively, compared to the model trained without embeddings). Unified embeddings have the additional benefit of reducing the overall memory footprint and the model size by a factor of \\sim4.", "The trend is similar on the VIDEO dataset, with training times starting around 20 minutes, starting to increase at 2048 extra samples (to 24 minutes) and quickly above thereafter. This means that the proposed method can be used with zero to little additional cost in practice, unlike data augmentation methods. It is also clear that GRU4Rec can work just as well with a few thousands of negative examples as with the whole itemset, thus it can be kept scalable.", "Finally, CLASS is a proprietary dataset containing item page view events from an online classified site. Datasets were subjugated to minor preprocessing then split into train and test sets so that a whole session either belongs to the train or to the test set. The split is based on the time of the first event of the sessions.", "If the number of items is large, the small sample size further hinders the chance of including all of the high scoring negative examples. (2) Mini-batch size has a direct effect on the training. E.g.\\ we found that training with smaller mini-batch sizes (30-100) produces more accurate models, but training with larger ones is faster on the GPU due to parallelization.", "Unlike our solutions, data augmentation greatly increases training times. Data augmentation and our improvements are not mutually exclusive, thus it is possible that by combining the two methods, even better results can be achieved. A recent paper proposes the Bayesian version of GRU4Rec and reports \\sim0.61 and \\sim0.25 in recall@20 and MRR@20 when using 100 units.", "The figure depicts the median negative gradient w.r.t.\\ the target score in two cases, measured on a dataset sample during the 1^{st and 10^{th epochs (i.e.\\ beginning and end of the training): (left) no additional samples were used, only the other examples from a mini-batch of size 32; (middle \\& right) 2048 additional negative samples were added. The rightmost figure focuses on the first 200 ranks of the figure in the middle. The gradient is slightly higher for BPR when there are more relevant samples (i.e.\\ high ranks).", "For example, the full training of the network is around 6-7 minutes, which does not increase with even 512 extra samples. At the point of diminishing returns, i.e.\\ at 2048 extra samples, training time is still around 7-8 minutes, which is only a slight increase. After that, training times grow quickly, due to exceeding the parallelization capabilities of the GPU we used."], "referred_text_chunks": ["Please note the logarithmic scale. The actual training time depends on not just the dataset, but on the model parameters as well (especially mini-batch size) and how certain operators used for computing the loss are supported by the framework. The trend, however, is similar for all losses."]}, "4": {"question": "What is the purpose of negative sampling?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 3, 'Rationale': 'Image 6 visually depicts mini-batch based negative sampling, which is directly related to the purpose of negative sampling. Text 3 discusses the purpose of negative sampling, emphasizing how negative samples with high scores produce high gradients, which are important for learning.', 'Answer': 'The purpose of negative sampling is to select negative samples with high scores to produce high gradients, aiding in more effective learning by ensuring that the model differentiates between relevant and irrelevant items.'}", "all_figures_names": ["1706.03847v3-Table1-1.png", "1706.03847v3-Figure4-1.png", "1706.03847v3-Figure2-1.png", "1706.03847v3-Figure5-1.png", "1706.03847v3-Figure3-1.png", "1706.03847v3-Table3-1.png", "1706.03847v3-Figure1-1.png", "1706.03847v3-Table2-1.png"], "referred_figures_names": ["1706.03847v3-Figure1-1.png"], "answer": "Negative sampling is a technique used to reduce the number of negative examples that need to be considered during training.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Mini-batch sampling is basically a form of popularity-based sampling, since the training iterates through all events, thus the probability of an item acting as a negative sample is proportional to its support. The problem with popularity-based sampling is that learning can slow down after the algorithm learns to (generally) rank target items above popular ones, and thus can still be inaccurate for ranking long tail high scoring items. On the other hand, uniform sampling slows down learning, due to the high number of low scoring negative samples, but might produce an overall more accurate model if trained indefinitely.", "This means that even if all negative samples would be relevant, their updates would still diminish as their number grows. \\caption{Median negative gradients of BPR and BPR-max w.r.t.\\ the target score against the rank of the target item. Left: only minibatch samples are used (minibatch size: 32); Center: 2048 additional negative samples were added to the minibatch samples; Right: same setting as the center, focusing on ranks 0-200.", "For example, when comparing a niche target to a very popular sample -- especially during the early phase of learning -- the target score might be much lower than the sample score. We concentrated on the gradients w.r.t.\\ the target score, but a similar issue can be observed for the gradients on the negative scores. The gradient w.r.t.\\ the score of a negative sample is the gradient of the pairwise loss between the target and the sample divided by the number of negative samples.", "With pairwise losses, one generally wants to have negative samples with high scores, as those samples produce high gradients. Or intuitively, if the score of the negative sample is already well below that of the target, there is nothing to learn from that negative sample anymore. For this discussion we will denote samples where r_j\\ll r_i irrelevant.", "GRU4Rec introduced mini-batch based sampling . For each example in the mini-batch, the other examples of the same mini-batch serve as negative examples (see Figure~). This method is practical from an implementation point of view and can be also implemented efficiently for GPUs.", "For example the gradient of BPR-max w.r.t.\\ a negative sample's score is as follows: Figure~ depicts how the gradients of BPR and BPR-max behave given the rank of the target item. The rank of the target is the number of negative scores exceeding it, e.g.\\ rank 0 means that the target score is higher than all sample scores. Lower rank means that there are fewer negative samples that are relevant.", "The first part aims to push the target score above the score of the samples, while the second part lowers the score of negative samples towards zero. The latter acts as a regularizer, but instead of constraining the model weights directly, it penalizes high scores on the negative examples. Since all items act as a negative score in one training example or another, it generally pushes the scores down.", "The relative importance of negative samples j and k is \\frac{\\sigma(r_i-r_j)s_j{\\sigma(r_i-r_k)s_k=\\frac{e^{r_j+e^{-r_i+r_j+r_k{e^{r_k+e^{-r_i+r_j+r_k, which behaves like softmax weights if r_i\\gg r_j+r_k or if both r_i and r_k are small. Otherwise it is a smoothed softmax. This means that while r_i is small, the weights are distributed more evenly, yet clear emphasis will be given to higher sample scores."], "referred_text_chunks": ["Mini-batch sampling is basically a form of popularity-based sampling, since the training iterates through all events, thus the probability of an item acting as a negative sample is proportional to its support. The problem with popularity-based sampling is that learning can slow down after the algorithm learns to (generally) rank target items above popular ones, and thus can still be inaccurate for ranking long tail high scoring items. On the other hand, uniform sampling slows down learning, due to the high number of low scoring negative samples, but might produce an overall more accurate model if trained indefinitely."]}, "5": {"question": "How does the training time of the different losses change as the number of additional samples increases?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 directly shows the training times for different losses with increasing sample sizes, which is precisely the query. Text chunk 0 discusses training time behavior with additional samples.', 'Answer': 'Training time increases with more samples, remaining nearly constant up to around 2048 samples, then growing quickly beyond that point due to parallelization limits.'}", "all_figures_names": ["1706.03847v3-Figure4-1.png", "1706.03847v3-Figure3-1.png", "1706.03847v3-Figure2-1.png", "1706.03847v3-Figure1-1.png", "1706.03847v3-Table3-1.png", "1706.03847v3-Figure5-1.png", "1706.03847v3-Table2-1.png", "1706.03847v3-Table1-1.png"], "referred_figures_names": ["1706.03847v3-Figure4-1.png"], "answer": "The training time of all losses increases as the number of additional samples increases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["For example, the full training of the network is around 6-7 minutes, which does not increase with even 512 extra samples. At the point of diminishing returns, i.e.\\ at 2048 extra samples, training time is still around 7-8 minutes, which is only a slight increase. After that, training times grow quickly, due to exceeding the parallelization capabilities of the GPU we used.", "BPR-max improves with more samples all the way, but slightly loses accuracy when all items are used. Adding extra samples increases computational cost, yet due to easy parallelization on modern GPUs most of this cost is alleviated. Figure~ shows the training times at different sample sizes.", "On the other hand, all three of the other losses react well to adding more samples. The point of diminishing returns is around a few thousand of extra samples for cross-entropy. TOP1-max starts to slightly lose accuracy after that.", "Please note the logarithmic scale. The actual training time depends on not just the dataset, but on the model parameters as well (especially mini-batch size) and how certain operators used for computing the loss are supported by the framework. The trend, however, is similar for all losses.", "The trend is similar on the VIDEO dataset, with training times starting around 20 minutes, starting to increase at 2048 extra samples (to 24 minutes) and quickly above thereafter. This means that the proposed method can be used with zero to little additional cost in practice, unlike data augmentation methods. It is also clear that GRU4Rec can work just as well with a few thousands of negative examples as with the whole itemset, thus it can be kept scalable.", "As we discussed earlier, this latter scenario is a more theoretical one, because it is not scalable. As theory suggests (see Section~), the TOP1 loss does not cope well with lots of samples. There is a slight increase in performance with a few extra samples, as the chance of having relevant samples increases; but performance quickly degrades as sample size grows, thus lots of irrelevant samples are included.", "Adding more samples naturally increases the complexity, since N_O increases from N_B to N_A+N_B. However, the computations are easily parallelizable, thus there is no actual increase in the training time on modern GPUs up to a certain sample size (see Section~). The efficient implementation of this sampling however is not trivial.", "Unlike our solutions, data augmentation greatly increases training times. Data augmentation and our improvements are not mutually exclusive, thus it is possible that by combining the two methods, even better results can be achieved. A recent paper proposes the Bayesian version of GRU4Rec and reports \\sim0.61 and \\sim0.25 in recall@20 and MRR@20 when using 100 units."], "referred_text_chunks": ["For example, the full training of the network is around 6-7 minutes, which does not increase with even 512 extra samples. At the point of diminishing returns, i.e.\\ at 2048 extra samples, training time is still around 7-8 minutes, which is only a slight increase. After that, training times grow quickly, due to exceeding the parallelization capabilities of the GPU we used."]}, "6": {"question": "Which dataset has the highest Recall@20 and MRR@20?", "referred_figures_indices": [5], "response": "{'Image': 4, 'Text': 1, 'Rationale': 'Image 4 provides detailed metrics on Recall@20 and MRR@20 across all datasets with various enhancements, directly relevant to finding the dataset with highest metrics. Text 1 discusses improvements in recall and MRR for the CLASS dataset, highlighting its significant performance.', 'Answer': 'The VIDXL dataset has the highest Recall@20 (0.8058) and MRR@20 (0.5066).'}", "all_figures_names": ["1706.03847v3-Figure6-1.png", "1706.03847v3-Figure4-1.png", "1706.03847v3-Figure5-1.png", "1706.03847v3-Figure2-1.png", "1706.03847v3-Table2-1.png", "1706.03847v3-Table3-1.png", "1706.03847v3-Figure1-1.png", "1706.03847v3-Table1-1.png"], "referred_figures_names": ["1706.03847v3-Table3-1.png"], "answer": "VIDXL has the highest Recall@20 and MRR@20.", "figure_type": "table", "content_type": "table", "text_chunks": ["We essentially devise a new class of loss functions that combines learnings from the deep learning and the learning to rank literature. Experimental results on several datasets coming from industry validate these improvements by showing significant increase in recommendation accuracy measured by Mean Reciprocal Rank (MRR) and Recall@20. With these improvements the difference between RNNs and conventional memory-based collaborative filtering jumps to 53\\% in terms of MRR and Recall@20, demonstrating the potential that deep learning methods bring to the area of Recommender Systems.", "Preliminary experiments (Table~) show additional slight improvements in recall@20 and slight decrease in MRR@20 for most of the datasets. However, for the CLASS dataset both recall and MRR are increased significantly when unified embeddings are used (+18.74\\% and +29.44\\% in recall and MRR respectively, compared to the model trained without embeddings). Unified embeddings have the additional benefit of reducing the overall memory footprint and the model size by a factor of \\sim4.", "The second metric used in the experiments is MRR@20 (Mean Reciprocal Rank). That is the average of reciprocal ranks of the desired items. The reciprocal rank is set to zero if the rank is above 20.", "The increase with sampling and the proper loss function is stunning as the best results exceed the accuracy of the original GRU4Rec by 18-37.5\\% and that of item-kNN by up to 55\\%. BPR-max performs similarly (2 of 4) or better (2 of 4; +2-6\\% improvement) than cross-entropy when extra samples are used for both method. On RSC15, reported \\sim0.685 and \\sim0.29 in recall@20 and MRR@20 respectively using data augmentation.", "Unlike our solutions, data augmentation greatly increases training times. Data augmentation and our improvements are not mutually exclusive, thus it is possible that by combining the two methods, even better results can be achieved. A recent paper proposes the Bayesian version of GRU4Rec and reports \\sim0.61 and \\sim0.25 in recall@20 and MRR@20 when using 100 units.", "While this evaluation for VIDXL overestimates the performance, the comparison of algorithms remain fair . As recommender systems can only recommend a few items at once, the actual item a user might pick should be amongst the first few items of the list. Therefore, our primary evaluation metric is recall@20 that is the proportion of cases having the desired item amongst the top-20 items in all test cases.", "Recall does not consider the actual rank of the item as long as it is amongst the top-N. This models certain practical scenarios well where there is no highlighting of recommendations and the absolute order does not matter. Recall also usually correlates well with important online KPIs, such as click-through rate (CTR).", "MRR takes into account the rank of the item, which is important in cases where the order of recommendations matter (e.g.\\ the lower ranked items are only visible after scrolling). The natural baseline we use is the original GRU4Rec algorithm, upon which we aim to improve. We consider the results with the originally proposed TOP1 loss and tanh activation function on the output to be the baseline."], "referred_text_chunks": ["We essentially devise a new class of loss functions that combines learnings from the deep learning and the learning to rank literature. Experimental results on several datasets coming from industry validate these improvements by showing significant increase in recommendation accuracy measured by Mean Reciprocal Rank (MRR) and Recall@20. With these improvements the difference between RNNs and conventional memory-based collaborative filtering jumps to 53\\% in terms of MRR and Recall@20, demonstrating the potential that deep learning methods bring to the area of Recommender Systems."]}, "7": {"question": "How does the addition of negative samples affect the gradient of BPR and BPR-max with respect to the target score?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 1, 'Rationale': 'Image 5 shows the gradients of BPR and BPR-max with negative samples added, directly visualizing their changes. Text chunk 1 explains these changes in gradient and how additional negative samples affect them.', 'Answer': 'The addition of negative samples increases the gradient magnitude of BPR and BPR-max concerning the target score, especially when more relevant samples are included, as indicated by higher ranks.'}", "all_figures_names": ["1706.03847v3-Figure4-1.png", "1706.03847v3-Figure1-1.png", "1706.03847v3-Figure3-1.png", "1706.03847v3-Figure6-1.png", "1706.03847v3-Table3-1.png", "1706.03847v3-Figure2-1.png", "1706.03847v3-Table1-1.png", "1706.03847v3-Figure5-1.png"], "referred_figures_names": ["1706.03847v3-Figure2-1.png"], "answer": "The addition of negative samples increases the gradient of BPR and BPR-max with respect to the target score.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["This means that even if all negative samples would be relevant, their updates would still diminish as their number grows. \\caption{Median negative gradients of BPR and BPR-max w.r.t.\\ the target score against the rank of the target item. Left: only minibatch samples are used (minibatch size: 32); Center: 2048 additional negative samples were added to the minibatch samples; Right: same setting as the center, focusing on ranks 0-200.", "The figure depicts the median negative gradient w.r.t.\\ the target score in two cases, measured on a dataset sample during the 1^{st and 10^{th epochs (i.e.\\ beginning and end of the training): (left) no additional samples were used, only the other examples from a mini-batch of size 32; (middle \\& right) 2048 additional negative samples were added. The rightmost figure focuses on the first 200 ranks of the figure in the middle. The gradient is slightly higher for BPR when there are more relevant samples (i.e.\\ high ranks).", "For example the gradient of BPR-max w.r.t.\\ a negative sample's score is as follows: Figure~ depicts how the gradients of BPR and BPR-max behave given the rank of the target item. The rank of the target is the number of negative scores exceeding it, e.g.\\ rank 0 means that the target score is higher than all sample scores. Lower rank means that there are fewer negative samples that are relevant.", "As r_i becomes higher, the focus shifts quickly to the samples with high scores. This is an ideal behaviour. The gradient w.r.t.\\ a negative sample -- with both the BPR-max and TOP1-max -- is proportional to the softmax score of the example, meaning that only the items, near the maximum will be updated.", "Of course, if all samples are irrelevant, the gradient becomes near zero, but this is not a problem, since if the target score is greater than all sample scores, there is nothing to be learned. Unfortunately, the sensitivity to large sample scores of TOP1 is still an issue as it is the consequence of the TOP1 pairwise loss and not the aggregation. BPR-max: Going back to the probability interpretation of BPR, the goal is to maximize the probability of the target score being higher than the maximal sample score r_{\\mathrm{max=\\max_{j{r_j.", "This is beneficial, because if the score of a negative sample is low, it doesn't need to be updated. If the score of a sample is much higher than that of the others it will be the only one updated and the gradient will coincide with the gradient of the pairwise loss between the target and the sample score. In a more balanced setting the gradient is between the aforementioned gradient and 0.", "For example, when comparing a niche target to a very popular sample -- especially during the early phase of learning -- the target score might be much lower than the sample score. We concentrated on the gradients w.r.t.\\ the target score, but a similar issue can be observed for the gradients on the negative scores. The gradient w.r.t.\\ the score of a negative sample is the gradient of the pairwise loss between the target and the sample divided by the number of negative samples.", "The point of vanishing is relative to the total sample size. With small sample size, BPR's gradient starts vanishing around rank 5 (the BPR-max does not vanish until rank 0); meanwhile, with more samples, the BPR gradient is very low, even for rank 100-500 (again, the gradient BPR-max starts decreasing significantly later). This means that BPR can hardly push target scores up in the ranking after a certain point, which comes earlier as the number of sample size increases."], "referred_text_chunks": ["This means that even if all negative samples would be relevant, their updates would still diminish as their number grows. \\caption{Median negative gradients of BPR and BPR-max w.r.t.\\ the target score against the rank of the target item. Left: only minibatch samples are used (minibatch size: 32); Center: 2048 additional negative samples were added to the minibatch samples; Right: same setting as the center, focusing on ranks 0-200."]}}