{"0": {"question": "What is the relationship between projection sparsity and normalized reconstruction error?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 directly shows the relationship between NMF reconstruction error and projection sparsity, p. Text 0 discusses the trade-off between reconstruction error and the sparsity parameter p of projection matrices.', 'Answer': 'As projection sparsity p increases, the normalized reconstruction error initially decreases, reaches a minimum, and then starts to increase or stabilize.'}", "all_figures_names": ["1706.08146v3-Figure5-1.png", "1706.08146v3-Figure6-1.png", "1706.08146v3-Figure1-1.png", "1706.08146v3-Figure4-1.png", "1706.08146v3-Table1-1.png", "1706.08146v3-Figure2-1.png", "1706.08146v3-Figure3-1.png"], "referred_figures_names": ["1706.08146v3-Figure5-1.png"], "answer": "The normalized reconstruction error decreases as the projection sparsity increases, up to a certain point. After that, the error starts to increase again.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["NMF Reconstruction Error and Projection Matrix Column Sparsity (p) We investigated the trade-off between reconstruction error (as measured by normalized Frobenius loss) and the sparsity parameter p of the binary random projections P. Recall that P\\in\\{0,1\\^{d\\times n is a randomly sampled sparse binary matrix where p distinct entries in each column are selected uniformly at random and set to 1. In Figure~, we plot the normalized reconstruction error achieved by NMF using \\textsc{Factorize-Recover on the lung carcinoma gene expression dataset \\citep{bhattacharjee2001classification at a fixed compression level of 5.", "\\textsc{Recover-Factorize (RF) achieves slightly lower error at a given projection dimension: at 28\\times compression (d=1000), RF achieves a normalized error of 0.819 vs. 0.834 for \\textsc{Factorize-Recover. However, RF is three orders of magnitude slower than \\textsc{Factorize-Recover on this dataset due to the large number of calls required to the sparse recovery algorithm (once for each frequency bin/channel pair, or 303\\times 23 = 6969) to fully recover the data tensor. Due to the computational expense of recovering the full collection of compressed time series, we did not compare RF to FR over the full range of compression factors plotted in the figure for FR.", "For a fixed number of measurements d, we observe that the \\textsc{Factorize-Recover procedure achieves lower approximation error than the alternative method of recovering prior to factorizing (Figure~). While this phenomenon is perhaps counter-intuitive, it can be understood as a consequence of the sparsifying effect of NMF. Recall that for NMF, we model each column of the compressed data \\tilde{M as a nonnegative linear combination of the columns of \\tilde{W. Due to the nonnegativity constraint on the entries of \\tilde{W, we expect the average sparsity of the columns of \\tilde{W to be at least that of the columns of \\tilde{M. Therefore, if \\tilde{W is a good approximation of PW, we should expect that the sparse recovery algorithm will recover the columns of W at least as accurately as the columns of M, given a fixed number of measurements.", "Since we observed that the cost of sparse recovery increases roughly linearly with p, we aimed to select a small value of p that achieves good reconstruction accuracy. We found that the setting p=5 was a reasonable choice for our experiments. Preprocessing of EEG Data Each channel is individually whitened with a mean and standard deviation estimated from segments of data known to not contain any periods of seizure.", "Such projections are known to preserve geometric properties such as pairwise distances \\citep{kane2014sparser and even singular vectors and singular vectors \\citep{halko2011finding. Here, we showed that maximally sparse solutions to certain factorization problems are preserved by sparse binary random projections. Therefore, our results indicate that random projections can also, in a sense, preserve certain solutions of non-convex, NP-Hard problems like NMF \\citep{vavasis2009complexity.", "We found that the tensor decomposition yields a factor that correlates with the onset of seizures (Figure~). At 5\\times compression, the recovered factor qualitatively retains the interpretability of the factor obtained by decomposing the tensor in the original space. \\vspace{-2pt Discussion and Conclusion \\vspace{-2pt We briefly discuss our theoretical results on the uniqueness of sparse matrix factorizations in the context of dimensionality reduction via random projections.", "At projection dimension d=1000, we find that \\textsc{Factorize-Recover achieves comparable error to \\textsc{Recover-Factorize (normalized Frobenius error of 0.83 vs. 0.82). However, RF is three orders of magnitude slower than FR on this task due to the large number of sparse recovery invocations required (once for each frequency bin/channel pair, or 303\\times23 = 6969).\\\\ \\noindent{Factor Interpretability.\\hspace{0.25em The EEG time series data was recorded from patients suffering from epileptic seizures \\citep{shoeb2010application.", "More efficient recovery algorithms than the LP for solving the problem are also known~\\citep{berinde2008practical,indyk2008near,berinde2009sequential. However, these algorithms typically require more measurements in the compressed domain to achieve the same reconstruction accuracy as the LP formulation \\citep{berinde2009sequential.\\\\ \\noindentMeasurement Matrices.\\hspace{0.25em In this work, we consider sparse, binary measurement (or projection) matrices P\\in\\{0,1\\^{d \\times n where each column of P has p non-zero entries chosen uniformly and independently at random."], "referred_text_chunks": ["NMF Reconstruction Error and Projection Matrix Column Sparsity (p) We investigated the trade-off between reconstruction error (as measured by normalized Frobenius loss) and the sparsity parameter p of the binary random projections P. Recall that P\\in\\{0,1\\^{d\\times n is a randomly sampled sparse binary matrix where p distinct entries in each column are selected uniformly at random and set to 1. In Figure~, we plot the normalized reconstruction error achieved by NMF using \\textsc{Factorize-Recover on the lung carcinoma gene expression dataset \\citep{bhattacharjee2001classification at a fixed compression level of 5."]}, "1": {"question": "What is the relationship between compression factor and reconstruction error?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 0, 'Rationale': 'Image 1 and Text 0 both directly address the relationship between compression factor and reconstruction error. Image 1 shows the increase in normalized reconstruction error with higher compression factors for EEG data. Text 0 discusses the trade-off between compression and reconstruction error, providing detailed numerical insights about the increase in error with higher compression.', 'Answer': 'As the compression factor increases, the normalized reconstruction error also increases, indicating a trade-off where higher compression results in larger reconstruction errors.'}", "all_figures_names": ["1706.08146v3-Figure5-1.png", "1706.08146v3-Figure6-1.png", "1706.08146v3-Figure1-1.png", "1706.08146v3-Figure4-1.png", "1706.08146v3-Table1-1.png", "1706.08146v3-Figure2-1.png", "1706.08146v3-Figure3-1.png"], "referred_figures_names": ["1706.08146v3-Figure6-1.png"], "answer": "The reconstruction error increases as the compression factor increases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["We concatenate groups of size three. Tensor Decomposition of Compressed EEG Data In Figure~ (left) , we plot the normalized Frobenius errors of the recovered factorization against the compression factor n/d. Due to the sparsity of the data, we can achieve over 10\\times compression for the cost of a 10\\% increase in reconstruction error relative to the baseline decomposition on the uncompressed data, or approximately 28\\times compression for a 15\\% increase.", "\\textsc{Recover-Factorize (RF) achieves slightly lower error at a given projection dimension: at 28\\times compression (d=1000), RF achieves a normalized error of 0.819 vs. 0.834 for \\textsc{Factorize-Recover. However, RF is three orders of magnitude slower than \\textsc{Factorize-Recover on this dataset due to the large number of calls required to the sparse recovery algorithm (once for each frequency bin/channel pair, or 303\\times 23 = 6969) to fully recover the data tensor. Due to the computational expense of recovering the full collection of compressed time series, we did not compare RF to FR over the full range of compression factors plotted in the figure for FR.", "We found that the tensor decomposition yields a factor that correlates with the onset of seizures (Figure~). At 5\\times compression, the recovered factor qualitatively retains the interpretability of the factor obtained by decomposing the tensor in the original space. \\vspace{-2pt Discussion and Conclusion \\vspace{-2pt We briefly discuss our theoretical results on the uniqueness of sparse matrix factorizations in the context of dimensionality reduction via random projections.", "NMF Reconstruction Error and Projection Matrix Column Sparsity (p) We investigated the trade-off between reconstruction error (as measured by normalized Frobenius loss) and the sparsity parameter p of the binary random projections P. Recall that P\\in\\{0,1\\^{d\\times n is a randomly sampled sparse binary matrix where p distinct entries in each column are selected uniformly at random and set to 1. In Figure~, we plot the normalized reconstruction error achieved by NMF using \\textsc{Factorize-Recover on the lung carcinoma gene expression dataset \\citep{bhattacharjee2001classification at a fixed compression level of 5.", "Figure~ (right) shows the median Pearson correlations between the columns of the recovered temporal factor and those computed on the original, uncompressed data (paired via maximum bipartite matching). At up to 10\\times compression, the recovered temporal factors match the temporal factors obtained by factorization on the uncompressed data with a median correlation above 0.85. Thus, compressed tensor factorization is able to successfully recover an approximation to the factorization of the uncompressed tensor.", "Thus, we demonstrate the practical applicability of \\textsc{Factorize-Recover through experiments on both synthetic benchmarks as well as several real-world gene expression datasets. We find that performing NMF on compressed data achieves reconstruction accuracy comparable to or better than factorizing the recovered (uncompressed) data at a fraction of the computation time. In addition to our results on matrix factorization, we show the following analog to Theorem~ for compressed CP tensor decomposition.", "For a fixed number of measurements d, we observe that the \\textsc{Factorize-Recover procedure achieves lower approximation error than the alternative method of recovering prior to factorizing (Figure~). While this phenomenon is perhaps counter-intuitive, it can be understood as a consequence of the sparsifying effect of NMF. Recall that for NMF, we model each column of the compressed data \\tilde{M as a nonnegative linear combination of the columns of \\tilde{W. Due to the nonnegativity constraint on the entries of \\tilde{W, we expect the average sparsity of the columns of \\tilde{W to be at least that of the columns of \\tilde{M. Therefore, if \\tilde{W is a good approximation of PW, we should expect that the sparse recovery algorithm will recover the columns of W at least as accurately as the columns of M, given a fixed number of measurements.", "Note that the \\textsc{Factorize-Recover approach is guaranteed to work if the factorization of the compressed matrix \\tilde{M yields the factors \\tilde{W=PW and \\tilde{H=H, since we assume that the columns of W are sparse and hence can be recovered from the columns of \\tilde{W using sparse recovery. Thus, the success of the \\textsc{Factorize-Recover approach depends on finding this particular factorization of \\tilde{M. Since matrix factorizations are not unique in general, we ask: under what conditions is it possible to recover the ``correct'' factorization \\tilde{M=(PW)H of the compressed data, from which the original factors can be successfully recovered?\\\\ \\noindentContributions.\\hspace{0.5em In this work, we establish conditions under which \\textsc{Factorize-Recover provably succeeds, in both the matrix and tensor factorization domains."], "referred_text_chunks": ["We concatenate groups of size three. Tensor Decomposition of Compressed EEG Data In Figure~ (left) , we plot the normalized Frobenius errors of the recovered factorization against the compression factor n/d. Due to the sparsity of the data, we can achieve over 10\\times compression for the cost of a 10\\% increase in reconstruction error relative to the baseline decomposition on the uncompressed data, or approximately 28\\times compression for a 15\\% increase."]}, "2": {"question": "What are the three steps involved in compressed matrix factorization?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 5, 'Rationale': 'Image 2 clearly illustrates the three steps in the process of compressed matrix factorization: compression, direct factorization, and sparse recovery. Text 5 provides an explanation of these steps, particularly focusing on the Factorize-Recover method, which is the core of the question.', 'Answer': 'The three steps in compressed matrix factorization are: (i) compress the matrix M to M\u0303, (ii) factorize M\u0303 to obtain W\u0303 and H\u0303, and (iii) recover the original factors by approximating W through sparse recovery of the columns of W\u0303.'}", "all_figures_names": ["1706.08146v3-Figure5-1.png", "1706.08146v3-Figure6-1.png", "1706.08146v3-Figure1-1.png", "1706.08146v3-Figure4-1.png", "1706.08146v3-Table1-1.png", "1706.08146v3-Figure2-1.png", "1706.08146v3-Figure3-1.png"], "referred_figures_names": ["1706.08146v3-Figure1-1.png"], "answer": "The three steps involved in compressed matrix factorization are: \n\n1. Compress the full data matrix M to obtain a compressed matrix M\u0303. \n2. Factorize M\u0303 to obtain matrices W\u0303 and H\u0303. \n3. Approximate the left factor of M via sparse recovery on each column of W\u0303.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["We consider the natural approach of performing matrix factorization directly in the compressed domain (Fig.~): first factorize the compressed matrix \\tilde{M to obtain factors \\tilde{W and \\tilde{H, and then approximately recover each column of W from the columns of \\tilde{W using a sparse recovery algorithm that leverages the sparsity of the factors. We refer to this ``compressed factorization'' method as \\textsc{Factorize-Recover. This approach has clear computational benefits over the alternative \\textsc{Recover-Factorize method of first recovering the matrix M from the compressed measurements, and then performing low-rank factorization on the recovered matrix.", "Hence, the uncompressed matrix M\\in \\mathbb{R^{n\\times m has m columns corresponding to m samples, and we assume that it has some rank-r factorization: M=WH, where {W \\in \\mathbb{R^{n \\times r, {H \\in \\mathbb{R^{r \\times m, and the columns of W are k-sparse. We are given the compressed matrix \\tilde{M=PM corresponding to the d-dimensional projection Pv for each sample v\\in \\mathbb{R^n. We then compute a low-rank factorization using the following algorithm: \\vspace{0.5em \\vspace{0.5em \\noindentCP Tensor Decomposition.\\hspace{0.25em", "Although their setting differs from ours, the technical cores of both analyses involve characterizing the sparsity patterns of linear combinations of random sparse vectors. Compressed Factorization In this section, we first establish preliminaries on compressive sensing, followed by a description of the measurement matrices used to compress the input data. Then, we specify the algorithms for compressed matrix and tensor factorization that we study in the remainder of the paper.\\\\ \\noindentNotation.\\hspace{0.25em Let [n] denote the set \\{1,2,\\dots,n\\.", "We complement our theoretical results with experimental validation that demonstrates both the accuracy of the recovered factors, as well as the computational speedup resulting from \\textsc{Factorize-Recover versus the alternative approach of first recovering the data in the original uncompressed domain, and then factorizing the result. Our main theoretical guarantee for sparse matrix factorizations, formally stated in Section~, provides a simple condition under which the factors of the compressed data are the compressed factors. While the result is intuitive, the proof is delicate, and involves characterizing the likely sparsity of linear combinations of sparse vectors, exploiting graph theoretic properties of expander graphs.", "We then demonstrate the practical applicability of compressed factorization with experiments on gene expression data~(\\S) and EEG time series~(\\S). Synthetic Data We provide empirical evidence that standard algorithms for sparse PCA and NMF converge in practice to the desired sparse factorization \\tilde{M = (PW)H---in order to achieve accurate sparse recovery in the subsequent step, it is necessary that the compressed factor \\tilde{W be a good approximation of PW. For sparse PCA, we use alternating minimization with LARS \\citep{zou2006sparse, and for NMF, we use projected gradient descent \\citep{lin2007projected.", "Note that the \\textsc{Factorize-Recover approach is guaranteed to work if the factorization of the compressed matrix \\tilde{M yields the factors \\tilde{W=PW and \\tilde{H=H, since we assume that the columns of W are sparse and hence can be recovered from the columns of \\tilde{W using sparse recovery. Thus, the success of the \\textsc{Factorize-Recover approach depends on finding this particular factorization of \\tilde{M. Since matrix factorizations are not unique in general, we ask: under what conditions is it possible to recover the ``correct'' factorization \\tilde{M=(PW)H of the compressed data, from which the original factors can be successfully recovered?\\\\ \\noindentContributions.\\hspace{0.5em In this work, we establish conditions under which \\textsc{Factorize-Recover provably succeeds, in both the matrix and tensor factorization domains.", "Consider a high-dimensional data matrix M\\in\\mathbb{R^{n \\times m that has a rank-r factorization M = WH, where W\\in\\mathbb{R^{n\\times r, H \\in \\mathbb{R^{r \\times m, and W is sparse. We are given the compressed measurements \\tilde{M=PM for a known measurement matrix P\\in\\mathbb{R^{d\\times n, where d < n. Our goal is to approximately recover the original factors W and H given the compressed data \\tilde{M as accurately and efficiently as possible. This setting of compressed data with sparse factors arises in a number of important practical domains.", "To conclude, in this work we analyzed low-rank matrix and tensor decomposition on compressed data. Our main theoretical contribution is a novel uniqueness result for the matrix factorization case that relates sparse solutions in the original and compressed domains. We provided empirical evidence on real and synthetic data that accurate recovery can be achieved in practice."], "referred_text_chunks": ["We consider the natural approach of performing matrix factorization directly in the compressed domain (Fig.~): first factorize the compressed matrix \\tilde{M to obtain factors \\tilde{W and \\tilde{H, and then approximately recover each column of W from the columns of \\tilde{W using a sparse recovery algorithm that leverages the sparsity of the factors. We refer to this ``compressed factorization'' method as \\textsc{Factorize-Recover. This approach has clear computational benefits over the alternative \\textsc{Recover-Factorize method of first recovering the matrix M from the compressed measurements, and then performing low-rank factorization on the recovered matrix."]}, "3": {"question": "Which dataset would likely benefit the most from using the Fac.-Recover approach instead of Recover-Fac. in terms of computational efficiency?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 3, 'Rationale': \"Image 4 shows runtime comparisons for different datasets, indicating computational efficiency differences between Factorize-Recover and Recover-Factorize approaches. Text 3 explains that Factorize-Recover requires significantly fewer calls to the sparse recovery algorithm, resulting in better recovery error and efficiency, especially when the number of samples (m) is large compared to a small constant (r).\", 'Answer': 'The leukemia dataset would likely benefit the most from using the Factorize-Recover approach, as it has the highest number of features and samples, leading to notable computational efficiency gains.'}", "all_figures_names": ["1706.08146v3-Figure5-1.png", "1706.08146v3-Figure6-1.png", "1706.08146v3-Figure1-1.png", "1706.08146v3-Figure4-1.png", "1706.08146v3-Table1-1.png", "1706.08146v3-Figure2-1.png", "1706.08146v3-Figure3-1.png"], "referred_figures_names": ["1706.08146v3-Table1-1.png"], "answer": "The Leukemia dataset would likely benefit the most from using the Fac.-Recover approach.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["We ran projected gradient descent \\citep{lin2007projected for 250 iterations, which was sufficient for convergence.\\\\ \\noindentComputation Time.\\hspace{0.25em Computation time for NMF on all 3 datasets (Table~) is dominated by the cost of solving instances of the LP (). % As a result, \\textsc{Factorize-Recover achieves much lower runtime as it requires a factor of m / r fewer calls to the sparse recovery procedure. % While fast iterative recovery procedures such as SSMP \\citep{berinde2009sequential achieve faster recovery times, we found that they require approximately 2\\times the number of measurements to achieve comparable accuracy to LP-based sparse recovery.\\\\ \\noindentReconstruction Error.\\hspace{0.25em", "\\textsc{Recover-Factorize (RF) achieves slightly lower error at a given projection dimension: at 28\\times compression (d=1000), RF achieves a normalized error of 0.819 vs. 0.834 for \\textsc{Factorize-Recover. However, RF is three orders of magnitude slower than \\textsc{Factorize-Recover on this dataset due to the large number of calls required to the sparse recovery algorithm (once for each frequency bin/channel pair, or 303\\times 23 = 6969) to fully recover the data tensor. Due to the computational expense of recovering the full collection of compressed time series, we did not compare RF to FR over the full range of compression factors plotted in the figure for FR.", "Thus, we demonstrate the practical applicability of \\textsc{Factorize-Recover through experiments on both synthetic benchmarks as well as several real-world gene expression datasets. We find that performing NMF on compressed data achieves reconstruction accuracy comparable to or better than factorizing the recovered (uncompressed) data at a fraction of the computation time. In addition to our results on matrix factorization, we show the following analog to Theorem~ for compressed CP tensor decomposition.", "In particular, \\textsc{Factorize-Recover requires only r calls to the sparse recovery algorithm, in contrast to m \\gg r calls for the alternative. This difference is significant in practice, e.g. when m is the number of samples and r is a small constant. Furthermore, we demonstrate empirically that \\textsc{Factorize-Recover also achieves better recovery error in practice on several real-world datasets.", "For a fixed number of measurements d, we observe that the \\textsc{Factorize-Recover procedure achieves lower approximation error than the alternative method of recovering prior to factorizing (Figure~). While this phenomenon is perhaps counter-intuitive, it can be understood as a consequence of the sparsifying effect of NMF. Recall that for NMF, we model each column of the compressed data \\tilde{M as a nonnegative linear combination of the columns of \\tilde{W. Due to the nonnegativity constraint on the entries of \\tilde{W, we expect the average sparsity of the columns of \\tilde{W to be at least that of the columns of \\tilde{M. Therefore, if \\tilde{W is a good approximation of PW, we should expect that the sparse recovery algorithm will recover the columns of W at least as accurately as the columns of M, given a fixed number of measurements.", "At projection dimension d=1000, we find that \\textsc{Factorize-Recover achieves comparable error to \\textsc{Recover-Factorize (normalized Frobenius error of 0.83 vs. 0.82). However, RF is three orders of magnitude slower than FR on this task due to the large number of sparse recovery invocations required (once for each frequency bin/channel pair, or 303\\times23 = 6969).\\\\ \\noindent{Factor Interpretability.\\hspace{0.25em The EEG time series data was recorded from patients suffering from epileptic seizures \\citep{shoeb2010application.", "In such settings, Theorem~ guarantees that we can efficiently recover the correct factorization. For other matrix factorization problems such as NMF, the current algorithmic understanding of how to recover the factorization is incomplete even for uncompressed data, and guarantees for provable recovery require strong assumptions such as separability \\citep{arora2012learning. As the original problem (computing NMF of the uncompressed matrix M) is itself NP-hard \\citep{vavasis2009complexity, hence one should not expect an analog of Theorem~ to avoid solving a computationally hard problem and guarantee efficient recovery in general.", "Note that the \\textsc{Factorize-Recover approach is guaranteed to work if the factorization of the compressed matrix \\tilde{M yields the factors \\tilde{W=PW and \\tilde{H=H, since we assume that the columns of W are sparse and hence can be recovered from the columns of \\tilde{W using sparse recovery. Thus, the success of the \\textsc{Factorize-Recover approach depends on finding this particular factorization of \\tilde{M. Since matrix factorizations are not unique in general, we ask: under what conditions is it possible to recover the ``correct'' factorization \\tilde{M=(PW)H of the compressed data, from which the original factors can be successfully recovered?\\\\ \\noindentContributions.\\hspace{0.5em In this work, we establish conditions under which \\textsc{Factorize-Recover provably succeeds, in both the matrix and tensor factorization domains."], "referred_text_chunks": ["We ran projected gradient descent \\citep{lin2007projected for 250 iterations, which was sufficient for convergence.\\\\ \\noindentComputation Time.\\hspace{0.25em Computation time for NMF on all 3 datasets (Table~) is dominated by the cost of solving instances of the LP (). % As a result, \\textsc{Factorize-Recover achieves much lower runtime as it requires a factor of m / r fewer calls to the sparse recovery procedure. % While fast iterative recovery procedures such as SSMP \\citep{berinde2009sequential achieve faster recovery times, we found that they require approximately 2\\times the number of measurements to achieve comparable accuracy to LP-based sparse recovery.\\\\ \\noindentReconstruction Error.\\hspace{0.25em"]}, "4": {"question": "What is the effect of increasing the projection dimension d on the approximation error for sparse PCA and NMF?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 4, 'Rationale': 'Image 5 and Text Chunk 4 directly address how increasing the projection dimension affects approximation errors for sparse PCA and NMF.', 'Answer': 'Increasing the projection dimension d generally improves approximation accuracy for both sparse PCA and NMF, leading to lower approximation errors for recovered factors.'}", "all_figures_names": ["1706.08146v3-Figure5-1.png", "1706.08146v3-Figure6-1.png", "1706.08146v3-Figure1-1.png", "1706.08146v3-Figure4-1.png", "1706.08146v3-Table1-1.png", "1706.08146v3-Figure2-1.png", "1706.08146v3-Figure3-1.png"], "referred_figures_names": ["1706.08146v3-Figure2-1.png"], "answer": "Increasing the projection dimension d decreases the approximation error for both sparse PCA and NMF.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["For a fixed number of measurements d, we observe that the \\textsc{Factorize-Recover procedure achieves lower approximation error than the alternative method of recovering prior to factorizing (Figure~). While this phenomenon is perhaps counter-intuitive, it can be understood as a consequence of the sparsifying effect of NMF. Recall that for NMF, we model each column of the compressed data \\tilde{M as a nonnegative linear combination of the columns of \\tilde{W. Due to the nonnegativity constraint on the entries of \\tilde{W, we expect the average sparsity of the columns of \\tilde{W to be at least that of the columns of \\tilde{M. Therefore, if \\tilde{W is a good approximation of PW, we should expect that the sparse recovery algorithm will recover the columns of W at least as accurately as the columns of M, given a fixed number of measurements.", "Additionally, we evaluate the quality of the factors obtained after sparse recovery by measuring the approximation error of the recovered factors \\hat{W relative to the true factors W. We generate synthetic data following the conditions of Theorem 1. For sparse PCA, we sample matrices W = B \\odot Y and H, where each column of B \\in \\{0,1\\^{n\\times r has k non-zero entries chosen uniformly at random, Y_{ij\\overset{\\text{iid{\\sim N(0, 1), and H_{ij\\overset{\\text{iid{\\sim N(0, 1). For NMF, an elementwise absolute value function is applied to the values sampled from this distribution.", "We then demonstrate the practical applicability of compressed factorization with experiments on gene expression data~(\\S) and EEG time series~(\\S). Synthetic Data We provide empirical evidence that standard algorithms for sparse PCA and NMF converge in practice to the desired sparse factorization \\tilde{M = (PW)H---in order to achieve accurate sparse recovery in the subsequent step, it is necessary that the compressed factor \\tilde{W be a good approximation of PW. For sparse PCA, we use alternating minimization with LARS \\citep{zou2006sparse, and for NMF, we use projected gradient descent \\citep{lin2007projected.", "NMF Reconstruction Error and Projection Matrix Column Sparsity (p) We investigated the trade-off between reconstruction error (as measured by normalized Frobenius loss) and the sparsity parameter p of the binary random projections P. Recall that P\\in\\{0,1\\^{d\\times n is a randomly sampled sparse binary matrix where p distinct entries in each column are selected uniformly at random and set to 1. In Figure~, we plot the normalized reconstruction error achieved by NMF using \\textsc{Factorize-Recover on the lung carcinoma gene expression dataset \\citep{bhattacharjee2001classification at a fixed compression level of 5.", "For small column sparsities k relative to the projection dimension d, the estimated compressed left factors \\tilde{W are good approximations to the desired solutions PW. Encouragingly, we find that the recovered solutions \\hat{W=\\mathcal{R(\\tilde{W) are typically only slightly worse in approximation error than \\mathcal{R(PW), the solution recovered when the projection of W is known exactly. Thus, we perform almost as well as the idealized setting where we are given the correct factorization (PW)H. \\vspace{-0pt NMF on Gene Expression Data NMF is a commonly-used method for clustering gene expression data, yielding interpretable factors in practice \\citep{gao2005improving,kim2007sparse.", "Such projections are known to preserve geometric properties such as pairwise distances \\citep{kane2014sparser and even singular vectors and singular vectors \\citep{halko2011finding. Here, we showed that maximally sparse solutions to certain factorization problems are preserved by sparse binary random projections. Therefore, our results indicate that random projections can also, in a sense, preserve certain solutions of non-convex, NP-Hard problems like NMF \\citep{vavasis2009complexity.", "We first formally state our theorem: \\vspace{-4pt Theorem~ shows that if the columns of W are k-sparse, then projecting into \\Omega((r+k)\\log n) dimensions preserves uniqueness, with failure probability at most (r/n)e^{-\\beta k+(1/n)^5, for some constant \\beta>0. As real-world matrices have been empirically observed to be typically close to low rank, the (r/n) term is usually small for practical applications. Note that the requirement for the projection dimension being at least \\Omega((r+k)\\log n) is close to optimal, as even being able to uniquely recover a k-sparse n-dimensional vector x from its projection Px requires the projection dimension to be at least \\Omega(k\\log n); we also cannot hope for uniqueness for projections to dimensions below the rank r.", "\\textsc{Recover-Factorize (RF) achieves slightly lower error at a given projection dimension: at 28\\times compression (d=1000), RF achieves a normalized error of 0.819 vs. 0.834 for \\textsc{Factorize-Recover. However, RF is three orders of magnitude slower than \\textsc{Factorize-Recover on this dataset due to the large number of calls required to the sparse recovery algorithm (once for each frequency bin/channel pair, or 303\\times 23 = 6969) to fully recover the data tensor. Due to the computational expense of recovering the full collection of compressed time series, we did not compare RF to FR over the full range of compression factors plotted in the figure for FR."], "referred_text_chunks": ["For a fixed number of measurements d, we observe that the \\textsc{Factorize-Recover procedure achieves lower approximation error than the alternative method of recovering prior to factorizing (Figure~). While this phenomenon is perhaps counter-intuitive, it can be understood as a consequence of the sparsifying effect of NMF. Recall that for NMF, we model each column of the compressed data \\tilde{M as a nonnegative linear combination of the columns of \\tilde{W. Due to the nonnegativity constraint on the entries of \\tilde{W, we expect the average sparsity of the columns of \\tilde{W to be at least that of the columns of \\tilde{M. Therefore, if \\tilde{W is a good approximation of PW, we should expect that the sparse recovery algorithm will recover the columns of W at least as accurately as the columns of M, given a fixed number of measurements."]}, "5": {"question": "Which method achieves lower approximation error when the compression factor is greater than 3?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 5, 'Rationale': 'Image 6 shows the comparison between Factorize-Recover (FR) and Recover-Factorize (RF) methods for different compression factors, indicating lower error for FR when the compression factor is greater than 3. Text 5 provides information on the accuracy and advantage of using NMF for clustering, supporting the effectiveness of FR.', 'Answer': 'Factorize-Recover achieves lower approximation error when the compression factor is greater than 3.'}", "all_figures_names": ["1706.08146v3-Figure5-1.png", "1706.08146v3-Figure6-1.png", "1706.08146v3-Figure1-1.png", "1706.08146v3-Figure4-1.png", "1706.08146v3-Table1-1.png", "1706.08146v3-Figure2-1.png", "1706.08146v3-Figure3-1.png"], "referred_figures_names": ["1706.08146v3-Figure3-1.png"], "answer": "Factorize-Recover", "figure_type": "plot", "content_type": "figure", "text_chunks": ["For a fixed number of measurements d, we observe that the \\textsc{Factorize-Recover procedure achieves lower approximation error than the alternative method of recovering prior to factorizing (Figure~). While this phenomenon is perhaps counter-intuitive, it can be understood as a consequence of the sparsifying effect of NMF. Recall that for NMF, we model each column of the compressed data \\tilde{M as a nonnegative linear combination of the columns of \\tilde{W. Due to the nonnegativity constraint on the entries of \\tilde{W, we expect the average sparsity of the columns of \\tilde{W to be at least that of the columns of \\tilde{M. Therefore, if \\tilde{W is a good approximation of PW, we should expect that the sparse recovery algorithm will recover the columns of W at least as accurately as the columns of M, given a fixed number of measurements.", "\\textsc{Recover-Factorize (RF) achieves slightly lower error at a given projection dimension: at 28\\times compression (d=1000), RF achieves a normalized error of 0.819 vs. 0.834 for \\textsc{Factorize-Recover. However, RF is three orders of magnitude slower than \\textsc{Factorize-Recover on this dataset due to the large number of calls required to the sparse recovery algorithm (once for each frequency bin/channel pair, or 303\\times 23 = 6969) to fully recover the data tensor. Due to the computational expense of recovering the full collection of compressed time series, we did not compare RF to FR over the full range of compression factors plotted in the figure for FR.", "We then demonstrate the practical applicability of compressed factorization with experiments on gene expression data~(\\S) and EEG time series~(\\S). Synthetic Data We provide empirical evidence that standard algorithms for sparse PCA and NMF converge in practice to the desired sparse factorization \\tilde{M = (PW)H---in order to achieve accurate sparse recovery in the subsequent step, it is necessary that the compressed factor \\tilde{W be a good approximation of PW. For sparse PCA, we use alternating minimization with LARS \\citep{zou2006sparse, and for NMF, we use projected gradient descent \\citep{lin2007projected.", "We concatenate groups of size three. Tensor Decomposition of Compressed EEG Data In Figure~ (left) , we plot the normalized Frobenius errors of the recovered factorization against the compression factor n/d. Due to the sparsity of the data, we can achieve over 10\\times compression for the cost of a 10\\% increase in reconstruction error relative to the baseline decomposition on the uncompressed data, or approximately 28\\times compression for a 15\\% increase.", "For small column sparsities k relative to the projection dimension d, the estimated compressed left factors \\tilde{W are good approximations to the desired solutions PW. Encouragingly, we find that the recovered solutions \\hat{W=\\mathcal{R(\\tilde{W) are typically only slightly worse in approximation error than \\mathcal{R(PW), the solution recovered when the projection of W is known exactly. Thus, we perform almost as well as the idealized setting where we are given the correct factorization (PW)H. \\vspace{-0pt NMF on Gene Expression Data NMF is a commonly-used method for clustering gene expression data, yielding interpretable factors in practice \\citep{gao2005improving,kim2007sparse.", "Thus, we demonstrate the practical applicability of \\textsc{Factorize-Recover through experiments on both synthetic benchmarks as well as several real-world gene expression datasets. We find that performing NMF on compressed data achieves reconstruction accuracy comparable to or better than factorizing the recovered (uncompressed) data at a fraction of the computation time. In addition to our results on matrix factorization, we show the following analog to Theorem~ for compressed CP tensor decomposition.", "For our theoretical results, we set p=O(\\log n). Although the first results on compressive sensing only held for dense matrices , subsequent work has shown that sparse, binary matrices can also be used for compressive sensing . % In particular, Theorem 3 of \\citet{berinde2008combining shows that the recovery procedure in \\eqref{eq:basis-pursuit succeeds with high probability for the class of P we consider if the original signal is k-sparse and d=\\Omega(k\\log n).", "We ran projected gradient descent \\citep{lin2007projected for 250 iterations, which was sufficient for convergence.\\\\ \\noindentComputation Time.\\hspace{0.25em Computation time for NMF on all 3 datasets (Table~) is dominated by the cost of solving instances of the LP (). % As a result, \\textsc{Factorize-Recover achieves much lower runtime as it requires a factor of m / r fewer calls to the sparse recovery procedure. % While fast iterative recovery procedures such as SSMP \\citep{berinde2009sequential achieve faster recovery times, we found that they require approximately 2\\times the number of measurements to achieve comparable accuracy to LP-based sparse recovery.\\\\ \\noindentReconstruction Error.\\hspace{0.25em"], "referred_text_chunks": ["For a fixed number of measurements d, we observe that the \\textsc{Factorize-Recover procedure achieves lower approximation error than the alternative method of recovering prior to factorizing (Figure~). While this phenomenon is perhaps counter-intuitive, it can be understood as a consequence of the sparsifying effect of NMF. Recall that for NMF, we model each column of the compressed data \\tilde{M as a nonnegative linear combination of the columns of \\tilde{W. Due to the nonnegativity constraint on the entries of \\tilde{W, we expect the average sparsity of the columns of \\tilde{W to be at least that of the columns of \\tilde{M. Therefore, if \\tilde{W is a good approximation of PW, we should expect that the sparse recovery algorithm will recover the columns of W at least as accurately as the columns of M, given a fixed number of measurements."]}}