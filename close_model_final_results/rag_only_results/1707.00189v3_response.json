{"0": {"question": "Which model performs best when trained on the NYT dataset and evaluated on the WT14 dataset, and how does its performance compare to the baselines?", "referred_figures_indices": [1], "response": "```json\n{\n  \"Image\": 1,\n  \"Text\": 0,\n  \"Rationale\": \"Image 1 specifically compares model performance on WT14, showing that Conv-KNRM performs best with NYT training and provides baseline comparisons. Text 0 discusses relative performance based on training sources, emphasizing PACRR's strength with NYT.\",\n  \"Answer\": \"Conv-KNRM performs best when trained on NYT with an nDCG@20 of 0.3215 for WT14, significantly outperforming the baselines BM25, WT10, and AOL.\"\n}\n```", "all_figures_names": ["1707.00189v3-Table2-1.png", "1707.00189v3-Table1-1.png"], "referred_figures_names": ["1707.00189v3-Table1-1.png"], "answer": "The Conv-KNRM model performs best when trained on the NYT dataset and evaluated on the WT14 dataset, achieving an nDCG@20 score of 0.3215. This performance is significantly better than all the baselines: BM25 (B), WT10 (W), and AOL (A).", "figure_type": "N/A", "content_type": "table", "text_chunks": ["When trained on NYT, PACRR performs significantly better. KNRM performs worse when trained using the content-based sources, sometimes significantly. These results suggest that these content-based training sources contain relevance signals where n-grams are useful, and it is valuable for these models to see a wide variety of n-gram relevance signals when training.", "When comparing the performance of the content-based training sources, we observe that the NYT source usually performs better than Wiki. We suspect that this is due to the web domain being more similar to the newswire domain than the complex answer retrieval domain. For instance, the document lengths of news articles are more similar to web documents, and precise term matches are less common in the complex answer retrieval domain~.", "Baselines and benchmarks. As baselines, we use the AOL ranking-based source as a weakly supervised baseline~, WT10 as a manual relevance judgment baseline, and BM25 as an unsupervised baseline. The two supervised baselines are trained using the same conditions as our approach, and the BM25 baselines is tuned on each testing set with Anserini~, representing the best-case performance of BM25.", "We present filtering performance on NYT and Wiki for each ranking architecture in Table~. In terms of absolute score, the filters almost always improve the content-based data sources, and in many cases this difference is statistically significant. The one exception is for Conv-KNRM on NYT. One possible explanation is that the filters caused the training data to become too homogeneous, reducing the ranker's ability to generalize.", "We measure the performance of the models using the \\textsc{Trec Web Track 2012--2014 (WT12--14) queries (topics + descriptions) and manual relevance judgments. These cover two target collections: ClueWeb09 and ClueWeb12. Akin to~, the trained models are used to re-rank the top 100 results from a query-likelihood model (QL, Indri~ version).", "Following the \\textsc{Trec Web Track, we use nDCG@20 and ERR@20 for evaluation. \\renewcommand{\\arraystretch{1 Results In Table~, we present the performance of the rankers when trained using content-based sources without filtering. In terms of absolute score, we observe that the two n-gram models (PACRR and ConvKNRM) always perform better when trained on content-based sources than when trained on the limited sample of in-domain data.", "We suspect that Conv-KNRM is particularly susceptible to this problem because of language-dependent convolutional filters; the other two models rely only on term similarity scores. We note that Wiki tends to do better with the 2max filter, with significant improvements seen for Conv-KNRM and KNRM. In thse models, the discriminator filter may be learning surface characteristics of the dataset, rather than more valuable notions of relevance.", "We show that our methods can significantly outperform various neural rankers when trained using a query log source (as proposed by~), the ranker when trained on a limited amount of manually-labeled in-domain data (as one would encounter in a new domain), and well-tuned conventional baselines. In summary, we (1) address existing shortcomings of weak supervision to train neural rankers by using training sources from text pairs, (2) address limitations related to domain differences when training rankers on these sources using novel filtering techniques, and (3) demonstrate the effectiveness of our methods for ad-hoc retrieval when limited in-domain training data is available. Our code is public for validation and further comparisons."], "referred_text_chunks": ["When trained on NYT, PACRR performs significantly better. KNRM performs worse when trained using the content-based sources, sometimes significantly. These results suggest that these content-based training sources contain relevance signals where n-grams are useful, and it is valuable for these models to see a wide variety of n-gram relevance signals when training."]}}