{"0": {"question": "Which model performs the best for caption retrieval in terms of R@1 and MEDR? Briefly explain why the performance might be better than other models.", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 1, 'Rationale': 'Image 0 provides the specific retrieval accuracies and median rank results for caption and image retrieval tasks on COCO5K, making it directly relevant to the question. Text 1 discusses comparisons between models, focusing on their performance in caption and image retrieval tasks and aligns with the evaluation of the models mentioned.', 'Answer': 'The Cap2Img model performs the best for caption retrieval with R@1=27.1 and MEDR=4. Its performance might be better than other models because it likely has a more effective sentence encoder, as suggested by its superior results in caption retrieval, indicating a well-learned representation of captions.'}", "all_figures_names": ["1707.06320v2-Table1-1.png", "1707.06320v2-Table2-1.png", "1707.06320v2-Table3-1.png", "1707.06320v2-Table4-1.png", "1707.06320v2-Table5-1.png", "1707.06320v2-Figure1-1.png"], "referred_figures_names": ["1707.06320v2-Table1-1.png"], "answer": "Cap2Img performs the best for caption retrieval in terms of both R@1 (27.1) and MEDR (4.0). This suggests that the model is more successful at retrieving the most relevant caption for a given image compared to other models.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["As the results show, Cap2Img performs very well on this task, outperforming the compared models on caption retrieval and being very close to order embeddings on image retrieval. The fact that the system outperforms Order Embeddings on caption retrieval suggests that it has a better sentence encoder. Cap2Both does not work as well on this task as the image-only case, probably because interference from the language signal makes the problem harder to optimize.", "Results Although it is not the primary aim of this work to learn a state-of-the-art image and caption retrieval system, it is important to first establish the capability of our system to do what it is trained to do. Table shows the results on the COCO5K caption and image retrieval tasks for the two models that predict image features. We compare our system against several well-known approaches, namely Deep Visual-Semantic Alignments (DVSA) , Fisher Vectors (FV) and Order Embeddings (OE) .", "Compared to e.g. an l_2 regression loss, \\Cref{eq:loss:ci is less susceptible to error incurred by subspaces of the visual representation that are irrelevant to the high level visual semantics. Empirically, we found it to be more robust to overfitting. Cap2Cap Let x = (x_1, \\ldots, x_T), y = (y_1, \\ldots, y_S) be a caption pair that describes the same image.", "In the cases of CR and MRPC (F1), it appears that improved performance is due to having more data or ensemble effects. For the other datasets, grounding clearly yields better results. These results indicate that grounding does indeed capture qualitatively different information, yielding better universal sentence representations.", "we use the usual multiclass classification approach over the vocabulary of the corpus \\Set{V such that Here, \\vec{y_s = \\psi(W_V\\vec{g_s + \\vec{b) and \\vec{g_s is hidden state of the decoder LSTM at time s. \\noindent To learn the model parameters, we minimize the negative log-likelihood over all caption pairs, i.e., Cap2Both Finally, we also integrate both concepts of grounding into a joint model, where we optimize the following loss function: Grounded universal representations On their own, features from this system are likely to suffer from the fact that training on COCO introduces biases: aside from the inherent dataset bias in COCO itself, the system will only have coverage for concrete concepts. COCO is also a much smaller dataset than e.g. the Toronto Books Corpus often used in purely text-based methods . As such, grounded representations are potentially less ``universal'' than text-based alternatives, which also cover abstract concepts.", "We achieved good performance on caption and image retrieval tasks on the large-scale COCO dataset. We subsequently showed how the sentence encodings that the system learns can be transferred to various NLP tasks, and that grounded universal sentence representations lead to improved performance. We analyzed the source of improvements from grounding, and showed that the increased performance appears to be due to the introduction of qualitatively different information (i.e., grounding), rather than simply having more parameters or applying ensemble methods.", "Although it is not the primary aim of this work, it is important to first examine how well this system achieves what it is trained to do, by evaluating on the COCO5K image and caption retrieval task. We then analyze the performance of grounded representations on a variety of sentence-level semantic transfer tasks, showing that grounding increases performance over text-only representations. We then investigate an important open question in multi-modal semantics: to what extent are improvements in semantic performance due to grounding, rather than to having more data or data from a different distribution?", "An alternative method is to exploit the fact that one image in COCO has multiple captions (Cap2Cap), and to learn to predict which other captions are valid descriptions of the same image. This approach is strictly speaking not perceptually grounded, but exploits the fact that there is an implicit association between the captions and the shared underlying image, and so could be considered a weaker version of grounding. Finally, we experiment with a model that optimizes both these objectives jointly: that is, we predict both images and alternative captions for the same image (Cap2Both)."], "referred_text_chunks": ["As the results show, Cap2Img performs very well on this task, outperforming the compared models on caption retrieval and being very close to order embeddings on image retrieval. The fact that the system outperforms Order Embeddings on caption retrieval suggests that it has a better sentence encoder. Cap2Both does not work as well on this task as the image-only case, probably because interference from the language signal makes the problem harder to optimize."]}, "1": {"question": "Which type of grounding appears to be most beneficial for the MRPC task, and how does its performance compare to the baseline model (ST-LN)?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 2, 'Rationale': 'Image 1 provides the results on the MRPC task, comparing different grounding strategies against the ST-LN baseline. Text 2 explains that grounding yields better results in datasets like MRPC, indicating its benefit.', 'Answer': 'For MRPC, GroundSent-Cap and GroundSent-Both both perform better than the baseline ST-LN. GroundSent-Both performs slightly better, with results of 72.7/82.5 compared to ST-LN\u2019s 69.6/81.2.'}", "all_figures_names": ["1707.06320v2-Table1-1.png", "1707.06320v2-Table2-1.png", "1707.06320v2-Table3-1.png", "1707.06320v2-Table4-1.png", "1707.06320v2-Table5-1.png", "1707.06320v2-Figure1-1.png"], "referred_figures_names": ["1707.06320v2-Table2-1.png"], "answer": "GroundSent-Cap appears to be most beneficial for the MRPC task, achieving an accuracy of 72.9/82.2 compared to the baseline model ST-LN's 69.6/81.2.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["This explains why grounding can clearly be seen to help in these cases. For the semantic classification tasks, the more concrete datasets are MRPC and SST. The picture is less clear for the first, but in SST we see that the grounded representations definitely do work better.", "Note that all systems use the exact same evaluation pipeline, which makes them directly comparable. We can see that in all cases, grounding increases the performance. The question of which type of grounding works best is more difficult: generally, grounding with Cap2Cap and Cap2Both appears to do slightly better on most tasks, but on e.g. SST, Cap2Img works better.", "In the cases of CR and MRPC (F1), it appears that improved performance is due to having more data or ensemble effects. For the other datasets, grounding clearly yields better results. These results indicate that grounding does indeed capture qualitatively different information, yielding better universal sentence representations.", "The entailment task results (SNLI and SICK in Table ) show a similar picture: in all cases grounding improves performance. It is important to note that, in this work, we are not necessarily concerned with replacing the state-of-the-art on these tasks: there are systems that perform better. We are primarily interested in whether grounding helps relative to text-only baselines.", "As Table shows, a more nuanced picture emerges in this comparison: grounding helps more for some datasets than for others. Grounded models outperform the STb-1024 model (which uses much more data---the Toronto Books Corpus is much larger than COCO) in all cases, often already without concatenating the textual modality. The ensemble of two STb-1024 models performs better than the individual one, and so does the higher-dimensional one.", "We find that it does. The contribution of grounding An important open question is whether the increase in performance in multi-modal semantic models is due to qualitatively different information from grounding, or simply due to the fact that we have more parameters or data from a different distribution. In order to examine this, we implement a SkipThought-like model that also uses a bidirectional LSTM with element-wise max on the final hidden layer (henceforth referred to as STb).", "This model is architecturally identical to the sentence encoder used before: it can be thought of as Cap2Cap, but where the objective is not to predict an alternative caption, but to predict the previous and next sentence in the Toronto Books Corpus, just like SkipThought . We train a 1024-dimensional and 2048-dimensional STb model (for one full iteration, with all other hyperparameters identical to Cap2Cap) to compare against: if grounding improves results because it introduces qualitatively different information, rather than just from having more parameters (i.e., a higher embedding dimensionality), we should expect the multi-modal GroundSent models to perform better not only than STb-1024, but also than STb-2048, which has the same number of parameters (recall that GroundSent models are combinations of grounded and linguistic-only representations). In addition, we compare against an ``ensemble'' of two different STb-1024 models (i.e., a concatenation of two separately trained STb-1024), to check that we are not (just) observing an ensemble effect.", "Although it is not the primary aim of this work, it is important to first examine how well this system achieves what it is trained to do, by evaluating on the COCO5K image and caption retrieval task. We then analyze the performance of grounded representations on a variety of sentence-level semantic transfer tasks, showing that grounding increases performance over text-only representations. We then investigate an important open question in multi-modal semantics: to what extent are improvements in semantic performance due to grounding, rather than to having more data or data from a different distribution?"], "referred_text_chunks": ["This explains why grounding can clearly be seen to help in these cases. For the semantic classification tasks, the more concrete datasets are MRPC and SST. The picture is less clear for the first, but in SST we see that the grounded representations definitely do work better."]}, "2": {"question": "Which model performs best on the SNLI dataset, and how much does grounding contribute to its performance compared to the baseline STb-1024 model?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 3, 'Rationale': 'Image 2 shows the performance of various models on the SNLI dataset, clearly indicating the accuracy of each. Text chunk 3 discusses grounding improvements across classification tasks, including SNLI.', 'Answer': 'The GroundSent-Img and GroundSent-Both models both achieve the best performance on SNLI with an accuracy of 76.1. Compared to the baseline STb-1024 model, which has an accuracy of 67.3, grounding contributes a significant improvement of 8.8 points.'}", "all_figures_names": ["1707.06320v2-Table1-1.png", "1707.06320v2-Table2-1.png", "1707.06320v2-Table3-1.png", "1707.06320v2-Table4-1.png", "1707.06320v2-Table5-1.png", "1707.06320v2-Figure1-1.png"], "referred_figures_names": ["1707.06320v2-Table3-1.png"], "answer": "The GroundSent-Both model performs best on the SNLI dataset, achieving an accuracy of 72.0%. Grounding contributes to an improvement of 4.7% compared to the baseline STb-1024 model, which achieves 67.3%.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["As Table shows, a more nuanced picture emerges in this comparison: grounding helps more for some datasets than for others. Grounded models outperform the STb-1024 model (which uses much more data---the Toronto Books Corpus is much larger than COCO) in all cases, often already without concatenating the textual modality. The ensemble of two STb-1024 models performs better than the individual one, and so does the higher-dimensional one.", "Note that all systems use the exact same evaluation pipeline, which makes them directly comparable. We can see that in all cases, grounding increases the performance. The question of which type of grounding works best is more difficult: generally, grounding with Cap2Cap and Cap2Both appears to do slightly better on most tasks, but on e.g. SST, Cap2Img works better.", "This model is architecturally identical to the sentence encoder used before: it can be thought of as Cap2Cap, but where the objective is not to predict an alternative caption, but to predict the previous and next sentence in the Toronto Books Corpus, just like SkipThought . We train a 1024-dimensional and 2048-dimensional STb model (for one full iteration, with all other hyperparameters identical to Cap2Cap) to compare against: if grounding improves results because it introduces qualitatively different information, rather than just from having more parameters (i.e., a higher embedding dimensionality), we should expect the multi-modal GroundSent models to perform better not only than STb-1024, but also than STb-2048, which has the same number of parameters (recall that GroundSent models are combinations of grounded and linguistic-only representations). In addition, we compare against an ``ensemble'' of two different STb-1024 models (i.e., a concatenation of two separately trained STb-1024), to check that we are not (just) observing an ensemble effect.", "The entailment task results (SNLI and SICK in Table ) show a similar picture: in all cases grounding improves performance. It is important to note that, in this work, we are not necessarily concerned with replacing the state-of-the-art on these tasks: there are systems that perform better. We are primarily interested in whether grounding helps relative to text-only baselines.", "We find that it does. The contribution of grounding An important open question is whether the increase in performance in multi-modal semantic models is due to qualitatively different information from grounding, or simply due to the fact that we have more parameters or data from a different distribution. In order to examine this, we implement a SkipThought-like model that also uses a bidirectional LSTM with element-wise max on the final hidden layer (henceforth referred to as STb).", "This explains why grounding can clearly be seen to help in these cases. For the semantic classification tasks, the more concrete datasets are MRPC and SST. The picture is less clear for the first, but in SST we see that the grounded representations definitely do work better.", "In the cases of CR and MRPC (F1), it appears that improved performance is due to having more data or ensemble effects. For the other datasets, grounding clearly yields better results. These results indicate that grounding does indeed capture qualitatively different information, yielding better universal sentence representations.", "That is, we concatenate Cap2Cap, Cap2Img or Cap2Both and Skip-Thought with Layer Normalization (ST-LN) representations, yielding GroundSent-Cap, GroundSent-Img and GroundSent-Both representations, respectively. We report performance of ST-LN using SentEval, which led to slightly different numbers than what is reported in their paper. Table shows the results for the semantic classification and entailment tasks."], "referred_text_chunks": ["As Table shows, a more nuanced picture emerges in this comparison: grounding helps more for some datasets than for others. Grounded models outperform the STb-1024 model (which uses much more data---the Toronto Books Corpus is much larger than COCO) in all cases, often already without concatenating the textual modality. The ensemble of two STb-1024 models performs better than the individual one, and so does the higher-dimensional one."]}, "3": {"question": "How do the word embeddings learned by the Cap2Img model compare to the original GloVe embeddings in terms of semantic similarity?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 1, 'Rationale': 'Image 4 directly shows the comparison of performance on semantic similarity benchmarks between GloVe and Cap2Img, which is central to the question. Text 1 mentions the evaluation of grounded word representations on semantic similarity tasks, connecting well with the query.', 'Answer': 'The Cap2Img model learns word embeddings that outperform the original GloVe embeddings on semantic similarity benchmarks MEN, SimLex, RW, and W353. This indicates improved semantic similarity in the learned embeddings.'}", "all_figures_names": ["1707.06320v2-Table1-1.png", "1707.06320v2-Table2-1.png", "1707.06320v2-Table3-1.png", "1707.06320v2-Table4-1.png", "1707.06320v2-Table5-1.png", "1707.06320v2-Figure1-1.png"], "referred_figures_names": ["1707.06320v2-Table5-1.png"], "answer": "The word embeddings learned by the Cap2Img model outperform the original GloVe embeddings in terms of semantic similarity.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["As the results show, Cap2Img performs very well on this task, outperforming the compared models on caption retrieval and being very close to order embeddings on image retrieval. The fact that the system outperforms Order Embeddings on caption retrieval suggests that it has a better sentence encoder. Cap2Both does not work as well on this task as the image-only case, probably because interference from the language signal makes the problem harder to optimize.", "We omit Cap2Cap for reasons of space (it performs similarly to Cap2Both). As shown in Table , the grounded word projections that our network learns yield higher-quality word embeddings on four standard lexical semantic similarity benchmarks: MEN , SimLex-999 , Rare Words and WordSim-353 . Conclusion We have investigated grounding for universal sentence representations.", "Concatenation has been proven to be a strong and straightforward mid-level multi-modal fusion method, previously explored in multi-modal semantics for word representations . We call the combined system GroundSent (GS), and distinguish between sentences perceptually grounded in images (GroundSent-Img), weakly grounded in captions (GroundSent-Cap) or grounded in both (GroundSent-Both). Implementation details We use 300-dimensional GloVe embeddings, trained on WebCrawl, for the initial word representations and optimize using Adam .", "Discussion There are a few other important questions to investigate. The average abstractness or concreteness of the evaluation datasets may have a large impact on performance. In addition, word embeddings from the learned projection from GloVe input embeddings, which now provides a generic word-embedding grounding method even for words that are not present in the image-caption training data, can be examined.", "This model is architecturally identical to the sentence encoder used before: it can be thought of as Cap2Cap, but where the objective is not to predict an alternative caption, but to predict the previous and next sentence in the Toronto Books Corpus, just like SkipThought . We train a 1024-dimensional and 2048-dimensional STb model (for one full iteration, with all other hyperparameters identical to Cap2Cap) to compare against: if grounding improves results because it introduces qualitatively different information, rather than just from having more parameters (i.e., a higher embedding dimensionality), we should expect the multi-modal GroundSent models to perform better not only than STb-1024, but also than STb-2048, which has the same number of parameters (recall that GroundSent models are combinations of grounded and linguistic-only representations). In addition, we compare against an ``ensemble'' of two different STb-1024 models (i.e., a concatenation of two separately trained STb-1024), to check that we are not (just) observing an ensemble effect.", "That is, we concatenate Cap2Cap, Cap2Img or Cap2Both and Skip-Thought with Layer Normalization (ST-LN) representations, yielding GroundSent-Cap, GroundSent-Img and GroundSent-Both representations, respectively. We report performance of ST-LN using SentEval, which led to slightly different numbers than what is reported in their paper. Table shows the results for the semantic classification and entailment tasks.", "Concreteness values make it easier to analyze performance, but are apparently not always direct indicators of improvements with grounding. Grounded word embeddings Our models contain a projection layer that maps the GloVe word embeddings that they receive as inputs to a different embedding space. There has been a lot of interest in grounded word representations in recent years, so it is interesting to examine what kind of word representations our models learn.", "In the first method, we try to predict the image features (Cap2Img). That is, we learn to map the caption to the same space as the image features that represent the correct image. We call this strong perceptual grounding, where we take the visual input directly into account."], "referred_text_chunks": ["As the results show, Cap2Img performs very well on this task, outperforming the compared models on caption retrieval and being very close to order embeddings on image retrieval. The fact that the system outperforms Order Embeddings on caption retrieval suggests that it has a better sentence encoder. Cap2Both does not work as well on this task as the image-only case, probably because interference from the language signal makes the problem harder to optimize."]}, "4": {"question": "What is the role of the \"max\" function in the model architecture?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 1, 'Rationale': 'Image 5 visually shows the use of the max function in the model architecture, which is directly tied to the model in question. Text 1 explains the role of specific components, including reference to the architecture and its objectives.', 'Answer': 'The \"max\" function in the model architecture is used for combining information from different LSTM directions, providing a single representation by element-wise selecting the maximum value at each position.'}", "all_figures_names": ["1707.06320v2-Table1-1.png", "1707.06320v2-Table2-1.png", "1707.06320v2-Table3-1.png", "1707.06320v2-Table4-1.png", "1707.06320v2-Table5-1.png", "1707.06320v2-Figure1-1.png"], "referred_figures_names": ["1707.06320v2-Figure1-1.png"], "answer": "The \"max\" function is used to select the most probable word at each time step in the decoding process.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["We find that it does. The contribution of grounding An important open question is whether the increase in performance in multi-modal semantic models is due to qualitatively different information from grounding, or simply due to the fact that we have more parameters or data from a different distribution. In order to examine this, we implement a SkipThought-like model that also uses a bidirectional LSTM with element-wise max on the final hidden layer (henceforth referred to as STb).", "By jointly training the BiLSTM with these latent projections, we can then ground the language model in its visual counterpart. In particular, let \\Theta = \\Theta_{\\text{BiLSTM \\cup \\{P_\\ell\\_{\\ell=1^L be the parameters of the BiLSTM as well as the projection layers. We then minimize the following ranking loss: where where [x]_+ = \\max(0, x) denotes the threshold function at zero and \\gamma defines the margin.", "This model is architecturally identical to the sentence encoder used before: it can be thought of as Cap2Cap, but where the objective is not to predict an alternative caption, but to predict the previous and next sentence in the Toronto Books Corpus, just like SkipThought . We train a 1024-dimensional and 2048-dimensional STb model (for one full iteration, with all other hyperparameters identical to Cap2Cap) to compare against: if grounding improves results because it introduces qualitatively different information, rather than just from having more parameters (i.e., a higher embedding dimensionality), we should expect the multi-modal GroundSent models to perform better not only than STb-1024, but also than STb-2048, which has the same number of parameters (recall that GroundSent models are combinations of grounded and linguistic-only representations). In addition, we compare against an ``ensemble'' of two different STb-1024 models (i.e., a concatenation of two separately trained STb-1024), to check that we are not (just) observing an ensemble effect.", "we use the usual multiclass classification approach over the vocabulary of the corpus \\Set{V such that Here, \\vec{y_s = \\psi(W_V\\vec{g_s + \\vec{b) and \\vec{g_s is hidden state of the decoder LSTM at time s. \\noindent To learn the model parameters, we minimize the negative log-likelihood over all caption pairs, i.e., Cap2Both Finally, we also integrate both concepts of grounding into a joint model, where we optimize the following loss function: Grounded universal representations On their own, features from this system are likely to suffer from the fact that training on COCO introduces biases: aside from the inherent dataset bias in COCO itself, the system will only have coverage for concrete concepts. COCO is also a much smaller dataset than e.g. the Toronto Books Corpus often used in purely text-based methods . As such, grounded representations are potentially less ``universal'' than text-based alternatives, which also cover abstract concepts.", "As Table shows, a more nuanced picture emerges in this comparison: grounding helps more for some datasets than for others. Grounded models outperform the STb-1024 model (which uses much more data---the Toronto Books Corpus is much larger than COCO) in all cases, often already without concatenating the textual modality. The ensemble of two STb-1024 models performs better than the individual one, and so does the higher-dimensional one.", "We then learn a linear map U \\in \\R^{n \\times n such that \\vx = U \\overline{\\vx and use \\vx as input to the BiLSTM. The linear map U and the BiLSTM are trained jointly. Cap2Img Let \\vv \\in \\R^I be the latent representation of an image (e.g.the final layer of a ResNet).", "In the first method, we try to predict the image features (Cap2Img). That is, we learn to map the caption to the same space as the image features that represent the correct image. We call this strong perceptual grounding, where we take the visual input directly into account.", "Purely text-based semantic models, which represent word meaning as a distribution over other words , suffer from the grounding problem . It has been shown that grounding leads to improved performance on a variety of word-level tasks . Unsupervised sentence representation models are often doubly exposed to the grounding problem, especially if they represent sentence meanings as a distribution over other sentences, as in SkipThought ."], "referred_text_chunks": ["We find that it does. The contribution of grounding An important open question is whether the increase in performance in multi-modal semantic models is due to qualitatively different information from grounding, or simply due to the fact that we have more parameters or data from a different distribution. In order to examine this, we implement a SkipThought-like model that also uses a bidirectional LSTM with element-wise max on the final hidden layer (henceforth referred to as STb)."]}}