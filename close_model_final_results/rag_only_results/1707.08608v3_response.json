{"0": {"question": "Which genre in the SRL-NW network has the lowest failure rate and how does its inference time compare to other genres within the same network?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 directly provides the failure rates and inference times across different genres, which is relevant to the question. Text 0 discusses genre-specific failure rates which aligns with comparing performance across genres.', 'Answer': 'The genre PT (Pivot Corpus) has the lowest failure rate of 10.01% in the SRL-NW network. Its inference time is 8 minutes (Viterbi), 19 minutes (GBI), and 15 minutes (A*), which are relatively lower compared to other genres within the same network.'}", "all_figures_names": ["1707.08608v3-Table8-1.png", "1707.08608v3-Table9-1.png", "1707.08608v3-Table5-1.png", "1707.08608v3-Table11-1.png", "1707.08608v3-Table2-1.png", "1707.08608v3-Table4-1.png", "1707.08608v3-Table10-1.png", "1707.08608v3-Table7-1.png"], "referred_figures_names": ["1707.08608v3-Table11-1.png"], "answer": "The PT genre within the SRL-NW network has the lowest failure rate at 10.01%. Its inference time is also the lowest across all genres in the SRL-NW network for all three inference procedures (Viterbi, GBI, and A*).", "figure_type": "N/A", "content_type": "table", "text_chunks": ["For SRL, we train a SOTA network with ELMo embedding on the NewsWire (NW) section of the OntoNotes v5.0 English PropBank corpus and then test on the other genres provided in the corpus: BC, BN, PT, TC, WB. The failure rate on the within genre data (test set of NW) is 18.10\\%. We can see from Table , the failure rate for the NW trained SRL network in general is higher for out-of-genre data with the highest being 26.86\\% for BC (vs. 18.10\\% NW).", "Further, by enforcing constraints, we see significant gains on the failure set in terms of F1 score across all genres (ranging from 9.39-16.5 F1), thus, providing additional evidences for answering Q5. As we did for SRL, we train a GMNT seq2seq model on the WSJ NW section in OntoNotes v5.0 Treebank which shares the same genre classification with PropBank. The F1 on the within-genre data (test set of WSJ) is 85.03, but the F1 on these genres is much lower, ranging from the mid-forties on BC (46.2--78.5", "Finally, to address (Q3), we evaluate the quality (F1 and exact match) of the output predictions on the network's failure-set both before and after applying our method. From Table , we can see that by applying GBI on SRL-100, the avgerage disagreement rate on the failure set goes down from 44.85\\% to 24.92\\% which results in an improvement of 11.7 F1 and 19.90\\% in terms of exact match on the same set. These improvements answer Q1-3 favorably.", "Consequently, for example, the GNMT network improves (Net2) on the failure-set from 73.54 to 79.68 F1, resulting in an overall improvement from 86.54 to 87.57 F1 (entire test-set). These improvements are similar to those we observe in the SRL task, and provide additional evidence for answering Q1-3 favorably. We also measure how many iterations of our algorithm it takes to convert the examples that have constraint-violations.", "On SRL, for example, the method successfully injects truth-conveying side-information via constraints, improving SOTA network in SRL on which we apply our technique has been advanced by 1.7 F1 points ~. However, this is a training time improvement which is orthogonal to our work. by 1.03 F1 ~.", "For {\\em SRL, we simulate low-resource models by limiting the training data portion to 1\\%, 10\\%, 40\\%, and 70\\% resulting in F1 score range of 67.28-83.55. Similarly, for {\\em syntactic parsing, we train additional low-quality models {\\texttt Net3-5 with a simpler uni-directional encoders/decoders, and on different training data portions of 25\\%, 75\\%, and 100\\% (Table ). We evaluate GBI on each of them in Table~,~ and find further evidence in support of favorable answers to Q2 (satisfying constraints) and Q3 (improving F1 accuracy) by favorably answering Q4.", "To address (Q2) we evaluate our method on the {\\em failure-set (i.e., the set of output sentences for which the original network produces constraint-violating outputs) and measure our method's {\\em conversion rate; that is, the percentage of failures for which our method is able to completely satisfy the constraints (or ``convert''). Finally, to address (Q3), we evaluate the quality (e.g., accuracy or F1) of the output predictions on the network's {\\em failure-set both before and after applying our method. Semantic Role Labeling We employ the AllenNLP SRL network with ELMo embeddings, which is a multi-layer highway bi-LSTM that produces BIO output predictions for each input token .", "In Table , we see that GBI is able to convert 42.25 \\% of failure set, and this boosts the overall F1 measure by 1.23 point over the SOTA network (SRL-100) which does not incorporate the constraints (they report 84.6 F1, we obtain a similar 84.4 F1 with their network, and achieve 85.63 after enforcing constraints with our inference). Further, to address (Q1) we measure the sentence-level {\\it failure rate as well as span-level disagreement rate (i.e., the ratio of predicted spans in a sentence that disagree with the spans implied by the true syntactic parse of the sentence). To address (Q2) we evaluate our method on the failure set (i.e., the set of sentences for which disagreement rate is nonzero) and measure our method's avgerage disagreement rate."], "referred_text_chunks": ["For SRL, we train a SOTA network with ELMo embedding on the NewsWire (NW) section of the OntoNotes v5.0 English PropBank corpus and then test on the other genres provided in the corpus: BC, BN, PT, TC, WB. The failure rate on the within genre data (test set of NW) is 18.10\\%. We can see from Table , the failure rate for the NW trained SRL network in general is higher for out-of-genre data with the highest being 26.86\\% for BC (vs. 18.10\\% NW)."]}, "1": {"question": "Which genre shows the **largest absolute improvement** in F1 score on the failure set after applying GBI for **both** syntactic parsing and SRL?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 2, 'Rationale': 'Image 7 provides a detailed comparison of F1 improvements across different genres for both syntactic parsing and SRL, which directly answers the question. Text 2 discusses significant gains in F1 score across all genres by applying constraints, which complements the information in Image 7.', 'Answer': 'Pivot Corpus (PT) shows the largest absolute improvement in F1 score after applying GBI: +4.4 for syntactic parsing and +16.5 for SRL.'}", "all_figures_names": ["1707.08608v3-Table2-1.png", "1707.08608v3-Table11-1.png", "1707.08608v3-Table1-1.png", "1707.08608v3-Table3-1.png", "1707.08608v3-Table8-1.png", "1707.08608v3-Table9-1.png", "1707.08608v3-Table6-1.png", "1707.08608v3-Table5-1.png"], "referred_figures_names": ["1707.08608v3-Table5-1.png"], "answer": "Pivot Corpus (PT) shows the largest absolute improvement in F1 score on the failure set after applying GBI for both syntactic parsing and SRL.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["In Table , we see that GBI is able to convert 42.25 \\% of failure set, and this boosts the overall F1 measure by 1.23 point over the SOTA network (SRL-100) which does not incorporate the constraints (they report 84.6 F1, we obtain a similar 84.4 F1 with their network, and achieve 85.63 after enforcing constraints with our inference). Further, to address (Q1) we measure the sentence-level {\\it failure rate as well as span-level disagreement rate (i.e., the ratio of predicted spans in a sentence that disagree with the spans implied by the true syntactic parse of the sentence). To address (Q2) we evaluate our method on the failure set (i.e., the set of sentences for which disagreement rate is nonzero) and measure our method's avgerage disagreement rate.", "For {\\em SRL, we simulate low-resource models by limiting the training data portion to 1\\%, 10\\%, 40\\%, and 70\\% resulting in F1 score range of 67.28-83.55. Similarly, for {\\em syntactic parsing, we train additional low-quality models {\\texttt Net3-5 with a simpler uni-directional encoders/decoders, and on different training data portions of 25\\%, 75\\%, and 100\\% (Table ). We evaluate GBI on each of them in Table~,~ and find further evidence in support of favorable answers to Q2 (satisfying constraints) and Q3 (improving F1 accuracy) by favorably answering Q4.", "Further, by enforcing constraints, we see significant gains on the failure set in terms of F1 score across all genres (ranging from 9.39-16.5 F1), thus, providing additional evidences for answering Q5. As we did for SRL, we train a GMNT seq2seq model on the WSJ NW section in OntoNotes v5.0 Treebank which shares the same genre classification with PropBank. The F1 on the within-genre data (test set of WSJ) is 85.03, but the F1 on these genres is much lower, ranging from the mid-forties on BC (46.2--78.5", "Finally, to address (Q3), we evaluate the quality (F1 and exact match) of the output predictions on the network's failure-set both before and after applying our method. From Table , we can see that by applying GBI on SRL-100, the avgerage disagreement rate on the failure set goes down from 44.85\\% to 24.92\\% which results in an improvement of 11.7 F1 and 19.90\\% in terms of exact match on the same set. These improvements answer Q1-3 favorably.", "We can see that the overall performance (F1-score) for \\Astar drops drastically (-6.92) in the presence of noisy constraints while we still see gains with GBI (+0.47). We further analyze the improvement of GBI by looking at the precision and recall scores individually. We see that recall drops slightly for GBI which suggests that noisy constraints do inhibit predicting actual argument spans.", "For SRL, we train a SOTA network with ELMo embedding on the NewsWire (NW) section of the OntoNotes v5.0 English PropBank corpus and then test on the other genres provided in the corpus: BC, BN, PT, TC, WB. The failure rate on the within genre data (test set of NW) is 18.10\\%. We can see from Table , the failure rate for the NW trained SRL network in general is higher for out-of-genre data with the highest being 26.86\\% for BC (vs. 18.10\\% NW).", "depending on the subcategory) to the low-eighties on BN (68.3--81.3. depending on the subcategory). Indeed, we find that overall the F1 is lower and in some cases, like WB, the failure rate is much higher (17.6\\% for WB {\\em vs. 11.9\\% for WSJ).", "For data we use OntoNotes v5.0, which has ground-truth for both SRL and syntactic parsing . We evaluate GBI on the test-set (25.6k examples), out of which consistent parse information is available for 81.25\\% examples (we only include side-information in terms of constraints for this subset). We repeat the same experimental procedure over multiple networks, SRL-X, while varying the portion (X\\%) of the training dataset."], "referred_text_chunks": ["In Table , we see that GBI is able to convert 42.25 \\% of failure set, and this boosts the overall F1 measure by 1.23 point over the SOTA network (SRL-100) which does not incorporate the constraints (they report 84.6 F1, we obtain a similar 84.4 F1 with their network, and achieve 85.63 after enforcing constraints with our inference). Further, to address (Q1) we measure the sentence-level {\\it failure rate as well as span-level disagreement rate (i.e., the ratio of predicted spans in a sentence that disagree with the spans implied by the true syntactic parse of the sentence). To address (Q2) we evaluate our method on the failure set (i.e., the set of sentences for which disagreement rate is nonzero) and measure our method's avgerage disagreement rate."]}, "2": {"question": "How did the accuracy of the model change as the iterations progressed?", "referred_figures_indices": [7], "response": "{'Image': 0, 'Text': 3, 'Rationale': 'Image 0 clearly displays the change in accuracy over iterations, with a final accuracy of 100%. Text 3 discusses the improvement in prediction quality and accuracy after applying constraints, aligning with the question on accuracy change.', 'Answer': 'As iterations progressed, the accuracy of the model increased from 33.3% initially to 100.0% after enforcing constraints.'}", "all_figures_names": ["1707.08608v3-Table8-1.png", "1707.08608v3-Table2-1.png", "1707.08608v3-Table4-1.png", "1707.08608v3-Table10-1.png", "1707.08608v3-Table11-1.png", "1707.08608v3-Table7-1.png", "1707.08608v3-Table6-1.png", "1707.08608v3-Table9-1.png"], "referred_figures_names": ["1707.08608v3-Table9-1.png"], "answer": "The accuracy of the model increased from 66.7% to 100% as the iterations progressed.", "figure_type": "table", "content_type": "table", "text_chunks": ["The runtime increase to 556 mins for GBI-30 as opposed to 288 mins of GBI-10. However, GBI-30 improves significantly in all accuracy metrics compared to GBI-10: overall F1 (+0.34), F1 on failure set (+3.4), exact match (+4.35\\%), and conversion rate (+11.24\\%). As can be seen from the demonstration, there is a clear tradeoff between runtime and accuracy as controlled by the maximum number of epochs M.", "Accuracy in each of the three tasks was improved by respecting constraints. Additionally, for SRL, we employed GBI on a model trained with similar constraint enforcing loss as GBI's ~, and observe that the additional test-time optimization of GBI still significantly improves the model output whereas \\Astar does not. We believe this is because GBI searches in the proximity of the provided model weights; however, theoretical analysis of this hypothesis is left as a future work.", "Consequently, for example, the GNMT network improves (Net2) on the failure-set from 73.54 to 79.68 F1, resulting in an overall improvement from 86.54 to 87.57 F1 (entire test-set). These improvements are similar to those we observe in the SRL task, and provide additional evidence for answering Q1-3 favorably. We also measure how many iterations of our algorithm it takes to convert the examples that have constraint-violations.", "Finally, to address (Q3), we evaluate the quality (F1 and exact match) of the output predictions on the network's failure-set both before and after applying our method. From Table , we can see that by applying GBI on SRL-100, the avgerage disagreement rate on the failure set goes down from 44.85\\% to 24.92\\% which results in an improvement of 11.7 F1 and 19.90\\% in terms of exact match on the same set. These improvements answer Q1-3 favorably.", "Across all the experimental conditions (Table , ), the conversion rates are high, often above 80 and sometimes above 90 supporting Q2. Note that beam search alone can also increase constraint satisfaction with conversion rates reaching as high as 51.74\\% (164/317) in the case of Net3 with beam size 9. However, as the quality of the model increases, the conversion rate becomes minuscule; in the case of Net1,2 the conversion rate is less than 14\\% with beam 9; in Net1 converting 26 out of 187 and in Net2 converting just 1 out of 287 instances from failure set.", "Thus, enforcing constraints at test-time is important. To satisfy constraints, we employ GBI with a constraint loss \\closs, a length-normalized quadratic (3x_a-y_a)^2/(m+n) that is zero when the number of \\texttt{a's in the output (y_a) is exactly three times the number in the input (x_a) with m,n denoting input, output, respectively. GBI achieves a conversion rate of 65.2\\% after 100 iterations, while also improving the accuracy on the failure-set from 75.2\\% to 82.4\\%.", "On SRL, for example, the method successfully injects truth-conveying side-information via constraints, improving SOTA network in SRL on which we apply our technique has been advanced by 1.7 F1 points ~. However, this is a training time improvement which is orthogonal to our work. by 1.03 F1 ~.", "Much like scoped-learning, the algorithm customizes the weights for each example at test-time , but does so in a way to satisfy the constraints. We study GBI on three tasks: semantic role labeling (SRL), syntactic constituency parsing and a synthetic sequence transduction problem and find that the algorithm performs favorably on all three tasks. In summary, our contributions are that we: For all the tasks, we find that GBI satisfies a large percentage of the constraints (up to 98\\%) and that in almost every case (out-of-domain data, state-of-the art networks, and even for the lower-quality networks), enforcing the constraints improves the accuracy."], "referred_text_chunks": ["The runtime increase to 556 mins for GBI-30 as opposed to 288 mins of GBI-10. However, GBI-30 improves significantly in all accuracy metrics compared to GBI-10: overall F1 (+0.34), F1 on failure set (+3.4), exact match (+4.35\\%), and conversion rate (+11.24\\%). As can be seen from the demonstration, there is a clear tradeoff between runtime and accuracy as controlled by the maximum number of epochs M."]}, "3": {"question": "What is the effect of enforcing syntactic constraints on the semantic role labeling output?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 7, 'Rationale': 'Image 1 directly illustrates the impact of enforcing syntactic constraints on semantic role labeling, showing correction of spans and roles. Text 7 provides an explanation of using parse tree constraints to improve semantic labeling accuracy.', 'Answer': 'Enforcing syntactic constraints on semantic role labeling improves accuracy by correcting span inconsistencies and aligning semantic roles with the parse tree structure.'}", "all_figures_names": ["1707.08608v3-Table8-1.png", "1707.08608v3-Table7-1.png", "1707.08608v3-Table4-1.png", "1707.08608v3-Table10-1.png", "1707.08608v3-Table5-1.png", "1707.08608v3-Table3-1.png", "1707.08608v3-Table11-1.png", "1707.08608v3-Table6-1.png"], "referred_figures_names": ["1707.08608v3-Table7-1.png"], "answer": "Enforcing syntactic constraints can correct the number of agreeing spans, and also change the semantic roles assigned to tokens.", "figure_type": "table", "content_type": "table", "text_chunks": ["Semantic Role labeling The \\closs(\\outs,\\lang^x) for SRL factorizes into per-span constraints \\closs_i. For the ith span s_i, if s_i is consistent with any node in the parse tree, \\closs_i(s_i,\\lang^x)=0, otherwise \\closs_i(s_i,\\lang^x)=1/n_{s_i where n_{s_i is defined as the number of tokens in s_i. Overall, where k is number of spans on output \\hat{\\outs.", "Much like scoped-learning, the algorithm customizes the weights for each example at test-time , but does so in a way to satisfy the constraints. We study GBI on three tasks: semantic role labeling (SRL), syntactic constituency parsing and a synthetic sequence transduction problem and find that the algorithm performs favorably on all three tasks. In summary, our contributions are that we: For all the tasks, we find that GBI satisfies a large percentage of the constraints (up to 98\\%) and that in almost every case (out-of-domain data, state-of-the art networks, and even for the lower-quality networks), enforcing the constraints improves the accuracy.", "We provide example-based case studies for each application in Appendix , . Semantic Role Labeling As a first illustrative example, consider SRL. SRL focuses on identifying shallow semantic information about phrases.", "Our work instead focuses on enforcing constraints at inference-time. More specifically, for SRL, previous work for enforcing % constraints have focused on constrained \\Astar decoding or integer linear programming . For parsing, previous work in enforcing hard constraints has focused on post-processing or building them into the decoder transitions or search constraints .", "However, GBI does not try to reduce {\\em expected reward and terminates upon discovery of an output that satisfies all constraints. Furthermore, GBI also works on sequence-tagging problem, SRL (Section ), where next output does not depend on the current output, which is far from REINFORCE setting. Applications There are multiple applications that involve hard-constraints and we provide two illustrative examples that we later employ as case-studies in our experiments: SRL and syntactic parsing.", "All tasks require hard constraints, but they play a different role in each. In the transduction task they force the output to belong to a particular input-dependent regular expression, in SRL, constraints provide side-information about possible true-spans and in parsing, constraints ensure that the outputs encode valid trees. While the SRL task involves more traditional recurrent neural networks that have exactly one output per input token, the parsing and transduction tasks provide an opportunity to study the algorithm on various seq2seq networks .", "To address (Q2) we evaluate our method on the {\\em failure-set (i.e., the set of output sentences for which the original network produces constraint-violating outputs) and measure our method's {\\em conversion rate; that is, the percentage of failures for which our method is able to completely satisfy the constraints (or ``convert''). Finally, to address (Q3), we evaluate the quality (e.g., accuracy or F1) of the output predictions on the network's {\\em failure-set both before and after applying our method. Semantic Role Labeling We employ the AllenNLP SRL network with ELMo embeddings, which is a multi-layer highway bi-LSTM that produces BIO output predictions for each input token .", "In particular, each node of the parse tree represents a contiguous segment of tokens that could be a candidate for a semantic role. Therefore, we can include as side-information constraints that force the BIO-encoded class labeling to produce segments of text that each agree with some segment of text expressed by a node in the parse tree. To continue with our example, the original SRL sequence-labeling might incorrectly label ``really like this'' as the second argument rather than ``like this.''"], "referred_text_chunks": ["Semantic Role labeling The \\closs(\\outs,\\lang^x) for SRL factorizes into per-span constraints \\closs_i. For the ith span s_i, if s_i is consistent with any node in the parse tree, \\closs_i(s_i,\\lang^x)=0, otherwise \\closs_i(s_i,\\lang^x)=1/n_{s_i where n_{s_i is defined as the number of tokens in s_i. Overall, where k is number of spans on output \\hat{\\outs."]}, "4": {"question": "What is the relationship between the number of shifts and the accuracy of the output?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 4, 'Rationale': 'Image 5 explicitly shows iterations that increase the number of shifts to meet constraints, resulting in improved accuracy. Text chunk 4 describes how correcting initial mistakes with added shifts improves global output, directly addressing the question.', 'Answer': 'Increasing the number of shifts to meet constraints improves the accuracy of the output by correcting initial errors and reducing propagation throughout the sequence.'}", "all_figures_names": ["1707.08608v3-Table6-1.png", "1707.08608v3-Table4-1.png", "1707.08608v3-Table11-1.png", "1707.08608v3-Table9-1.png", "1707.08608v3-Table1-1.png", "1707.08608v3-Table8-1.png", "1707.08608v3-Table10-1.png", "1707.08608v3-Table3-1.png"], "referred_figures_names": ["1707.08608v3-Table8-1.png"], "answer": "The accuracy of the output increases as the number of shifts increases.", "figure_type": "table", "content_type": "table", "text_chunks": ["The runtime increase to 556 mins for GBI-30 as opposed to 288 mins of GBI-10. However, GBI-30 improves significantly in all accuracy metrics compared to GBI-10: overall F1 (+0.34), F1 on failure set (+3.4), exact match (+4.35\\%), and conversion rate (+11.24\\%). As can be seen from the demonstration, there is a clear tradeoff between runtime and accuracy as controlled by the maximum number of epochs M.", "Thus, enforcing constraints at test-time is important. To satisfy constraints, we employ GBI with a constraint loss \\closs, a length-normalized quadratic (3x_a-y_a)^2/(m+n) that is zero when the number of \\texttt{a's in the output (y_a) is exactly three times the number in the input (x_a) with m,n denoting input, output, respectively. GBI achieves a conversion rate of 65.2\\% after 100 iterations, while also improving the accuracy on the failure-set from 75.2\\% to 82.4\\%.", "Accuracy in each of the three tasks was improved by respecting constraints. Additionally, for SRL, we employed GBI on a model trained with similar constraint enforcing loss as GBI's ~, and observe that the additional test-time optimization of GBI still significantly improves the model output whereas \\Astar does not. We believe this is because GBI searches in the proximity of the provided model weights; however, theoretical analysis of this hypothesis is left as a future work.", "Finally, to address (Q3), we evaluate the quality (F1 and exact match) of the output predictions on the network's failure-set both before and after applying our method. From Table , we can see that by applying GBI on SRL-100, the avgerage disagreement rate on the failure set goes down from 44.85\\% to 24.92\\% which results in an improvement of 11.7 F1 and 19.90\\% in terms of exact match on the same set. These improvements answer Q1-3 favorably.", "Rather than fixing the constraint by inserting these extra commands at the end of the sequence as the greedy decoder must do, GBI inserts them at the beginning of the sequence where the initial mistake was made, thereby correcting the initial mistake. Moreover, this correction propagates to a mistake made later in the sequence (viz., the the sequence of three reduces after the four shifts) and fixes them too. This example provides evidence that GBI can indeed enforce constraints holistically and that doing so improves the output in a global sense.", "Consequently, for example, the GNMT network improves (Net2) on the failure-set from 73.54 to 79.68 F1, resulting in an overall improvement from 86.54 to 87.57 F1 (entire test-set). These improvements are similar to those we observe in the SRL task, and provide additional evidence for answering Q1-3 favorably. We also measure how many iterations of our algorithm it takes to convert the examples that have constraint-violations.", "The user can control M by the runtime constraint the system has: lower M when the serving time is most important and larger M when accuracy is more important than the serving time.", "For the provided transduction function, the number of \\texttt{a's in the output should be three times the number of \\texttt{a's in the input. To express this constraint, we define following constraint loss \\closs, a length-normalized quadratic that is zero when the number of \\texttt{a's in the output (y_a) is exactly three times the number in the input (x_a) with m,n denoting input length, output length, respectively. Analyzing the behavior of different inference procedures in the presence of noisy constraints Table reports the performance of GBI and \\Astar in the presence of noisy constraints."], "referred_text_chunks": ["The runtime increase to 556 mins for GBI-30 as opposed to 288 mins of GBI-10. However, GBI-30 improves significantly in all accuracy metrics compared to GBI-10: overall F1 (+0.34), F1 on failure set (+3.4), exact match (+4.35\\%), and conversion rate (+11.24\\%). As can be seen from the demonstration, there is a clear tradeoff between runtime and accuracy as controlled by the maximum number of epochs M."]}, "5": {"question": "Which inference method consistently leads to the highest F1 score on the failure set across all three networks (Net3, Net4, and Net5)?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 4, 'Rationale': \"Image 6 shows a table evaluating different inference methods across Net3, Net4, and Net5, with F1 scores before and after applying GBI. Text chunk 4 compares inference procedures, highlighting GBI's performance boost. Both provide specific insights into inference methods and their impact on F1 scores.\", 'Answer': 'GBI consistently leads to the highest F1 score on the failure set across Net3, Net4, and Net5.'}", "all_figures_names": ["1707.08608v3-Table3-1.png", "1707.08608v3-Table7-1.png", "1707.08608v3-Table9-1.png", "1707.08608v3-Table5-1.png", "1707.08608v3-Table10-1.png", "1707.08608v3-Table6-1.png", "1707.08608v3-Table4-1.png", "1707.08608v3-Table2-1.png"], "referred_figures_names": ["1707.08608v3-Table4-1.png"], "answer": "Beam search with a width of 9 consistently leads to the highest F1 score on the failure set across all three networks.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["In total, we train five networks \\texttt{Net1-5 for this study, that we describe below. We train our two best baseline models ({\\texttt Net1,2) using a highly competitive seq2seq architecture for machine translation, GNMT with F1 scores, 86.78 and 87.33, respectively. And, to study a wider range of accuracies, we train a simpler architecture with different hyper parameters and obtain nets ({Net3-5).", "Consequently, for example, the GNMT network improves (Net2) on the failure-set from 73.54 to 79.68 F1, resulting in an overall improvement from 86.54 to 87.57 F1 (entire test-set). These improvements are similar to those we observe in the SRL task, and provide additional evidence for answering Q1-3 favorably. We also measure how many iterations of our algorithm it takes to convert the examples that have constraint-violations.", "Further, by enforcing constraints, we see significant gains on the failure set in terms of F1 score across all genres (ranging from 9.39-16.5 F1), thus, providing additional evidences for answering Q5. As we did for SRL, we train a GMNT seq2seq model on the WSJ NW section in OntoNotes v5.0 Treebank which shares the same genre classification with PropBank. The F1 on the within-genre data (test set of WSJ) is 85.03, but the F1 on these genres is much lower, ranging from the mid-forties on BC (46.2--78.5", "Finally, to address (Q3), we evaluate the quality (F1 and exact match) of the output predictions on the network's failure-set both before and after applying our method. From Table , we can see that by applying GBI on SRL-100, the avgerage disagreement rate on the failure set goes down from 44.85\\% to 24.92\\% which results in an improvement of 11.7 F1 and 19.90\\% in terms of exact match on the same set. These improvements answer Q1-3 favorably.", "For all models, we employ Glorot initialization, and basic attention . See Table~ for a summary of the networks, hyper-parameters, and their performance. We report the behavior of the constraint-satisfaction method in Table~ for Net1-2, and in Table~ for Net3-5.", "For SRL, we train a SOTA network with ELMo embedding on the NewsWire (NW) section of the OntoNotes v5.0 English PropBank corpus and then test on the other genres provided in the corpus: BC, BN, PT, TC, WB. The failure rate on the within genre data (test set of NW) is 18.10\\%. We can see from Table , the failure rate for the NW trained SRL network in general is higher for out-of-genre data with the highest being 26.86\\% for BC (vs. 18.10\\% NW).", "In Table , we see that GBI is able to convert 42.25 \\% of failure set, and this boosts the overall F1 measure by 1.23 point over the SOTA network (SRL-100) which does not incorporate the constraints (they report 84.6 F1, we obtain a similar 84.4 F1 with their network, and achieve 85.63 after enforcing constraints with our inference). Further, to address (Q1) we measure the sentence-level {\\it failure rate as well as span-level disagreement rate (i.e., the ratio of predicted spans in a sentence that disagree with the spans implied by the true syntactic parse of the sentence). To address (Q2) we evaluate our method on the failure set (i.e., the set of sentences for which disagreement rate is nonzero) and measure our method's avgerage disagreement rate.", "Across all the experimental conditions (Table , ), the conversion rates are high, often above 80 and sometimes above 90 supporting Q2. Note that beam search alone can also increase constraint satisfaction with conversion rates reaching as high as 51.74\\% (164/317) in the case of Net3 with beam size 9. However, as the quality of the model increases, the conversion rate becomes minuscule; in the case of Net1,2 the conversion rate is less than 14\\% with beam 9; in Net1 converting 26 out of 187 and in Net2 converting just 1 out of 287 instances from failure set."], "referred_text_chunks": ["In total, we train five networks \\texttt{Net1-5 for this study, that we describe below. We train our two best baseline models ({\\texttt Net1,2) using a highly competitive seq2seq architecture for machine translation, GNMT with F1 scores, 86.78 and 87.33, respectively. And, to study a wider range of accuracies, we train a simpler architecture with different hyper parameters and obtain nets ({Net3-5)."]}, "6": {"question": "How does GBI compare to A* in terms of reducing disagreement rate on the SRL-100 network's failure set?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 directly compares the average disagreement rates before and after applying GBI and A* on the SRL-100 network, which addresses the question. Text 0 discusses the improvement in disagreement rate when using GBI, specifically addressing the failure set, which is relevant to the question.', 'Answer': 'GBI reduces the average disagreement rate on the SRL-100 network\\'s failure set from 44.85% to 24.92%, while A* reduces it to 33.91%. This indicates that GBI is more effective than A* at reducing the disagreement rate.'}", "all_figures_names": ["1707.08608v3-Table5-1.png", "1707.08608v3-Table11-1.png", "1707.08608v3-Table3-1.png", "1707.08608v3-Table1-1.png", "1707.08608v3-Table8-1.png", "1707.08608v3-Table6-1.png", "1707.08608v3-Table10-1.png", "1707.08608v3-Table9-1.png"], "referred_figures_names": ["1707.08608v3-Table1-1.png"], "answer": "GBI is more effective than A* in reducing the disagreement rate on the SRL-100 network's failure set. After applying GBI, the average disagreement rate drops to 24.92%, while A* only reduces it to 33.91%. This represents an 19.93% greater reduction in disagreement rate when using GBI compared to A*.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Finally, to address (Q3), we evaluate the quality (F1 and exact match) of the output predictions on the network's failure-set both before and after applying our method. From Table , we can see that by applying GBI on SRL-100, the avgerage disagreement rate on the failure set goes down from 44.85\\% to 24.92\\% which results in an improvement of 11.7 F1 and 19.90\\% in terms of exact match on the same set. These improvements answer Q1-3 favorably.", "In Table , we see that GBI is able to convert 42.25 \\% of failure set, and this boosts the overall F1 measure by 1.23 point over the SOTA network (SRL-100) which does not incorporate the constraints (they report 84.6 F1, we obtain a similar 84.4 F1 with their network, and achieve 85.63 after enforcing constraints with our inference). Further, to address (Q1) we measure the sentence-level {\\it failure rate as well as span-level disagreement rate (i.e., the ratio of predicted spans in a sentence that disagree with the spans implied by the true syntactic parse of the sentence). To address (Q2) we evaluate our method on the failure set (i.e., the set of sentences for which disagreement rate is nonzero) and measure our method's avgerage disagreement rate.", "This synthetic experiment provides additional evidence in support of Q2 and Q3, on a simpler small-capacity network. GBI on wide range of reference models The foregoing experimental results provide evidence that GBI is a viable method for enforcing constraints. However, we hitherto study GBI on high quality reference networks such as SRL-100.", "Robustness and Runtime analysis We perform additional experiments to analyze the robustness and runtime of GBI. First, to measure robustness, we consider a variant of the SRL task in which we include noisy constraints, and compare GBI to \\Astar (Appendix ). We find that in this case, \\Astar performs significantly worse than the baseline, while GBI improves over the same baseline, thus showing the robustness of GBI.", "Accuracy in each of the three tasks was improved by respecting constraints. Additionally, for SRL, we employed GBI on a model trained with similar constraint enforcing loss as GBI's ~, and observe that the additional test-time optimization of GBI still significantly improves the model output whereas \\Astar does not. We believe this is because GBI searches in the proximity of the provided model weights; however, theoretical analysis of this hypothesis is left as a future work.", "One exception is the BC domain where GBI is just slightly faster than A*. We hypothesize it might be due to the difficulty of the constraint violations as its failure rate is higher than usual. GBI will spend more time searching for the correct output (more iterations) if it is harder to find the solution.", "In terms of runtime, GBI is generally faster than \\Astar, though, the difference is less clear on smaller evaluation sets (Appendix ). In the case study with noisy constraints, the runtimes are similar; however, GBI has much better accuracy, showing similar gains as the noise-free setting. Lastly, in appendix , we discuss GBI's trade off between runtime and accuracy by varying the max epoch M. Related work Recent work has considered applying neural networks to structured prediction; for example, structured prediction energy networks (SPENs) .", "To enforce constraints during inference, \\citeauthor{he2017deep proposed to employ constrained-\\Astar decoding. For the sake of a fair comparison with GBI, we consider \\Astar decoding as used in and report results for the SRL-X networks. We see from Table , that the GBI procedure consistently outperforms \\Astar decoding on all evaluation metrics, thus demonstrating the superiority of the approach."], "referred_text_chunks": ["Finally, to address (Q3), we evaluate the quality (F1 and exact match) of the output predictions on the network's failure-set both before and after applying our method. From Table , we can see that by applying GBI on SRL-100, the avgerage disagreement rate on the failure set goes down from 44.85\\% to 24.92\\% which results in an improvement of 11.7 F1 and 19.90\\% in terms of exact match on the same set. These improvements answer Q1-3 favorably."]}}