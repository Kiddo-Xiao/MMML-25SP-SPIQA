{"0": {"question": "What are the different steps involved in reconstructing implicit warrants for argument reasoning comprehension?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 visually outlines the eight-step methodology for reconstructing implicit warrants, directly addressing the question about steps. Text 0 describes a specific methodology involving eight steps, aligning perfectly with the question.', 'Answer': 'The eight steps involved in reconstructing implicit warrants are:\\n1. Stance annotation\\n2. Reason span annotations\\n3. Reason gist summarization\\n4. Reason disambiguation\\n5. Alternative warrant\\n6. Alternative warrant validation\\n7. Warrant for original claim\\n8. Warrant validation.'}", "all_figures_names": ["1708.01425v4-Figure2-1.png", "1708.01425v4-Figure3-1.png", "1708.01425v4-Table2-1.png", "1708.01425v4-Figure4-1.png", "1708.01425v4-Table1-1.png", "1708.01425v4-Figure5-1.png"], "referred_figures_names": ["1708.01425v4-Figure2-1.png"], "answer": "The different steps involved in reconstructing implicit warrants for argument reasoning comprehension are:\n1. Sampling comments\n2. Stance annotation\n3. Reason span annotations\n4. Reason gist summarization\n5. Reason disambiguation\n6. Alternative warrant\n7. Alternative warrant validation\n8. Warrant for original claim\n9. Warrant validation", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Moreover, we proposed a methodology to systematically reconstruct implicit warrants in eight consecutive steps. So far, we implemented the methodology in a manual crowdsourcing process, along with a strategy that enables standard~inter-annotator agreement measures in crowdsourcing. Following the process, we constructed a new dataset with 1,970 instances for the task.", "The reason behind is probably that the opposing claims are lexically very close, usually negated, and the models cannot pick this up. This underlines that argument reasoning comprehension cannot be solved by simply looking at the occurring words or phrases. Conclusion and Outlook We presented a new task called argument reasoning comprehension that tackles the core of reasoning in natural language argumentation --- implicit warrants.", "The study results in 1,970 high-quality instances for a new task that we call argument reasoning comprehension: Given a reason and a claim, identify the correct warrant from two opposing options. An example is given in Figure . A solution to this task will represent a substantial step towards automatic warrant reconstruction.", "As a result, trying to give a plausible reasoning for the opposite claim \\neg C either leads to nonsense or to a proposition that resembles a rebuttal rather than a warrant . However, if both W and AW are available, they usually capture the core of a reason's relevance and reveal the implicit presuppositions (examples follow further below). Based on our key hypothesis, we define the argument reasoning comprehension task as: Given a reason R and a claim C along with the title and a short description of the debate they occur in, identify the correct warrant W from two candidates: the correct warrant W and an incorrect alternative warrant AW.", "Examples Below, we show three examples in which implicit common-sense presuppositions were revealed during the construction of the alternative warrant AW and the original warrant~W. For brevity, we omit the debate title and description here. A full walk-through example is found in the supplementary material. \\medskip \\makebox[\\linewidth]{\\rule{\\linewidth{0.1pt \\makebox[\\linewidth]{\\rule{\\linewidth{0.1pt Experiments Given the dataset, we performed first experiments to assess the complexity of argument reasoning comprehension.", "In a direct fashion, \\newcite{Boltuzic. Snajder.2016.ArgMinWS let annotators write down implicit warrants, but they concluded only with a preliminary analysis due to large variance in the responses. In an indirect fashion, implicit warrants correspond to major premises in argumentation schemes; a concept heavily referenced in argumentation theory .", "It has been claimed that for reconstructing and evaluating natural language arguments, one has to fully `roll out' their implicit premises \\citep[Chap.~3.2]{vanEemeren.et.al.2014 and leverage knowledge bases \\citep{Wyner.et.al.2016.ArgCompJournal. We believe that a system that can distinguish between the wrong and the right warrant given its context will be helpful in filtering out good candidates in argument reconstruction. For the moment, we just made a first empirical step towards exploring how much common-sense reasoning is necessary in argumentation and how much common sense there might be at all.", "However, we present experiments with several neural attention and language models which reveal that current approaches based on the words and phrases in arguments and warrants do not suffice to solve the task. The main contributions of this paper are (1) a methodology for obtaining implicit warrants realized by means of scalable crowdsourcing and (2) a new task along with a high-quality dataset. In addition, we provide (a) 2,884 user-generated arguments annotated for their stance, covering 50+ controversial topics, (b) 2,026 arguments with annotated reasons supporting the stance, (c) 4,235 rephrased reason gists, useful for argument summarization and sentence compression, and (d) a method for checking the reliability of crowdworkers in document and span labeling using traditional inter-annotator agreement measures."], "referred_text_chunks": ["Moreover, we proposed a methodology to systematically reconstruct implicit warrants in eight consecutive steps. So far, we implemented the methodology in a manual crowdsourcing process, along with a strategy that enables standard~inter-annotator agreement measures in crowdsourcing. Following the process, we constructed a new dataset with 1,970 instances for the task."]}, "1": {"question": "What is the relationship between the number of workers per \"expert\" and Cohen's kappa agreement for stance annotation?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 3, 'Rationale': 'Image 1 directly shows the relationship between workers per \"expert\" and Cohen\u2019s \u03ba agreement for stance annotation. Text 3 describes how agreement scores are influenced by varying the crowd size. Together, they provide the most focused information.', 'Answer': 'Cohen\\'s kappa agreement increases with more workers per \"expert\", showing higher reliability in stance annotation as crowd size increases.'}", "all_figures_names": ["1708.01425v4-Figure2-1.png", "1708.01425v4-Figure3-1.png", "1708.01425v4-Table2-1.png", "1708.01425v4-Figure4-1.png", "1708.01425v4-Table1-1.png", "1708.01425v4-Figure5-1.png"], "referred_figures_names": ["1708.01425v4-Figure3-1.png"], "answer": "The Cohen's kappa agreement for stance annotation increases as the number of workers per \"expert\" increases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Figure shows the Cohen's \\kappa agreement for stance annotation with respect to the crowd size computed by our method. As MACE also includes a threshold for keeping only the most confident predictions in order to benefit precision, we tuned this parameter, too. Deciding on the number of workers per task is a trade-off between the desired quality and the budget.", "For example, reason span annotation is a harder task; however, the results for six workers are comparable to those for the expert annotations of \\newcite{Habernal. Gurevych.2017.COLI.% which refers to stance annotation. Table lists statistics of the entire crowdsourcing process carried out for our dataset, including tasks for which we created data as a by-product.", "A more detailed description is available in the readme file of the source code. \\end{table* Agreement and Dataset Statistics To strictly assess quality in the entire crowdsourcing process, we propose an evaluation method that enables `classic' inter-annotator agreement measures for crowdsourcing, such as Fleiss' \\kappa or Krippendorff's \\alpha.", "Having two independent `experts' from the crowd allowed us to compute standard agreement scores. We also varied the size of the sub-sample from each group from 1 to 9 by repeated random sampling of assignments. This revealed how the score varies with respect to the crowd size per `expert'.", "All 2,884 comments in our dataset classified as stance-taking by the crowdworkers were then also annotated as to whether being sarcastic or ironic; both pose challenges in analyzing argumentation not solved so far . \\paragraph{2.\\ Reason Span Annotation For all comments taking a stance, the next step is to select those spans that give a reason for the claim (with a single EDU as the minimal unit). In our dataset, the workers found 5,119 reason spans, of which 2,026 lay within arguments.", "Applying \\kappa and \\alpha directly to crowdsourced data has been disputed . For estimating gold labels from the crowd, several models have been proposed; we rely on MACE . Given a number of noisy workers, MACE outputs best estimates, outperforming simple majority votes.", "Before, each comment was split into elementary discourse units (EDUs) using SistaNLP . \\paragraph{1.\\ Stance Annotation For each comment, we first classify what stance it is taking (recall that we always have two explicit claims with opposing stance). Alternatively, it may be neutral (considering both sides) or may not take any stance.% who showed that the majority of claims in user-generated arguments are implicit.", "This number might not seem large (e.g., compared to 0.5M from SNLI), but tasks with hand-crafted data are of a similar size (e.g., 3,744 Story Cloze Test instances). Also, the crowdsourcing process is scalable and is limited only by the budget. Moreover, we created several data `by-products' that are valuable for argumentation research: 5,000 comments annotated with stance, which outnumbers the 4,163 tweets for stance detection of \\newcite{Mohammad.et.al.2016.SemEval; 2,026 arguments with 4,235 annotated reasons, which is six times larger than the 340 documents of \\newcite{Habernal.Gurevych.2017.COLI; and 4,235 summarized reason gists --- we are not aware of any other hand-crafted dataset for abstractive argument summarization built upon authentic arguments."], "referred_text_chunks": ["Figure shows the Cohen's \\kappa agreement for stance annotation with respect to the crowd size computed by our method. As MACE also includes a threshold for keeping only the most confident predictions in order to benefit precision, we tuned this parameter, too. Deciding on the number of workers per task is a trade-off between the desired quality and the budget."]}, "2": {"question": "Which approach performs best on the development set?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 3, 'Rationale': 'Image 2 directly shows the accuracy of different approaches on the development set. Text 3 provides relevant insights into the performance of neural attention models versus language models.', 'Answer': 'Intra-warrant attention with context performs best on the development set with an accuracy of 0.637.'}", "all_figures_names": ["1708.01425v4-Figure2-1.png", "1708.01425v4-Figure3-1.png", "1708.01425v4-Table2-1.png", "1708.01425v4-Figure4-1.png", "1708.01425v4-Table1-1.png", "1708.01425v4-Figure5-1.png"], "referred_figures_names": ["1708.01425v4-Table2-1.png"], "answer": "Intra-warrant attention with context.", "figure_type": "table", "content_type": "table", "text_chunks": ["To this end, we split the 1,970 instances into three sets based on the year of the debate they were taken from: 2011--2015 became the training set (1,210 instances), 2016 the development set (316 instances), and 2017 the test set (444 instances). This follows the paradigm of learning on past data and predicting on new ones. In addition, it removes much lexical and topical overlap.", "Intra-warrant attention with rich context outperforms standard neural models with a simple attention, but it only slightly beats the language model on the dev set. The language model is basically random on the test set. A manual error analysis of 50 random wrong predictions (a single run of the best-performing system on the dev set) revealed no explicit pattern of encountered errors.", "Our more elaborated version used {\\em intra-warrant attention, as shown in Figure . Both versions were also extended with the debate title and description added as context to the attention layer (w/ context). We trained the resulting four models using the ADAM optimizer, with heavy dropout (0.9) and early stopping (5 epochs), tuned on the development set.", "Based on the dataset, we evaluated human performance in argument reasoning comprehension. Our findings suggest that the task is harder for people without formal argumentation training, while being solvable without knowing the topic. We also found that neural attention models outperform language models on the task.", "If the label is 0, W_0 is the correct warrant, otherwise W_1. Recall that we have two warrants W and AW whose correctness depends on the claim: W is correct for R and the original claim C, whereas AW would be correct for R and the opposite claim \\neg C. We thus doubled the training data by adding a permuted instance (R, C, W_1, W_0) with the respective correct label; this led to increased performance. The overall results of all approaches (humans and systems) are shown in Table .", "Figure shows the Cohen's \\kappa agreement for stance annotation with respect to the crowd size computed by our method. As MACE also includes a threshold for keeping only the most confident predictions in order to benefit precision, we tuned this parameter, too. Deciding on the number of workers per task is a trade-off between the desired quality and the budget.", "Applying \\kappa and \\alpha directly to crowdsourced data has been disputed . For estimating gold labels from the crowd, several models have been proposed; we rely on MACE . Given a number of noisy workers, MACE outputs best estimates, outperforming simple majority votes.", "Introduction Most house cats face enemies. Russia has the opposite objectives of the US. There is much innovation in 3-d printing and it is sustainable."], "referred_text_chunks": ["To this end, we split the 1,970 instances into three sets based on the year of the debate they were taken from: 2011--2015 became the training set (1,210 instances), 2016 the development set (316 instances), and 2017 the test set (444 instances). This follows the paradigm of learning on past data and predicting on new ones. In addition, it removes much lexical and topical overlap."]}, "3": {"question": "Does formal training in reasoning, logic, or argumentation seem to have a significant effect on argument reasoning comprehension accuracy for people with graduate degrees?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 2, 'Rationale': 'Image 3 directly illustrates the effect of formal training on argument reasoning comprehension accuracy across different educational levels, including graduate degrees. Text 2 provides quantitative data on the impact of formal training on reasoning accuracy, showing higher scores for those with such training.', 'Answer': 'Yes, formal training in reasoning has a significant effect on argument reasoning comprehension accuracy for people with graduate degrees, as shown by higher accuracy scores in the presence of extensive training.'}", "all_figures_names": ["1708.01425v4-Figure2-1.png", "1708.01425v4-Figure3-1.png", "1708.01425v4-Table2-1.png", "1708.01425v4-Figure4-1.png", "1708.01425v4-Table1-1.png", "1708.01425v4-Figure5-1.png"], "referred_figures_names": ["1708.01425v4-Figure4-1.png"], "answer": "No, it does not appear to have a significant effect.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Based on the dataset, we evaluated human performance in argument reasoning comprehension. Our findings suggest that the task is harder for people without formal argumentation training, while being solvable without knowing the topic. We also found that neural attention models outperform language models on the task.", "Not really, i.e., we found no significant (Spearman) correlation between the mean score and familiarity of a participant in almost all education/training configurations. This suggests that argument reasoning comprehension skills are likely to be independent of topic-specific knowledge. Computational Models To assess the complexity of computationally approaching argument reasoning comprehension, we carried out first experiments with systems based on the following models.", "Whereas education had almost negligible influence on the performance, the more extensive formal training in reasoning the participants had, the higher their score was. Overall, 30 of the 173 participants scored 100\\%. The mean score for those with extensive formal training was 90.9\\%.", "While the understanding of semantic inference is crucial in language comprehension, argumentation also requires coping with phenomena beyond~semantics. \\newcite{Rajpurkar.et.al.2016.EMNLP presented a large dataset for reading comprehension by answering questions over Wikipedia articles (SQuAD). In an analysis of this dataset \\newcite{Sugawara.Aizawa.2016.EMNLP-WS found, though, that only 6.2\\% of the questions require causal reasoning, 1.2\\% logical reasoning, and 0\\% analogy.", "The reason behind is probably that the opposing claims are lexically very close, usually negated, and the models cannot pick this up. This underlines that argument reasoning comprehension cannot be solved by simply looking at the occurring words or phrases. Conclusion and Outlook We presented a new task called argument reasoning comprehension that tackles the core of reasoning in natural language argumentation --- implicit warrants.", "The study results in 1,970 high-quality instances for a new task that we call argument reasoning comprehension: Given a reason and a claim, identify the correct warrant from two opposing options. An example is given in Figure . A solution to this task will represent a substantial step towards automatic warrant reconstruction.", "It has been claimed that for reconstructing and evaluating natural language arguments, one has to fully `roll out' their implicit premises \\citep[Chap.~3.2]{vanEemeren.et.al.2014 and leverage knowledge bases \\citep{Wyner.et.al.2016.ArgCompJournal. We believe that a system that can distinguish between the wrong and the right warrant given its context will be helpful in filtering out good candidates in argument reconstruction. For the moment, we just made a first empirical step towards exploring how much common-sense reasoning is necessary in argumentation and how much common sense there might be at all.", "Since their logical formalism builds upon an enhanced version of Aristotle's syllogisms, its applicability to natural language argumentation remains limited (see our discussion above). In contrast to our data source, a few synthetic datasets for general natural language reasoning have been recently introduced, such as answers to questions over a described physical world or an evaluation set of 100 questions in the Winograd Schema Challenge . Finally, we note that, although being related, research on argument mining, argumentation quality, and stance classification is not in the immediate scope of this paper."], "referred_text_chunks": ["Based on the dataset, we evaluated human performance in argument reasoning comprehension. Our findings suggest that the task is harder for people without formal argumentation training, while being solvable without knowing the topic. We also found that neural attention models outperform language models on the task."]}, "4": {"question": "Which step in the methodology resulted in the largest decrease in the size of the dataset?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 0, 'Rationale': 'Image 4 provides specific dataset sizes for each step, allowing identification of the largest decrease. Text 0 states the process of reducing instances by filtering with a logic score.', 'Answer': 'Step 7 resulted in the largest decrease from 5,342 to 2,613 due to filtering with a logic score in Step 6.'}", "all_figures_names": ["1708.01425v4-Figure2-1.png", "1708.01425v4-Figure3-1.png", "1708.01425v4-Table2-1.png", "1708.01425v4-Figure4-1.png", "1708.01425v4-Table1-1.png", "1708.01425v4-Figure5-1.png"], "referred_figures_names": ["1708.01425v4-Table1-1.png"], "answer": "Step 4, Reason disambiguation.", "figure_type": "table", "content_type": "table", "text_chunks": ["We ended up with 1,970 instances to be used for the argument reasoning comprehension task. \\end{scriptsize \\caption{ Details and statistics of the datasets resulting from the eight steps of our methodology implemented in a crowdsourcing process. *Input instances were filtered by their `logic score' assigned in Step 6, such that the weakest 30\\% were discarded.", "The methodology we propose consists of eight consecutive steps that are illustrated in Figure~ and detailed below. Each step can be operationalized with crowdsourcing. For our dataset, we performed crowdsourcing on 5,000 randomly sampled comments using Amazon Mechanical Turk (AMT) from December 2016 to April 2017.", "To this end, we split the 1,970 instances into three sets based on the year of the debate they were taken from: 2011--2015 became the training set (1,210 instances), 2016 the development set (316 instances), and 2017 the test set (444 instances). This follows the paradigm of learning on past data and predicting on new ones. In addition, it removes much lexical and topical overlap.", "Moreover, we proposed a methodology to systematically reconstruct implicit warrants in eight consecutive steps. So far, we implemented the methodology in a manual crowdsourcing process, along with a strategy that enables standard~inter-annotator agreement measures in crowdsourcing. Following the process, we constructed a new dataset with 1,970 instances for the task.", "Having two independent `experts' from the crowd allowed us to compute standard agreement scores. We also varied the size of the sub-sample from each group from 1 to 9 by repeated random sampling of assignments. This revealed how the score varies with respect to the crowd size per `expert'.", "Our more elaborated version used {\\em intra-warrant attention, as shown in Figure . Both versions were also extended with the debate title and description added as context to the attention layer (w/ context). We trained the resulting four models using the ADAM optimizer, with heavy dropout (0.9) and early stopping (5 epochs), tuned on the development set.", "We computed log-likelihood of the candidate warrants and picked the one with lower score. To specifically appoach the given task, we implemented two neural models based on a bidirectional LSTM. In the standard {\\em attention version, we encoded the reason and claim using a BiLSTM and provided it as an attention vector after max-pooling to LSTM layers from the two available warrants W_0 and W_1 (corresponding to W and AW, see below).", "Figure shows the Cohen's \\kappa agreement for stance annotation with respect to the crowd size computed by our method. As MACE also includes a threshold for keeping only the most confident predictions in order to benefit precision, we tuned this parameter, too. Deciding on the number of workers per task is a trade-off between the desired quality and the budget."], "referred_text_chunks": ["We ended up with 1,970 instances to be used for the argument reasoning comprehension task. \\end{scriptsize \\caption{ Details and statistics of the datasets resulting from the eight steps of our methodology implemented in a crowdsourcing process. *Input instances were filtered by their `logic score' assigned in Step 6, such that the weakest 30\\% were discarded."]}, "5": {"question": "How does the intra-warrant attention mechanism work?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 2, 'Rationale': 'Image 5 visually depicts the intra-warrant attention mechanism using LSTM layers, which is directly relevant to understanding how the mechanism works. Text 2 discusses the implementation of attention using BiLSTM and attention vectors, providing insight into the workings of the intra-warrant attention system.', 'Answer': 'The intra-warrant attention mechanism involves using BiLSTM to encode reasons and claims, and generating attention vectors for each warrant, which are used in conjunction with LSTM layers. This allows for better context processing and reasoning in argument comprehension.'}", "all_figures_names": ["1708.01425v4-Figure2-1.png", "1708.01425v4-Figure3-1.png", "1708.01425v4-Table2-1.png", "1708.01425v4-Figure4-1.png", "1708.01425v4-Table1-1.png", "1708.01425v4-Figure5-1.png"], "referred_figures_names": ["1708.01425v4-Figure5-1.png"], "answer": "The intra-warrant attention mechanism uses a BiLSTM to encode the reason and claim, and then provides this encoded information as an attention vector to LSTM layers for each warrant. The attention vector allows the model to focus on specific parts of the reason and claim that are most relevant to each warrant.", "figure_type": "Schematic", "content_type": "figure", "text_chunks": ["Intra-warrant attention with rich context outperforms standard neural models with a simple attention, but it only slightly beats the language model on the dev set. The language model is basically random on the test set. A manual error analysis of 50 random wrong predictions (a single run of the best-performing system on the dev set) revealed no explicit pattern of encountered errors.", "Our more elaborated version used {\\em intra-warrant attention, as shown in Figure . Both versions were also extended with the debate title and description added as context to the attention layer (w/ context). We trained the resulting four models using the ADAM optimizer, with heavy dropout (0.9) and early stopping (5 epochs), tuned on the development set.", "We computed log-likelihood of the candidate warrants and picked the one with lower score. To specifically appoach the given task, we implemented two neural models based on a bidirectional LSTM. In the standard {\\em attention version, we encoded the reason and claim using a BiLSTM and provided it as an attention vector after max-pooling to LSTM layers from the two available warrants W_0 and W_1 (corresponding to W and AW, see below).", "However, we present experiments with several neural attention and language models which reveal that current approaches based on the words and phrases in arguments and warrants do not suffice to solve the task. The main contributions of this paper are (1) a methodology for obtaining implicit warrants realized by means of scalable crowdsourcing and (2) a new task along with a high-quality dataset. In addition, we provide (a) 2,884 user-generated arguments annotated for their stance, covering 50+ controversial topics, (b) 2,026 arguments with annotated reasons supporting the stance, (c) 4,235 rephrased reason gists, useful for argument summarization and sentence compression, and (d) a method for checking the reliability of crowdworkers in document and span labeling using traditional inter-annotator agreement measures.", "Moreover, we proposed a methodology to systematically reconstruct implicit warrants in eight consecutive steps. So far, we implemented the methodology in a manual crowdsourcing process, along with a strategy that enables standard~inter-annotator agreement measures in crowdsourcing. Following the process, we constructed a new dataset with 1,970 instances for the task.", "Warrant \\rightarrow (therefore) Claim. The warrant takes the role of an inference rule, similar to the major premise in Walton's terminology . In principle, the chain Reason \\rightarrow {\\em Warrant \\rightarrow {\\em Claim is applicable to deductive arguments and syllogisms, which allows us to validate arguments properly formalized in propositional logic.", "All 5,342 alternative warrants in our dataset are written under CC-0 license. \\paragraph{6.\\ Alternative Warrant Validation As the previous step produces largely uncontrolled writings, we validate each fabricated alternative warrant AW as to whether it actually relates to the reason~R. To this end, we show AW and \\neg C together with two alternatives: R itself and a distracting reason. Only instances with correctly validated R are kept.", "This phenomenon is also referred to as common knowledge , enthymemes , tacit major premises , or implicit {\\em warrants . \\newcite{Wilson. Sperber.2004 suggest that, when we comprehend arguments, we reconstruct their warrants driven by the cognitive principle of relevance."], "referred_text_chunks": ["Intra-warrant attention with rich context outperforms standard neural models with a simple attention, but it only slightly beats the language model on the dev set. The language model is basically random on the test set. A manual error analysis of 50 random wrong predictions (a single run of the best-performing system on the dev set) revealed no explicit pattern of encountered errors."]}}