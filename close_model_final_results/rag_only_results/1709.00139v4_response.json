{"0": {"question": "Which method generally achieved a lower objective function value (OFV) for the different datasets, FISVDD or Incremental SVM? Does this imply that one method is definitively better than the other?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 1, 'Rationale': 'Image 0 shows the OFV for both FISVDD and Incremental SVM across different datasets, which is directly related to the question. Text [1] provides context on the performance of FISVDD versus Incremental SVM, mentioning their objective function values and efficiency.', 'Answer': 'Incremental SVM generally achieved a lower objective function value across the datasets. However, this does not imply it is definitively better, as FISVDD is significantly more efficient with only a small loss in objective function value.'}", "all_figures_names": ["1709.00139v4-Table1-1.png", "1709.00139v4-Figure1-1.png"], "referred_figures_names": ["1709.00139v4-Table1-1.png"], "answer": "For all datasets presented, Incremental SVM achieved a slightly lower OFV compared to FISVDD. However, this does not necessarily mean that Incremental SVM is definitively better.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["\\tref{table_compare shows that for the same Gaussian bandwidth, the FISVDD method is much faster than the incremental SVM method, with only a tiny sacrifice in the objective function value. Because incremental SVM achieves global optimal solutions, the solutions provided by FISVDD are very close to the global optimal solutions. Even with interior points discarded after each iteration, FISVDD is faster than incremental SVM for the data sets in our experiments.", "The purpose of our experiments is to show that compared to the incremental SVM method (which can achieve global optimal solutions), the FISVDD method does not lose much in either objective function value or outlier detection accuracy while it demonstrates significant gains in efficiency. Our experiments used 4/5 of the normal data, randomly chosen, for training. The remaining normal data and the outliers together form the testing sets.", "In this paper, we compare FISVDD with the one-class incremental SVM method \\citep{laskov2006incremental, a well-known technique for performing global optimal SVDD. For each method, the following quantities are measured in \\tref{table_compare: The time consumed by the incremental SVM method with interior points discarded after each iteration is listed in parentheses. \\tref{table_compare also lists the settings for the experiments, including Gaussian bandwidth (Sigma), number of training observations (\\#Train obs), number of testing observations (\\#Test obs), and number of variables (\\#Var).", "The results from \\cref{alphanorm help make our FISVDD algorithm more efficient. Fast Incremental SVDD Learning Algorithm We propose a fast incremental algorithm of SVDD (FISVDD). The central idea of FISVDD is to minimize the objective function () by quickly updating the inverse of similarity matrices in each iteration.", "Conclusion This paper introduces a fast incremental SVDD learning algorithm (FISVDD), which is more efficient than existing SVDD algorithms. In each iteration, FISVDD considers only the incoming data point and the support vectors that were determined in the previous iteration. The essential calculations of FISVDD are contributed from incremental and decremental updates of a similar matrix inverse \\mathbf{A^{-1.", "As explained in \\sref{fisvdd, FISVDD is faster because it is based solely on matrix manipulation and thus many calculations are saved. \\fref{pics shows plots of the F-1 measure \\citep{tan2007introduction of the accuracy of FISVDD and incremental SVM with different training sizes. The plots show that by discarding interior points at the end of each iteration, there is almost no loss in the quality of outlier detection.", "First, FISVDD tries to obtain the optimal solution in each iteration without using the interior points, similar to the idea mentioned in \\citet{syed1999incremental. Results from many experiments show that if a proper Gaussian bandwidth is chosen, then the number of support vectors should be far smaller than the total number of observations. FISVDD takes advantage of this fact by calculating only the similarities between the new data points and the support vectors.", "The FISVDD Algorithm The FISVDD algorithm is shown in \\aref{fisvddalg. It contains three parts of FISVDD: expanding (which is shown in \\aref{expand), shrinking (which is shown in \\aref{shrink), and bookkeeping. \\subsubsection{Stage 1, Expanding When a new data point \\mathbf{x_{k+1 comes in, it is scored to determine whether it falls in the interior."], "referred_text_chunks": ["\\tref{table_compare shows that for the same Gaussian bandwidth, the FISVDD method is much faster than the incremental SVM method, with only a tiny sacrifice in the objective function value. Because incremental SVM achieves global optimal solutions, the solutions provided by FISVDD are very close to the global optimal solutions. Even with interior points discarded after each iteration, FISVDD is faster than incremental SVM for the data sets in our experiments."]}}