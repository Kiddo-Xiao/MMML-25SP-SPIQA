{"0": {"question": "How does the validation accuracy of the SRU model compare to that of the cuDNN LSTM and CNN models?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 2, 'Rationale': 'Image 0 shows validation accuracies and training times for SRU, cuDNN LSTM, and CNN models, directly addressing the comparison of validation accuracy asked in the question. Text 2 discusses the validation accuracy of the SRU model compared to other models, emphasizing how SRU is more effective in sequence modeling.', 'Answer': 'The SRU model exhibits better or comparable validation accuracy compared to the cuDNN LSTM and CNN models, with more stable performance and requiring fewer layers.'}", "all_figures_names": ["1709.02755v5-Figure3-1.png", "1709.02755v5-Figure1-1.png", "1709.02755v5-Table2-1.png", "1709.02755v5-Table6-1.png", "1709.02755v5-Figure6-1.png", "1709.02755v5-Figure2-1.png", "1709.02755v5-Table4-1.png", "1709.02755v5-Table1-1.png"], "referred_figures_names": ["1709.02755v5-Figure3-1.png"], "answer": "The SRU model achieves comparable or slightly higher validation accuracy than the cuDNN LSTM and CNN models on all six benchmarks.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Despite our simple setup, SRU outperforms most previous methods and achieves comparable results compared to the state-of-the-art but more sophisticated model of~\\citet{zhao2015self. Figure~ shows validation performance relative to training time for SRU, cuDNN LSTM and the CNN model. Our SRU implementation runs 5--9 times faster than cuDNN LSTM, and 6--40\\% faster than the CNN model of~\\citet{Kim14.", "SRU achieves 71.4\\% exact match and 80.2\\% F1 score, outperforming the bidirectional LSTM model by 1.9\\% (EM) and 1.4\\% (F1) respectively. SRU also exhibits over 5x speed-up over LSTM and 53--63\\% reduction in total training time. In comparison with QRNN, SRU obtains 0.8\\% improvement on exact match and 0.6\\% on F1 score, and runs 60\\% faster.", "SRU also exhibits more stable performance, with smaller variance over 3 runs. Figure~ further compares the validation accuracy of different models. These results confirm that SRU is better at sequence modeling compared to the original feed-forward network (FFN), requiring fewer layers to achieve similar accuracy.", "On classification and question answering datasets, SRU outperforms common recurrent and non-recurrent architectures, while achieving 5--9x speed-up compared to cuDNN LSTM. Stacking additional layers further improves performance, while incurring relatively small costs owing to the cheap computation of a single layer. We also obtain an average improvement of 0.7 BLEU score on the English to German translation task by incorporating SRU into Transformer~\\citep{vaswani2017attention.", "\\citet{shazeer2017outrageously and \\citet{kuchaiev2017factorization improve the computation via conditional computing and matrix factorization respectively. Our implementation for SRU is inspired by the cuDNN-optimized LSTM~\\citep{cudnnlstm, but enables more parallelism -- while cuDNN LSTM requires six optimization steps, SRU achieves more significant speed-up via two optimizations. The design of recurrent networks, such as SRU and related architectures, raises questions about representational power and interpretability~\\citep{chen2017recurrent,peng2018emnlp.", "The 8-layer SRU model achieves validation and test bits per character (BPC) of 1.21, outperforming previous best reported results of LSTM, QRNN and recurrent highway networks (RHN). Increasing the layer of SRU to 12 and using a longer context of 256 characters in training further improves the BPC to 1.19 Ablation Analysis We perform ablation analyses on SRU by successively disabling different components: We train model variants on the classification and question answering datasets. Table~ and Figure~ confirm the impact of our design decisions -- removing these components result in worse classification accuracies and exact match scores.", "\\paragraph{Results Table~ shows the translation results. When SRU is incorporated into the architecture, both the 4-layer and 5-layer model outperform the Transformer base model. For instance, our 5-layer model obtains an average improvement of 0.7 test BLEU score and an improvement of 0.5 BLEU score by comparing the best results of each model achieved across three runs.", "We substitute the feed-forward network using our SRU implementation: The intuition is that SRU can better capture sequential information as a recurrent network, and potentially achieve better performance while requiring fewer layers. We keep the model configuration the same as \\citet{vaswani2017attention: the model dimension is d_\\text{model=512, the feed-forward and SRU layer has inner dimensionality d_\\text{ff=d_\\text{sru=2048, and positional encoding~ is applied on the input word embeddings. The base model without SRU has 6 layers, while we set the number of layers to 4 and 5 when SRU is added."], "referred_text_chunks": ["Despite our simple setup, SRU outperforms most previous methods and achieves comparable results compared to the state-of-the-art but more sophisticated model of~\\citet{zhao2015self. Figure~ shows validation performance relative to training time for SRU, cuDNN LSTM and the CNN model. Our SRU implementation runs 5--9 times faster than cuDNN LSTM, and 6--40\\% faster than the CNN model of~\\citet{Kim14."]}, "1": {"question": "How does the processing time of SRU compare to that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 1, 'Rationale': 'Image 1 directly compares the processing time of SRU, cuDNN LSTM, and word-level convolutions with filter widths k=2 and k=3. Text chunk 1 provides specific details about SRU\u2019s speed compared to cuDNN LSTM and CNN models.', 'Answer': 'SRU runs 5-9 times faster than cuDNN LSTM and 6-40% faster than convolutional models with filter widths k=2 and k=3.'}", "all_figures_names": ["1709.02755v5-Figure3-1.png", "1709.02755v5-Figure1-1.png", "1709.02755v5-Table2-1.png", "1709.02755v5-Table6-1.png", "1709.02755v5-Figure6-1.png", "1709.02755v5-Figure2-1.png", "1709.02755v5-Table4-1.png", "1709.02755v5-Table1-1.png"], "referred_figures_names": ["1709.02755v5-Figure1-1.png"], "answer": "The processing time of SRU is significantly faster than that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["On classification and question answering datasets, SRU outperforms common recurrent and non-recurrent architectures, while achieving 5--9x speed-up compared to cuDNN LSTM. Stacking additional layers further improves performance, while incurring relatively small costs owing to the cheap computation of a single layer. We also obtain an average improvement of 0.7 BLEU score on the English to German translation task by incorporating SRU into Transformer~\\citep{vaswani2017attention.", "Despite our simple setup, SRU outperforms most previous methods and achieves comparable results compared to the state-of-the-art but more sophisticated model of~\\citet{zhao2015self. Figure~ shows validation performance relative to training time for SRU, cuDNN LSTM and the CNN model. Our SRU implementation runs 5--9 times faster than cuDNN LSTM, and 6--40\\% faster than the CNN model of~\\citet{Kim14.", "\\citet{shazeer2017outrageously and \\citet{kuchaiev2017factorization improve the computation via conditional computing and matrix factorization respectively. Our implementation for SRU is inspired by the cuDNN-optimized LSTM~\\citep{cudnnlstm, but enables more parallelism -- while cuDNN LSTM requires six optimization steps, SRU achieves more significant speed-up via two optimizations. The design of recurrent networks, such as SRU and related architectures, raises questions about representational power and interpretability~\\citep{chen2017recurrent,peng2018emnlp.", "SRU achieves 71.4\\% exact match and 80.2\\% F1 score, outperforming the bidirectional LSTM model by 1.9\\% (EM) and 1.4\\% (F1) respectively. SRU also exhibits over 5x speed-up over LSTM and 53--63\\% reduction in total training time. In comparison with QRNN, SRU obtains 0.8\\% improvement on exact match and 0.6\\% on F1 score, and runs 60\\% faster.", "Discussion This work presents Simple Recurrent Unit (SRU), a scalable recurrent architecture that operates as fast as feed-forward and convolutional units. We confirm the effectiveness of SRU on multiple natural language tasks ranging from classification to translation.", "We substitute the feed-forward network using our SRU implementation: The intuition is that SRU can better capture sequential information as a recurrent network, and potentially achieve better performance while requiring fewer layers. We keep the model configuration the same as \\citet{vaswani2017attention: the model dimension is d_\\text{model=512, the feed-forward and SRU layer has inner dimensionality d_\\text{ff=d_\\text{sru=2048, and positional encoding~ is applied on the input word embeddings. The base model without SRU has 6 layers, while we set the number of layers to 4 and 5 when SRU is added.", "Finally, adding SRU does not affect the parallelization or speed of Transformer -- the 4-layer model exhibits 10\\% speed improvement, while the 5-layer model is only 5\\% slower compared to the base model. We present more results and discussion in Appendix~. Character-level Language Modeling \\paragraph{Dataset We use Enwik8, a large dataset for character-level language modeling. Following standard practice, we use the first 90M characters for training and the remaining 10M split evenly for validation and test.", "We use Adam~\\citep{Kingma:14adam with the default learning rate 0.001, a weight decay 0 and a hidden dimension of 128. We compare SRU with a wide range of methods on these datasets, including various convolutional models~\\citep{kalchbrenner:2014,Kim14,zhang2015sensitivity and a hierarchical sentence model~\\citep{zhao2015self reported as the state of the art on these datasets~\\citep{conneau17. Their setups are not exactly the same as ours, and may involve more tuning on word embeddings and other regularizations."], "referred_text_chunks": ["On classification and question answering datasets, SRU outperforms common recurrent and non-recurrent architectures, while achieving 5--9x speed-up compared to cuDNN LSTM. Stacking additional layers further improves performance, while incurring relatively small costs owing to the cheap computation of a single layer. We also obtain an average improvement of 0.7 BLEU score on the English to German translation task by incorporating SRU into Transformer~\\citep{vaswani2017attention."]}, "2": {"question": "According to Table 1 and the passage, how does the performance of the SRU model compare to the LSTM model in terms of both accuracy and training speed on the SQuAD dataset?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 1, 'Rationale': 'Image 2 provides exact match and F1 scores comparison between SRU and LSTM on the SQuAD dataset. Text 1 directly compares SRU with LSTM regarding accuracy and training speed.', 'Answer': 'On the SQuAD dataset, SRU achieves higher exact match (71.4% vs. 69.5%) and F1 scores (80.2% vs. 78.8%) compared to LSTM. SRU also shows over 5x speed-up in training compared to LSTM.'}", "all_figures_names": ["1709.02755v5-Figure3-1.png", "1709.02755v5-Figure1-1.png", "1709.02755v5-Table2-1.png", "1709.02755v5-Table6-1.png", "1709.02755v5-Figure6-1.png", "1709.02755v5-Figure2-1.png", "1709.02755v5-Table4-1.png", "1709.02755v5-Table1-1.png"], "referred_figures_names": ["1709.02755v5-Table2-1.png"], "answer": "The SRU model outperforms the LSTM model in both accuracy and training speed on the SQuAD dataset.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Despite our simple setup, SRU outperforms most previous methods and achieves comparable results compared to the state-of-the-art but more sophisticated model of~\\citet{zhao2015self. Figure~ shows validation performance relative to training time for SRU, cuDNN LSTM and the CNN model. Our SRU implementation runs 5--9 times faster than cuDNN LSTM, and 6--40\\% faster than the CNN model of~\\citet{Kim14.", "SRU achieves 71.4\\% exact match and 80.2\\% F1 score, outperforming the bidirectional LSTM model by 1.9\\% (EM) and 1.4\\% (F1) respectively. SRU also exhibits over 5x speed-up over LSTM and 53--63\\% reduction in total training time. In comparison with QRNN, SRU obtains 0.8\\% improvement on exact match and 0.6\\% on F1 score, and runs 60\\% faster.", "On the movie review (MR) dataset for instance, SRU completes 100 training epochs within 40 seconds, while LSTM takes over 320 seconds. Question Answering \\paragraph{Dataset We use the Stanford Question Answering Dataset~\\citep[SQuAD;][]{rajpurkar2016squad. SQuAD is a large machine comprehension dataset that includes over 100K question-answer pairs extracted from Wikipedia articles.", "On classification and question answering datasets, SRU outperforms common recurrent and non-recurrent architectures, while achieving 5--9x speed-up compared to cuDNN LSTM. Stacking additional layers further improves performance, while incurring relatively small costs owing to the cheap computation of a single layer. We also obtain an average improvement of 0.7 BLEU score on the English to German translation task by incorporating SRU into Transformer~\\citep{vaswani2017attention.", "\\paragraph{Results Table~ shows the translation results. When SRU is incorporated into the architecture, both the 4-layer and 5-layer model outperform the Transformer base model. For instance, our 5-layer model obtains an average improvement of 0.7 test BLEU score and an improvement of 0.5 BLEU score by comparing the best results of each model achieved across three runs.", "We also see that models with SRU exhibit much faster training progress with much lower training perplexity, suggesting the models could be tuned better with further training regularization. Character-level Language Modeling We train all models using a weight decay of 10^{-7 and a gradient clipping of 0.3. We set the learning rate factor of Noam scheduling to 3 and the warmup steps to 32,000.", "SRU also exhibits more stable performance, with smaller variance over 3 runs. Figure~ further compares the validation accuracy of different models. These results confirm that SRU is better at sequence modeling compared to the original feed-forward network (FFN), requiring fewer layers to achieve similar accuracy.", "Following the author suggestions, we use the Adamax optimizer~\\citep{Kingma:14adam and variational dropout~\\citep{Gal2016Theoretically during training. We compare with two alternative recurrent components: the bidirectional LSTM adopted in the original implementation of \\citet{chen2017reading and Quasi-RNN with highway connections for improved performance. \\paragraph{Results Table~ summarizes the results on SQuAD."], "referred_text_chunks": ["Despite our simple setup, SRU outperforms most previous methods and achieves comparable results compared to the state-of-the-art but more sophisticated model of~\\citet{zhao2015self. Figure~ shows validation performance relative to training time for SRU, cuDNN LSTM and the CNN model. Our SRU implementation runs 5--9 times faster than cuDNN LSTM, and 6--40\\% faster than the CNN model of~\\citet{Kim14."]}, "3": {"question": "Based on the table, how does the training process handle large vocabulary sizes?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 1, 'Rationale': 'Image 3 shows training configurations including parameters for handling vocabulary through embedding and batch processing, relevant to the question about managing large vocabulary sizes. Text 1 discusses embedding initialization, specifically for out-of-vocabulary words, which directly relates to handling large vocabulary sizes.', 'Answer': 'The training process manages large vocabulary sizes by using embeddings where out-of-vocabulary words are initialized with random vectors, and batch processing with a large batch size for efficiency.'}", "all_figures_names": ["1709.02755v5-Figure3-1.png", "1709.02755v5-Figure1-1.png", "1709.02755v5-Table2-1.png", "1709.02755v5-Table6-1.png", "1709.02755v5-Figure6-1.png", "1709.02755v5-Figure2-1.png", "1709.02755v5-Table4-1.png", "1709.02755v5-Table1-1.png"], "referred_figures_names": ["1709.02755v5-Table6-1.png"], "answer": "The training process uses several techniques to handle large vocabulary sizes. These include:\n\n1. **Token-based batching:** Instead of grouping sentences of similar lengths together, the training process batches together a fixed number of tokens (5120 tokens per batch). This approach ensures that the model sees a consistent amount of vocabulary regardless of sentence length variation.\n2. **Shared embedding:** This technique maps both source and target words to the same embedding space, effectively reducing the memory footprint needed to store word representations. \n3. **Positional encoding:** This method injects information about the position of words in a sentence into the model, helping it better understand long-range dependencies within the text. ", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Table~ shows the list of configuration options used for training. For evaluation, we use beam size 5 and length penalty 0.6. Table~ shows the averaged BLEU score of each model from 20th to 40th epoch.", "We use a batch size of 32 and a dropout probability of 0.5 for all models. In addition, we increment the dropout to 0.55 or 0.6 for the 8-layer SRU model. Following the implementation of~\\citep{Kim14, out-of-vocabulary words that are not in the pre-trained embeddings are initialized with random vectors with values from [-0.25, 0.25].", "Experiments We evaluate SRU on several natural language processing tasks and perform additional analyses of the model. The set of tasks includes text classification, question answering, machine translation, and character-level language modeling. Training time on these benchmarks ranges from minutes (classification) to days (translation), providing a variety of computation challenges.", "\\paragraph{Setup Similar to previous work, we use a batch size of 128 and an unroll size of 100 for truncated backpropagation during training. We also experiment with an unroll size of 256 and a batch size of 64 such that each training instance has longer context. We use a non-zero highway bias \\bb_r={-3 that is shown useful for training language model~\\citep{ZillySKS17.", "We also see that models with SRU exhibit much faster training progress with much lower training perplexity, suggesting the models could be tuned better with further training regularization. Character-level Language Modeling We train all models using a weight decay of 10^{-7 and a gradient clipping of 0.3. We set the learning rate factor of Noam scheduling to 3 and the warmup steps to 32,000.", "Previous methods employ different optimizers and learning rate schedulers for training. For simplicity and consistency, we use the Adam optimizer and the same learning rate scheduling (i.e., Noam scheduling) as the translation experiments. We train a maximum of 100 epochs (about 700,000 steps).", "Introduction Recurrent neural networks (RNN) are at the core of state-of-the-art approaches for a large number of natural language tasks, including machine translation~\\citep{cho-al-emnlp14,Bahdanau:14neuralmt,jean15,luong2015:EMNLP, language modeling~\\citep{zaremba2014recurrent,Gal2016Theoretically,zoph2016neural, opinion mining~\\citep{Irsoy:14opinion, and situated language understanding~\\citep{Mei:16neuralnavi,Misra:17instructions,Suhr:18context,Suhr:18situated. Key to many of these advancements are architectures of increased capacity and computation. For instance, the top-performing models for semantic role labeling and translation use eight recurrent layers, requiring days to train~\\citep{he2017deep,wu2016gnmt.", "On the movie review (MR) dataset for instance, SRU completes 100 training epochs within 40 seconds, while LSTM takes over 320 seconds. Question Answering \\paragraph{Dataset We use the Stanford Question Answering Dataset~\\citep[SQuAD;][]{rajpurkar2016squad. SQuAD is a large machine comprehension dataset that includes over 100K question-answer pairs extracted from Wikipedia articles."], "referred_text_chunks": ["Table~ shows the list of configuration options used for training. For evaluation, we use beam size 5 and length penalty 0.6. Table~ shows the averaged BLEU score of each model from 20th to 40th epoch."]}, "4": {"question": "How does the variance of the hidden state $h_t$ compare to the variance of the input $x_t$ in deep layers of the SRU model?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 2, 'Rationale': 'Image 4 directly shows the variance ratio of Var[c]/Var[x], illustrating how it approaches 1 in deeper SRU layers, which is related to hidden states. Text chunk 2 discusses variance reduction in hidden representations, relevant to the comparison of variance in deep models.', 'Answer': 'In deep layers of the SRU model, the variance of the hidden state $h_t$ is reduced compared to the variance of the input $x_t$, converging to a factor of 1/2, causing the output and gradient to vanish.'}", "all_figures_names": ["1709.02755v5-Figure3-1.png", "1709.02755v5-Figure1-1.png", "1709.02755v5-Table2-1.png", "1709.02755v5-Table6-1.png", "1709.02755v5-Figure6-1.png", "1709.02755v5-Figure2-1.png", "1709.02755v5-Table4-1.png", "1709.02755v5-Table1-1.png"], "referred_figures_names": ["1709.02755v5-Figure6-1.png"], "answer": "According to the passage, the variance of the hidden state is approximately equal to the variance of the input in deep layers.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["The internal state \\cc_t and \\h_t would be a weighted combination of inputs \\{\\x_1\\cdots \\x_t\\, which will increase the correlation of the state vectors at different steps. These state vectors are again fed into the next layer, and keep increasing the correlation. As a result, we expect the actual ratio between the variance of \\cc_t and that of the input of the current layer \\x_t lies between the two derived values, and would finally converge to the upper bound value of 1.", "The variance of c_{t,i however depends on the correlation between input vectors. When the input vectors are independent: However, the two vectors in the input sequence, for instance \\x_t and \\x_t', are not necessarily independent, for example because two words in an input sentence are often correlated. When the input vectors are perfectly correlated \\x_t=\\x_{t-1=\\cdots=\\x, on the other hand, In practice, multiple SRU layers are stacked to construct a deep network.", "This ensures the output variance remains approximately the same as the input variance after the matrix multiplication. However, the light recurrence and highway computation would still reduce the variance of hidden representations by a factor of 1/3 to 1/2: and the factor converges to 1/2 in deeper layers (see Appendix~). This implies the output \\h_t and the gradient would vanish in deep models.", "Figure~ confirms our expectation by computing the empirical value of \\text{Var[c]/\\text{Var[x] in deep SRU networks. Computing Var[\\h_t] Given the result in Equation (5), we proceed to compute \\text{Var[\\h_t].", "We open source our implementation to facilitate future NLP and deep learning research. \\paragraph{Trading capacity with layers SRU achieves high parallelization by simplifying the hidden-to-hidden dependency. This simplification is likely to reduce the representational power of a single layer and hence should be balanced to avoid performance loss.", "\\yae{This finding motivates the design of SRU for both high parallelization and representational power.{ SRU also relates to IRNN~\\citep{le2015simple, which uses an identity diagonal matrix to initialize hidden-to-hidden connections. SRU uses point-wise multiplication for hidden connections, which is equivalent to using a diagonal weight matrix.", "SRU also exhibits more stable performance, with smaller variance over 3 runs. Figure~ further compares the validation accuracy of different models. These results confirm that SRU is better at sequence modeling compared to the original feed-forward network (FFN), requiring fewer layers to achieve similar accuracy.", "We substitute the feed-forward network using our SRU implementation: The intuition is that SRU can better capture sequential information as a recurrent network, and potentially achieve better performance while requiring fewer layers. We keep the model configuration the same as \\citet{vaswani2017attention: the model dimension is d_\\text{model=512, the feed-forward and SRU layer has inner dimensionality d_\\text{ff=d_\\text{sru=2048, and positional encoding~ is applied on the input word embeddings. The base model without SRU has 6 layers, while we set the number of layers to 4 and 5 when SRU is added."], "referred_text_chunks": ["The internal state \\cc_t and \\h_t would be a weighted combination of inputs \\{\\x_1\\cdots \\x_t\\, which will increase the correlation of the state vectors at different steps. These state vectors are again fed into the next layer, and keep increasing the correlation. As a result, we expect the actual ratio between the variance of \\cc_t and that of the input of the current layer \\x_t lies between the two derived values, and would finally converge to the upper bound value of 1."]}, "5": {"question": "How does scaling correction affect the training of SRU models?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 5, 'Rationale': 'Image 5 and Text 5 both directly address the effect of scaling correction on the training of SRU models. Image 5 shows the training curves with and without scaling, directly illustrating improvements in training progress due to scaling correction, especially in deeper models. Text 5 explains the introduction of scaling correction, its purpose, and refers to a figure comparing training progress, which aligns with the content of Image 5.', 'Answer': 'Scaling correction improves the training progress of SRU models, notably in deeper models with more layers, by ensuring better convergence of training loss.'}", "all_figures_names": ["1709.02755v5-Figure3-1.png", "1709.02755v5-Figure1-1.png", "1709.02755v5-Table2-1.png", "1709.02755v5-Table6-1.png", "1709.02755v5-Figure6-1.png", "1709.02755v5-Figure2-1.png", "1709.02755v5-Table4-1.png", "1709.02755v5-Table1-1.png"], "referred_figures_names": ["1709.02755v5-Figure2-1.png"], "answer": "Scaling correction improves the training progress of SRU models, especially for deeper models with many stacked layers.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["We also see that models with SRU exhibit much faster training progress with much lower training perplexity, suggesting the models could be tuned better with further training regularization. Character-level Language Modeling We train all models using a weight decay of 10^{-7 and a gradient clipping of 0.3. We set the learning rate factor of Noam scheduling to 3 and the warmup steps to 32,000.", "The 8-layer SRU model achieves validation and test bits per character (BPC) of 1.21, outperforming previous best reported results of LSTM, QRNN and recurrent highway networks (RHN). Increasing the layer of SRU to 12 and using a longer context of 256 characters in training further improves the BPC to 1.19 Ablation Analysis We perform ablation analyses on SRU by successively disabling different components: We train model variants on the classification and question answering datasets. Table~ and Figure~ confirm the impact of our design decisions -- removing these components result in worse classification accuracies and exact match scores.", "Finally, adding SRU does not affect the parallelization or speed of Transformer -- the 4-layer model exhibits 10\\% speed improvement, while the 5-layer model is only 5\\% slower compared to the base model. We present more results and discussion in Appendix~. Character-level Language Modeling \\paragraph{Dataset We use Enwik8, a large dataset for character-level language modeling. Following standard practice, we use the first 90M characters for training and the remaining 10M split evenly for validation and test.", "The improvement over the Transformer base model is consistent across different epochs. Figure~ plots the training and validation perplexity of three models. With a higher dropout (0.2) used for the SRU, the 5-layer model gets consistent lower validation perplexity over the base model and the 4-layer model.", "SRU achieves 71.4\\% exact match and 80.2\\% F1 score, outperforming the bidirectional LSTM model by 1.9\\% (EM) and 1.4\\% (F1) respectively. SRU also exhibits over 5x speed-up over LSTM and 53--63\\% reduction in total training time. In comparison with QRNN, SRU obtains 0.8\\% improvement on exact match and 0.6\\% on F1 score, and runs 60\\% faster.", "To offset the problem, we introduce a scaling correction constant \\alpha in the highway connection where \\alpha is set to \\sqrt{3 such that \\text{Var[\\h_t] \\approx \\text{Var[\\x_t] at initialization. When the highway network is initialized with a non-zero bias \\bb_r = b, the scaling constant \\alpha can be accordingly set as: Figure~ compares the training progress with and without the scaling correction. See Appendix~ for the derivation and more discussion.", "We thank Alexander Rush and Yoon Kim for help with machine translation experiments, and Danqi Chen for help with SQuAD experiments. We thank Adam Yala, Howard Chen, Jeremy Wohlwend, Lili Yu, Kyle Swanson and Kevin Yang for providing useful feedback on the paper and the SRU implementation. A special thanks to Hugh Perkins for his support on the experimental environment setup and Runqi Yang for answering questions about his code.% \\bibliography{emnlp2018 \\bibliographystyle{acl_natbib_nourl \\newpage \\appendix Parameter Initialization Derivation Following the derivation of Glorot and Kaiming initialization~, we assume the values of each input vector \\x_t are i.i.d with zero mean and a small variance: We initialize each weight matrix with zero mean and a variance of 1/d. After a matrix multiplication \\mathbf{y = \\W\\x_t, each value y_i would have which means the scale of the values after matrix multiplication remains the same.", "However, these models can potentially benefit from SRU since RNNs are incorporated in the model architecture. We use the open source implementation of Document Reader in our experiments. We train models for up to 100 epochs, with a batch size of 32 and a hidden dimension of 128."], "referred_text_chunks": ["We also see that models with SRU exhibit much faster training progress with much lower training perplexity, suggesting the models could be tuned better with further training regularization. Character-level Language Modeling We train all models using a weight decay of 10^{-7 and a gradient clipping of 0.3. We set the learning rate factor of Noam scheduling to 3 and the warmup steps to 32,000."]}, "6": {"question": "How does the performance of SRU with 8 layers compare to the best reported results on the SUBJ dataset, and how does its training time compare to the other models in the \"Our setup\" section?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 7, 'Rationale': 'Image 7 provides the test accuracies of various models on the SUBJ dataset and compares their training times under a consistent setup. Text chunk 7 offers insight into the SRU model\u2019s speed and performance comparison to other models.', 'Answer': 'The 8-layer SRU achieves a test accuracy of 93.7% on the SUBJ dataset, slightly below the best reported result of 95.5%. Its training time is 879 seconds for 100 epochs, which is faster compared to LSTM (2409 seconds) but slower than QRNN (371 seconds).'}", "all_figures_names": ["1709.02755v5-Figure3-1.png", "1709.02755v5-Figure1-1.png", "1709.02755v5-Table2-1.png", "1709.02755v5-Table6-1.png", "1709.02755v5-Figure6-1.png", "1709.02755v5-Figure2-1.png", "1709.02755v5-Table4-1.png", "1709.02755v5-Table1-1.png"], "referred_figures_names": ["1709.02755v5-Table1-1.png"], "answer": "While SRU with 8 layers achieves high test accuracy within the \"Our setup\" section on the SUBJ dataset (93.7%), it falls slightly short of the best reported result of 95.5% achieved by Zhao et al. (2015). However, SRU's training time of 879 seconds for 100 epochs on the SST dataset is faster than the LSTM model (2409 seconds) but slower than the CNN model (417 seconds) and the QRNN models (345 and 371 seconds).", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Despite our simple setup, SRU outperforms most previous methods and achieves comparable results compared to the state-of-the-art but more sophisticated model of~\\citet{zhao2015self. Figure~ shows validation performance relative to training time for SRU, cuDNN LSTM and the CNN model. Our SRU implementation runs 5--9 times faster than cuDNN LSTM, and 6--40\\% faster than the CNN model of~\\citet{Kim14.", "\\paragraph{Results Table~ shows the translation results. When SRU is incorporated into the architecture, both the 4-layer and 5-layer model outperform the Transformer base model. For instance, our 5-layer model obtains an average improvement of 0.7 test BLEU score and an improvement of 0.5 BLEU score by comparing the best results of each model achieved across three runs.", "The 8-layer SRU model achieves validation and test bits per character (BPC) of 1.21, outperforming previous best reported results of LSTM, QRNN and recurrent highway networks (RHN). Increasing the layer of SRU to 12 and using a longer context of 256 characters in training further improves the BPC to 1.19 Ablation Analysis We perform ablation analyses on SRU by successively disabling different components: We train model variants on the classification and question answering datasets. Table~ and Figure~ confirm the impact of our design decisions -- removing these components result in worse classification accuracies and exact match scores.", "Finally, adding SRU does not affect the parallelization or speed of Transformer -- the 4-layer model exhibits 10\\% speed improvement, while the 5-layer model is only 5\\% slower compared to the base model. We present more results and discussion in Appendix~. Character-level Language Modeling \\paragraph{Dataset We use Enwik8, a large dataset for character-level language modeling. Following standard practice, we use the first 90M characters for training and the remaining 10M split evenly for validation and test.", "The improvement over the Transformer base model is consistent across different epochs. Figure~ plots the training and validation perplexity of three models. With a higher dropout (0.2) used for the SRU, the 5-layer model gets consistent lower validation perplexity over the base model and the 4-layer model.", "SRU also exhibits more stable performance, with smaller variance over 3 runs. Figure~ further compares the validation accuracy of different models. These results confirm that SRU is better at sequence modeling compared to the original feed-forward network (FFN), requiring fewer layers to achieve similar accuracy.", "On classification and question answering datasets, SRU outperforms common recurrent and non-recurrent architectures, while achieving 5--9x speed-up compared to cuDNN LSTM. Stacking additional layers further improves performance, while incurring relatively small costs owing to the cheap computation of a single layer. We also obtain an average improvement of 0.7 BLEU score on the English to German translation task by incorporating SRU into Transformer~\\citep{vaswani2017attention.", "SRU achieves 71.4\\% exact match and 80.2\\% F1 score, outperforming the bidirectional LSTM model by 1.9\\% (EM) and 1.4\\% (F1) respectively. SRU also exhibits over 5x speed-up over LSTM and 53--63\\% reduction in total training time. In comparison with QRNN, SRU obtains 0.8\\% improvement on exact match and 0.6\\% on F1 score, and runs 60\\% faster."], "referred_text_chunks": ["Despite our simple setup, SRU outperforms most previous methods and achieves comparable results compared to the state-of-the-art but more sophisticated model of~\\citet{zhao2015self. Figure~ shows validation performance relative to training time for SRU, cuDNN LSTM and the CNN model. Our SRU implementation runs 5--9 times faster than cuDNN LSTM, and 6--40\\% faster than the CNN model of~\\citet{Kim14."]}}