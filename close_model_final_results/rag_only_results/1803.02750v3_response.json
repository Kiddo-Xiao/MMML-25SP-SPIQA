{"0": {"question": "Which synchronization method is the most efficient in terms of CPU processing time?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 directly shows the CPU processing time ratio of different synchronization methods, while text 0 discusses CPU time related to processing CRDT updates, making them most relevant to the question.', 'Answer': 'State-based synchronization is the most efficient in terms of CPU processing time.'}", "all_figures_names": ["1803.02750v3-TableIII-1.png", "1803.02750v3-Figure2-1.png", "1803.02750v3-Figure6-1.png", "1803.02750v3-Figure1-1.png", "1803.02750v3-Figure13-1.png", "1803.02750v3-Figure3-1.png", "1803.02750v3-Figure5-1.png", "1803.02750v3-TableII-1.png"], "referred_figures_names": ["1803.02750v3-Figure1-1.png"], "answer": "The proposed method compared to state-based and delta-based methods.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["During the experiment we also measured the CPU time spent in processing CRDT updates, both producing and processing synchronization messages. Figure reports the CPU overhead of classic delta-based, when considering \\BP + \\RR as baseline.", "As seen previously, Scuttlebutt and Scuttlebutt-GC behave much better than state-based synchronization, yielding a reduction in the transmission cost between 46\\% and 91\\%, and 20\\% and 65\\%, respectively. This is due to the underlying precise reconciliation mechanism of Scuttlebutt. Operation-based synchronization leads to a transmission reduction between 35\\% and 80\\% since it is able to represent incremental changes to the CRDT as small operations.", "\\RR. State-based does not require synchronization metadata, and thus it is optimal in terms of memory usage. Classic delta-based and delta-based \\BP have an overhead of 1.1\\af{x-3.9\\af{x since the size of \\delta-groups in the \\delta-buffer is larger for these techniques. For \\af{GSet and \\af{GMap\\ 10\\%, Scuttlebutt-GC is close to \\BP +", "Since classic delta-based produces/processes larger messages than \\BP + \\RR, this results in a higher CPU cost: for the 1, 1.25 and 1.5 Zipf coefficients, classic delta-based incurs an overhead of 0.4\\af{x, 5.5\\af{x, and 7.9\\af{x respectively. Related Work In the context of remote file synchronization, rsync synchronizes two files placed on different machines, by generating file block signatures, and using these signatures to identify the missing blocks on the backup file. In this strategy, there's a trade-off between the size of the blocks to be signed, the number of signatures to be sent, and the size of the blocks to be received: bigger blocks to be signed implies fewer signatures to be sent, but the blocks received (deltas) can be bigger than necessary.", "Finally, delta-based \\BP + \\RR is able reduce the transmission costs by up-to 94\\%. In the extreme case of \\af{GMap\\ 100\\% (every key in the map is updated between synchronization rounds, which is a less likely workload in practical systems) and considering a partial-mesh, delta-based \\BP + \\RR provides a modest improvement in relation to state-based of about 18\\% less transmission, and its performance is below Scuttlebutt variants and operation-based synchronization.", "As baselines, we have state-based synchronization, classic delta-based synchronization, Scuttlebutt, a variation of Scuttlebutt, and operation-based synchronization. \\paragraph*{Scuttlebutt Scuttlebutt is an anti-entropy protocol used to reconcile changes in values of a key-value store. Each value is uniquely identified with a version \\tup{i, s \\in \\ids \\times \\nat, where the first component i \\in \\ids is the identifier of the replica responsible for the new value, and s \\in \\nat a sequence number, incremented on each local update, thus being unique.", "The results show a linear and quadratic cost (in terms of number of nodes) for Scuttlebutt and Scuttlebutt-GC (respectively), and a linear cost for operation-based synchronization (in terms of both number of nodes and pending updates still to be propagated). Given N nodes, P neighbors, and U pending updates, the metadata cost per node is: This cost may represent a large fraction of all data propagated during synchronization. For example, in our measurements with 32 nodes, this metadata represents 75\\%, 99\\%, and 97\\% of the transmission costs for Scuttlebutt, Scuttlebutt-GC and operation-based, respectively, while the overhead of delta-based synchronization is only 7.7\\%. \\subsubsection{Memory footprint", "With a partial-mesh, \\BP has little effect, and \\RR contributes most to the overall improvement. Given that the underlying topology leads to redundant communication (desired for fault-tolerance), and classic delta-based can never extract that redundancy, its transmission bandwidth is effectively similar to that of state-based synchronization. Scuttlebutt and Scuttlebutt-GC are more efficient than classic delta-based for \\af{GSet since both can precisely identify state changes between synchronization rounds."], "referred_text_chunks": ["During the experiment we also measured the CPU time spent in processing CRDT updates, both producing and processing synchronization messages. Figure reports the CPU overhead of classic delta-based, when considering \\BP + \\RR as baseline."]}, "1": {"question": "What is the difference between GCounter and GSet?", "referred_figures_indices": [6], "response": "{'Image': 1, 'Text': 4, 'Rationale': 'Image 1 provides a visual representation of both GCounter and GSet, illustrating their structures directly. Text 4 describes the specifications of GCounter and GSet, detailing their operations and characteristics.', 'Answer': 'GCounter is a grow-only counter where each replica maintains a counter that can only be incremented, and its value is the sum of all replicas. GSet is a grow-only set where elements can be added but not removed. GCounter involves numerical increments, while GSet involves adding unique elements.'}", "all_figures_names": ["1803.02750v3-Figure13-1.png", "1803.02750v3-Figure3-1.png", "1803.02750v3-Figure10-1.png", "1803.02750v3-TableIV-1.png", "1803.02750v3-Figure2-1.png", "1803.02750v3-Figure8-1.png", "1803.02750v3-TableI-1.png", "1803.02750v3-Figure9-1.png"], "referred_figures_names": ["1803.02750v3-TableI-1.png"], "answer": "GCounter measures the number of times an event has occurred, while GSet measures the number of unique elements in a set.", "figure_type": "table", "content_type": "table", "text_chunks": ["In \\af{GSet, the update event is the addition of a globally unique element to the set; in \\af{GCounter, an increment on the counter; and in \\af{GMap\\ K\\% each node updates \\frac{\\af{K{\\af{N\\% keys (\\af{N being the number of nodes/replicas), such that globally \\af{K\\% of all the keys in the grow-only map are modified within each synchronization interval. Note how the \\af{GCounter benchmark is a particular case of \\af{GMap\\ K\\%, in which \\af{K = 100. For \\af{GMap\\ K\\% we set the total number of keys to 1000, and for all benchmarks, the number of events per replica is set to 100.", "However, the results for \\af{GCounter reveal a limitation of this approach. Since Scuttlebutt treats propagated values as opaque, and does not understand that the changes in a \\af{GCounter compress naturally under lattice joins (only the highest sequence for each replica needs to be kept), it effectively behaves worse than state-based and classic delta-based in this case. Operation-based synchronization follows the same trend for the same reason: it improves state-based and classic delta-based for \\af{GSet but not for \\af{GCounter since the middleware is unable to compress multiple operations into a single, equivalent, operation.", "Note that two of the constructs, {\\hookrightarrow and \\mathcal{P, were used in Section to define \\af{GCounter and \\af{GSet, respectively. The use of these composition techniques and a catalog of CRDTs is presented in~ but that presentation (as well as CRDT designs in general) simply considers building join-semilattices (typically with bottom) from join-semilattices, never examining whether the result is more than a join-semilattice. \\newcommand\\maximalsbelow{ \\{ \\tup{\\seta, \\seta, \\tup{\\seta, \\setb, \\tup{\\setb, \\seta, \\tup{\\setb, \\setb \\", "\\newcommand\\mltext[2]{ \\vspace*{-.42cm \\end{minipage \\def\\colalen{1.5cm \\def\\colblen{2.4cm \\def\\colclen{2.6cm These micro-benchmarks are summarized in Table , along with the metric (to be used in transmission and memory measurements) we have defined: for \\af{GCounter and \\af{GMap\\ K\\% we count the number of map entries, while for \\af{GSet, the number of set elements. We setup this part of the evaluation with 15-node topologies (as in Figure ).", "Note how the decompositions of \\af{GCounter and \\af{GSet presented in Section are an application of these rules. As a further example, consider a positive-negative counter \\--- \\af{PNCounter \\--- a CRDT counter that allows both increments and decrements. In this CRDT, each replica identifier is mapped to a pair where the first component tracks the number of increments, and the second the number of decrements, i.e. \\af{PNCounter = \\ids \\map (\\nat \\times \\nat).", "Operation-based has a higher memory cost than Scuttlebutt-GC, since each operation in the transmission buffer is tagged with a vector, while in Scuttlebutt and Scuttlebutt-GC each delta is simply tagged with a version pair. Considering the results for \\af{GCounter, the three vector-based algorithms exhibit the highest memory consumption. This is justified by the same reason they perform poorly in terms of transmission bandwidth in this case (Figure~): these protocols are unable to compress incremental changes.", "Supporting generic operation-compression at the middleware level in operation-based CRDTs is an open research problem. The difference between these three approaches is related with the metadata cost associated to each, as we show in Section~. Even with the optimizations \\BP + \\RR proposed, the best result for \\af{GCounter is not much better than state-based. This is expected since most entries of the underlying map are being updated between each synchronization step: each node has almost always something new from every other node in the system to propagate (thus being similar to state-based in some cases).", "The value of the counter is the sum of each entry's value in the map. Mutator \\af{inc returns the updated map (the notation p\\{k \\mapsto v\\ indicates that only entry k in the map p is updated to a new value v, the remaining entries left unchanged), while the \\delta-mutator \\daf{inc only returns the updated entry. The join of two \\af{GCounters computes, for each key, the maximum of the associated values."], "referred_text_chunks": ["In \\af{GSet, the update event is the addition of a globally unique element to the set; in \\af{GCounter, an increment on the counter; and in \\af{GMap\\ K\\% each node updates \\frac{\\af{K{\\af{N\\% keys (\\af{N being the number of nodes/replicas), such that globally \\af{K\\% of all the keys in the grow-only map are modified within each synchronization interval. Note how the \\af{GCounter benchmark is a particular case of \\af{GMap\\ K\\%, in which \\af{K = 100. For \\af{GMap\\ K\\% we set the total number of keys to 1000, and for all benchmarks, the number of events per replica is set to 100."]}, "2": {"question": "Which of the algorithms is most efficient in terms of transmission in a tree topology?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 0, 'Rationale': 'Image 6 provides a clear comparison of transmission efficiencies for different algorithms in tree topologies, relevant to the question. Text 0 discusses the efficiency of synchronization mechanisms, highlighting delta-based synchronization efficiency with BP optimizations in tree topologies.', 'Answer': 'Delta-based BP (Bloom Pruning) is the most efficient algorithm in tree topologies.'}", "all_figures_names": ["1803.02750v3-Figure3-1.png", "1803.02750v3-Figure13-1.png", "1803.02750v3-Figure1-1.png", "1803.02750v3-Figure6-1.png", "1803.02750v3-TableIII-1.png", "1803.02750v3-TableII-1.png", "1803.02750v3-Figure7-1.png", "1803.02750v3-Figure5-1.png"], "referred_figures_names": ["1803.02750v3-Figure7-1.png"], "answer": "Op-based GSet.", "figure_type": "plot.", "content_type": "figure", "text_chunks": ["\\subsubsection{Transmission bandwidth Figure shows, for \\af{GSet and \\af{GCounter, the transmission ratio (of all synchronization mechanisms previously mentioned) with respect to delta-based synchronization with \\BP and \\RR optimizations enabled. The first observation is that classic delta-based synchronization presents almost no improvement, when compared to state-based synchronization. In the tree topology, \\BP is enough to attain the best result, because the underlying topology does not have cycles, and thus, \\BP is sufficient to prevent redundant state to be propagated.", "We label this approach \\texttt{Scuttlebutt. \\caption{Transmission of \\af{GMap\\ 10\\%, \\af{30\\%, \\af{60\\% and \\af{100\\% -- tree and mesh topologies. \\end{figure*", "The topology is a partial-mesh, with 50 nodes, each with 4 neighbors, as in Figure , and updates on objects follow a Zipf distribution, with coefficients ranging from 0.5 (low contention) to 1.5 (high contention) . Figure~ shows the transmission bandwidth and memory footprint of both algorithms, for different Zipf coefficient values. We can observe that in low contention workloads, classic delta-based behaves almost optimally when compared to \\BP + \\RR.", "The first topology exhibits redundancy in the links and tests the effect of cycles in the synchronization, while the second represents an optimal propagation scenario over a spanning tree. Micro-Benchmarks We have designed a set of micro-benchmarks, in which each node periodically (every second) synchronizes with neighbors and executes an update operation over a CRDT. The update operation depends on the CRDT type.", "With a partial-mesh, \\BP has little effect, and \\RR contributes most to the overall improvement. Given that the underlying topology leads to redundant communication (desired for fault-tolerance), and classic delta-based can never extract that redundancy, its transmission bandwidth is effectively similar to that of state-based synchronization. Scuttlebutt and Scuttlebutt-GC are more efficient than classic delta-based for \\af{GSet since both can precisely identify state changes between synchronization rounds.", "Finally, delta-based \\BP + \\RR is able reduce the transmission costs by up-to 94\\%. In the extreme case of \\af{GMap\\ 100\\% (every key in the map is updated between synchronization rounds, which is a less likely workload in practical systems) and considering a partial-mesh, delta-based \\BP + \\RR provides a modest improvement in relation to state-based of about 18\\% less transmission, and its performance is below Scuttlebutt variants and operation-based synchronization.", "Each machine has a Quad Core Intel Xeon 2.4 GHz and 12GB of RAM. The number of machines in the cluster is set such that two replicas are never scheduled to run in the same machine, i.e. there is at least one machine available for each replica in the experiment. \\paragraph*{Network Topologies Figure depicts the two network topologies employed in the experiments: a partial-mesh, in which each node has 4 neighbors; and a tree, with 3 neighbors per node, with the exception of the root node (2 neighbors) and leaf nodes (1 neighbor).", "In this approach, CRDTs need to be extended to maintain additional metadata for delta derivation, and if this metadata needs to be garbage collected, the mechanism falls-back to standard bidirectional full state transmission. In the context of anti-entropy gossip protocols, Scuttlebutt proposes a push-pull algorithm to be used to synchronize a set of values between participants, but considers each value as opaque, and does not try to represent recent changes to these values as deltas. Other solutions try to minimize the communication overhead of anti-entropy gossip-based protocols by exploiting either hash functions~ or a combination of Bloom filters, Merkle trees, and Patricia tries~."], "referred_text_chunks": ["\\subsubsection{Transmission bandwidth Figure shows, for \\af{GSet and \\af{GCounter, the transmission ratio (of all synchronization mechanisms previously mentioned) with respect to delta-based synchronization with \\BP and \\RR optimizations enabled. The first observation is that classic delta-based synchronization presents almost no improvement, when compared to state-based synchronization. In the tree topology, \\BP is enough to attain the best result, because the underlying topology does not have cycles, and thus, \\BP is sufficient to prevent redundant state to be propagated."]}, "3": {"question": "What is the role of the RR optimization in the delta-based synchronization of a GSet?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 3, 'Rationale': 'Image 2 illustrates the delta-based synchronization with RR optimization in a GSet, while Text 3 describes how RR removes redundant state in received delta-groups, providing direct information related to RR optimization.', 'Answer': 'The RR optimization in delta-based synchronization of a GSet removes redundant state in received delta-groups before adding them to the delta-buffer, reducing unnecessary transmission.'}", "all_figures_names": ["1803.02750v3-Figure6-1.png", "1803.02750v3-TableI-1.png", "1803.02750v3-Figure5-1.png", "1803.02750v3-Figure9-1.png", "1803.02750v3-Figure8-1.png", "1803.02750v3-TableIV-1.png", "1803.02750v3-Figure3-1.png", "1803.02750v3-Figure7-1.png"], "referred_figures_names": ["1803.02750v3-Figure5-1.png"], "answer": "The RR optimization helps to reduce the number of messages that need to be exchanged between replicas.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Finally, delta-based \\BP + \\RR is able reduce the transmission costs by up-to 94\\%. In the extreme case of \\af{GMap\\ 100\\% (every key in the map is updated between synchronization rounds, which is a less likely workload in practical systems) and considering a partial-mesh, delta-based \\BP + \\RR provides a modest improvement in relation to state-based of about 18\\% less transmission, and its performance is below Scuttlebutt variants and operation-based synchronization.", "\\subsubsection{Transmission bandwidth Figure shows, for \\af{GSet and \\af{GCounter, the transmission ratio (of all synchronization mechanisms previously mentioned) with respect to delta-based synchronization with \\BP and \\RR optimizations enabled. The first observation is that classic delta-based synchronization presents almost no improvement, when compared to state-based synchronization. In the tree topology, \\BP is enough to attain the best result, because the underlying topology does not have cycles, and thus, \\BP is sufficient to prevent redundant state to be propagated.", "With a partial-mesh, \\BP has little effect, and \\RR contributes most to the overall improvement. Given that the underlying topology leads to redundant communication (desired for fault-tolerance), and classic delta-based can never extract that redundancy, its transmission bandwidth is effectively similar to that of state-based synchronization. Scuttlebutt and Scuttlebutt-GC are more efficient than classic delta-based for \\af{GSet since both can precisely identify state changes between synchronization rounds.", "However, part of this \\delta-group has already been in the \\delta-buffer (namely b), and thus, has already been propagated. This observation hints for another optimization: remove redundant state in received \\delta-groups (\\RR), before adding them to the \\delta-buffer. Both \\BP and \\RR optimizations are detailed in Section , where we incorporate them into the delta-based synchronization algorithm with few changes.", "For further coverage of delta-based CRDTs see . Synchronization Cost Problem Figures and illustrate possible distributed executions of the classic delta-based synchronization algorithm , with replicas of a grow-only-set, all starting with a bottom value \\bot = \\varnothing. (This classic algorithm is captured in Algorithm , covered in Section .)", "\\RR. State-based does not require synchronization metadata, and thus it is optimal in terms of memory usage. Classic delta-based and delta-based \\BP have an overhead of 1.1\\af{x-3.9\\af{x since the size of \\delta-groups in the \\delta-buffer is larger for these techniques. For \\af{GSet and \\af{GMap\\ 10\\%, Scuttlebutt-GC is close to \\BP +", "Since classic delta-based produces/processes larger messages than \\BP + \\RR, this results in a higher CPU cost: for the 1, 1.25 and 1.5 Zipf coefficients, classic delta-based incurs an overhead of 0.4\\af{x, 5.5\\af{x, and 7.9\\af{x respectively. Related Work In the context of remote file synchronization, rsync synchronizes two files placed on different machines, by generating file block signatures, and using these signatures to identify the missing blocks on the backup file. In this strategy, there's a trade-off between the size of the blocks to be signed, the number of signatures to be sent, and the size of the blocks to be received: bigger blocks to be signed implies fewer signatures to be sent, but the blocks received (deltas) can be bigger than necessary.", "During the experiment we also measured the CPU time spent in processing CRDT updates, both producing and processing synchronization messages. Figure reports the CPU overhead of classic delta-based, when considering \\BP + \\RR as baseline."], "referred_text_chunks": ["Finally, delta-based \\BP + \\RR is able reduce the transmission costs by up-to 94\\%. In the extreme case of \\af{GMap\\ 100\\% (every key in the map is updated between synchronization rounds, which is a less likely workload in practical systems) and considering a partial-mesh, delta-based \\BP + \\RR provides a modest improvement in relation to state-based of about 18\\% less transmission, and its performance is below Scuttlebutt variants and operation-based synchronization."]}, "4": {"question": "How does the average metadata required per node for the Op-based BP+RR approach change as the number of nodes in the network increases?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 3, 'Rationale': 'Image 5 shows how the average metadata required per node changes as the number of nodes increases, specifically for Op-based BP+RR. Text 3 discusses metadata costs for synchronization, which is relevant to the question.', 'Answer': 'As the number of nodes in the network increases, the average metadata required per node for Op-based BP+RR remains constant at 0.2 MB.'}", "all_figures_names": ["1803.02750v3-TableIII-1.png", "1803.02750v3-Figure13-1.png", "1803.02750v3-Figure12-1.png", "1803.02750v3-TableIV-1.png", "1803.02750v3-Figure8-1.png", "1803.02750v3-Figure9-1.png", "1803.02750v3-Figure6-1.png", "1803.02750v3-Figure10-1.png"], "referred_figures_names": ["1803.02750v3-Figure9-1.png"], "answer": "The average metadata required per node for the Op-based BP+RR approach increases as the number of nodes in the network increases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Supporting generic operation-compression at the middleware level in operation-based CRDTs is an open research problem. The difference between these three approaches is related with the metadata cost associated to each, as we show in Section~. Even with the optimizations \\BP + \\RR proposed, the best result for \\af{GCounter is not much better than state-based. This is expected since most entries of the underlying map are being updated between each synchronization step: each node has almost always something new from every other node in the system to propagate (thus being similar to state-based in some cases).", "\\RR is required, in order to avoid redundant state propagation. For example, with a 1.25 coefficient, bandwidth is reduced from 1.46GB/s to 0.06GB/s per node, and memory footprint per node drops from 1.58GB to 0.62GB (right side of the plots). Also, as we increase the Zipf coefficient, we note that the bandwidth consumption continues to rise, leading to an unsustainable situation in the case of classic delta-based, as it can never reduce the size of \\delta-groups being transmitted.", "In delta-based synchronization, the size of \\delta-groups being propagated not only affects the network bandwidth consumption, but also the memory required to store them in the \\delta-buffer for further propagation. During the experiments, we periodically measure the amount of state (both CRDT state and metadata required for synchronization) stored in memory for each node. Figure~ reports the average memory ratio with respect to \\BP +", "Vector-based protocols (Scuttlebutt and operation-based) however, have an inherent scalability problem. When increasing the number of nodes in the system, the transmission costs may become dominated by the size of metadata required for synchronization, as we show next. \\subsubsection{Metadata Cost Figure~ shows the size of metadata required for synchronization per node while varying the total number of replicas (i.e. nodes).", "The topology is a partial-mesh, with 50 nodes, each with 4 neighbors, as in Figure , and updates on objects follow a Zipf distribution, with coefficients ranging from 0.5 (low contention) to 1.5 (high contention) . Figure~ shows the transmission bandwidth and memory footprint of both algorithms, for different Zipf coefficient values. We can observe that in low contention workloads, classic delta-based behaves almost optimally when compared to \\BP + \\RR.", "During the experiment we also measured the CPU time spent in processing CRDT updates, both producing and processing synchronization messages. Figure reports the CPU overhead of classic delta-based, when considering \\BP + \\RR as baseline.", "The results show a linear and quadratic cost (in terms of number of nodes) for Scuttlebutt and Scuttlebutt-GC (respectively), and a linear cost for operation-based synchronization (in terms of both number of nodes and pending updates still to be propagated). Given N nodes, P neighbors, and U pending updates, the metadata cost per node is: This cost may represent a large fraction of all data propagated during synchronization. For example, in our measurements with 32 nodes, this metadata represents 75\\%, 99\\%, and 97\\% of the transmission costs for Scuttlebutt, Scuttlebutt-GC and operation-based, respectively, while the overhead of delta-based synchronization is only 7.7\\%. \\subsubsection{Memory footprint", "\\RR. State-based does not require synchronization metadata, and thus it is optimal in terms of memory usage. Classic delta-based and delta-based \\BP have an overhead of 1.1\\af{x-3.9\\af{x since the size of \\delta-groups in the \\delta-buffer is larger for these techniques. For \\af{GSet and \\af{GMap\\ 10\\%, Scuttlebutt-GC is close to \\BP +"], "referred_text_chunks": ["Supporting generic operation-compression at the middleware level in operation-based CRDTs is an open research problem. The difference between these three approaches is related with the metadata cost associated to each, as we show in Section~. Even with the optimizations \\BP + \\RR proposed, the best result for \\af{GCounter is not much better than state-based. This is expected since most entries of the underlying map are being updated between each synchronization step: each node has almost always something new from every other node in the system to propagate (thus being similar to state-based in some cases)."]}, "5": {"question": "Which algorithm performs the best in terms of average memory ratio with respect to BP+RR for GMap 10%?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 2, 'Rationale': 'Image 2 and Text 2 both provide data on memory ratios for GMap 10% including comparisons with BP+RR. Image 2 specifically visualizes average memory ratios, while Text 2 summarizes these findings, highlighting BP+RR\u2019s effectiveness.', 'Answer': 'Delta-based BP+RR performs the best for GMap 10% in terms of average memory ratio.'}", "all_figures_names": ["1803.02750v3-Figure3-1.png", "1803.02750v3-Figure1-1.png", "1803.02750v3-Figure10-1.png", "1803.02750v3-TableIII-1.png", "1803.02750v3-Figure12-1.png", "1803.02750v3-TableII-1.png", "1803.02750v3-Figure4-1.png", "1803.02750v3-Figure9-1.png"], "referred_figures_names": ["1803.02750v3-Figure10-1.png"], "answer": "Delta-based BP+RR", "figure_type": "plot", "content_type": "figure", "text_chunks": ["\\RR. State-based does not require synchronization metadata, and thus it is optimal in terms of memory usage. Classic delta-based and delta-based \\BP have an overhead of 1.1\\af{x-3.9\\af{x since the size of \\delta-groups in the \\delta-buffer is larger for these techniques. For \\af{GSet and \\af{GMap\\ 10\\%, Scuttlebutt-GC is close to \\BP +", "Finally, delta-based \\BP + \\RR is able reduce the transmission costs by up-to 94\\%. In the extreme case of \\af{GMap\\ 100\\% (every key in the map is updated between synchronization rounds, which is a less likely workload in practical systems) and considering a partial-mesh, delta-based \\BP + \\RR provides a modest improvement in relation to state-based of about 18\\% less transmission, and its performance is below Scuttlebutt variants and operation-based synchronization.", "Overall, and ignoring state-based which doesn't present any metadata memory costs, \\BP + \\RR attains the best results. Retwis Application \\def\\colalen{1.5cm \\def\\colblen{2cm \\def\\colclen{1.7cm", "The topology is a partial-mesh, with 50 nodes, each with 4 neighbors, as in Figure , and updates on objects follow a Zipf distribution, with coefficients ranging from 0.5 (low contention) to 1.5 (high contention) . Figure~ shows the transmission bandwidth and memory footprint of both algorithms, for different Zipf coefficient values. We can observe that in low contention workloads, classic delta-based behaves almost optimally when compared to \\BP + \\RR.", "Supporting generic operation-compression at the middleware level in operation-based CRDTs is an open research problem. The difference between these three approaches is related with the metadata cost associated to each, as we show in Section~. Even with the optimizations \\BP + \\RR proposed, the best result for \\af{GCounter is not much better than state-based. This is expected since most entries of the underlying map are being updated between each synchronization step: each node has almost always something new from every other node in the system to propagate (thus being similar to state-based in some cases).", "This pattern represents a special case of a map in which \\af{100\\% of its keys are updated between state synchronizations. In Figure we study other update patterns, by measuring the transmission of \\af{GMap\\ 10\\%, \\af{30\\%, \\af{60\\%, and \\af{100\\%. These results are further evidence of what we have observed in the case of \\af{GSet: \\BP suffices if the network graph is acyclic, but \\RR is crucial in the more general case.", "We label this approach \\texttt{Scuttlebutt. \\caption{Transmission of \\af{GMap\\ 10\\%, \\af{30\\%, \\af{60\\% and \\af{100\\% -- tree and mesh topologies. \\end{figure*", "\\subsubsection{Transmission bandwidth Figure shows, for \\af{GSet and \\af{GCounter, the transmission ratio (of all synchronization mechanisms previously mentioned) with respect to delta-based synchronization with \\BP and \\RR optimizations enabled. The first observation is that classic delta-based synchronization presents almost no improvement, when compared to state-based synchronization. In the tree topology, \\BP is enough to attain the best result, because the underlying topology does not have cycles, and thus, \\BP is sufficient to prevent redundant state to be propagated."], "referred_text_chunks": ["\\RR. State-based does not require synchronization metadata, and thus it is optimal in terms of memory usage. Classic delta-based and delta-based \\BP have an overhead of 1.1\\af{x-3.9\\af{x since the size of \\delta-groups in the \\delta-buffer is larger for these techniques. For \\af{GSet and \\af{GMap\\ 10\\%, Scuttlebutt-GC is close to \\BP +"]}, "6": {"question": "What is the difference between the `inc_i(p)` and `inc_i'(p)` operations in the Grow-only Counter data type?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 1, 'Rationale': 'The image provides the exact definitions of inc_i(p) and inc_i\\'(p) in the GCounter, while the text elaborates on the result of the operations.', 'Answer': 'The difference is that inc_i(p) returns the updated map, updating only the entry for the increment, while inc_i\\'(p) returns only the updated entry, useful for delta operations.'}", "all_figures_names": ["1803.02750v3-TableIII-1.png", "1803.02750v3-Figure12-1.png", "1803.02750v3-TableI-1.png", "1803.02750v3-TableII-1.png", "1803.02750v3-Figure13-1.png", "1803.02750v3-Figure2-1.png", "1803.02750v3-Figure1-1.png", "1803.02750v3-Figure5-1.png"], "referred_figures_names": ["1803.02750v3-Figure2-1.png"], "answer": "The `inc_i(p)` operation increments the value associated with the key `i` in the counter `p`, while the `inc_i'(p)` operation increments the value associated with the key `i` in the counter `p` only if the key `i` is not already present in the counter.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["A CRDT counter that only allows increments is known as a grow-only counter (Figure ). In this data type, the set of replica identifiers \\ids is mapped to the set of natural numbers \\nat. Increments are tracked per replica i, individually, and stored in a map entry p(i).", "The value of the counter is the sum of each entry's value in the map. Mutator \\af{inc returns the updated map (the notation p\\{k \\mapsto v\\ indicates that only entry k in the map p is updated to a new value v, the remaining entries left unchanged), while the \\delta-mutator \\daf{inc only returns the updated entry. The join of two \\af{GCounters computes, for each key, the maximum of the associated values.", "In \\af{GSet, the update event is the addition of a globally unique element to the set; in \\af{GCounter, an increment on the counter; and in \\af{GMap\\ K\\% each node updates \\frac{\\af{K{\\af{N\\% keys (\\af{N being the number of nodes/replicas), such that globally \\af{K\\% of all the keys in the grow-only map are modified within each synchronization interval. Note how the \\af{GCounter benchmark is a particular case of \\af{GMap\\ K\\%, in which \\af{K = 100. For \\af{GMap\\ K\\% we set the total number of keys to 1000, and for all benchmarks, the number of events per replica is set to 100.", "Note how the decompositions of \\af{GCounter and \\af{GSet presented in Section are an application of these rules. As a further example, consider a positive-negative counter \\--- \\af{PNCounter \\--- a CRDT counter that allows both increments and decrements. In this CRDT, each replica identifier is mapped to a pair where the first component tracks the number of increments, and the second the number of decrements, i.e. \\af{PNCounter = \\ids \\map (\\nat \\times \\nat).", "Given a \\af{PNCounter state p = \\{ \\A{\\mapsto \\tup{2, 3, \\B{\\mapsto \\tup{5, 5 \\ (2 increments by \\A, 3 decrements by \\A, and an equal number of increments and decrements by \\B), the irredundant join decomposition of p is \\dec{p = \\{ \\{ \\A{\\mapsto \\tup{2, 0 \\, \\{ \\A{\\mapsto \\tup{0, 3 \\, \\{ \\B{\\mapsto \\tup{5, 0 \\, \\{ \\B{\\mapsto \\tup{0, 5 \\ \\.", "Operation-based has a higher memory cost than Scuttlebutt-GC, since each operation in the transmission buffer is tagged with a vector, while in Scuttlebutt and Scuttlebutt-GC each delta is simply tagged with a version pair. Considering the results for \\af{GCounter, the three vector-based algorithms exhibit the highest memory consumption. This is justified by the same reason they perform poorly in terms of transmission bandwidth in this case (Figure~): these protocols are unable to compress incremental changes.", "The lattice state evolution (either by mutation or join of two states) can also be understood by looking at the corresponding Hasse diagram (Figure ). For example, state \\countab{1{1 in Figure (where \\A_1 represents entry \\{\\A \\mapsto 1 \\ in the map, i.e. one increment registered by replica \\A), can result from an increment on \\counta{1 by \\B, from an increment on \\countb{1 by \\A, or from the join of these two states. A grow-only set, Figures and Figure , is a set data type that only allows element additions.", "Supporting generic operation-compression at the middleware level in operation-based CRDTs is an open research problem. The difference between these three approaches is related with the metadata cost associated to each, as we show in Section~. Even with the optimizations \\BP + \\RR proposed, the best result for \\af{GCounter is not much better than state-based. This is expected since most entries of the underlying map are being updated between each synchronization step: each node has almost always something new from every other node in the system to propagate (thus being similar to state-based in some cases)."], "referred_text_chunks": ["A CRDT counter that only allows increments is known as a grow-only counter (Figure ). In this data type, the set of replica identifiers \\ids is mapped to the set of natural numbers \\nat. Increments are tracked per replica i, individually, and stored in a map entry p(i)."]}, "7": {"question": "Which topology has the highest transmission rate for GMap 100%?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 4, 'Rationale': 'Image 4 shows the transmission for GMap 100% in both tree and mesh topologies, which directly relates to the question about transmission rates. Text 4 discusses the transmission costs for GMap 100% using different approaches, providing context and relevant information.', 'Answer': 'The mesh topology has the highest transmission rate for GMap 100%.'}", "all_figures_names": ["1803.02750v3-Figure7-1.png", "1803.02750v3-Figure4-1.png", "1803.02750v3-Figure13-1.png", "1803.02750v3-Figure1-1.png", "1803.02750v3-Figure8-1.png", "1803.02750v3-Figure3-1.png", "1803.02750v3-Figure12-1.png", "1803.02750v3-Figure10-1.png"], "referred_figures_names": ["1803.02750v3-Figure8-1.png"], "answer": "Mesh", "figure_type": "plot", "content_type": "figure", "text_chunks": ["We label this approach \\texttt{Scuttlebutt. \\caption{Transmission of \\af{GMap\\ 10\\%, \\af{30\\%, \\af{60\\% and \\af{100\\% -- tree and mesh topologies. \\end{figure*", "The topology is a partial-mesh, with 50 nodes, each with 4 neighbors, as in Figure , and updates on objects follow a Zipf distribution, with coefficients ranging from 0.5 (low contention) to 1.5 (high contention) . Figure~ shows the transmission bandwidth and memory footprint of both algorithms, for different Zipf coefficient values. We can observe that in low contention workloads, classic delta-based behaves almost optimally when compared to \\BP + \\RR.", "\\subsubsection{Transmission bandwidth Figure shows, for \\af{GSet and \\af{GCounter, the transmission ratio (of all synchronization mechanisms previously mentioned) with respect to delta-based synchronization with \\BP and \\RR optimizations enabled. The first observation is that classic delta-based synchronization presents almost no improvement, when compared to state-based synchronization. In the tree topology, \\BP is enough to attain the best result, because the underlying topology does not have cycles, and thus, \\BP is sufficient to prevent redundant state to be propagated.", "This pattern represents a special case of a map in which \\af{100\\% of its keys are updated between state synchronizations. In Figure we study other update patterns, by measuring the transmission of \\af{GMap\\ 10\\%, \\af{30\\%, \\af{60\\%, and \\af{100\\%. These results are further evidence of what we have observed in the case of \\af{GSet: \\BP suffices if the network graph is acyclic, but \\RR is crucial in the more general case.", "Finally, delta-based \\BP + \\RR is able reduce the transmission costs by up-to 94\\%. In the extreme case of \\af{GMap\\ 100\\% (every key in the map is updated between synchronization rounds, which is a less likely workload in practical systems) and considering a partial-mesh, delta-based \\BP + \\RR provides a modest improvement in relation to state-based of about 18\\% less transmission, and its performance is below Scuttlebutt variants and operation-based synchronization.", "\\newcommand\\mltext[2]{ \\vspace*{-.42cm \\end{minipage \\def\\colalen{1.5cm \\def\\colblen{2.4cm \\def\\colclen{2.6cm These micro-benchmarks are summarized in Table , along with the metric (to be used in transmission and memory measurements) we have defined: for \\af{GCounter and \\af{GMap\\ K\\% we count the number of map entries, while for \\af{GSet, the number of set elements. We setup this part of the evaluation with 15-node topologies (as in Figure ).", "\\RR. State-based does not require synchronization metadata, and thus it is optimal in terms of memory usage. Classic delta-based and delta-based \\BP have an overhead of 1.1\\af{x-3.9\\af{x since the size of \\delta-groups in the \\delta-buffer is larger for these techniques. For \\af{GSet and \\af{GMap\\ 10\\%, Scuttlebutt-GC is close to \\BP +", "With a partial-mesh, \\BP has little effect, and \\RR contributes most to the overall improvement. Given that the underlying topology leads to redundant communication (desired for fault-tolerance), and classic delta-based can never extract that redundancy, its transmission bandwidth is effectively similar to that of state-based synchronization. Scuttlebutt and Scuttlebutt-GC are more efficient than classic delta-based for \\af{GSet since both can precisely identify state changes between synchronization rounds."], "referred_text_chunks": ["We label this approach \\texttt{Scuttlebutt. \\caption{Transmission of \\af{GMap\\ 10\\%, \\af{30\\%, \\af{60\\% and \\af{100\\% -- tree and mesh topologies. \\end{figure*"]}, "8": {"question": "If a user with 100 followers posts a tweet, how many CRDT updates will be performed in total, and what percentage of the overall workload does this represent?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 2, 'Rationale': 'Image 7 directly provides information on the number of updates and workload percentage for posting a tweet, relevant to the question. Text 2 gives a detailed description of the structure and operations involved in a Twitter-like application, making it relevant for understanding the CRDT updates.', 'Answer': 'If a user with 100 followers posts a tweet, there will be 101 CRDT updates (1 for the post itself, plus 1 for each of the 100 followers). This represents 35% of the overall workload.'}", "all_figures_names": ["1803.02750v3-Figure5-1.png", "1803.02750v3-Figure13-1.png", "1803.02750v3-TableI-1.png", "1803.02750v3-Figure1-1.png", "1803.02750v3-Figure12-1.png", "1803.02750v3-Figure3-1.png", "1803.02750v3-TableIV-1.png", "1803.02750v3-TableII-1.png"], "referred_figures_names": ["1803.02750v3-TableII-1.png"], "answer": "Posting a tweet will result in 1 + 100 = 101 CRDT updates. This represents 35% of the overall workload.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["We run this benchmark with 10K users, and thus, 30K CRDT objects overall. The size of tweet identifiers and content is 31B and 270B, respectively. These sizes are representative of real workloads, as shown in an analysis of Facebook's general-purpose key-value store .", "In this paper we advance the foundations of state-based CRDTs by introducing minimal deltas that precisely track state changes. We also present and micro-benchmark two optimizations, avoid back-propagation of \\delta-groups and remove redundant state in received \\delta-groups, that solve inefficiencies in classic delta-based synchronization algorithms. Further evaluation shows the improvement our solution can bring to a small scale Twitter clone deployed in a 50-node cluster, a relevant application scenario.", "We now compare classic delta-based with delta-based \\BP + \\RR using Retwis , a popular open-source Twitter clone. In Table we describe the application workload, similar to the one used in : user a can follow user b by updating the set of followers of user b; users can post a new tweet, by writing it in their wall and in the timeline of all their followers; and finally, users can read their timeline, fetching the 10 most recent tweets. Each user has 3 objects associated with it: 1) a set of followers stored in a \\af{GSet; 2) a wall stored in a \\af{GMap mapping tweet identifiers to their content; and 3) a timeline stored in a \\af{GMap mapping tweet timestamps to tweet identifiers.", "During the experiment we also measured the CPU time spent in processing CRDT updates, both producing and processing synchronization messages. Figure reports the CPU overhead of classic delta-based, when considering \\BP + \\RR as baseline.", "The first topology exhibits redundancy in the links and tests the effect of cycles in the synchronization, while the second represents an optimal propagation scenario over a spanning tree. Micro-Benchmarks We have designed a set of micro-benchmarks, in which each node periodically (every second) synchronizes with neighbors and executes an update operation over a CRDT. The update operation depends on the CRDT type.", "In operation-based CRDTs , operations are disseminated assuming a reliable dissemination layer that ensures exactly-once causal delivery of operations. State-based CRDTs need fewer guarantees from the communication channel: messages can be dropped, duplicated, and reordered. When an update operation occurs, the local state is updated through a mutator, and from time to time (since we can disseminate the state at a lower rate than the rate of the updates) the full (local) state is propagated to other replicas.", "\\B then sends the computed delta along with a digest of its local state, allowing \\A to compute a delta for \\B. Convergence is achieved after 2 and 3 messages in state-driven and digest-driven, respectively. These two techniques also exploit the concept of join decomposition presented in this paper. Similarly to digest-driven synchronization, \\Delta-CRDTs exchange metadata used to compute a delta that reflects missing updates.", "This pattern represents a special case of a map in which \\af{100\\% of its keys are updated between state synchronizations. In Figure we study other update patterns, by measuring the transmission of \\af{GMap\\ 10\\%, \\af{30\\%, \\af{60\\%, and \\af{100\\%. These results are further evidence of what we have observed in the case of \\af{GSet: \\BP suffices if the network graph is acyclic, but \\RR is crucial in the more general case."], "referred_text_chunks": ["We run this benchmark with 10K users, and thus, 30K CRDT objects overall. The size of tweet identifiers and content is 31B and 270B, respectively. These sizes are representative of real workloads, as shown in an analysis of Facebook's general-purpose key-value store ."]}, "9": {"question": "If we use the lexicographic product with a chain as the first component and a distributive lattice as the second component to design a CRDT, will the resulting CRDT lattice be guaranteed to be distributive and satisfy the descending chain condition (DCC)?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 1, 'Rationale': 'Image 1 visually presents information on when a lexicographic product lattice satisfies DCC and distributivity, which is directly relevant to the question. Text 1 discusses the properties of lexicographic pairs, including non-distributivity depending on component arrangement. Both sources relate directly to whether the resulting CRDT lattice meets distributivity and DCC conditions.', 'Answer': 'No, using a lexicographic product with a chain as the first component and a distributive lattice as the second component does not ensure distributivity, though it does satisfy DCC.'}", "all_figures_names": ["1803.02750v3-Figure12-1.png", "1803.02750v3-TableIII-1.png", "1803.02750v3-Figure7-1.png", "1803.02750v3-Figure6-1.png", "1803.02750v3-TableI-1.png", "1803.02750v3-Figure3-1.png", "1803.02750v3-Figure8-1.png", "1803.02750v3-Figure9-1.png"], "referred_figures_names": ["1803.02750v3-TableIII-1.png"], "answer": "Yes, the resulting CRDT lattice will be guaranteed to be both distributive and satisfy the DCC.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["This principle is followed by Cassandra counters . In such typical usages of the lexicographic product, with a chain as first component, the distributivity of the second component is propagated to the resulting construct. Table summarizes these remarks about how almost always these CRDT composition techniques yield lattices satisfying DCC and distributive lattices, and thus, have unique irredundant decompositions, by Proposition .", "As an example, in Figure~ we depict the Hasse diagram of a non-distributive lexicographic pair. This lattice is non-distributive since, e.g., for x = \\tup{\\seta, \\seta, y = \\tup{\\seta, \\varnothing and z = \\tup{\\setb, \\varnothing, we have x = x \\meet (y \\join z) \\neq (x \\meet y) \\join (x \\meet z) = y. For the join-reducible \\tup{\\setab, \\varnothing, the set of the maximals of the join-irreducibles below it (i.e. \\maximalsbelow) is a redundant decomposition (as well as some of its subsets), and there are several alternative irredundant decompositions: \\end{minipage \\end{minipage \\vspace{4pt \\end{center Fortunately, the typical use of lexicographic products to design CRDTs is with a chain (total order) as the first component, to allow an actor which is ``owner'' of part of the state (the single-writer principle~) to either inflate the second component, or to change it to some arbitrary value, while increasing a ``version number'' (first component).", "Having DCC and distributivity, even if it always occurs in practice, is not enough to directly apply Proposition~, as it holds for finite lattices. However if the sublattice given by the ideal \\down x = \\{ y | y \\pleq x \\ is finite, then we can apply that proposition to this finite lattice (for which x is now the top element) to compute \\dec{x. Again, finiteness yields from all constructs, with the exception of the lexicographic product and linear sum.", "In fact, all those constructs yield lattices with bottom when starting from lattices with bottom. Moreover, all these constructs yield lattices satisfying DCC, when starting from lattices satisfying DCC (such as booleans and naturals). Also, it is easily seen that most yield distributive lattices when applied to distributive lattices, with the exception of the lexicographic product with an arbitrary first component.", "Although Proposition is stated for finite lattices, it can be applied to typical CRDTs defined over infinite lattices, as we show next. Lattice Compositions in CRDTs We now show that unique irredundant join decompositions (and therefore, optimal deltas and delta-mutators) can be obtained for almost all state-based CRDTs used in practice. Most CRDT designs define the lattice state starting from lattice chains (booleans and natural numbers), unordered sets, partial orders, and obtain more complex states by lattice composition through: cartesian product \\times, lexicographic product \\boxtimes, linear sum \\oplus, finite functions {\\hookrightarrow from a set to a lattice, powersets \\mathcal{P, and sets of maximal elements \\mathcal{M (in a partial order).", "For example, \\af{Left a \\in A \\oplus B becomes \\tup{\\af{Left, a.. Decomposing Compositions In this section we show that for each composition technique there is a corresponding decomposition rule. As the lattice join \\join of a composite CRDT is defined in terms of the lattice join of its components , decomposition rules of a composite CRDT follow the same idea and resort to the decomposition of its smaller parts. We now present such rules for all lattice compositions covered in Tables and .", "Jo\\~{ao Leit\\~{ao was partially supported by project NG-STORAGE through FCT grant PTDC/CCI-INF/32038/2017, and by NOVA LINCS through the FCT grant UID/CEC/04516/2013. \\bibliographystyle{IEEEtran \\bibliography{bib \\appendix Existence of Unique Irredundant Decompositions In this section we present sufficient conditions for the existence of unique irredundant join decompositions, and show how they can be obtained. For almost all CRDTs used in practice, the state is not merely a join-semilattice, but a distributive lattice satisfying DCC (Appendix ). Therefore, from Proposition , we have a unique irredundant join decomposition for each CRDT state.", "Given lattice \\crdt, we use \\mathcal{J(\\crdt) for the set of all join-irreducible elements of \\crdt. \\end{minipage \\end{minipage \\end{center Only P_4 and S_4 are irredundant join decompositions of p and s. P_1 and S_1 are not decompositions since their join does not result in p and s, respectively; P_2 and S_2 are decompositions but contain redundant elements, \\countb{6 and \\setb, respectively; P_3 and S_3 do not have redundancy, but contain reducible elements (S_2 fails to be an irredundant join decomposition for the same reason, since its element \\setab is also reducible). \\end{example As we show in Appendix and , these irredundant decompositions exist, are unique, and can be obtained for CRDTs used in practice."], "referred_text_chunks": ["This principle is followed by Cassandra counters . In such typical usages of the lexicographic product, with a chain as first component, the distributivity of the second component is propagated to the resulting construct. Table summarizes these remarks about how almost always these CRDT composition techniques yield lattices satisfying DCC and distributive lattices, and thus, have unique irredundant decompositions, by Proposition ."]}, "10": {"question": "How does the CPU overhead of classic delta-based compare to delta-based BP+RR as the Zipf coefficient increases?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 1, 'Rationale': \"Image 0 and Text 1 both specifically discuss CPU overhead related to classic delta-based synchronization compared to BP+RR as the Zipf coefficient increases, directly addressing the question.\", 'Answer': \"The CPU overhead of classic delta-based synchronization increases compared to delta-based BP+RR as the Zipf coefficient increases.\"}", "all_figures_names": ["1803.02750v3-Figure12-1.png", "1803.02750v3-Figure13-1.png", "1803.02750v3-Figure4-1.png", "1803.02750v3-Figure6-1.png", "1803.02750v3-Figure8-1.png", "1803.02750v3-Figure2-1.png", "1803.02750v3-Figure3-1.png", "1803.02750v3-Figure10-1.png"], "referred_figures_names": ["1803.02750v3-Figure12-1.png"], "answer": "The CPU overhead of classic delta-based is consistently higher than that of delta-based BP+RR as the Zipf coefficient increases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Since classic delta-based produces/processes larger messages than \\BP + \\RR, this results in a higher CPU cost: for the 1, 1.25 and 1.5 Zipf coefficients, classic delta-based incurs an overhead of 0.4\\af{x, 5.5\\af{x, and 7.9\\af{x respectively. Related Work In the context of remote file synchronization, rsync synchronizes two files placed on different machines, by generating file block signatures, and using these signatures to identify the missing blocks on the backup file. In this strategy, there's a trade-off between the size of the blocks to be signed, the number of signatures to be sent, and the size of the blocks to be received: bigger blocks to be signed implies fewer signatures to be sent, but the blocks received (deltas) can be bigger than necessary.", "During the experiment we also measured the CPU time spent in processing CRDT updates, both producing and processing synchronization messages. Figure reports the CPU overhead of classic delta-based, when considering \\BP + \\RR as baseline.", "\\RR is required, in order to avoid redundant state propagation. For example, with a 1.25 coefficient, bandwidth is reduced from 1.46GB/s to 0.06GB/s per node, and memory footprint per node drops from 1.58GB to 0.62GB (right side of the plots). Also, as we increase the Zipf coefficient, we note that the bandwidth consumption continues to rise, leading to an unsustainable situation in the case of classic delta-based, as it can never reduce the size of \\delta-groups being transmitted.", "Finally, delta-based \\BP + \\RR is able reduce the transmission costs by up-to 94\\%. In the extreme case of \\af{GMap\\ 100\\% (every key in the map is updated between synchronization rounds, which is a less likely workload in practical systems) and considering a partial-mesh, delta-based \\BP + \\RR provides a modest improvement in relation to state-based of about 18\\% less transmission, and its performance is below Scuttlebutt variants and operation-based synchronization.", "\\RR. State-based does not require synchronization metadata, and thus it is optimal in terms of memory usage. Classic delta-based and delta-based \\BP have an overhead of 1.1\\af{x-3.9\\af{x since the size of \\delta-groups in the \\delta-buffer is larger for these techniques. For \\af{GSet and \\af{GMap\\ 10\\%, Scuttlebutt-GC is close to \\BP +", "The topology is a partial-mesh, with 50 nodes, each with 4 neighbors, as in Figure , and updates on objects follow a Zipf distribution, with coefficients ranging from 0.5 (low contention) to 1.5 (high contention) . Figure~ shows the transmission bandwidth and memory footprint of both algorithms, for different Zipf coefficient values. We can observe that in low contention workloads, classic delta-based behaves almost optimally when compared to \\BP + \\RR.", "With a partial-mesh, \\BP has little effect, and \\RR contributes most to the overall improvement. Given that the underlying topology leads to redundant communication (desired for fault-tolerance), and classic delta-based can never extract that redundancy, its transmission bandwidth is effectively similar to that of state-based synchronization. Scuttlebutt and Scuttlebutt-GC are more efficient than classic delta-based for \\af{GSet since both can precisely identify state changes between synchronization rounds.", "\\subsubsection{Transmission bandwidth Figure shows, for \\af{GSet and \\af{GCounter, the transmission ratio (of all synchronization mechanisms previously mentioned) with respect to delta-based synchronization with \\BP and \\RR optimizations enabled. The first observation is that classic delta-based synchronization presents almost no improvement, when compared to state-based synchronization. In the tree topology, \\BP is enough to attain the best result, because the underlying topology does not have cycles, and thus, \\BP is sufficient to prevent redundant state to be propagated."], "referred_text_chunks": ["Since classic delta-based produces/processes larger messages than \\BP + \\RR, this results in a higher CPU cost: for the 1, 1.25 and 1.5 Zipf coefficients, classic delta-based incurs an overhead of 0.4\\af{x, 5.5\\af{x, and 7.9\\af{x respectively. Related Work In the context of remote file synchronization, rsync synchronizes two files placed on different machines, by generating file block signatures, and using these signatures to identify the missing blocks on the backup file. In this strategy, there's a trade-off between the size of the blocks to be signed, the number of signatures to be sent, and the size of the blocks to be received: bigger blocks to be signed implies fewer signatures to be sent, but the blocks received (deltas) can be bigger than necessary."]}}