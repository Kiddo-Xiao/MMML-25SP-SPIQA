{"0": {"question": "Which model performs the best at SNR=0dB?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 1, 'Rationale': \"Image 0 shows NMSE results at SNR=0dB, and Text 1 discusses NMSE and identifies that graph structure enhances prediction performance significantly under noisy conditions. Both highlight the best-performing model at SNR=0dB.\", 'Answer': 'GPG-K performs the best at SNR=0dB.'}", "all_figures_names": ["1803.05776v2-Figure1-1.png"], "referred_figures_names": ["1803.05776v2-Figure1-1.png"], "answer": "GPG-K", "figure_type": "plot", "content_type": "figure", "text_chunks": ["We generate noisy training data by adding zero-mean white Gaussian noise at SNR of 5 dB and 0 dB to the true temperature measurements. In Figure , we show the NMSE obtained for testing data by averaging over 100 different random partitioning of the total dataset into training and testing datasets. We observe that GPG outperforms GP for both linear and kernel regression cases.", "We compute the NMSE for the different approaches by averaging over 100 different randomly drawn training subsets of size N from the full training set of size N_{ts=30. We plot the NMSE as a function of N in Figures (b)-(c) at SNR levels of 5dB and 0dB. We observe that graph structure enhances the prediction performance signficantly under noisy and low sample size conditions.", "The average NMSE for testing datasets is shown in Figure at SNR levels of 5 dB and 0 dB. We once again observe the same trend that GPG outperforms GP for both linear and kernel cases at low sample sizes corrupted with noise. \\caption{Results for flow-cytometry data (a) Adjacency matrix, (b) NMSE for testing data as a function of training data size at SNR=5dB, and (c) at SNR=0dB. \\end{figure* {Prediction for atmospheric tracer diffusion data Our next experiment is on the atmospheric tracer diffusion measurements obtained from the European Tracer Experiment (ETEX) which tracked the concentration of perfluorocarbon tracers released into the atmosphere starting from a fixed location (Rennes, France). The observations were collected from two experiments over a span of 72 hours at 168 ground stations over Europe, giving two sets of 30 measurements, in total 60 measurements.", "\\caption{Results for ETEX data (a) Schematic showing the ground stations for the experiment, (b) NMSE for testing data as a function of training data size at SNR=5 dB, and (c) at SNR=0 dB. \\end{figure * Reproducible research In the spirit of reproducible research, all the codes relevant to the experiments in this article are made available at https://www.researchgate.net/profile/Arun\\textunderscore Venkitaraman and https://www.kth.se/ise/research/reproducibleresearch-1.433797.", "This implies that the GPG models the observed training samples better than conventional GP. Since \\mathbf{K is positive semidefinite by construction as a kernel matrix, we have that \\mathrm{tr(\\Delta\\mathbf{C_N)\\geq0 if and only if \\mathrm{tr(\\mathbf{I_M-\\mathbf{B^2)\\geq 0. Let \\{\\theta_i\\_{i=1^{N and \\{\\mathbf{u_i\\_{i=1^{N\\in\\mathbb{R^{N denote the eigenvalues and eigenvectors of \\mathbf{K, respectively.", "We have a total of 295 graph signals corresponding to different measurements from a single subject. We use a portion of the signals for training and the remaining for testing. We construct noisy training targets by adding white Gaussian noise at SNR-levels of 10 dB and 0 dB. The NMSE of the prediction mean for testing data, averaged over 100 different random choices of training and testing sets is shown in (b) and (c); this is Monte Carlo simulation to check robustness.", "We illustrate the ground station locations in a schematic in Figure (a). The output nodes which correspond to the the target are shown in red markers with corresponding edges, whereas the rest of the markers denote the input. We simulate noisy training by adding white Gaussian noise at different SNR levels to the training data.", "In the case of regression output being smooth graph signals in the sense of \\mathbf{G=\\mathbf{L, we have that J^2(i)=\\lambda_i and s=K for a graph with K-connected components or disjoint subgraphs. This is because the number of zero eigenvalues of \\mathbf{L is equal to the nunber of connected components in the graph . \\end{proof \\end{theorem"], "referred_text_chunks": ["We generate noisy training data by adding zero-mean white Gaussian noise at SNR of 5 dB and 0 dB to the true temperature measurements. In Figure , we show the NMSE obtained for testing data by averaging over 100 different random partitioning of the total dataset into training and testing datasets. We observe that GPG outperforms GP for both linear and kernel regression cases."]}}