{"0": {"question": "What is the role of the Joint Attention Module in the model?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 2, 'Rationale': \"Image 6 provides an overview of the model including the Joint Attention Module, showing its role in processing both visual and textual features. Text 2 describes the use of joint features and attention mechanism, closely related to the role of the Joint Attention Module.\", 'Answer': 'The Joint Attention Module in the model combines visual and textual features to generate spatial attention maps, facilitating the localization of concepts within the images.'}", "all_figures_names": ["1803.06506v3-Figure3-1.png", "1803.06506v3-Figure1-1.png", "1803.06506v3-Figure7-1.png", "1803.06506v3-Figure5-1.png", "1803.06506v3-Figure4-1.png", "1803.06506v3-Table1-1.png", "1803.06506v3-Figure2-1.png", "1803.06506v3-Figure6-1.png"], "referred_figures_names": ["1803.06506v3-Figure2-1.png"], "answer": "The Joint Attention Module takes the embedded image and phrase features as input and uses them to induce a parameterization for spatial attention. This spatial attention map is then used by the decoder to predict the common concept.", "figure_type": "", "content_type": "figure", "text_chunks": ["It is important to note that the attention module is shared across all \\mathbf{V^{i and \\mathbf{t^{i. Thus the encoder is common for all pairs in the concept batch. Next, we describe a decoding mechanism to predict the common and independent concept.", "Here n is the number of regions over which attention is defined and d is the dimensionality of each region with respective to \\mathbf{X. Thus, we use a 4 layered non linear perceptron to calculate attention for each of the n regions . In contrast to~, we compute attention over the spatial regions of the last feature maps from VGG16 instead of computing it over bounding boxes. The four \\Phi(\\cdot) layers gradually decrease the dimensionality of the concatenated joint features from (m+l) \\to p \\to q \\to r \\to s where s=1.", "The encoder uses an attention mechanism similar to~ using the joint features from visual and textual modalities. To maintain fair comparison with previous work, the image features are extracted from the last convolution layer of a VGG16 model~ pre-trained on ImageNet. Similarly, the phrase features are extracted from a language model trained on next word prediction on the Google 1 Billion dataset~ and the MS COCO captions dataset~. As done in~, both the model weights are frozen during training and aren't fine tuned.", "Both the visual and textual feature extractors are fixed during training. Before the attention module, both the features are normalized using a batch-normalization layer~. The concept vocabulary used for training with the softmax based loss is taken from the most frequently occurring nouns.", "The proxy task for the model is to decode the common concept present within each concept batch. We induce a parametrization which, given the input text and image, can generate an attention map to localize a region. These localized regions are then used to predict the common concept.", "In this section we show that even though the visual features are fixed during training, our model learns attention maps which are spatially distinct from the VGG16 feature maps used as input. We use the channel averaged VGG16 baseline model for visualizing the VGG16 heatmap. Figure~ shows the comparison between this and our predictions.", "As evident from the examples, our method produces attention maps which can localize regions which were weak or even non-existent in the activations of the input maps. This shows that our model doesn't simply amplify the activations present in VGG16 channels but learns a phrase dependent attention map. \\vspace{-0.5em Alignment of the selected, predicted and true concept \\vspace{-0.5em In this section, we qualitatively cover four cases for summarizing the effects of the selected concept and the predicted concept and how these two relate to what the actual entity to be localized was.", "Given the attention weights \\mathbf{f_{attn^{i \\in \\mathbb{R^{1 \\times n, the visual attention for common concept prediction (\\mathbf{f_{vac) is computed by taking the weighted sum with the original visual features. We find that aggregating the visual attention across regions, which is commonly done in the past attention literature degrades performance for our task. Therefore we retain the spatial information and only aggregate the features across the concept batch."], "referred_text_chunks": ["It is important to note that the attention module is shared across all \\mathbf{V^{i and \\mathbf{t^{i. Thus the encoder is common for all pairs in the concept batch. Next, we describe a decoding mechanism to predict the common and independent concept."]}, "1": {"question": "How does the quality of the output heatmap change when the selected concept, predicted concept, and the real entity to be grounded are all aligned?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 0, 'Rationale': 'Image 5 and Text 0 both focus on the alignment of selected, predicted, and true concepts and its effect on heatmap quality. The image shows how the alignment influences the heatmap, while the text explains that correct grounding results in good localization.', 'Answer': 'When the selected concept, predicted concept, and the real entity to be grounded are all aligned, the quality of the output heatmap improves, resulting in accurate localization of the phrase.'}", "all_figures_names": ["1803.06506v3-Figure3-1.png", "1803.06506v3-Figure7-1.png", "1803.06506v3-Figure4-1.png", "1803.06506v3-Table3-1.png", "1803.06506v3-Figure1-1.png", "1803.06506v3-Figure6-1.png", "1803.06506v3-Table1-1.png", "1803.06506v3-Figure2-1.png"], "referred_figures_names": ["1803.06506v3-Figure6-1.png"], "answer": "When the selected concept, predicted concept, and the real entity to be grounded are all aligned, the generated heatmap produces a good localization of the phrase.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["\\newline First row: Correct grounding of phrase. In cases where the selected concept and all predicted concepts coincide with the true concept to be localized, the generated heatmap produces a good localization of the phrase. This is shown in the first row of Figure~ with the concept `headlight' and `picture'.", "The third row shows some small and challenging entities to be grounded. Finally, the fourth and fifth row highlight the ability of the model to output completely different heatmaps for the same image having differing phrases. The grounded heatmap appear to identify regularities like localizing `television' towards the periphery near a wall or localizing `phone' near the hands of a person.", "Many other works like~ and~ only allow for word-level grounding, thus making them average over the heatmaps to get a phrase-level output. In contrast, our formulation does not suffer from these limitations. Our proxy task deals with the full phrase and forces the model to limit the attention to areas which can explain the concept to be grounded, thus aligning the objective better with the task of visual grounding.", "Note that our proxy loss is trained with the selected concept as the ground truth and predicts the common concept and independent concept. In Figure~, common concept, independent concept and selected concept are denoted by red, blue and gray blocks respectively. For the remainder of this section we use the term true concept to refer to the actual entity to be localized.", "Since our model generates localization in the form of a heatmap, we evaluate our model with the pointing game metric~, similar to the previous work of~. Pointing game measures how accurate the most confident region in the predicted heatmap is with respect to the ground truth bounding box. For a given input image-phrase pair, the predicted localization heatmap is considered a Hit if the pixel with the maximum value lies within the bounding box, else it's considered a Miss. Therefore it purely evaluates the spatial accuracy of the heatmap instead of measuring the extent of localization.", "As evident from the examples, our method produces attention maps which can localize regions which were weak or even non-existent in the activations of the input maps. This shows that our model doesn't simply amplify the activations present in VGG16 channels but learns a phrase dependent attention map. \\vspace{-0.5em Alignment of the selected, predicted and true concept \\vspace{-0.5em In this section, we qualitatively cover four cases for summarizing the effects of the selected concept and the predicted concept and how these two relate to what the actual entity to be localized was.", "Similarly, the visual attention for independent concept prediction, \\mathbf{f_{vai^{i is given by the element-wise product of the attention weights and visual features. Finally, both the attended features are flattened and separately connected to a fully connected layer, leading to a softmax over the concepts. In practice, we also down-sample the dimensionality of \\mathbf{f_{vai^{i using 1*1 convolutions before we aggregate and flatten the features.", "We then choose the maximum activated grid for the pointing game. Apart from these three baselines, we also compare against weakly supervised works of~ and~ who use an MIL based approach and an excitation backprop scheme respectively for localizing single-word labels. As done in~, we average the heatmaps generated for tokens present in their dictionary for obtaining the final heatmap."], "referred_text_chunks": ["\\newline First row: Correct grounding of phrase. In cases where the selected concept and all predicted concepts coincide with the true concept to be localized, the generated heatmap produces a good localization of the phrase. This is shown in the first row of Figure~ with the concept `headlight' and `picture'."]}, "2": {"question": "Why does the Semantic self-supervision model perform better on ReferIt (mask) compared to ReferIt (bbox), and how does this relate to the difference in performance between Visual Genome and Flickr30k datasets? ", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 7, 'Rationale': 'Table 2 provides specific accuracy comparisons for the ReferIt mask and bbox, showcasing performance differences. Text 7 discusses the evaluation settings and relevance of mask-based annotations for precise localization.', 'Answer': 'The Semantic self-supervision model performs better on ReferIt (mask) compared to ReferIt (bbox) because mask annotations provide more precise and accurate localization compared to bounding boxes. This precision benefits the model in tasks requiring fine-grained localization. In contrast, datasets like Flickr30k tend to have higher overall performance because they annotate multiple bounding boxes for multiple phrase instances, giving a more extensive training signal and potentially easier localization tasks.'}", "all_figures_names": ["1803.06506v3-Figure6-1.png", "1803.06506v3-Figure7-1.png", "1803.06506v3-Table2-1.png", "1803.06506v3-Figure3-1.png", "1803.06506v3-Figure2-1.png", "1803.06506v3-Figure1-1.png", "1803.06506v3-Figure5-1.png", "1803.06506v3-Table3-1.png"], "referred_figures_names": ["1803.06506v3-Table2-1.png"], "answer": "The Semantic self-supervision model performs better on ReferIt (mask) compared to ReferIt (bbox) because mask-based annotations are considered more precise and accurate for measuring localization performance than bounding boxes. Masks tightly encompass the specific region referred to by the phrase, whereas bounding boxes can include extraneous areas. \n\nThis relates to the difference in performance between Visual Genome and Flickr30k because both Visual Genome and ReferIt (mask) contain phrases that refer to very specific regions or non-salient objects. Precise localization is crucial for achieving high accuracy on these datasets. Flickr30k, on the other hand, annotates all bounding boxes referring to a phrase, leading to potentially less precise localization requirements and generally higher performance across methods. ", "figure_type": "N/A", "content_type": "table", "text_chunks": ["We test our method on the Visual Genome~, the ReferItGame~ and the Flickr30k Entities~ datasets and there exist few important qualitative and quantitative differences between them. Visual Genome has a longer average description length than ReferIt which is in turn higher than Flickr30k. As mentioned in~, Visual Genome and ReferIt include description for regions which are less salient and hence harder to localize unlike Flickr30k.", "Since both Visual Genome and ReferIt contain phrases which: a) refer to very specific regions like `red car on corner' and b) refer to non-salient objects like `white crack in the sidewalk', both datasets have low performance with baselines like center and VGG. Our model outperforms all baselines on ReferIt too, improving upon the MIL based approach by 5.79\\%. \\newline Flickr30k Entities.", "On the other hand, Flickr30k provides multiple bounding box annotations for different description instances within an image. So for a phrase, `trees behind the lake', all instances of `trees' are annotated separately in Flickr30k whereas Visual Genome only annotates a single instance of the description. Unlike the others, ReferIt usually only refers to a specific object instance.", "Flickr30k dataset has higher performance across methods as compared to the other two datasets due to the two points mentioned in the previous subsection along with the fact that Flickr30k annotates all bboxes referring to a phrase as opposed to the other datasets which only have a one-to-one phrase-bbox mapping for an image. Our model outperforms most baselines and is just 1\\% less than the state-of-art work of~. Analysis of the approach In this section, we examine the effects of changing the hyperparameter k (concept batch size), the significance of the two surrogate losses and the effect of the concepts with which our model is trained, followed by some qualitative outputs of our model. All the analysis in the following sections is done on the Visual Genome dataset.", "We use remaining images of the Visual Genome dataset for training. For ReferIt and Flickr30k, we use the test sets for evaluation. \\newline Evaluation metric.", "Table~ shows some important dataset statistics which hint towards the complexity of the datasets. For example, notice that in Flickr30k, the average phrase length is just 2.3 words and average noun count is 1.2 which would mean that the region to be localized in most cases is directly present as a single word, thus changing the problem to an almost weakly supervised setting. To ensure fair comparison with the previous work of~, we use the images from the validation set of MS-COCO which have region annotations in the Visual Genome dataset as our test set.", "~ learn a joint embedding space for a complete sentence and an image using a CNN-LSTM based encoder and a neural language model based decoder. Since the release of the Flickr30k Entities dataset~ and subsequently the Visual Genome dataset~, availability of bounding box annotations of phrases has allowed many new attempts at the problem of visual grounding of phrases.~ provide a baseline for Flickr30k Entities dataset using Canonical Correlation Analysis (CCA) to compute the region-phrase similarity.~ construct a two-branch architecture that enforces a structure and bi-directional ranking constraint to improve upon the CCA baseline. Another recent work from~ departs from the standard usage of bounding box proposals and uses the primary entity of the phrase along with its context to regress localization coordinates.", "~ provide segmentation mask for each phrase-region pair and use them to obtain a bounding box (bbox) which envelopes the mask completely. They then use this for their evaluation on ReferIt. Though we provide evaluation for both bbox and mask settings, we believe that the mask based annotations are more precise and accurate for measuring localization performance."], "referred_text_chunks": ["We test our method on the Visual Genome~, the ReferItGame~ and the Flickr30k Entities~ datasets and there exist few important qualitative and quantitative differences between them. Visual Genome has a longer average description length than ReferIt which is in turn higher than Flickr30k. As mentioned in~, Visual Genome and ReferIt include description for regions which are less salient and hence harder to localize unlike Flickr30k."]}, "3": {"question": "How does the proposed method's attention map differ from the VGG16 feature map?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 2, 'Rationale': 'Image 2 directly compares VGG16 feature maps with attention maps, illustrating differences visually. Text 2 describes how the method produces attention maps that localize regions not present in VGG16, highlighting the functional differences.', 'Answer': \"The proposed method's attention map differs from the VGG16 feature map by localizing regions that are weak or non-existent in the VGG16 activations, showing that it learns phrase-dependent attention maps rather than simply amplifying existing features.\"}", "all_figures_names": ["1803.06506v3-Figure6-1.png", "1803.06506v3-Figure3-1.png", "1803.06506v3-Figure5-1.png", "1803.06506v3-Figure7-1.png", "1803.06506v3-Figure2-1.png", "1803.06506v3-Table2-1.png", "1803.06506v3-Table3-1.png", "1803.06506v3-Table1-1.png"], "referred_figures_names": ["1803.06506v3-Figure5-1.png"], "answer": "The attention map generated by proposed method is able to localize regions that were weak or non-existent in the activations of the input maps, while the VGG16 feature map simply amplifies the activations present in VGG16 channels.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["In this section we show that even though the visual features are fixed during training, our model learns attention maps which are spatially distinct from the VGG16 feature maps used as input. We use the channel averaged VGG16 baseline model for visualizing the VGG16 heatmap. Figure~ shows the comparison between this and our predictions.", "Here n is the number of regions over which attention is defined and d is the dimensionality of each region with respective to \\mathbf{X. Thus, we use a 4 layered non linear perceptron to calculate attention for each of the n regions . In contrast to~, we compute attention over the spatial regions of the last feature maps from VGG16 instead of computing it over bounding boxes. The four \\Phi(\\cdot) layers gradually decrease the dimensionality of the concatenated joint features from (m+l) \\to p \\to q \\to r \\to s where s=1.", "As evident from the examples, our method produces attention maps which can localize regions which were weak or even non-existent in the activations of the input maps. This shows that our model doesn't simply amplify the activations present in VGG16 channels but learns a phrase dependent attention map. \\vspace{-0.5em Alignment of the selected, predicted and true concept \\vspace{-0.5em In this section, we qualitatively cover four cases for summarizing the effects of the selected concept and the predicted concept and how these two relate to what the actual entity to be localized was.", "The encoder uses an attention mechanism similar to~ using the joint features from visual and textual modalities. To maintain fair comparison with previous work, the image features are extracted from the last convolution layer of a VGG16 model~ pre-trained on ImageNet. Similarly, the phrase features are extracted from a language model trained on next word prediction on the Google 1 Billion dataset~ and the MS COCO captions dataset~. As done in~, both the model weights are frozen during training and aren't fine tuned.", "Qualitative analysis In this section we show some of our qualitative results on the Visual Genome dataset. Figure shows the localization heatmap we obtain from our attention weights. It also shows the VGG16 activation heatmaps obtained by averaging over the channels.", "We believe this is due to the phrases often referring to some object in the image which the VGG16 features are already trained for recognizing. Our model outperforms all the baselines and improves upon the previous state-of-art work by~ by 5.63\\%. \\newline ReferItGame.", "Using our approach, we achieve state-of-art performance on multiple datasets. Finally, we note that as the complexity of these visual tasks increase, the role of language can become pivotal in learning richer representations, the task of unsupervised grounding being one of the case in point. {\\small \\bibliographystyle{ieee \\bibliography{egbib \\section*{\\centering Supplementary Material Introduction \\vspace{-0.5em In the supplementary material, we qualitatively highlight the following: \\vspace{-0.5em \\vspace{-1.5em Comparison with VGG16 feature maps \\vspace{-0.5em Our model uses a pre-trained VGG16 model to extract feature maps for our visual encoder.", "For the i^{th index in the concept batch, given visual features from VGG16, \\mathbf{V^{i = f_{VGG(I^{i) and textual features from the language model \\mathbf{t^{i = f_{LM(P^{i), the attention over visual regions is given by: where \\mathbf{V^{i \\in \\mathbb{R^{m \\times n, \\mathbf{t^{i \\in \\mathbb{R^{l \\times 1, \\mathbf{f_{joint(\\mathbf{V^{i,\\mathbf{t^{i) \\in \\mathbb{R^{1 \\times n, [\\mathbf{V^{i,\\mathbf{t^{i] is an index-wise concatenation operator (over the first dimension) between a matrix \\mathbf{V^{i and a vector \\mathbf{t^{i resulting in a matrix of size ((m+l) \\times n ). \\Phi(\\cdot) corresponds to a hidden layer of a neural network and is defined as: where ReLU(x)=max(x,0), \\mathbf{W_p \\in \\mathbb{R^{p \\times d, \\mathbf{b_p \\in \\mathbb{R^{p \\times 1 and \\mathbf{X \\in \\mathbb{R^{d \\times n."], "referred_text_chunks": ["In this section we show that even though the visual features are fixed during training, our model learns attention maps which are spatially distinct from the VGG16 feature maps used as input. We use the channel averaged VGG16 baseline model for visualizing the VGG16 heatmap. Figure~ shows the comparison between this and our predictions."]}, "4": {"question": "Which loss type performs best when the concept batch size is 5k?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 3, 'Rationale': 'Image 1 directly provides performance data for different loss types across concept batch sizes, including k=5. Text 3 explains performance patterns related to different loss combinations.', 'Answer': 'The \"Independent and common concept\" loss type performs best at k=5 with a performance of 28.91.'}", "all_figures_names": ["1803.06506v3-Figure6-1.png", "1803.06506v3-Table3-1.png", "1803.06506v3-Table1-1.png", "1803.06506v3-Figure1-1.png", "1803.06506v3-Figure4-1.png", "1803.06506v3-Figure3-1.png", "1803.06506v3-Table2-1.png", "1803.06506v3-Figure7-1.png"], "referred_figures_names": ["1803.06506v3-Table3-1.png"], "answer": "Independent and common concept", "figure_type": "table", "content_type": "table", "text_chunks": ["For all three settings, we vary the concept batch size k and observe some interesting trends. As shown in Table~, for a fixed loss type, the performance increases as we increase k, the CC loss being the exception to this trend. The performance for CC loss increases up to k=7, but goes down with k=9.", "This is corroborated with the fact that during training, we also observe a faster convergence of CC loss for k=9 than the lower concept batch sizes. This empirically highlights the importance of the IC loss term. It also highlights the usefulness of the concept batch formulation since it improves performance in general.", "Concept batch size and surrogate loss We perform ablative studies on the two loss terms and the concept batch size k and observe certain patterns. For the discussion in this section, we use the shorthand IC (independent concept only), CC (common concept only) and ICC (independent and common concept) for the three loss types from Table~. We train our model with the IC and CC loss separately, keeping everything else in the pipeline fixed. This means that the sampling procedure for a concept batch remains intact for all loss types, even when there is no common concept being decoded.", "For a fixed k, we also observe an expected pattern. IC loss usually achieves the least performance out of the three, with CC loss coming in next. The best performance is obtained with both the losses together.", "Lastly, we point out the slight increase in performance for IC loss as we increase k. We believe that even though this loss type does not jointly decode the common concept explicitly, within a training mini-batch, the gradients which are backpropagated are from the average loss of a concept batch having a common concept (see second term of Equation ~). Thus training with IC loss is not exactly the same as training with a batch of independent image-phrase pairs and simply decoding a concept from the phrase.", "A good proxy task improves performance on the final task when the surrogate loss is minimized. In this work we propose concept-learning as a substitute task for visual grounding. During training, we create concept batches of size k, consisting of k different phrase-image pairs, all containing a common concept (as illustrated in Figure~).", "We note a 4.7\\% drop in performance on Visual Genome for k=5. Since training of the original model enforces only concept-level discrimination, it's interesting to see that presence of complete phrases is useful for performance. This shows that our model is much more than a word-level concept-detector and utilizes the full phrase for grounding.", "This points to a common problem with self-supervised techniques where the model finds a shortcut to reduce the loss value without improving on the performance. With only the common concept loss, the network can learn a majority voting mechanism such that not all k concept representations need to be consistent with the common concept. Thus, the network can easily optimize the proxy objective, but is not forced to learn a robust grounding for all instances in the concept batch."], "referred_text_chunks": ["For all three settings, we vary the concept batch size k and observe some interesting trends. As shown in Table~, for a fixed loss type, the performance increases as we increase k, the CC loss being the exception to this trend. The performance for CC loss increases up to k=7, but goes down with k=9."]}, "5": {"question": "How does the performance of the model vary with respect to the bounding box area and the similarity of the concept with ImageNet classes?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 1, 'Rationale': 'Image 4 visually demonstrates the correlation between performance and bounding box area/similarity with ImageNet, directly addressing the question. Text 1 describes the correlation analysis of performance with bounding box size and similarity to ImageNet classes, which aligns directly with the question.', 'Answer': 'The model performance positively correlates with bounding box area (Pearson = 0.85) but shows no noticeable correlation with similarity to ImageNet classes (Pearson = -0.02).'}", "all_figures_names": ["1803.06506v3-Figure5-1.png", "1803.06506v3-Table1-1.png", "1803.06506v3-Figure6-1.png", "1803.06506v3-Figure1-1.png", "1803.06506v3-Figure3-1.png", "1803.06506v3-Table2-1.png", "1803.06506v3-Figure7-1.png", "1803.06506v3-Figure2-1.png"], "referred_figures_names": ["1803.06506v3-Figure3-1.png"], "answer": "The performance of the model increases with increasing bounding box area and decreases with increasing similarity of the concept with ImageNet classes.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["The first is the average bounding box size associated with each of these concepts. The second is the existing knowledge of concept labels present in the ImageNet classes which our model obtains through the VGG16 based visual encoder. Figure (left) shows the variation of performance with respect to the average bounding box area for each concept.", "We observe a strong positive correlation between the two variables, explaining the lower performance for concepts with small sizes. For computing the correlation of concept performance with the knowledge from ImageNet classes, we use a trained word2vec model~ and compute the maximum similarity of a particular concept across all the ImageNet classes. We plot this in Figure (right) which illustrates no noticeable correlation between the two variables.", "~ learn a joint embedding space for a complete sentence and an image using a CNN-LSTM based encoder and a neural language model based decoder. Since the release of the Flickr30k Entities dataset~ and subsequently the Visual Genome dataset~, availability of bounding box annotations of phrases has allowed many new attempts at the problem of visual grounding of phrases.~ provide a baseline for Flickr30k Entities dataset using Canonical Correlation Analysis (CCA) to compute the region-phrase similarity.~ construct a two-branch architecture that enforces a structure and bi-directional ranking constraint to improve upon the CCA baseline. Another recent work from~ departs from the standard usage of bounding box proposals and uses the primary entity of the phrase along with its context to regress localization coordinates.", "Since our model generates localization in the form of a heatmap, we evaluate our model with the pointing game metric~, similar to the previous work of~. Pointing game measures how accurate the most confident region in the predicted heatmap is with respect to the ground truth bounding box. For a given input image-phrase pair, the predicted localization heatmap is considered a Hit if the pixel with the maximum value lies within the bounding box, else it's considered a Miss. Therefore it purely evaluates the spatial accuracy of the heatmap instead of measuring the extent of localization.", "Flickr30k dataset has higher performance across methods as compared to the other two datasets due to the two points mentioned in the previous subsection along with the fact that Flickr30k annotates all bboxes referring to a phrase as opposed to the other datasets which only have a one-to-one phrase-bbox mapping for an image. Our model outperforms most baselines and is just 1\\% less than the state-of-art work of~. Analysis of the approach In this section, we examine the effects of changing the hyperparameter k (concept batch size), the significance of the two surrogate losses and the effect of the concepts with which our model is trained, followed by some qualitative outputs of our model. All the analysis in the following sections is done on the Visual Genome dataset.", "However, these approaches require expensive bounding box annotations for the phrase, which are difficult to scale since they are a function of scene context and grow exponentially with the number of entities present in the scene. Furthermore, bounding box annotations for phrases are subjective in nature and might contain non-relevant regions with respect to the phrase. This brings us to our main motivation, which is to explore new ways in which models can directly harness unlabelled data and its regularities to learn visual grounding of phrases.", "The problem of image-text alignment has received much attention in the vision community in the recent years. Early work like DeViSE~ focus on learning semantic visual embeddings which have a high similarity score with single-word labels. Similar to DeViSE,~ learn a multi-modal alignment by constructing a semantic embedding space, but instead of image-label correspondences, they learn region-label correspondences through a multiple-instance learning approach.", "They use a combination of a regression, a classification and a reinforcement learning based loss to train multiple networks in their framework. Prior to our work, there are two papers which take up the problem of unsupervised visual grounding of phrases.~ use reconstruction of the original phrase as a substitute objective function to improve visual attention. But the output predictions in their work is in the form of bounding boxes which, as noted by~, puts an upper bound on the performance."], "referred_text_chunks": ["The first is the average bounding box size associated with each of these concepts. The second is the existing knowledge of concept labels present in the ImageNet classes which our model obtains through the VGG16 based visual encoder. Figure (left) shows the variation of performance with respect to the average bounding box area for each concept."]}, "6": {"question": "Which dataset would you expect to be the easiest for a model to localize phrases in, and why?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 provides statistics that directly relate to the complexity of the datasets, showing word and noun counts per phrase, which influences ease of localization. Text 0 discusses these statistics and their implications for the task. Both highlight that shorter phrases in Flickr30k make the localization task simpler.', 'Answer': 'Flickr30k would be the easiest dataset for a model to localize phrases in because the average phrase length is shorter, often consisting of a single word, simplifying the localization task to a weakly supervised problem.'}", "all_figures_names": ["1803.06506v3-Figure6-1.png", "1803.06506v3-Table3-1.png", "1803.06506v3-Figure7-1.png", "1803.06506v3-Table1-1.png", "1803.06506v3-Figure5-1.png", "1803.06506v3-Figure2-1.png", "1803.06506v3-Figure1-1.png", "1803.06506v3-Table2-1.png"], "referred_figures_names": ["1803.06506v3-Table1-1.png"], "answer": "Flickr30k is likely the easiest dataset for a model to localize phrases in. \nFlickr30k has the shortest average phrase length (2.3 words) and the lowest average noun count (1.2) per phrase. This suggests that the phrases in this dataset are simpler and often directly refer to single objects present in the image. This makes the localization task easier, almost approaching a weakly supervised setting where the object to be localized is explicitly named in the phrase.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Table~ shows some important dataset statistics which hint towards the complexity of the datasets. For example, notice that in Flickr30k, the average phrase length is just 2.3 words and average noun count is 1.2 which would mean that the region to be localized in most cases is directly present as a single word, thus changing the problem to an almost weakly supervised setting. To ensure fair comparison with the previous work of~, we use the images from the validation set of MS-COCO which have region annotations in the Visual Genome dataset as our test set.", "Since both Visual Genome and ReferIt contain phrases which: a) refer to very specific regions like `red car on corner' and b) refer to non-salient objects like `white crack in the sidewalk', both datasets have low performance with baselines like center and VGG. Our model outperforms all baselines on ReferIt too, improving upon the MIL based approach by 5.79\\%. \\newline Flickr30k Entities.", "~ learn a joint embedding space for a complete sentence and an image using a CNN-LSTM based encoder and a neural language model based decoder. Since the release of the Flickr30k Entities dataset~ and subsequently the Visual Genome dataset~, availability of bounding box annotations of phrases has allowed many new attempts at the problem of visual grounding of phrases.~ provide a baseline for Flickr30k Entities dataset using Canonical Correlation Analysis (CCA) to compute the region-phrase similarity.~ construct a two-branch architecture that enforces a structure and bi-directional ranking constraint to improve upon the CCA baseline. Another recent work from~ departs from the standard usage of bounding box proposals and uses the primary entity of the phrase along with its context to regress localization coordinates.", "As evident from the examples, our method produces attention maps which can localize regions which were weak or even non-existent in the activations of the input maps. This shows that our model doesn't simply amplify the activations present in VGG16 channels but learns a phrase dependent attention map. \\vspace{-0.5em Alignment of the selected, predicted and true concept \\vspace{-0.5em In this section, we qualitatively cover four cases for summarizing the effects of the selected concept and the predicted concept and how these two relate to what the actual entity to be localized was.", "For example in the third row of Figure~, even though `window' and `tire' are correct selected concepts, the concept-learning inaccurately predicts `glass' and `car/vehicle' which in turn generates a localization corresponding to the predicted concept. \\newline Fourth row: Incorrect grounding due to challenging phrase-image pairs. Lastly, there are some cases where the entity to be localized is either ambiguous or simply too hard (due to a small size in the image or due to a complicated phrase structure).", "We hypothesize that if we induce a parametrization for localization of the phrase and use the localized regions to predict the concept present in an image, the parametrization will converge to the ground truth localization of the phrase. Given this proxy task, we're faced with two main challenges: 1) How do we identify the concept in a phrase? and 2) How do we learn concept representations in an unsupervised setting?", "A more sophisticated sampler can annotate and use the POS tags of the phrase to return the concept. Since most concepts to be localized are nouns, a POS tagger should perform better than random sampling of words. Though better techniques can be employed for concept identification, for the sake of simplicity, we choose the POS tagger to find all nouns in a phrase and randomly select one of them as the concept.", "Flickr30k dataset has higher performance across methods as compared to the other two datasets due to the two points mentioned in the previous subsection along with the fact that Flickr30k annotates all bboxes referring to a phrase as opposed to the other datasets which only have a one-to-one phrase-bbox mapping for an image. Our model outperforms most baselines and is just 1\\% less than the state-of-art work of~. Analysis of the approach In this section, we examine the effects of changing the hyperparameter k (concept batch size), the significance of the two surrogate losses and the effect of the concepts with which our model is trained, followed by some qualitative outputs of our model. All the analysis in the following sections is done on the Visual Genome dataset."], "referred_text_chunks": ["Table~ shows some important dataset statistics which hint towards the complexity of the datasets. For example, notice that in Flickr30k, the average phrase length is just 2.3 words and average noun count is 1.2 which would mean that the region to be localized in most cases is directly present as a single word, thus changing the problem to an almost weakly supervised setting. To ensure fair comparison with the previous work of~, we use the images from the validation set of MS-COCO which have region annotations in the Visual Genome dataset as our test set."]}}