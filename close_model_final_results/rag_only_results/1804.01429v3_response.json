{"0": {"question": "What is the difference between an \"agent-in-place\" action and a generic action category?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 1, 'Rationale': 'Image 3 shows clear examples of agent-in-place actions, illustrating agents interacting with places and actions as tuples. Text chunk 1 explains the concept of agent-in-place actions as actions defined by scene layouts, emphasizing interaction with places and moving directions.', 'Answer': 'Agent-in-place actions are defined with respect to scene layouts, focusing on interaction with places and moving directions. Generic action categories usually do not consider the specific layout of the scene but focus on the action and the agent performing it.'}", "all_figures_names": ["1804.01429v3-Figure2-1.png", "1804.01429v3-Figure9-1.png", "1804.01429v3-Figure10-1.png", "1804.01429v3-Figure1-1.png", "1804.01429v3-Figure4-1.png", "1804.01429v3-Table1-1.png", "1804.01429v3-Figure5-1.png", "1804.01429v3-Figure7-1.png"], "referred_figures_names": ["1804.01429v3-Figure1-1.png"], "answer": "An agent-in-place action is an action that is performed by an agent in a specific place, while a generic action category is a more general category of action that does not specify the place where the action is performed.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["So, what makes an action \"interesting\" is how the it interacts with the geometry and topology of the scene. Examples of these actions in outdoor home surveillance scenarios and the semantic segmentation maps of scenes are shown in Fig.. We refer to these actions as \"agent-in-place\" actions to distinguish them from the widely studied generic action categories.", "In contrast to these datasets that were designed to evaluate motion and appearance modeling, or human-object interactions, our Agent-in-Place Action (APA) dataset is the first one that focuses on actions that are defined with respect to scene layouts, including interaction with places and moving directions. Recognizing these actions requires the network to not only detect and recognize agent categories and motion patterns, but also how they interact with the layout of the semantic classes in the scene. With the large variations of scene layouts, it is critical to explicitly model scene layout in the network to improve generalization on unseen scenes.", "These videos are captured from 26 different outdoor cameras which cover various layouts of typical American front and back yards. We select 15 common agent-in-place actions to label and each is represented as a tuple containing an action, the agent performing it, and the place where it occurs. The agents, actions, and places involved in our dataset are: agent=\\{person, vehicle, pet\\; action=\\{move along, stay, move away (home), move toward (home), interact with vehicle, move across\\; place=\\{street, sidewalk, lawn, porch, walkway, driveway\\.", "We propose topological feature aggregation to dynamically aggregate the decomposed features within the place associated with that action and adjacent places. The aggregation controls the \"on/off\" state of neuron connections from generic place-based feature descriptions to action nodes to model scene layout based on topological connectivity among places. We created the Agent-in-Place Action (APA) dataset, which to the best of our knowledge, is the first dataset that addresses recognizing actions associated with scene layouts.", "The biggest challenge of agent-in-place action recognition is to generalize a model trained with limited scenes to unseen scenes. Previous work on knowledge transfer for both images and videos has been based on visual similarity, which requires a large amount of training data . For trajectory prediction, Ballan \\etal transferred the priors of statistics from training scenes to new scenes based on scene similarity.", "For some actions, especially the ones occurring on street and sidewalk, since they are relatively easy to recognize, adding DD or Topo-Agg upon the place-based feature descriptions (PD) does not help much. Overall, LIVR improves the generalization capability of the network, especially for actions that are more challenging, and are associated with moving directions. \\noindentQualitative Results.", "Dataset We introduce a video dataset for recognizing agent-in-place actions. We collected outdoor home surveillance videos from internal donors and webcams to obtain over 7,100 actions from around 5,000 15-second video clips with 1280\\times 720 resolution.", "LIVR outperforms the baseline methods by a large margin on almost all action categories. When comparing the orange and green bars in Fig., we observe that the proposed topological feature aggregation (Topo-Agg) leads to consistently better generalization for almost all actions. The blue dashed box highlights the actions that include moving directions, and consistent improvements are brought by distance-based place discretization (DD)."], "referred_text_chunks": ["So, what makes an action \"interesting\" is how the it interacts with the geometry and topology of the scene. Examples of these actions in outdoor home surveillance scenarios and the semantic segmentation maps of scenes are shown in Fig.. We refer to these actions as \"agent-in-place\" actions to distinguish them from the widely studied generic action categories."]}, "1": {"question": "What are the different types of features used by the Layout-induced Video Representation Network?", "referred_figures_indices": [6], "response": "{'Image': 1, 'Text': 4, 'Rationale': 'Image 1 and Text 4 both describe the framework of LIVR, highlighting its decomposition-aggregation strategy, semantic feature decomposition, distance-based discretization, and topological aggregation. These align with the question about the types of features used.', 'Answer': 'The Layout-induced Video Representation Network uses semantic feature decomposition, distance-based place discretization, and topological feature aggregation.'}", "all_figures_names": ["1804.01429v3-Figure5-1.png", "1804.01429v3-Figure2-1.png", "1804.01429v3-Figure1-1.png", "1804.01429v3-Figure8-1.png", "1804.01429v3-Table1-1.png", "1804.01429v3-Figure7-1.png", "1804.01429v3-Figure4-1.png", "1804.01429v3-Figure9-1.png"], "referred_figures_names": ["1804.01429v3-Figure4-1.png"], "answer": "The Layout-induced Video Representation Network uses three types of features: place-based features, distance-based features, and topological features.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Layout-Induced Video Representation Framework Overview The network architecture of layout-induced video representation is shown in Fig.. For each video, we stack sampled frames of a video clip into a 4-D tensor.", "From those examples we observe that although the types of place are limited (\\eg street, walkway, lawn), the layout (\\ie structure of places, including reference to location, size, appearance of places and their adjacent places) vary significantly from scene to scene. Without large-scale training data (which is hard to collect considering privacy issues), a naive method that directly learns from raw pixels in training videos without layout modeling can easily overfit to scene-specific textures and absolute pixel coordinates, and exhibit poor generalization on layouts of new scenes. To address the generalization problem, we propose the Layout-Induced Video Representation (LIVR), which explicitly models scene layout for action recognition by encoding the layout in the network architecture given semantic segmentation maps.", "We evaluate LIVR using the imperfect maps and observe some performance degradations (around 10\\%), but LIVR still outperforms the baselines by a large margin (around 20\\%), which demonstrate the effectiveness of our method even if the segmentation maps are imperfect. Details of how we generate the segmentation maps can be found in the supplementary materials. Conclusion To improve the generalization of a deep network that learns from limited training scenes, we explicitly model scene layout in a network by using layout-induced video representations, which abstracts away low-level appearance variance but encodes the semantics, geometry and topology of scene layouts.", "The baseline models share the same 3D ConvNets architecture with our proposed model, except that the last layer is fully connected instead of gated through topological feature aggregation. The difference between baselines is their input: B/L1 takes the raw frames as input; B/L2 incorporates the scene layout information by directly concatenating the 6 segmentation maps to the RGB channels in each frame (we call this method ConcateMap), resulting in an input of 9 channels per frame in total. We train the baseline models using the same setting as in the proposed model, and the performance of the baselines are shown in column 2-5 in Table .", "The representation has three components: 1) A semantic component represented by a set of bitmaps used for decomposing features in different \"places\" (\\eg walkway, street, \\etc), and force the network to learn place-based features that are independent of scene layout; 2) A geometric component represented by a set of coarsely quantized distance transforms of each semantic place incorporated into the network to model moving directions; 3) A topological component represented through the connection structure in a dynamically gated fully connected layer of the network--essentially aggregating representations from adjacent (more generally h-connected for h hops in the adjacency graph of the semantic map) places. By encoding layout information (class membership of places, layout geometry and topology) into the network architecture using this decompotision-aggregation framework, we encourage our model to abstract away low-level appearance variations and focus on modeling high-level scene layouts, and eliminate the need to collect massive amounts of training data. The first two components require semantic feature decomposition (Fig.).", "We show the path from the baselines to our method in Table . In column 6-9, we report the mAP of our models on observed scene validation set and unseen scene testing set. We observe three significant performance gaps, especially on unseen scenes: 1) from B/L2 to LIVR-V1, we obtain around 20\\% mAP improvement by applying the proposed semantic feature decomposition to extract place feature descriptions; 2) from LIVR-V1 to LIVR-V3, our model is further improved by explicitly modeling moving directions by place discretization; 3) when compared to using a fully connected layer for feature aggregation (V1 and V3), our topological method (V2 and V4) leads to another significant improvement, which shows the efficacy of feature aggregation based on scene layout connectivity.", "Place descriptions can be extracted from different levels of feature maps. L = 0 means the input level; L > 0 means after the L^{th 3D-conv blocks. A higher L generally allows the 3D ConvNet to observe more context and to abstract features.", "Kitani \\etal extracted static scene features to learn scene-specific motion dynamics for predicting human activities. Instead of utilizing low-level visual similarity for knowledge transfer, our video representation abstracts away appearance and location variance and models geometrical and topological relationships in a scene. which are more abstract and easier to generalize from limited training scenes."], "referred_text_chunks": ["Layout-Induced Video Representation Framework Overview The network architecture of layout-induced video representation is shown in Fig.. For each video, we stack sampled frames of a video clip into a 4-D tensor."]}, "2": {"question": "What is the role of the 3D ConvNet in the distance-based place discretization process?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 1, 'Rationale': \"Image 5 directly shows the use of 3D ConvNet in distance-based place discretization. Text 1 explains the role of 3D ConvNet in abstracting temporal information, which is essential for place discretization. Both image and text directly relate to the 3D ConvNet's function in the process.\", 'Answer': 'The 3D ConvNet abstracts temporal information to create place-based feature descriptions in the distance-based place discretization process.'}", "all_figures_names": ["1804.01429v3-Figure3-1.png", "1804.01429v3-Figure6-1.png", "1804.01429v3-Figure9-1.png", "1804.01429v3-Figure1-1.png", "1804.01429v3-Table1-1.png", "1804.01429v3-Figure5-1.png", "1804.01429v3-Figure7-1.png", "1804.01429v3-Figure8-1.png"], "referred_figures_names": ["1804.01429v3-Figure5-1.png"], "answer": "The 3D ConvNet is used to extract features from the input images. These features are then used to generate place-based feature descriptions.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Unlike traditional 3D ConvNets which conduct spatial-temporal max-pooling simultaneously, we found that decoupling the pooling into spatial-only and temporal-only leads to better performance (experimental results and discussions can be found in the supplementary materials). So, for each place-specific network that extracts place-based feature descriptions, we utilize nine blocks of 3D ConvNets with the first five blocks using spatial-only max pooling and the last four blocks using temporal-only max pooling. The first two blocks have one 3D-conv layer each, and there are two convolutional (conv) layers with ReLU in between for the remaining blocks.", "Then, we apply 3D-conv blocks with temporal-only max pooling to abstract temporal information. Finally, we obtain a 1-D place-based feature description after applying GMP (see Fig.). The final description obtained after distance-based place discretization has the same dimensionality as non-discretized place descriptions.", "We study different strategies for determining PL_{DT and the number of parts to discretize (k) per place. From our observations, including the anchor place--porch, the six places in our dataset can be clustered into three categories with regard to the distance to camera: C1 includes only porch, which is usually the closest place to camera; C2 includes lawn, walkway, driveway, and actions occurring in those places usually require modeling the moving direction directly; C3 includes sidewalk and street, which are usually far away from the house, and actions on them are not sensitive to directions (\\eg \"move along\"). We evaluate our method with two strategies to apply DD on: 1) all places belong to C2 and C3; 2) only places in C2.", "The most related work to ours is ReMotENet , which skips expensive object detection and utilizes 3D ConvNets to detect motion of an object-of-interest in outdoor home surveillance videos. We employ a similar 3D ConvNet model as proposed in as a backbone architecture for extracting place-based feature descriptions for our model. \\vskip 0.5em\\noindentKnowledge Transfer.", "For each place-specific network, we use 64 3\\times3\\times3 conv filters per 3D-conv layer. After conducting SGMP on features extracted by each place-specific network, the final concatenated 1-D feature dimension is 6\\times64 since there are 6 places in total. The inference is conducted with a gated fully connected layer, whose connections (\"on/off\" status) are determined by action labels and scene topology.", "However, not all places need to be segmented since some places (such as sidewalk, street) are not associated with any direction-related action (\\eg moving toward or away from the house). For these places, we still extract the whole-place feature descriptors f_{L,p. We discuss the effect of different choices of place discretization and the number of parts k, and show the robustness of our framework to these parameters in Sec.~. To preserve temporal ordering, we apply 3D-conv blocks with spatial-only max pooling to extract features from each discretized place, and concatenate them channel-wise.", "and show that the generalization capability of our model is improved when we allow the network to observe the entire video at input level, and decompose semantics at feature level (after the 2nd conv blocks). Generally, the improvements of PD are robust across different feature levels. \\noindentDistance-based Place Discretization.", "\\noindent Anchor Place. For our dataset, the directions mentioned are all relative to the house location, and porch is a strong indicator of the house location. So we only conduct distance transform to porch in a scene, we let the user to draw a line (click to generate two endpoints) to indicate its location.\\nopagebreak, but the distance-based place discretization method can be easily generalized to represent moving direction w.r.t any arbitrary anchor place."], "referred_text_chunks": ["Unlike traditional 3D ConvNets which conduct spatial-temporal max-pooling simultaneously, we found that decoupling the pooling into spatial-only and temporal-only leads to better performance (experimental results and discussions can be found in the supplementary materials). So, for each place-specific network that extracts place-based feature descriptions, we utilize nine blocks of 3D ConvNets with the first five blocks using spatial-only max pooling and the last four blocks using temporal-only max pooling. The first two blocks have one 3D-conv layer each, and there are two convolutional (conv) layers with ReLU in between for the remaining blocks."]}, "3": {"question": "How does the LIVR framework decompose semantic features into different places?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 3, 'Rationale': 'Image 1 and Text 3 both focus on the semantic feature decomposition aspect of the LIVR framework. Image 1 shows how semantic features are divided into places and then aggregated, and Text 3 explains how video representations are decomposed based on place semantics.', 'Answer': 'The LIVR framework decomposes semantic features into different places using bitmaps encoded with semantic labels. These decompositions help the network learn features of motion patterns independent of scene layouts, facilitating place-based feature descriptions.'}", "all_figures_names": ["1804.01429v3-Figure6-1.png", "1804.01429v3-Figure2-1.png", "1804.01429v3-Figure8-1.png", "1804.01429v3-Figure10-1.png", "1804.01429v3-Figure3-1.png", "1804.01429v3-Figure7-1.png", "1804.01429v3-Figure9-1.png", "1804.01429v3-Figure5-1.png"], "referred_figures_names": ["1804.01429v3-Figure2-1.png"], "answer": "The LIVR framework decomposes semantic features into different places by utilizing bitmaps encoded with the semantic labels of places. This decomposition encourages the network to learn features of generic place-based motion patterns that are independent of scene layouts.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["We show the path from the baselines to our method in Table . In column 6-9, we report the mAP of our models on observed scene validation set and unseen scene testing set. We observe three significant performance gaps, especially on unseen scenes: 1) from B/L2 to LIVR-V1, we obtain around 20\\% mAP improvement by applying the proposed semantic feature decomposition to extract place feature descriptions; 2) from LIVR-V1 to LIVR-V3, our model is further improved by explicitly modeling moving directions by place discretization; 3) when compared to using a fully connected layer for feature aggregation (V1 and V3), our topological method (V2 and V4) leads to another significant improvement, which shows the efficacy of feature aggregation based on scene layout connectivity.", "Place descriptions can be extracted from different levels of feature maps. L = 0 means the input level; L > 0 means after the L^{th 3D-conv blocks. A higher L generally allows the 3D ConvNet to observe more context and to abstract features.", "We evaluate LIVR using the imperfect maps and observe some performance degradations (around 10\\%), but LIVR still outperforms the baselines by a large margin (around 20\\%), which demonstrate the effectiveness of our method even if the segmentation maps are imperfect. Details of how we generate the segmentation maps can be found in the supplementary materials. Conclusion To improve the generalization of a deep network that learns from limited training scenes, we explicitly model scene layout in a network by using layout-induced video representations, which abstracts away low-level appearance variance but encodes the semantics, geometry and topology of scene layouts.", "We utilize bitmaps encoded with the semantic labels of places to decompose video representations into different places and train models to learn place-based feature descriptions. This decomposition encourages the network to learn features of generic place-based motion patterns that are independent of scene layouts. As part of the semantic feature decomposition, we encode scene geometry to model moving directions by discretizing a place into parts based on a quantized distance transform w.r.t.", "From those examples we observe that although the types of place are limited (\\eg street, walkway, lawn), the layout (\\ie structure of places, including reference to location, size, appearance of places and their adjacent places) vary significantly from scene to scene. Without large-scale training data (which is hard to collect considering privacy issues), a naive method that directly learns from raw pixels in training videos without layout modeling can easily overfit to scene-specific textures and absolute pixel coordinates, and exhibit poor generalization on layouts of new scenes. To address the generalization problem, we propose the Layout-Induced Video Representation (LIVR), which explicitly models scene layout for action recognition by encoding the layout in the network architecture given semantic segmentation maps.", "LIVR outperforms the baseline methods by a large margin on almost all action categories. When comparing the orange and green bars in Fig., we observe that the proposed topological feature aggregation (Topo-Agg) leads to consistently better generalization for almost all actions. The blue dashed box highlights the actions that include moving directions, and consistent improvements are brought by distance-based place discretization (DD).", "Topological Feature Aggregation (Topo-Agg) Semantic feature decomposition allows us to extract a feature description for each place individually. To explicitly model the layout of a scene, we need to aggregate these place features based on connectivity between places. Each action category is one-to-one mapped to a place.", "For some actions, especially the ones occurring on street and sidewalk, since they are relatively easy to recognize, adding DD or Topo-Agg upon the place-based feature descriptions (PD) does not help much. Overall, LIVR improves the generalization capability of the network, especially for actions that are more challenging, and are associated with moving directions. \\noindentQualitative Results."], "referred_text_chunks": ["We show the path from the baselines to our method in Table . In column 6-9, we report the mAP of our models on observed scene validation set and unseen scene testing set. We observe three significant performance gaps, especially on unseen scenes: 1) from B/L2 to LIVR-V1, we obtain around 20\\% mAP improvement by applying the proposed semantic feature decomposition to extract place feature descriptions; 2) from LIVR-V1 to LIVR-V3, our model is further improved by explicitly modeling moving directions by place discretization; 3) when compared to using a fully connected layer for feature aggregation (V1 and V3), our topological method (V2 and V4) leads to another significant improvement, which shows the efficacy of feature aggregation based on scene layout connectivity."]}, "4": {"question": "Why does the proposed method outperform the baselines for the action \"<person, move toward (home), walkway>\"?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 1, 'Rationale': 'Image 0 shows the performance of different methods for various actions, highlighting that PD+DD+Topo-Agg significantly improves accuracy for \"person, move toward (home), walkway\". Text 1 explains that place discretization and topological aggregation capture spatial-temporal dynamics and moving directions effectively.', 'Answer': 'The proposed method uses distance-based place discretization and topological feature aggregation to capture spatial-temporal dynamics and moving directions, leading to improved performance for \"person, move toward (home), walkway\".'}", "all_figures_names": ["1804.01429v3-Figure8-1.png", "1804.01429v3-Figure3-1.png", "1804.01429v3-Figure1-1.png", "1804.01429v3-Figure5-1.png", "1804.01429v3-Figure7-1.png", "1804.01429v3-Figure6-1.png", "1804.01429v3-Table1-1.png", "1804.01429v3-Figure9-1.png"], "referred_figures_names": ["1804.01429v3-Figure8-1.png"], "answer": "The proposed method outperforms the baselines for the action \"<person, move toward (home), walkway>\" because it is better at modeling moving directions.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Some example actions are visualized using three frames in temporal order and the predicted probabilities of the groundtruth actions using different methods are reported in Fig.. It is observed that for relatively easy actions such as <vehicle, move along, street>, performance is similar across approaches. However, for challenging actions, especially ones requiring modeling moving directions such as <person, move toward (home), walkway>, our method outperforms baselines significantly.", "LIVR outperforms the baseline methods by a large margin on almost all action categories. When comparing the orange and green bars in Fig., we observe that the proposed topological feature aggregation (Topo-Agg) leads to consistently better generalization for almost all actions. The blue dashed box highlights the actions that include moving directions, and consistent improvements are brought by distance-based place discretization (DD).", "another place. Fig. (brown) shows the discretized bitmaps of walkway w.r.t.", "These videos are captured from 26 different outdoor cameras which cover various layouts of typical American front and back yards. We select 15 common agent-in-place actions to label and each is represented as a tuple containing an action, the agent performing it, and the place where it occurs. The agents, actions, and places involved in our dataset are: agent=\\{person, vehicle, pet\\; action=\\{move along, stay, move away (home), move toward (home), interact with vehicle, move across\\; place=\\{street, sidewalk, lawn, porch, walkway, driveway\\.", "For example, walkway, street and driveway have different functionalities in daily life, which can be easily and efficiently differentiated by humans. However, they may confuse appearance-based methods due to their similar appearance. Furthermore, since home surveillance cameras are typically fixed, users can annotate one map per camera very efficiently.", "Introduction Recent advances in deep neural networks have brought significant improvements to many fundamental computer vision tasks, including video action recognition . Current action recognition methods are able to detect, recognize or localize general actions and identify the agents (people, vehicles, etc.) . However, in applications such as surveillance, relevant actions often involve locations and moving directions that relate to to scene layouts --for example, it might be of interest to detect (and issue an alert about) a person walking towards the front door of a house, but not to detect a person walking along the sidewalk.", "For some actions, especially the ones occurring on street and sidewalk, since they are relatively easy to recognize, adding DD or Topo-Agg upon the place-based feature descriptions (PD) does not help much. Overall, LIVR improves the generalization capability of the network, especially for actions that are more challenging, and are associated with moving directions. \\noindentQualitative Results.", "= \\{porch, walkway, driveway, lawn\\, C_2(porch) includes all except for street, and C_3(porch) covers all six places. It is worth noting that since the vocabulary of our actions is closed, \\mb{T is known at both training and testing time given the segmentation maps. We implement topological feature aggregation using a gated fully connected layer with customized connections determined by the action-place mapping \\mb{T. Given n_p"], "referred_text_chunks": ["Some example actions are visualized using three frames in temporal order and the predicted probabilities of the groundtruth actions using different methods are reported in Fig.. It is observed that for relatively easy actions such as <vehicle, move along, street>, performance is similar across approaches. However, for challenging actions, especially ones requiring modeling moving directions such as <person, move toward (home), walkway>, our method outperforms baselines significantly."]}, "5": {"question": "Which actions are most challenging for the network to recognize, and how do the proposed methods improve the performance on these actions?", "referred_figures_indices": [4], "response": "{'Image': 0, 'Text': 5, 'Rationale': 'Image 0 provides a clear comparison of predicted confidence scores for different methods on action recognition and highlights challenging actions. Text chunk 5 discusses the performance improvements of methods on challenging actions requiring modeling of moving directions.', 'Answer': 'The most challenging actions for the network to recognize are those involving moving directions, such as \"<person, move toward (home), walkway>\". The proposed methods, including place-based feature descriptions (PD), distance-based place discretization (DD), and topological feature aggregation (Topo-Agg), significantly improve performance on these challenging actions by capturing spatial and temporal dependencies effectively.'}", "all_figures_names": ["1804.01429v3-Figure8-1.png", "1804.01429v3-Figure9-1.png", "1804.01429v3-Figure4-1.png", "1804.01429v3-Figure2-1.png", "1804.01429v3-Figure7-1.png", "1804.01429v3-Figure1-1.png", "1804.01429v3-Figure3-1.png", "1804.01429v3-Figure6-1.png"], "referred_figures_names": ["1804.01429v3-Figure7-1.png"], "answer": "The actions that are most challenging for the network to recognize are those that include moving directions, such as \"person, move toward (home)\", \"person, move away (home)\", and \"vehicle, move toward (person)\". The proposed methods, distance-based place discretization (DD) and topological feature aggregation (Topo-Agg), significantly improve the average precision on almost all action categories, especially those that are more challenging and are associated with moving directions.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Introduction Recent advances in deep neural networks have brought significant improvements to many fundamental computer vision tasks, including video action recognition . Current action recognition methods are able to detect, recognize or localize general actions and identify the agents (people, vehicles, etc.) . However, in applications such as surveillance, relevant actions often involve locations and moving directions that relate to to scene layouts --for example, it might be of interest to detect (and issue an alert about) a person walking towards the front door of a house, but not to detect a person walking along the sidewalk.", "\\noindent Training and Testing Details. Our action recognition task is formulated as multi-label classification without mutual exclusion. The network is trained using the Adam optimizer with 0.001 initial learning rate.", "In contrast to these datasets that were designed to evaluate motion and appearance modeling, or human-object interactions, our Agent-in-Place Action (APA) dataset is the first one that focuses on actions that are defined with respect to scene layouts, including interaction with places and moving directions. Recognizing these actions requires the network to not only detect and recognize agent categories and motion patterns, but also how they interact with the layout of the semantic classes in the scene. With the large variations of scene layouts, it is critical to explicitly model scene layout in the network to improve generalization on unseen scenes.", "We observe that: 1) the testing performance gap between observed and unseen scenes is large, which reveals the poor generalization of the baseline models; 2) marginal improvements are obtained by incorporating scene layout information using ConcateMap, which suggests that it is difficulty for the network to learn the human-scene interactions directly from the raw pixels and segmentation maps. In addition, we also train a B/L2 model with 6\\times more filters per layer to evaluate whether model size is the key factor for the performance improvement. The result of this enlarged B/L2 model is shown in column 5 of Table .", "For some actions, especially the ones occurring on street and sidewalk, since they are relatively easy to recognize, adding DD or Topo-Agg upon the place-based feature descriptions (PD) does not help much. Overall, LIVR improves the generalization capability of the network, especially for actions that are more challenging, and are associated with moving directions. \\noindentQualitative Results.", "Some example actions are visualized using three frames in temporal order and the predicted probabilities of the groundtruth actions using different methods are reported in Fig.. It is observed that for relatively easy actions such as <vehicle, move along, street>, performance is similar across approaches. However, for challenging actions, especially ones requiring modeling moving directions such as <person, move toward (home), walkway>, our method outperforms baselines significantly.", "The biggest challenge of agent-in-place action recognition is to generalize a model trained with limited scenes to unseen scenes. Previous work on knowledge transfer for both images and videos has been based on visual similarity, which requires a large amount of training data . For trajectory prediction, Ballan \\etal transferred the priors of statistics from training scenes to new scenes based on scene similarity.", "An interesting future directions would be to include integrate the estimation of the semantic maps into the network architecture, which may require collecting more scenes for training. {\\small \\bibliographystyle{ieee \\bibliography{egbib"], "referred_text_chunks": ["Introduction Recent advances in deep neural networks have brought significant improvements to many fundamental computer vision tasks, including video action recognition . Current action recognition methods are able to detect, recognize or localize general actions and identify the agents (people, vehicles, etc.) . However, in applications such as surveillance, relevant actions often involve locations and moving directions that relate to to scene layouts --for example, it might be of interest to detect (and issue an alert about) a person walking towards the front door of a house, but not to detect a person walking along the sidewalk."]}}