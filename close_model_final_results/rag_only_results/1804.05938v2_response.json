{"0": {"question": "Which correction method resulted in the best performance in terms of nDCG@10 and ERR@10, and how does it compare to not using any correction method?", "referred_figures_indices": [5], "response": "{'Image': 1, 'Text': 5, 'Rationale': 'Image 1 and Text 5 directly compare the performance metrics for different correction methods, including nDCG@10 and ERR@10. They show that DLA consistently outperforms other methods in these metrics.', 'Answer': 'The DLA correction method resulted in the best performance in terms of nDCG@10 and ERR@10, achieving 0.729 and 0.447 respectively, compared to NoCorrect which had 0.704 and 0.431.'}", "all_figures_names": ["1804.05938v2-Figure1-1.png", "1804.05938v2-Table3-1.png", "1804.05938v2-Figure2-1.png", "1804.05938v2-Table2-1.png", "1804.05938v2-Table1-1.png", "1804.05938v2-Table4-1.png"], "referred_figures_names": ["1804.05938v2-Table4-1.png"], "answer": "The DNN trained with DLA achieved the best performance in terms of both nDCG@10 (0.421) and ERR@10 (0.582). Compared to not using any correction method (NoCorrect), DLA shows a significant improvement in both metrics, with nDCG@10 being higher by 0.063 and ERR@10 being higher by 0.082.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Even with no bias correction, the performance of Ranking SVM and DNN trained with click data are better than the Initial Ranker. Comparing the two ranking algorithms, the DNN with softmax cross entropy consistently outperformed Ranking SVM. After incorporating bias corrections with the propensity model estimated by result randomization (RandList), we observed approximately 3\\% improvements with respect to ERR@10 on Ranking SVM and DNN over the models trained with raw clicks.", "When \\eta is 1 (which is same with the click sampling process used in result randomization), the DNN with RandList outperformed the DNN with NoCorrect. When \\eta>1, the relative improvements of RandList over NoCorrect are positive but keep decreasing as \\eta increases. Although the models with RandList underestimated the real presentation bias, they are still better than those with no bias correction.", "Figure~ depicts the performance of the DNN models trained with NoCorrect, RandList and DLA with respect to different \\eta. The presentation bias is severe when \\eta is large and vanishes when \\eta=0. As shown in Figure~, the performance of the DNN with NoCorrect is negative correlated with the value of \\eta.", "The evaluation of retrieval performance for baselines and DLA are conducted on the test set of Yahoo! LETOR data with expert judged relevance labels. The evaluation metrics we used include the mean average precision (MAP), the normalized Discounted Cumulative Gain (nDCG)~ and the Expected Reciprocal Rank (ERR)~. For both nDCG and ERR, we reported the results at rank 1, 3, 5 and 10 to show the performance of models on different positions.", "When \\eta < 1, however, the DNN with RandList performed poorly and are worse than the DNN with NoCorrect. When click biases in the training data are not as severe as they are in the randomization experiments, RandList overestimated the real biases and introduced extra noise into the training of ranking models. In comparison, the performance of the DNN with DLA is robust to changes of \\eta and significantly outperformed NoCorrect and RandList in most cases.", "It obtained more than 10\\% improvements over DBN and UBM in terms of ERR@10. The fact that the DNN model trained with DLA produced better results than those trained with click models indicates that the joint learning of bias correction and ranking models is promising for unbiased learning to rank. First, the joint learning paradigm simplifies the design of search engines as it directly utilizes raw click data without requiring separate experiments to debias user feedback.", "Without bias correction, the training of ranking models are exposed to click bias. Thus, an increase of \\eta will hurt the performance of models trained with raw clicks. Compared to NoCorrect, the effect of misspecified \\eta on the DNN with RandList is more complicated.", "We trained our models and baselines on the training set with clicks and evaluated their performance on the test set with human annotations. Similar to the simulation experiments, we reported the value of MAP, nDCG and ERR for all models in Section~. Please refer to for the data used in this paper. Results and analysis In this section, we discuss the results of our simulation experiments and real-world experiments."], "referred_text_chunks": ["Even with no bias correction, the performance of Ranking SVM and DNN trained with click data are better than the Initial Ranker. Comparing the two ranking algorithms, the DNN with softmax cross entropy consistently outperformed Ranking SVM. After incorporating bias corrections with the propensity model estimated by result randomization (RandList), we observed approximately 3\\% improvements with respect to ERR@10 on Ranking SVM and DNN over the models trained with raw clicks."]}, "1": {"question": "Which method performs better in terms of mean square error?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 0, 'Rationale': 'Image 2 shows the MSE between true click propensity and estimates by DLA and RandList, directly answering the performance question. Text 0 discusses the MSE results, explaining why DLA is more robust and accurate in estimating propensity weights compared to RandList.', 'Answer': 'DLA performs better in terms of mean square error (MSE) than RandList.'}", "all_figures_names": ["1804.05938v2-Figure1-1.png", "1804.05938v2-Table3-1.png", "1804.05938v2-Figure2-1.png", "1804.05938v2-Table2-1.png", "1804.05938v2-Table1-1.png", "1804.05938v2-Table4-1.png"], "referred_figures_names": ["1804.05938v2-Figure2-1.png"], "answer": "DLA performs better than RandList in terms of mean square error.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Because it automatically and directly estimates propensity model from training data, DLA is adaptive to changes in click biases and more robust in practice. To explain why DLA produced a better unbiased ranker than RandList, we computed the mean square error (MSE) between the true inverse propensity weights (\\rho_0^\\eta/\\rho_i^\\eta) and the inverse propensity weights (g_0/g_i) estimated by DLA or RandList: MSE = \\frac{1{|\\pi_q| \\sum_{i=0^{|\\pi_q|-1 \\big(\\frac{g_0{g_i - \\frac{\\rho_0^\\eta{\\rho_i^\\eta\\big)^2 As shown in Figure~, the MSE of RandList is small when \\eta=1 but large otherwise.", "Even with no bias correction, the performance of Ranking SVM and DNN trained with click data are better than the Initial Ranker. Comparing the two ranking algorithms, the DNN with softmax cross entropy consistently outperformed Ranking SVM. After incorporating bias corrections with the propensity model estimated by result randomization (RandList), we observed approximately 3\\% improvements with respect to ERR@10 on Ranking SVM and DNN over the models trained with raw clicks.", "When \\eta is 1 (which is same with the click sampling process used in result randomization), the DNN with RandList outperformed the DNN with NoCorrect. When \\eta>1, the relative improvements of RandList over NoCorrect are positive but keep decreasing as \\eta increases. Although the models with RandList underestimated the real presentation bias, they are still better than those with no bias correction.", "Randomization-based Estimation ~ The key of the IPW algorithm is the estimation of propensity model P(o_q^x =1| \\pi_q).", "This, however, does not hurt the effectiveness of model training. The predicted values for P(r_q^x=1|\\pi_q) and P(o_q^x=1|\\pi_q) have a minor effect on the unbiased learning process as long as their relative proportions are correct. In fact, the actual inverse propensity weighted loss functions used in the DLA are: \\end{equation where P(o_q^1=1|\\pi_q) and P(r_q^1=1|\\pi_q) are the marginal probabilities for the first document in \\pi_q.", "The expected values of \\hat{l_{IRW(E,q) and \\hat{l_{IPW(S,q) are proportional to l(S,q), l(E,q), which doesn't affect the effectiveness of unbiased learning discussed in Equation~\\&. Finally, the empirical loss of S and E can be computed as: To compute the loss on a batch of queries Q' \\subseteq Q, we can simply replace Q with Q' in Equation~(). An overview of the complete algorithm is shown in Algorithm~. In DLA, we first initialize all parameters to zero.", "The evaluation of retrieval performance for baselines and DLA are conducted on the test set of Yahoo! LETOR data with expert judged relevance labels. The evaluation metrics we used include the mean average precision (MAP), the normalized Discounted Cumulative Gain (nDCG)~ and the Expected Reciprocal Rank (ERR)~. For both nDCG and ERR, we reported the results at rank 1, 3, 5 and 10 to show the performance of models on different positions.", "To sample o_q^x, we adopted the presentation bias \\bm{\\rho estimated by Joachims et al.~ through eye-tracking experiments: P(o_q^x=1|\\pi_q) = P(o_i=1) = \\rho_i^\\eta where \\eta\\in[0,+\\infty] is a hyper-parameter that controls the severity of presentation biases. We set \\eta=1.0 if not discussed explicitly."], "referred_text_chunks": ["Because it automatically and directly estimates propensity model from training data, DLA is adaptive to changes in click biases and more robust in practice. To explain why DLA produced a better unbiased ranker than RandList, we computed the mean square error (MSE) between the true inverse propensity weights (\\rho_0^\\eta/\\rho_i^\\eta) and the inverse propensity weights (g_0/g_i) estimated by DLA or RandList: MSE = \\frac{1{|\\pi_q| \\sum_{i=0^{|\\pi_q|-1 \\big(\\frac{g_0{g_i - \\frac{\\rho_0^\\eta{\\rho_i^\\eta\\big)^2 As shown in Figure~, the MSE of RandList is small when \\eta=1 but large otherwise."]}, "2": {"question": "Explain the difference between the features \"TF-IDF\" and \"BM25\".", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 3, 'Rationale': 'Image 3 directly describes TF-IDF and BM25 as part of the features extracted for experiments. Text 3 discusses the extraction of features from documents, which is relevant to understanding how TF-IDF and BM25 are applied.', 'Answer': 'TF-IDF is a statistical measure that evaluates the importance of a term in a document relative to a corpus. It considers the term frequency and the inverse document frequency. BM25, on the other hand, is a ranking function used in information retrieval that improves over the TF-IDF by considering document length and term saturation.'}", "all_figures_names": ["1804.05938v2-Figure1-1.png", "1804.05938v2-Table3-1.png", "1804.05938v2-Figure2-1.png", "1804.05938v2-Table2-1.png", "1804.05938v2-Table1-1.png", "1804.05938v2-Table4-1.png"], "referred_figures_names": ["1804.05938v2-Table2-1.png"], "answer": "Both TF-IDF and BM25 are features used to estimate the relevance of a document to a query. However, they differ in their underlying calculations.\n\nTF-IDF: This feature represents the average product of term frequency (TF) and inverse document frequency (IDF) for each query term within different document sections (URL, title, content, and whole document). TF measures how often a term appears in a specific document section, while IDF measures how important that term is across the entire document collection.\n\nBM25: This feature utilizes the BM25 ranking function, which is a probabilistic model that considers term frequency, document length, and average document length to estimate relevance. While it also considers term frequency like TF-IDF, it incorporates additional factors to improve the weighting scheme.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Evaluation. The evaluation of unbiased learning-to-rank algorithms requires human judgments of query-document relevance. In our experiments, we constructed a separate test dataset with 100 queries and recruited professional assessors to judge the relevance of top 100 documents retrieved by BM25~ in five level (irrelevant, fair, good, excellent and perfect).", "\\iftrue Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.", "DBN assumes that users will keep reading documents sequentially and click them if they look attractive. If not satisfied, users will have a constant probability to return to the search result page and continue reading. Both UBM and DBN use the Expectation-Maximization algorithm~ to estimate their parameters based on click logs.", "For the training of learning-to-rank algorithms, we manually extracted features based on the text of queries and documents. Following a similar methodology used by Microsoft Letor data~, we designed features based on url, title, content and the whole text of the documents. In total, we have 33 features for each query-document pair.", "It introduces a counterfactual model that removes the effect of click bias. Let \\mathbf{o_q, \\mathbf{c_q be the sets of Bernoulli variables that represent whether the documents in \\pi_q are observed and clicked by a user. For simplicity, we assume that x will be clicked (c_q^x=1) when it is observed (o_q^x=1) and relevant (y=1).", "To insure the quality of relevance estimation with click models, we removed ranked lists with less than 10 click sessions in the cleaning process described previously. The final baselines are the DNN models (in Section~~) trained with the relevance signals extracted by UBM and DBN. Other training settings are the same as those used in the simulation experiments.", "For instance, it has been shown that presenting randomized ranked lists and collecting user clicks accordingly can reveal the true preferences between swapped documents~. Yue and Joachims~ used result interleaving to collect reliable user feedback and used Dual Bandit Gradient Descent (DBCG) to learn ranking models in an online manner. Schuth et al.~ extended DBCG and proposed Multileave Gradient Descent to compare multiple rankings and find good rankers. The effectiveness of these learning paradigms has been justified in theory~, but they are not popular in practice because result interleaving hurts ranking quality and introduces non-deterministic factors into search engines~.", "\\fi \\balance \\bibliographystyle{ACM-Reference-Format \\bibliography{sigproc"], "referred_text_chunks": ["Evaluation. The evaluation of unbiased learning-to-rank algorithms requires human judgments of query-document relevance. In our experiments, we constructed a separate test dataset with 100 queries and recruited professional assessors to judge the relevance of top 100 documents retrieved by BM25~ in five level (irrelevant, fair, good, excellent and perfect)."]}}