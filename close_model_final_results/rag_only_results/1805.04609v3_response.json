{"0": {"question": "Which algorithm has the highest percentage of switched instances?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 2, 'Rationale': 'Image 1 directly shows the percentage of switched instances for each algorithm, which is relevant to the question. Text 2 describes the different synthesis algorithms, which are the subject of the question regarding the percentage of switched instances.', 'Answer': 'The algorithm with the highest percentage of switched instances is US-HC-MQ.'}", "all_figures_names": ["1805.04609v3-Figure1-1.png", "1805.04609v3-Figure3-1.png", "1805.04609v3-Figure2-1.png"], "referred_figures_names": ["1805.04609v3-Figure3-1.png"], "answer": "US-HC-MQ", "figure_type": "plot", "content_type": "figure", "text_chunks": ["We randomly chose a core set of 10 instances for each dataset and used each synthesis algorithm to generate 50 examples. The score for each algorithm is the portion of examples it generated that crossed the classification boundary. We repeat the experiment 20 times with different core sets and show the average results on all available datasets.", "The cross-validation accuracy of each artificial expert on its dataset is 86.8\\% in CMR, 94.4\\% in SUBJ, 86.2\\% in SST, 91.0\\% in HS, and 94.5\\% in KS. The expert achieves close to state-of-the-art performance for each dataset. One potential problematic aspect of this methodology is that the performance of the artificial expert was tested only on real examples from the datasets, and not on artificially generated instances.", "In this experiment, we tested the effect of our synthesis algorithms on the number of label changes they cause. Three algorithms were compared, Uncertainty hill-climbing (US-HC-MQ), Stochastic hill-climbing (S-HC-MQ), and Stochastic synthesis (S-MQ). US-HC-MQ uses a heuristic function to direct its search, S-HC-MQ applies multiple operators randomly, and S-MQ randomly applies only one operator at a time, just as described in Algorithms and .", "A simple way of utilizing the modification operators is to apply them in random to the core set of instances. The algorithm maintains a set of instances \\Omega. We initialize \\Omega with the core set C. At each step, we randomly choose an instance from \\Omega and apply a random operator to it.", "The accuracy achieved was very close to the one achieved for the real examples (with a maximum of 2\\% difference). This confirmed that the artificial expert can operate on our artificial examples. \\subsubsection{Compared Methods In the following experiments, we have used 3 variations of our methods:", "To account for this, we performed the following experiment. We used our algorithm to generate a test set of 100 instances for each of the 5 datasets and asked humans to label these instances. We then applied the artificial expert on these test sets.", "Our work tackles the problems in membership query synthesis and presents a novel algorithm for synthesizing new instances by defining a search space over the set of instances, actively searching for the most informative instances. Our method is capable of generating near-misses \u2014 negative examples that are only slightly different from the positive ones. Such examples have been shown to be helpful for learning .", "We would also like a single operator to minimally change the instance, as this will enable the generation of near-miss examples by consecutively applying operators which will slowly move the instance over the classification boundary. We will later discuss the implementation of the modification operators for the textual domain. \\subsubsection{Stochastic Query Synthesis"], "referred_text_chunks": ["We randomly chose a core set of 10 instances for each dataset and used each synthesis algorithm to generate 50 examples. The score for each algorithm is the portion of examples it generated that crossed the classification boundary. We repeat the experiment 20 times with different core sets and show the average results on all available datasets."]}, "1": {"question": "How does the accuracy of the US-BS-MQ method compare to that of the S-MQ method when adding SST examples?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 2, 'Rationale': 'Image 2 directly compares the accuracy of US-BS-MQ and S-MQ across datasets, including SST. Text 2 discusses the performance of search-based approaches like US-BS-MQ compared to S-MQ.', 'Answer': 'US-BS-MQ achieves higher accuracy than S-MQ when adding SST examples.'}", "all_figures_names": ["1805.04609v3-Figure1-1.png", "1805.04609v3-Figure3-1.png", "1805.04609v3-Figure2-1.png"], "referred_figures_names": ["1805.04609v3-Figure2-1.png"], "answer": "The US-BS-MQ method achieves higher accuracy than the S-MQ method when adding SST examples.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Uncertainty sampling beam-search MQ synthesis (US-BS-MQ): Same as (a) but using the beam search algorithm. Stochastic Synthesis (S-MQ):", "In this experiment, we tested the effect of our synthesis algorithms on the number of label changes they cause. Three algorithms were compared, Uncertainty hill-climbing (US-HC-MQ), Stochastic hill-climbing (S-HC-MQ), and Stochastic synthesis (S-MQ). US-HC-MQ uses a heuristic function to direct its search, S-HC-MQ applies multiple operators randomly, and S-MQ randomly applies only one operator at a time, just as described in Algorithms and .", "The two search-based approaches (US-HC-MQ and US-BS-MQ) both exhibited excellent performance across the 5 datasets. The comparison of the search-based approaches to S-MQ showed that, as we expected, more valuable examples are obtained when using the utility function in the generation process. WNA performed admirably considering that in principle it is using only a small semantic neighborhood and therefore receives only synonyms.", "The cross-validation accuracy of each artificial expert on its dataset is 86.8\\% in CMR, 94.4\\% in SUBJ, 86.2\\% in SST, 91.0\\% in HS, and 94.5\\% in KS. The expert achieves close to state-of-the-art performance for each dataset. One potential problematic aspect of this methodology is that the performance of the artificial expert was tested only on real examples from the datasets, and not on artificially generated instances.", "a) Uncertainty sampling hill-climbing MQ synthesis (US-HC-MQ): The proposed search-based synthesis method, using hill-climbing search and uncertainty sampling-based heuristic function. b)", "Figure shows a clear hierarchy, where US-HC-MQ has the most label changes, followed by S-HC-MQ, and then by S-MQ. This result reinforces our hypothesis that using multiple operators on a single sentence as well as using heuristic functions during generation results in more diverse sentences as well shows that our framework does not only perform augmentations but significantly changes the meaning of the sentences. Human Evaluation - Sentence Generation", "This approach holds strong theoretical promise as its learning model is more robust than the standard PAC learning model in many cases , since it does not depend on the assumption that the training and testing instances are drawn from the same distribution. There is one major obstacle, however, when using the MQ model for real-world learning tasks. Traditionally, training instances, such as images or natural-language sentences, are mapped into feature vectors.", "The resulting new instance is added to \\Omega. When enough instances have been generated, we return \\Omega \\backslash C as the MQs. The pseudo-code for stochastic query synthesis is listed in Algorithm ."], "referred_text_chunks": ["Uncertainty sampling beam-search MQ synthesis (US-BS-MQ): Same as (a) but using the beam search algorithm. Stochastic Synthesis (S-MQ):"]}}