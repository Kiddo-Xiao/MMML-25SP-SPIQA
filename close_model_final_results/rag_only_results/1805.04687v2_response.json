{"0": {"question": "How does increasing the training set size affect the performance of the lane marking and drivable area segmentation tasks?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 4, 'Rationale': 'Image 2 shows a clear comparison of results for different training set sizes on lane marking and drivable area segmentation. Text 4 discusses the specific effects of training set size on the performance of these tasks.', 'Answer': 'Increasing the training set size improves the mean ODS-F score for lane marking from 45.41 to 54.24 and the drivable IoU from 64.23 to 72.21, indicating better segmentation performance with larger datasets.'}", "all_figures_names": ["1805.04687v2-Table13-1.png", "1805.04687v2-Figure7-1.png", "1805.04687v2-Table5-1.png", "1805.04687v2-Figure12-1.png", "1805.04687v2-Figure11-1.png", "1805.04687v2-Figure5-1.png", "1805.04687v2-Figure3-1.png", "1805.04687v2-Figure1-1.png"], "referred_figures_names": ["1805.04687v2-Table5-1.png"], "answer": "Increasing the training set size generally leads to improved performance for both lane marking and drivable area segmentation tasks.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["However, the improvement of jointly training on the drivable area detection task, from 64.23 to 64.37, is marginal compared to the individual task. As we increase the number of training images to 20K and 70K, the difference between jointly training and single-task training becomes insignificant, though the performance numbers are generally higher than those trained on 10K images. One hypothesis for the results is that the drivable area detection task and the lane marking task share a similar prediction structure, referred as the homogeneous tasks, and therefore the additional supervision may fail to bring new information to each individual task.", "\\minisection{Semantic segmentation with other tasks. Following a similar manner, we fine-tune a base semantic segmentation model by jointly training semantic segmentation with detection and lane marking/drivable area as shown in Table~. We observe that training with the additional 70K object detection dataset improves the overall mIoU from 56.9 to 58.3, with the improvement mostly attributed to the object classes that are present in the object detection dataset. When jointly training with the lane marking and drivable area sets, the IOU of the stuff classes (e.g., road and sidewalk) improves though the overall IOU across all classes decreases.", "Homogeneous Multitask Learning We first investigate the effects of jointly performing tasks with similar output structures. The BDD100K lane marking and drivable area datasets share the same set of 70K training images. Drivable area annotations consist of 2 foreground classes and lane marking annotations have 3 attributes (direction, continuity, and category).", "The detailed evaluation results for the lane marking benchmark are in Table~. Drivable area detection is a new task, so we show results of a baseline method on the task here. First, the drivable area detection is converted to 3-way segmentation task (background, directly, and alternatively drivable) by ignoring the region ID. Then, we train DRN-D-22 model~ on the 70,000 training images.", "We employ the weighted cross-entropy loss with foreground weight 10 for the lane marking heads, and the gradient-based non-maximum suppression for post-processing. We construct three train sets with 10K, 20K and the full 70K images and report the evaluation results of models trained on individual tasks and both tasks in Table~. Full evaluation results for lane marking are shown in the Appendix. We observe that when training with only 10K images, the mean ODS-F score of lane marking prediction improves from 45.41 to 50.40 when jointly training with the drivable area task.", "We find that after learning from the large-scale image dataset, the model learns to split the road according to the lanes and extrapolate the drivable area to unmarked space. The mIoU for directly and alternatively drivable areas is 79.4\\% and 63.3\\%. However, the same model can achieve much higher accuracy on road segmentation, which indicates that techniques beyond segmentation may be required to solve the drivable area problem.", "However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops. \\vspace{-3mm \\end{table* \\minisection{Object detection and instance segmentation . The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images.", "Table~ shows that joint training of detection and multiple object tracking improves the single-task MOT model with detection AP increasing from 28.1 to 30.7 and MOTA from 55.0 to 56.7, with a slight increase in identity switch."], "referred_text_chunks": ["However, the improvement of jointly training on the drivable area detection task, from 64.23 to 64.37, is marginal compared to the individual task. As we increase the number of training images to 20K and 70K, the difference between jointly training and single-task training becomes insignificant, though the performance numbers are generally higher than those trained on 10K images. One hypothesis for the results is that the drivable area detection task and the lane marking task share a similar prediction structure, referred as the homogeneous tasks, and therefore the additional supervision may fail to bring new information to each individual task."]}, "1": {"question": "How does joint training with the object detection set affect instance segmentation performance, and what is the likely reason for this effect?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 1, 'Rationale': 'Image 2 directly shows the impact of joint training with object detection on instance segmentation performance, highlighting improvements in AP scores. Text 1 provides specific evidence of increased AP scores due to joint training, confirming the observation from the image.', 'Answer': 'Joint training with the object detection set improves instance segmentation performance, as evidenced by an increase in AP from 21.8 to 24.5. The likely reason is the additional localization supervision provided by the object detection annotations.'}", "all_figures_names": ["1805.04687v2-Figure13-1.png", "1805.04687v2-Figure1-1.png", "1805.04687v2-Table6-1.png", "1805.04687v2-Table2-1.png", "1805.04687v2-Table4-1.png", "1805.04687v2-Figure7-1.png", "1805.04687v2-Figure6-1.png", "1805.04687v2-Table8-1.png"], "referred_figures_names": ["1805.04687v2-Table6-1.png"], "answer": "Joint training with the object detection set improves instance segmentation performance significantly. This is evidenced by the increase in AP, AP50, and AP75 metrics in Table 1 when comparing \"Inst-Seg\" and \"Inst-Seg + Det\" rows.\n\nThe passage explains that this improvement is likely due to the richer diversity of images and object examples in the detection set. This allows the instance segmentation model to learn better object appearance features and localization.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["To understand the potential and difficulty, we study joint training for multiple object tracking and segmentation, a downstream task to object detection, instance segmentation, and multiple object tracking. Since the MOTS dataset requires time-consuming instance segmentation annotations at each frame, the dataset is relatively limited in video diversity, with 12K frames from 60 videos in the training set. We aim to improve the performance on the task of MOTS by leveraging the diversity from the detection set with 70K images from 70K videos, the MOT set with 278K frames from 1,400 videos, and the instance segmentation set with 7K images from 7K videos.", "We first study whether adding more object detection annotations can help instance segmentation. We use Mask R-CNN~ with ResNet-50~ as the backbone, and train detection and instance segmentation in a batch-level round-robin manner. As shown in Table~, AP increases from 21.8 to 24.5 with joint training.", "\\minisection{Semantic segmentation with other tasks. Following a similar manner, we fine-tune a base semantic segmentation model by jointly training semantic segmentation with detection and lane marking/drivable area as shown in Table~. We observe that training with the additional 70K object detection dataset improves the overall mIoU from 56.9 to 58.3, with the improvement mostly attributed to the object classes that are present in the object detection dataset. When jointly training with the lane marking and drivable area sets, the IOU of the stuff classes (e.g., road and sidewalk) improves though the overall IOU across all classes decreases.", "The instance segmentation model is able to learn better object appearance features and localization from the detection set with a much richer diversity of images and object examples. Zhou~\\etal~ explore the shape priors in the detection supervision and improve the semi-supervised instance segmentation results further. \\minisection{MOT and object detection.", "Fine-tuning from the pre-trained MOT model, on the other hand, reduces identity switch (IDSW). The extra training examples from the instance segmentation and MOT datasets improve the segmentation and box propagation respectively, thus improving the overall MOTSA results by a large margin. We finally fine-tune the jointly trained detection and tracking model mentioned in Table by jointly training the four tasks together.", "We report instance segmentation AP and multi-object tracking and segmentation accuracy (MOTSA), precision (MOTSP), and other metrics used by in Table . We first fine-tune the MOTS model from pre-trained models of upstream tasks. Compared with training MOTS from scratch, fine-tuning from the pre-trained instance segmentation model improves segmentation AP and MOTSP.", "However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops. \\vspace{-3mm \\end{table* \\minisection{Object detection and instance segmentation . The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images.", "Table~ shows that joint training of detection and multiple object tracking improves the single-task MOT model with detection AP increasing from 28.1 to 30.7 and MOTA from 55.0 to 56.7, with a slight increase in identity switch."], "referred_text_chunks": ["To understand the potential and difficulty, we study joint training for multiple object tracking and segmentation, a downstream task to object detection, instance segmentation, and multiple object tracking. Since the MOTS dataset requires time-consuming instance segmentation annotations at each frame, the dataset is relatively limited in video diversity, with 12K frames from 60 videos in the training set. We aim to improve the performance on the task of MOTS by leveraging the diversity from the detection set with 70K images from 70K videos, the MOT set with 278K frames from 1,400 videos, and the instance segmentation set with 7K images from 7K videos."]}, "2": {"question": "Based on Table 1, which category of objects has the largest total number of annotations in the BDD100K MOT dataset, considering both bounding boxes and instance tracks? ", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 shows the relevant table with detailed annotations by category from the BDD100K MOT dataset. Text 0 gives an overview of the dataset components, useful for understanding context.', 'Answer': 'The category \"car\" has the largest total number of annotations in the BDD100K MOT dataset with 2.6M bounding boxes and 97K instance tracks.'}", "all_figures_names": ["1805.04687v2-Figure7-1.png", "1805.04687v2-Table3-1.png", "1805.04687v2-Figure13-1.png", "1805.04687v2-Table11-1.png", "1805.04687v2-Figure8-1.png", "1805.04687v2-Table14-1.png", "1805.04687v2-Table4-1.png", "1805.04687v2-Figure15-1.png"], "referred_figures_names": ["1805.04687v2-Table11-1.png"], "answer": "Cars have the largest total number of annotations.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["We observe a total number of 130.6K track identities and 3.3M bounding boxes in the training and validation set. The dataset splits are 1400 videos for training, 200 videos for validation and 400 videos for testing. Table~ shows the comparison of BDD100K with previous MOT datasets.", "Our bounding box tracking annotations cover more than one hundred thousand instances with more than two million bounding boxes, and the segmentation tracking set contains more than six thousand instances with one hundred thousand polygons. We showed in the paper submission, our tracking annotation set is one of the largest out there, in addition to our advantage in multitask and diversity. Model Details In this section, we present more implementation details for benchmark models.", "BDD100K MOT is diverse in object scale. Figure~ (left) plots the cumulative distribution of box size, defined as \\sqrt{wh for a bounding box with width w and height h. Figure~ (middle) shows the cumulative distribution of the ratio between the maximum box size and the minimum box size along each track, and Figure~ (right) shows that of the length of each track. The distributions show that the MOT dataset is not only diverse in visual scale among and within tracks, but also in the temporal range of each track.", "Object Detection Locating objects is a fundamental task for not only autonomous driving but the general visual recognition. We provide bounding box annotations of 10 categories for each of the reference frames of 100K videos. The instance statistics is shown in Figure~. We provide visibility attributes including ``occluded'' and ``truncated'' in Figure~ and Figure~. Lane", "MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~ require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS dataset.", "We build BDD100K, a new, diverse, and large-scale dataset of visual driving scenes, together with various tasks, to overcome the limitations. We have been able to collect and annotate the largest available dataset of annotated driving scenes, consisting of over 100K diverse video clips. BDD100K covers more realistic driving scenarios and captures more of the ``long-tail'' of appearance variation and pose configuration of categories of interest in diverse environmental domains.", "BDD100K MOT has 278K training frames from 1,400 videos, whereas the detection set contains 70K images sampled from 70K videos. For the detection and MOT models, we use a modified version of Faster R-CNN~ with a shared DLA-34~ backbone. The implementation details of the tracking model are shown in the Appendix.", "Our tracking benchmark provides one order-of-magnitude bigger than the previously popular tracking dataset, MOT17~. A recent dataset released by Waymo~ has fewer tracking sequences (1150 vs 2000) and fewer frames (230K vs 398K) in total, compared to ours. But Waymo data has more 2D boxes (9.9M vs 4.2M), while ours has better diversity including different weather conditions and more locations. Distributions of tracks and bounding boxes by category are shown in the Appendix."], "referred_text_chunks": ["We observe a total number of 130.6K track identities and 3.3M bounding boxes in the training and validation set. The dataset splits are 1400 videos for training, 200 videos for validation and 400 videos for testing. Table~ shows the comparison of BDD100K with previous MOT datasets."]}, "3": {"question": "How does the segmentation model perform in areas with no lane markings?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 0, 'Rationale': \"Image 1 shows the segmentation model's drivable area predictions, highlighting its ability to handle regions with no lane markings, which relates directly to the question. Text 0 discusses the model's performance in segmenting roads without explicit lane markings, making it the most pertinent to understanding its capabilities in those conditions.\", 'Answer': \"The segmentation model performs adequately in areas with no lane markings by learning to interpolate and identify drivable areas beyond marked lanes, according to Image 1 and Text 0.\"}", "all_figures_names": ["1805.04687v2-Table10-1.png", "1805.04687v2-Figure11-1.png", "1805.04687v2-Figure2-1.png", "1805.04687v2-Figure4-1.png", "1805.04687v2-Table7-1.png", "1805.04687v2-Figure3-1.png", "1805.04687v2-Table11-1.png", "1805.04687v2-Figure12-1.png"], "referred_figures_names": ["1805.04687v2-Figure11-1.png"], "answer": "The segmentation model learns to interpolate in areas that have no lane markings.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["We find that after learning from the large-scale image dataset, the model learns to split the road according to the lanes and extrapolate the drivable area to unmarked space. The mIoU for directly and alternatively drivable areas is 79.4\\% and 63.3\\%. However, the same model can achieve much higher accuracy on road segmentation, which indicates that techniques beyond segmentation may be required to solve the drivable area problem.", "The detailed evaluation results for the lane marking benchmark are in Table~. Drivable area detection is a new task, so we show results of a baseline method on the task here. First, the drivable area detection is converted to 3-way segmentation task (background, directly, and alternatively drivable) by ignoring the region ID. Then, we train DRN-D-22 model~ on the 70,000 training images.", "\\minisection{Semantic segmentation with other tasks. Following a similar manner, we fine-tune a base semantic segmentation model by jointly training semantic segmentation with detection and lane marking/drivable area as shown in Table~. We observe that training with the additional 70K object detection dataset improves the overall mIoU from 56.9 to 58.3, with the improvement mostly attributed to the object classes that are present in the object detection dataset. When jointly training with the lane marking and drivable area sets, the IOU of the stuff classes (e.g., road and sidewalk) improves though the overall IOU across all classes decreases.", "We employ DLA-34~ as the base model for the segmentation tasks. We implement the segmentation head with four 3\\times3 convolution blocks followed by an 1\\times1 convolution to produce segmentation maps in a 4x down-sampled scale, and use bilinear interpolation to upsample the output to the original scale. For lane marking, we use three segmentation heads for the three attributes.", "We formulate the detection of drivable area as segmentation and lane marking as contour detection. We evaluate drivable area segmentation by mean IoU, and lane marking by the Optimal Dataset Scale F-measure (ODS-F) for each category of the three attributes using the Structured Edge Detection Toolbox~ with tolerance \\tau = 1, 2, and 10 pixels. We employ morphological thinning for each score threshold during evaluation.", "However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops. \\vspace{-3mm \\end{table* \\minisection{Object detection and instance segmentation . The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images.", "Not surprisingly, on highways or city streets, where traffic is closely regulated, drivable areas are mostly within lanes and they do not overlap with the vehicles or objects on the road. However, in residential areas, the lanes are sparse. Our annotators can find the drivable areas based on the surroundings.", "The data is suitable to train and test imitation learning algorithms on real driving data. Semantic Instance Segmentation Figure~ shows the distribution of number of instances observed in the segmentation annotations. BDD100K has a good coverage on rare categories (e.g. trailer, train) and large number of instances of common traffic objects such as persons and cars."], "referred_text_chunks": ["We find that after learning from the large-scale image dataset, the model learns to split the road according to the lanes and extrapolate the drivable area to unmarked space. The mIoU for directly and alternatively drivable areas is 79.4\\% and 63.3\\%. However, the same model can achieve much higher accuracy on road segmentation, which indicates that techniques beyond segmentation may be required to solve the drivable area problem."]}, "4": {"question": "What is the most common type of scene in the dataset?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 3, 'Rationale': 'Image 2 directly shows the distribution of scenes, making it the most helpful to identify the most common scene type. Text 3 describes the dataset composition and mentions diverse scenes like city streets and residential areas.', 'Answer': 'City street is the most common type of scene in the dataset.'}", "all_figures_names": ["1805.04687v2-Figure7-1.png", "1805.04687v2-Table9-1.png", "1805.04687v2-Figure10-1.png", "1805.04687v2-Figure9-1.png", "1805.04687v2-Table7-1.png", "1805.04687v2-Figure14-1.png", "1805.04687v2-Table1-1.png", "1805.04687v2-Table14-1.png"], "referred_figures_names": ["1805.04687v2-Figure10-1.png"], "answer": "City Street", "figure_type": "plot", "content_type": "figure", "text_chunks": ["\\vspace{-4mm \\caption{\\small Distribution of images in weather, scene, and day hours categories. \\vspace{-3mm \\end{figure* Object Detection \\vspace{-2mm Table~ compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes.", "The videos contain large portions of extreme weather conditions, such as snow and rain. They also include a diverse number of different scenes across the world. Notably, our dataset contains approximately an equal number of day-time and night-time videos.", "We build BDD100K, a new, diverse, and large-scale dataset of visual driving scenes, together with various tasks, to overcome the limitations. We have been able to collect and annotate the largest available dataset of annotated driving scenes, consisting of over 100K diverse video clips. BDD100K covers more realistic driving scenarios and captures more of the ``long-tail'' of appearance variation and pose configuration of categories of interest in diverse environmental domains.", "The dataset contains not only images with high resolution (720p) and high frame rate (30fps), but also GPS/IMU recordings to preserve the driving trajectories. In total, we have 100K driving videos (40 seconds each) collected from more than 50K rides, covering New York, San Francisco Bay Area, and other regions as shown in Figure~. The dataset contains diverse scene types such as city streets, residential areas, and highways. Furthermore, the videos were recorded in diverse weather conditions at different times of the day.", "The entire label set consists of 40 object classes, which are chosen to capture the diversity of objects in road scenes as well as maximizing the number of labeled pixels in each image. Besides a large number of labels, our dataset exceeds previous efforts in terms of scene diversity and complexity. The whole set is split into 3 parts: 7K images for training, 1K images for validation, and 2K images for testing.", "Like Vistas, our data is crowdsourced, however, our dataset is collected solely from drivers, with each annotated image corresponding to a video sequence, which enables interesting applications for modeling temporal dynamics. \\vspace{-2mm \\caption{\\small Instance statistics of our object categories. (a) Number of instances of each category, which follows a long-tail distribution.", "{\\small \\bibliographystyle{abbrv \\bibliography{reference \\section*{Appendix \\appendix Dataset Details We present more details about our dataset and annotations in this section. Image Tagging Figure~ shows the distribution of weather, scene, and time of day attributes in BDD100K. The distribution demonstrates visual diversity of the images and thus provides an opportunity to study visual transfer between different domains.", "The distribution of classes in the semantic instance segmentation dataset is shown in the Appendix. Multiple Object Tracking To understand the temporal association of objects within the videos, we provide a multiple object tracking (MOT) dataset including 2,000 videos with about 400K frames. Each video is approximately 40 seconds and annotated at 5 fps, resulting in approximately 200 frames per video."], "referred_text_chunks": ["\\vspace{-4mm \\caption{\\small Distribution of images in weather, scene, and day hours categories. \\vspace{-3mm \\end{figure* Object Detection \\vspace{-2mm Table~ compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes."]}, "5": {"question": "What are the different types of annotations that are included in the dataset?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 4, 'Rationale': 'Image 6 provides a comprehensive overview of the types of annotations in the dataset. Text 4 details the comprehensive tracking and segmentation annotations, highlighting the scope and variety of annotations.', 'Answer': 'The dataset includes scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.'}", "all_figures_names": ["1805.04687v2-Table2-1.png", "1805.04687v2-Figure2-1.png", "1805.04687v2-Figure7-1.png", "1805.04687v2-Table11-1.png", "1805.04687v2-Figure11-1.png", "1805.04687v2-Figure6-1.png", "1805.04687v2-Figure1-1.png", "1805.04687v2-Figure14-1.png"], "referred_figures_names": ["1805.04687v2-Figure1-1.png"], "answer": "The dataset includes a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["They are general-purpose and include millions of images with image-level categorical labels. These large datasets are useful in learning representations for image recognition, but most of the complex visual understanding tasks in the real world require more fine-grained recognition such as object localization and segmentation~. Our proposed dataset provides these multi-granularity annotations for more in-depth visual reasoning.", "We build BDD100K, a new, diverse, and large-scale dataset of visual driving scenes, together with various tasks, to overcome the limitations. We have been able to collect and annotate the largest available dataset of annotated driving scenes, consisting of over 100K diverse video clips. BDD100K covers more realistic driving scenarios and captures more of the ``long-tail'' of appearance variation and pose configuration of categories of interest in diverse environmental domains.", "In addition, we provide these annotations in the context of videos, which provides an additional dimension of visual information. Although large video datasets exist~, they usually are restricted to image-level labels. \\minisection{Driving datasets have received increasing attention in the recent years, due to the popularity of autonomous vehicle technology.", "{\\small \\bibliographystyle{abbrv \\bibliography{reference \\section*{Appendix \\appendix Dataset Details We present more details about our dataset and annotations in this section. Image Tagging Figure~ shows the distribution of weather, scene, and time of day attributes in BDD100K. The distribution demonstrates visual diversity of the images and thus provides an opportunity to study visual transfer between different domains.", "Our bounding box tracking annotations cover more than one hundred thousand instances with more than two million bounding boxes, and the segmentation tracking set contains more than six thousand instances with one hundred thousand polygons. We showed in the paper submission, our tracking annotation set is one of the largest out there, in addition to our advantage in multitask and diversity. Model Details In this section, we present more implementation details for benchmark models.", "Homogeneous Multitask Learning We first investigate the effects of jointly performing tasks with similar output structures. The BDD100K lane marking and drivable area datasets share the same set of 70K training images. Drivable area annotations consist of 2 foreground classes and lane marking annotations have 3 attributes (direction, continuity, and category).", "MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~ require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS dataset.", "Like Vistas, our data is crowdsourced, however, our dataset is collected solely from drivers, with each annotated image corresponding to a video sequence, which enables interesting applications for modeling temporal dynamics. \\vspace{-2mm \\caption{\\small Instance statistics of our object categories. (a) Number of instances of each category, which follows a long-tail distribution."], "referred_text_chunks": ["They are general-purpose and include millions of images with image-level categorical labels. These large datasets are useful in learning representations for image recognition, but most of the complex visual understanding tasks in the real world require more fine-grained recognition such as object localization and segmentation~. Our proposed dataset provides these multi-granularity annotations for more in-depth visual reasoning."]}, "6": {"question": "Explain why the model trained on both MOT and detection sets has a higher number of identity switches (IDS) compared to the model trained only on the MOT set, even though it achieves better performance in terms of AP, MOTA, and MOTP?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 shows the relevant data comparing performance metrics between MOT and MOT+Det, and Text 0 discusses how joint training improves performance metrics despite increased identity switches.', 'Answer': 'The model trained on both MOT and detection sets has a higher number of identity switches because it benefits from the diverse examples in the detection set, which improves performance metrics like AP, MOTA, and MOTP. However, this diversity can introduce more complexity in tracking, leading to increased identity switches.'}", "all_figures_names": ["1805.04687v2-Table11-1.png", "1805.04687v2-Figure6-1.png", "1805.04687v2-Table9-1.png", "1805.04687v2-Table7-1.png", "1805.04687v2-Table5-1.png", "1805.04687v2-Table2-1.png", "1805.04687v2-Figure11-1.png", "1805.04687v2-Table4-1.png"], "referred_figures_names": ["1805.04687v2-Table7-1.png"], "answer": "While the model trained on both MOT and detection sets shows improved performance in detection and tracking metrics (AP, MOTA, MOTP), it also exhibits a higher number of identity switches (IDS). This can be attributed to the increased diversity of instances introduced by the detection set. Although the MOT set provides a larger number of bounding boxes for training, the detection set adds varied examples that may lead to more frequent identity switches during tracking, even as it improves the model's overall performance.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Table~ shows that joint training of detection and multiple object tracking improves the single-task MOT model with detection AP increasing from 28.1 to 30.7 and MOTA from 55.0 to 56.7, with a slight increase in identity switch.", "Fine-tuning from the pre-trained MOT model, on the other hand, reduces identity switch (IDSW). The extra training examples from the instance segmentation and MOT datasets improve the segmentation and box propagation respectively, thus improving the overall MOTSA results by a large margin. We finally fine-tune the jointly trained detection and tracking model mentioned in Table by jointly training the four tasks together.", "To understand the potential and difficulty, we study joint training for multiple object tracking and segmentation, a downstream task to object detection, instance segmentation, and multiple object tracking. Since the MOTS dataset requires time-consuming instance segmentation annotations at each frame, the dataset is relatively limited in video diversity, with 12K frames from 60 videos in the training set. We aim to improve the performance on the task of MOTS by leveraging the diversity from the detection set with 70K images from 70K videos, the MOT set with 278K frames from 1,400 videos, and the instance segmentation set with 7K images from 7K videos.", "We report instance segmentation AP and multi-object tracking and segmentation accuracy (MOTSA), precision (MOTSP), and other metrics used by in Table . We first fine-tune the MOTS model from pre-trained models of upstream tasks. Compared with training MOTS from scratch, fine-tuning from the pre-trained instance segmentation model improves segmentation AP and MOTSP.", "BDD100K MOT has 278K training frames from 1,400 videos, whereas the detection set contains 70K images sampled from 70K videos. For the detection and MOT models, we use a modified version of Faster R-CNN~ with a shared DLA-34~ backbone. The implementation details of the tracking model are shown in the Appendix.", "However, the improvement of jointly training on the drivable area detection task, from 64.23 to 64.37, is marginal compared to the individual task. As we increase the number of training images to 20K and 70K, the difference between jointly training and single-task training becomes insignificant, though the performance numbers are generally higher than those trained on 10K images. One hypothesis for the results is that the drivable area detection task and the lane marking task share a similar prediction structure, referred as the homogeneous tasks, and therefore the additional supervision may fail to bring new information to each individual task.", "We observe a total number of 130.6K track identities and 3.3M bounding boxes in the training and validation set. The dataset splits are 1400 videos for training, 200 videos for validation and 400 videos for testing. Table~ shows the comparison of BDD100K with previous MOT datasets.", "Detailed distributions of the MOTS dataset by category are shown in the Appendix. Imitation Learning GPS/IMU recordings in our dataset show the human driver action given the visual input and the driving trajectories. We can use those recordings as a demonstration supervision for the imitation learning algorithms and use perplexity to measure the similarity of driving behaviors on the validation and testing set."], "referred_text_chunks": ["Table~ shows that joint training of detection and multiple object tracking improves the single-task MOT model with detection AP increasing from 28.1 to 30.7 and MOTA from 55.0 to 56.7, with a slight increase in identity switch."]}, "7": {"question": "Which domain discrepancy has a larger impact on object detection performance: city vs. non-city or daytime vs. nighttime?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 0, 'Rationale': 'Image 6 contains data specifically comparing domain discrepancies for city vs. non-city and daytime vs. nighttime for object detection, which directly relates to the question. Text 0 explains the significance of the discrepancy between daytime and nighttime, which is relevant to understanding its impact on object detection.', 'Answer': 'The domain discrepancy between daytime and nighttime has a larger impact on object detection performance.'}", "all_figures_names": ["1805.04687v2-Figure9-1.png", "1805.04687v2-Table9-1.png", "1805.04687v2-Figure4-1.png", "1805.04687v2-Figure15-1.png", "1805.04687v2-Table2-1.png", "1805.04687v2-Table10-1.png", "1805.04687v2-Table4-1.png", "1805.04687v2-Figure10-1.png"], "referred_figures_names": ["1805.04687v2-Table4-1.png"], "answer": "Daytime vs. nighttime has a larger impact on object detection performance.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["We find that there is indeed a domain discrepancy between image sets from different conditions, as shown in Table~. The difference between city and non-city is significant, but the gap between daytime and nighttime is much bigger. Although this is not completely surprising, the results indicate that more work is necessary to bridge the gap. \\vspace{-1mm Semantic Segmentation \\vspace{-1mm We also compare the models trained on Cityscapes and ours, to understand the difference between our new datasets and existing driving datasets.", "While in semantic segmentation, we investigate the domains between our data and Cityscapes~. Object Detection Our dataset has an advantage in diversity, compared to other popular driving datasets. We investigate the influence of domain differences on object detection. The whole dataset is partitioned into several domains based on time of day and scene types.", "City street and daytime are chosen as validation domains. The training sets have the same number of images (30K) in the training set. We then train Faster-RCNN~ based on ResNet-50 on those domains and evaluate the result with COCO API .", "Cityscapes data is collected in German cities, while our data is mainly from the US. We observe that there is a dramatic domain shift between the two datasets for semantic segmentation models. The models perform much worse when tested on a different dataset.", "\\vspace{-4mm \\caption{\\small Distribution of images in weather, scene, and day hours categories. \\vspace{-3mm \\end{figure* Object Detection \\vspace{-2mm Table~ compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes.", "However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops. \\vspace{-3mm \\end{table* \\minisection{Object detection and instance segmentation . The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images.", "Such diversity allows us to study domain transfer and generalize our object detection model well on new test sets. Detailed distributions of images with weather, scene, and day hours tags are shown in the Appendix. We provide image tagging classification results using DLA-34~ in Figure~. The average classification accuracy across different weather and scenes are around 50 to 60\\%.", "However, the improvement of jointly training on the drivable area detection task, from 64.23 to 64.37, is marginal compared to the individual task. As we increase the number of training images to 20K and 70K, the difference between jointly training and single-task training becomes insignificant, though the performance numbers are generally higher than those trained on 10K images. One hypothesis for the results is that the drivable area detection task and the lane marking task share a similar prediction structure, referred as the homogeneous tasks, and therefore the additional supervision may fail to bring new information to each individual task."], "referred_text_chunks": ["We find that there is indeed a domain discrepancy between image sets from different conditions, as shown in Table~. The difference between city and non-city is significant, but the gap between daytime and nighttime is much bigger. Although this is not completely surprising, the results indicate that more work is necessary to bridge the gap. \\vspace{-1mm Semantic Segmentation \\vspace{-1mm We also compare the models trained on Cityscapes and ours, to understand the difference between our new datasets and existing driving datasets."]}, "8": {"question": "Why does the proposed dataset have a lower number of persons per image compared to the Cityscapes dataset, even though it has a much higher total number of pedestrians?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 0, 'Rationale': 'Image 5 directly compares the number of pedestrians per image across datasets, while Text 0 explains that the dataset includes non-city scenes like highways, leading to fewer people per image.', 'Answer': 'The proposed dataset has a lower number of persons per image because it includes non-city scenes such as highways, which typically have fewer pedestrians compared to city scenes like those in the Cityscapes dataset.'}", "all_figures_names": ["1805.04687v2-Figure8-1.png", "1805.04687v2-Figure5-1.png", "1805.04687v2-Figure2-1.png", "1805.04687v2-Table8-1.png", "1805.04687v2-Figure15-1.png", "1805.04687v2-Table10-1.png", "1805.04687v2-Figure13-1.png", "1805.04687v2-Table14-1.png"], "referred_figures_names": ["1805.04687v2-Table10-1.png"], "answer": "The proposed dataset contains non-city scenes like highways, which typically have fewer pedestrians per image compared to cityscapes.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["\\vspace{-4mm \\caption{\\small Distribution of images in weather, scene, and day hours categories. \\vspace{-3mm \\end{figure* Object Detection \\vspace{-2mm Table~ compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes.", "Cityscapes data is collected in German cities, while our data is mainly from the US. We observe that there is a dramatic domain shift between the two datasets for semantic segmentation models. The models perform much worse when tested on a different dataset.", "The goal is to understand the challenge of computer vision systems in the context of self-driving. Some of the datasets focus on particular objects such as pedestrians~. Cityscapes~ provides instance-level semantic segmentation on sampled frames of videos collected by their own vehicle. RobotCar~ and KITTI~ also provide data of multiple sources such as LiDAR scanned points.", "However, the same model trained on our dataset does not suffer these problems. Also, the model of Cityscapes may over-fit the hood of the data collecting vehicle and produces erroneous segmentation for the lower part of the images. Multiple Object Tracking and Segmentation Table and Table shows the label distributions by categories.", "This suggests that even for the domain of other datasets, our new dataset is complementary, which augments existing datasets. Figure~ shows the discrepancy visually. We can observe that the model trained on Cityscape can not recognize the traffic sign in the US.", "However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops. \\vspace{-3mm \\end{table* \\minisection{Object detection and instance segmentation . The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images.", "We find that there is indeed a domain discrepancy between image sets from different conditions, as shown in Table~. The difference between city and non-city is significant, but the gap between daytime and nighttime is much bigger. Although this is not completely surprising, the results indicate that more work is necessary to bridge the gap. \\vspace{-1mm Semantic Segmentation \\vspace{-1mm We also compare the models trained on Cityscapes and ours, to understand the difference between our new datasets and existing driving datasets.", "MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~ require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS dataset."], "referred_text_chunks": ["\\vspace{-4mm \\caption{\\small Distribution of images in weather, scene, and day hours categories. \\vspace{-3mm \\end{figure* Object Detection \\vspace{-2mm Table~ compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes."]}, "9": {"question": "How does the performance of lane marking detection change with different thresholds (\u03c4) for direction, continuity, and category?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 2, 'Rationale': 'Image 0 directly provides the evaluation results (ODS-F scores) for lane marking tasks at different thresholds \u03c4 for direction, continuity, and category. Text 2 specifies the evaluation methodology and the use of thresholds in evaluating lane markings.', 'Answer': 'The performance of lane marking detection, measured using ODS-F scores, generally improves as the threshold (\u03c4) increases from 1 to 10 pixels. Each attribute (direction, continuity, category) shows consistent improvement with higher \u03c4 values across different training sets.'}", "all_figures_names": ["1805.04687v2-Table14-1.png", "1805.04687v2-Table13-1.png", "1805.04687v2-Table9-1.png", "1805.04687v2-Table10-1.png", "1805.04687v2-Table3-1.png", "1805.04687v2-Table4-1.png", "1805.04687v2-Figure12-1.png", "1805.04687v2-Figure15-1.png"], "referred_figures_names": ["1805.04687v2-Table14-1.png"], "answer": "As the threshold (\u03c4) increases, the ODS-F scores for direction, continuity, and category generally increase as well. This indicates that the model performs better in detecting lane markings with higher thresholds, meaning it can tolerate larger deviations from the ground truth annotations.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["We label the attributes of continuity (full or dashed) and direction (parallel or perpendicular). Shown in Table~, our lane marking annotations cover a diverse set of classes. Detailed distributions of types of lane markings and drivable areas are shown in the Appendix.", "However, the improvement of jointly training on the drivable area detection task, from 64.23 to 64.37, is marginal compared to the individual task. As we increase the number of training images to 20K and 70K, the difference between jointly training and single-task training becomes insignificant, though the performance numbers are generally higher than those trained on 10K images. One hypothesis for the results is that the drivable area detection task and the lane marking task share a similar prediction structure, referred as the homogeneous tasks, and therefore the additional supervision may fail to bring new information to each individual task.", "We formulate the detection of drivable area as segmentation and lane marking as contour detection. We evaluate drivable area segmentation by mean IoU, and lane marking by the Optimal Dataset Scale F-measure (ODS-F) for each category of the three attributes using the Structured Edge Detection Toolbox~ with tolerance \\tau = 1, 2, and 10 pixels. We employ morphological thinning for each score threshold during evaluation.", "Marking The lane marking detection is critical for vision-based vehicle localization and trajectory planning. However, available datasets are often limited in scale and diversity. For example, the Caltech Lanes Dataset~ only contains 1,224 images, and the Road Marking Dataset~ has 1,443 images labeled in 11 classes of lane markings.", "The detailed evaluation results for the lane marking benchmark are in Table~. Drivable area detection is a new task, so we show results of a baseline method on the task here. First, the drivable area detection is converted to 3-way segmentation task (background, directly, and alternatively drivable) by ignoring the region ID. Then, we train DRN-D-22 model~ on the 70,000 training images.", "We employ the weighted cross-entropy loss with foreground weight 10 for the lane marking heads, and the gradient-based non-maximum suppression for post-processing. We construct three train sets with 10K, 20K and the full 70K images and report the evaluation results of models trained on individual tasks and both tasks in Table~. Full evaluation results for lane marking are shown in the Appendix. We observe that when training with only 10K images, the mean ODS-F score of lane marking prediction improves from 45.41 to 50.40 when jointly training with the drivable area task.", "Homogeneous Multitask Learning We first investigate the effects of jointly performing tasks with similar output structures. The BDD100K lane marking and drivable area datasets share the same set of 70K training images. Drivable area annotations consist of 2 foreground classes and lane marking annotations have 3 attributes (direction, continuity, and category).", "Similarly, the direction of a lane marking is also significant for autonomous driving. For example, if a lane marking is parallel to the passing car, it may serve to guide cars and separate lanes; if it is perpendicular, it can be treated as a sign of deceleration or stop. The distribution of the number of annotations in varied driving scenes are shown in Figure~, Figure~, and Figure~."], "referred_text_chunks": ["We label the attributes of continuity (full or dashed) and direction (parallel or perpendicular). Shown in Table~, our lane marking annotations cover a diverse set of classes. Detailed distributions of types of lane markings and drivable areas are shown in the Appendix."]}, "10": {"question": "Which weather condition has the highest classification accuracy?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 0, 'Rationale': \"Image 6 shows the classification accuracy of different weather conditions, which directly addresses the question. Text chunk 0 mentions classification accuracy in relation to weather but does not specify which condition has the highest accuracy. However, it supports the image by discussing classification accuracy. Thus, they are both relevant sources.\", 'Answer': 'The weather condition with the highest classification accuracy is clear with 93.4%.'}", "all_figures_names": ["1805.04687v2-Table1-1.png", "1805.04687v2-Figure15-1.png", "1805.04687v2-Figure2-1.png", "1805.04687v2-Figure14-1.png", "1805.04687v2-Figure6-1.png", "1805.04687v2-Table13-1.png", "1805.04687v2-Figure4-1.png", "1805.04687v2-Figure11-1.png"], "referred_figures_names": ["1805.04687v2-Figure4-1.png"], "answer": "Clear weather.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Such diversity allows us to study domain transfer and generalize our object detection model well on new test sets. Detailed distributions of images with weather, scene, and day hours tags are shown in the Appendix. We provide image tagging classification results using DLA-34~ in Figure~. The average classification accuracy across different weather and scenes are around 50 to 60\\%.", "The videos contain large portions of extreme weather conditions, such as snow and rain. They also include a diverse number of different scenes across the world. Notably, our dataset contains approximately an equal number of day-time and night-time videos.", "The videos are split into training (70K), validation (10K) and testing (20K) sets. The frame at the 10\\textsuperscript{th second in each video is annotated for image tasks and the entire sequences are used for tracking tasks. \\vspace{-1mm Image Tagging \\vspace{-1mm We have collected image-level annotation on six weather conditions, six scene types, and three distinct times of day, for each image.", "{\\small \\bibliographystyle{abbrv \\bibliography{reference \\section*{Appendix \\appendix Dataset Details We present more details about our dataset and annotations in this section. Image Tagging Figure~ shows the distribution of weather, scene, and time of day attributes in BDD100K. The distribution demonstrates visual diversity of the images and thus provides an opportunity to study visual transfer between different domains.", "Because it is very difficult to collect data that covers a broad range of time and location, the data diversity of these datasets is limited. For a vehicle perception system to be robust, it needs to learn from a variety of road conditions in numerous cities. Our data was collected from the same original source as the videos in~.", "Cityscapes data is collected in German cities, while our data is mainly from the US. We observe that there is a dramatic domain shift between the two datasets for semantic segmentation models. The models perform much worse when tested on a different dataset.", "We report instance segmentation AP and multi-object tracking and segmentation accuracy (MOTSA), precision (MOTSP), and other metrics used by in Table . We first fine-tune the MOTS model from pre-trained models of upstream tasks. Compared with training MOTS from scratch, fine-tuning from the pre-trained instance segmentation model improves segmentation AP and MOTSP.", "The entire label set consists of 40 object classes, which are chosen to capture the diversity of objects in road scenes as well as maximizing the number of labeled pixels in each image. Besides a large number of labels, our dataset exceeds previous efforts in terms of scene diversity and complexity. The whole set is split into 3 parts: 7K images for training, 1K images for validation, and 2K images for testing."], "referred_text_chunks": ["Such diversity allows us to study domain transfer and generalize our object detection model well on new test sets. Detailed distributions of images with weather, scene, and day hours tags are shown in the Appendix. We provide image tagging classification results using DLA-34~ in Figure~. The average classification accuracy across different weather and scenes are around 50 to 60\\%."]}, "11": {"question": "Which type of object is the most common in the dataset?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 2, 'Rationale': 'Image 3 shows the distribution of classes, indicating the relative frequency of objects, and Text 2 discusses the dataset object classes, which helps determine the most common type.', 'Answer': 'The most common object in the dataset is \"car\".'}", "all_figures_names": ["1805.04687v2-Figure9-1.png", "1805.04687v2-Table9-1.png", "1805.04687v2-Figure11-1.png", "1805.04687v2-Figure14-1.png", "1805.04687v2-Figure4-1.png", "1805.04687v2-Table7-1.png", "1805.04687v2-Figure2-1.png", "1805.04687v2-Table11-1.png"], "referred_figures_names": ["1805.04687v2-Figure14-1.png"], "answer": "Cars are the most common object in the dataset.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["\\vspace{-4mm \\caption{\\small Distribution of images in weather, scene, and day hours categories. \\vspace{-3mm \\end{figure* Object Detection \\vspace{-2mm Table~ compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes.", "Objects in our tracking data also present complicated occlusion and reappearing patterns are shown in Figure~. An object may be fully occluded or move out of the frame, and then reappear later. We observe 49,418 occurrences of occlusion in the dataset, or one occurrence of occlusion every 3.51 tracks. Our dataset shows the real challenges of object re-identification for tracking in autonomous driving.", "The entire label set consists of 40 object classes, which are chosen to capture the diversity of objects in road scenes as well as maximizing the number of labeled pixels in each image. Besides a large number of labels, our dataset exceeds previous efforts in terms of scene diversity and complexity. The whole set is split into 3 parts: 7K images for training, 1K images for validation, and 2K images for testing.", "The distribution of classes in the semantic instance segmentation dataset is shown in the Appendix. Multiple Object Tracking To understand the temporal association of objects within the videos, we provide a multiple object tracking (MOT) dataset including 2,000 videos with about 400K frames. Each video is approximately 40 seconds and annotated at 5 fps, resulting in approximately 200 frames per video.", "To understand the potential and difficulty, we study joint training for multiple object tracking and segmentation, a downstream task to object detection, instance segmentation, and multiple object tracking. Since the MOTS dataset requires time-consuming instance segmentation annotations at each frame, the dataset is relatively limited in video diversity, with 12K frames from 60 videos in the training set. We aim to improve the performance on the task of MOTS by leveraging the diversity from the detection set with 70K images from 70K videos, the MOT set with 278K frames from 1,400 videos, and the instance segmentation set with 7K images from 7K videos.", "MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~ require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS dataset.", "They are general-purpose and include millions of images with image-level categorical labels. These large datasets are useful in learning representations for image recognition, but most of the complex visual understanding tasks in the real world require more fine-grained recognition such as object localization and segmentation~. Our proposed dataset provides these multi-granularity annotations for more in-depth visual reasoning.", "While in semantic segmentation, we investigate the domains between our data and Cityscapes~. Object Detection Our dataset has an advantage in diversity, compared to other popular driving datasets. We investigate the influence of domain differences on object detection. The whole dataset is partitioned into several domains based on time of day and scene types."], "referred_text_chunks": ["\\vspace{-4mm \\caption{\\small Distribution of images in weather, scene, and day hours categories. \\vspace{-3mm \\end{figure* Object Detection \\vspace{-2mm Table~ compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes."]}, "12": {"question": "Based on the table, which approach achieved the highest mean IoU for semantic segmentation, and how did it perform compared to the baseline Sem-Seg model?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 contains a table with mean IoU for different semantic segmentation approaches, which is directly relevant to the question. Text 0 discusses improvements in mean IoU with different approaches, corresponding to the data shown in Image 7.', 'Answer': 'The highest mean IoU for semantic segmentation was achieved by the \"Sem-Seg + Det\" approach at 58.3, compared to the baseline \"Sem-Seg\" model with 56.9 mean IoU.'}", "all_figures_names": ["1805.04687v2-Figure11-1.png", "1805.04687v2-Figure1-1.png", "1805.04687v2-Table13-1.png", "1805.04687v2-Figure14-1.png", "1805.04687v2-Figure9-1.png", "1805.04687v2-Figure10-1.png", "1805.04687v2-Figure8-1.png", "1805.04687v2-Table8-1.png"], "referred_figures_names": ["1805.04687v2-Table8-1.png"], "answer": "The **Sem-Seg + Det** approach achieved the highest mean IoU of 58.3, which is an improvement of 1.4 points compared to the baseline Sem-Seg model with a mean IoU of 56.9.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["\\minisection{Semantic segmentation with other tasks. Following a similar manner, we fine-tune a base semantic segmentation model by jointly training semantic segmentation with detection and lane marking/drivable area as shown in Table~. We observe that training with the additional 70K object detection dataset improves the overall mIoU from 56.9 to 58.3, with the improvement mostly attributed to the object classes that are present in the object detection dataset. When jointly training with the lane marking and drivable area sets, the IOU of the stuff classes (e.g., road and sidewalk) improves though the overall IOU across all classes decreases.", "We formulate the detection of drivable area as segmentation and lane marking as contour detection. We evaluate drivable area segmentation by mean IoU, and lane marking by the Optimal Dataset Scale F-measure (ODS-F) for each category of the three attributes using the Structured Edge Detection Toolbox~ with tolerance \\tau = 1, 2, and 10 pixels. We employ morphological thinning for each score threshold during evaluation.", "We find that after learning from the large-scale image dataset, the model learns to split the road according to the lanes and extrapolate the drivable area to unmarked space. The mIoU for directly and alternatively drivable areas is 79.4\\% and 63.3\\%. However, the same model can achieve much higher accuracy on road segmentation, which indicates that techniques beyond segmentation may be required to solve the drivable area problem.", "We report instance segmentation AP and multi-object tracking and segmentation accuracy (MOTSA), precision (MOTSP), and other metrics used by in Table . We first fine-tune the MOTS model from pre-trained models of upstream tasks. Compared with training MOTS from scratch, fine-tuning from the pre-trained instance segmentation model improves segmentation AP and MOTSP.", "\\vspace{-3mm \\caption{\\small Evaluation results for semantic segmentation. We explore segmentation joint-training with different tasks. Detection can improve the overall accuracy of segmentation, although their output structures are different.", "Cityscapes data is collected in German cities, while our data is mainly from the US. We observe that there is a dramatic domain shift between the two datasets for semantic segmentation models. The models perform much worse when tested on a different dataset.", "We first study whether adding more object detection annotations can help instance segmentation. We use Mask R-CNN~ with ResNet-50~ as the backbone, and train detection and instance segmentation in a batch-level round-robin manner. As shown in Table~, AP increases from 21.8 to 24.5 with joint training.", "We employ DLA-34~ as the base model for the segmentation tasks. We implement the segmentation head with four 3\\times3 convolution blocks followed by an 1\\times1 convolution to produce segmentation maps in a 4x down-sampled scale, and use bilinear interpolation to upsample the output to the original scale. For lane marking, we use three segmentation heads for the three attributes."], "referred_text_chunks": ["\\minisection{Semantic segmentation with other tasks. Following a similar manner, we fine-tune a base semantic segmentation model by jointly training semantic segmentation with detection and lane marking/drivable area as shown in Table~. We observe that training with the additional 70K object detection dataset improves the overall mIoU from 56.9 to 58.3, with the improvement mostly attributed to the object classes that are present in the object detection dataset. When jointly training with the lane marking and drivable area sets, the IOU of the stuff classes (e.g., road and sidewalk) improves though the overall IOU across all classes decreases."]}, "13": {"question": "Which training approach achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 1, 'Rationale': 'Image 6 directly compares training sets with respect to AP, MOTSA, FN, FP, and IDSW, showing the best-performing approach. Text 1 describes how training on multiple tasks improves segmentation and tracking, reducing identity switches.', 'Answer': 'Joint training of detection, tracking, and segmentation (Det + T + I + S) achieves the best balance with AP of 23.3, MOTSA of 41.4, minimizing FN (5132) and maintaining FP (6228).'}", "all_figures_names": ["1805.04687v2-Figure7-1.png", "1805.04687v2-Table8-1.png", "1805.04687v2-Figure2-1.png", "1805.04687v2-Table3-1.png", "1805.04687v2-Table10-1.png", "1805.04687v2-Figure10-1.png", "1805.04687v2-Table9-1.png", "1805.04687v2-Table4-1.png"], "referred_figures_names": ["1805.04687v2-Table9-1.png"], "answer": "The training approach \"Det + T + I + S\" achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Table~ shows that joint training of detection and multiple object tracking improves the single-task MOT model with detection AP increasing from 28.1 to 30.7 and MOTA from 55.0 to 56.7, with a slight increase in identity switch.", "Fine-tuning from the pre-trained MOT model, on the other hand, reduces identity switch (IDSW). The extra training examples from the instance segmentation and MOT datasets improve the segmentation and box propagation respectively, thus improving the overall MOTSA results by a large margin. We finally fine-tune the jointly trained detection and tracking model mentioned in Table by jointly training the four tasks together.", "To understand the potential and difficulty, we study joint training for multiple object tracking and segmentation, a downstream task to object detection, instance segmentation, and multiple object tracking. Since the MOTS dataset requires time-consuming instance segmentation annotations at each frame, the dataset is relatively limited in video diversity, with 12K frames from 60 videos in the training set. We aim to improve the performance on the task of MOTS by leveraging the diversity from the detection set with 70K images from 70K videos, the MOT set with 278K frames from 1,400 videos, and the instance segmentation set with 7K images from 7K videos.", "We report instance segmentation AP and multi-object tracking and segmentation accuracy (MOTSA), precision (MOTSP), and other metrics used by in Table . We first fine-tune the MOTS model from pre-trained models of upstream tasks. Compared with training MOTS from scratch, fine-tuning from the pre-trained instance segmentation model improves segmentation AP and MOTSP.", "We employ the weighted cross-entropy loss with foreground weight 10 for the lane marking heads, and the gradient-based non-maximum suppression for post-processing. We construct three train sets with 10K, 20K and the full 70K images and report the evaluation results of models trained on individual tasks and both tasks in Table~. Full evaluation results for lane marking are shown in the Appendix. We observe that when training with only 10K images, the mean ODS-F score of lane marking prediction improves from 45.41 to 50.40 when jointly training with the drivable area task.", "We first study whether adding more object detection annotations can help instance segmentation. We use Mask R-CNN~ with ResNet-50~ as the backbone, and train detection and instance segmentation in a batch-level round-robin manner. As shown in Table~, AP increases from 21.8 to 24.5 with joint training.", "However, the improvement of jointly training on the drivable area detection task, from 64.23 to 64.37, is marginal compared to the individual task. As we increase the number of training images to 20K and 70K, the difference between jointly training and single-task training becomes insignificant, though the performance numbers are generally higher than those trained on 10K images. One hypothesis for the results is that the drivable area detection task and the lane marking task share a similar prediction structure, referred as the homogeneous tasks, and therefore the additional supervision may fail to bring new information to each individual task.", "\\minisection{Semantic segmentation with other tasks. Following a similar manner, we fine-tune a base semantic segmentation model by jointly training semantic segmentation with detection and lane marking/drivable area as shown in Table~. We observe that training with the additional 70K object detection dataset improves the overall mIoU from 56.9 to 58.3, with the improvement mostly attributed to the object classes that are present in the object detection dataset. When jointly training with the lane marking and drivable area sets, the IOU of the stuff classes (e.g., road and sidewalk) improves though the overall IOU across all classes decreases."], "referred_text_chunks": ["Table~ shows that joint training of detection and multiple object tracking improves the single-task MOT model with detection AP increasing from 28.1 to 30.7 and MOTA from 55.0 to 56.7, with a slight increase in identity switch."]}, "14": {"question": "Which category of objects has the highest total number of annotations, and is there evidence that this category might be more challenging to annotate accurately?", "referred_figures_indices": [3], "response": "{'Image': 1, 'Text': 1, 'Rationale': 'Image 1 provides the total number of annotations by category, such as cars and persons, indicating which has the highest total annotations. Text 1 explains the difficulty in obtaining certain detailed annotations like those used in instance segmentation.', 'Answer': 'The category \"car\" has the highest total number of annotations with 2.6M. More complex annotations like instance segmentation are challenging for this category.'}", "all_figures_names": ["1805.04687v2-Table9-1.png", "1805.04687v2-Table11-1.png", "1805.04687v2-Figure11-1.png", "1805.04687v2-Table12-1.png", "1805.04687v2-Figure13-1.png", "1805.04687v2-Table2-1.png", "1805.04687v2-Figure5-1.png", "1805.04687v2-Table4-1.png"], "referred_figures_names": ["1805.04687v2-Table12-1.png"], "answer": "The category with the highest total number of annotations is \"Masks,\" with 129K annotations. There is evidence that this category might be more challenging to annotate accurately because it also has the highest number of annotations in the \"Occluded\" subcategory, indicating that a large portion of these objects are partially hidden in the images.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["These results further motivate us to study multitask learning of heterogeneous tasks with diverse prediction structure and annotation types in this work. Cascaded Multitask Learning Certain tasks such as object tracking and instance segmentation are more time-consuming to annotate. But they can depend on predictions of simple tasks.", "Additionally, models trained on existing datasets tend to overfit specific domain characteristics~. Real-world applications require performing a combination of perception tasks with different complexities, instead of only homogeneous multiple tasks with the same prediction structure~. Although it may be feasible to label a large number of images with simple annotations such as drivable areas and object bounding boxes~, it remains challenging to obtain more complicated annotations such as instance segmentation~, not to mention multi-object detection and segmentation tracking~.", "They are general-purpose and include millions of images with image-level categorical labels. These large datasets are useful in learning representations for image recognition, but most of the complex visual understanding tasks in the real world require more fine-grained recognition such as object localization and segmentation~. Our proposed dataset provides these multi-granularity annotations for more in-depth visual reasoning.", "Our bounding box tracking annotations cover more than one hundred thousand instances with more than two million bounding boxes, and the segmentation tracking set contains more than six thousand instances with one hundred thousand polygons. We showed in the paper submission, our tracking annotation set is one of the largest out there, in addition to our advantage in multitask and diversity. Model Details In this section, we present more implementation details for benchmark models.", "This connection has been studied as cascaded multitask learning. For example, more accurate object detection can locate the object candidates better for tracking. A natural question is whether to spend all the annotation efforts for the complicated tasks, or to allocate some resources for the basic tasks.", "Object Detection Locating objects is a fundamental task for not only autonomous driving but the general visual recognition. We provide bounding box annotations of 10 categories for each of the reference frames of 100K videos. The instance statistics is shown in Figure~. We provide visibility attributes including ``occluded'' and ``truncated'' in Figure~ and Figure~. Lane", "To understand the potential and difficulty, we study joint training for multiple object tracking and segmentation, a downstream task to object detection, instance segmentation, and multiple object tracking. Since the MOTS dataset requires time-consuming instance segmentation annotations at each frame, the dataset is relatively limited in video diversity, with 12K frames from 60 videos in the training set. We aim to improve the performance on the task of MOTS by leveraging the diversity from the detection set with 70K images from 70K videos, the MOT set with 278K frames from 1,400 videos, and the instance segmentation set with 7K images from 7K videos.", "We first study whether adding more object detection annotations can help instance segmentation. We use Mask R-CNN~ with ResNet-50~ as the backbone, and train detection and instance segmentation in a batch-level round-robin manner. As shown in Table~, AP increases from 21.8 to 24.5 with joint training."], "referred_text_chunks": ["These results further motivate us to study multitask learning of heterogeneous tasks with diverse prediction structure and annotation types in this work. Cascaded Multitask Learning Certain tasks such as object tracking and instance segmentation are more time-consuming to annotate. But they can depend on predictions of simple tasks."]}, "15": {"question": "What percentage of occlusions last for more than 10 frames?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 1, 'Rationale': 'Image 4 includes a graph showing occlusion durations, which is directly relevant to the question. Text 1 discusses occlusion patterns in the dataset and object re-identification, which relates to understanding occlusions in frames.', 'Answer': \"The percentage of occlusions lasting more than 10 frames can be seen in the graph in Image 4. Exact values are not provided in the text, but the graph suggests it is a small percentage.\"}", "all_figures_names": ["1805.04687v2-Figure6-1.png", "1805.04687v2-Table13-1.png", "1805.04687v2-Table14-1.png", "1805.04687v2-Table12-1.png", "1805.04687v2-Figure8-1.png", "1805.04687v2-Table6-1.png", "1805.04687v2-Figure10-1.png", "1805.04687v2-Figure12-1.png"], "referred_figures_names": ["1805.04687v2-Figure8-1.png"], "answer": "Approximately 80%", "figure_type": "plot", "content_type": "figure", "text_chunks": ["(b) Roughly half of the instances are occluded. (c) About 7\\% of the instances are truncated. \\vspace{-4mm \\end{figure*", "Objects in our tracking data also present complicated occlusion and reappearing patterns are shown in Figure~. An object may be fully occluded or move out of the frame, and then reappear later. We observe 49,418 occurrences of occlusion in the dataset, or one occurrence of occlusion every 3.51 tracks. Our dataset shows the real challenges of object re-identification for tracking in autonomous driving.", "The videos are split into training (70K), validation (10K) and testing (20K) sets. The frame at the 10\\textsuperscript{th second in each video is annotated for image tasks and the entire sequences are used for tracking tasks. \\vspace{-1mm Image Tagging \\vspace{-1mm We have collected image-level annotation on six weather conditions, six scene types, and three distinct times of day, for each image.", "Object Detection Locating objects is a fundamental task for not only autonomous driving but the general visual recognition. We provide bounding box annotations of 10 categories for each of the reference frames of 100K videos. The instance statistics is shown in Figure~. We provide visibility attributes including ``occluded'' and ``truncated'' in Figure~ and Figure~. Lane", "However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops. \\vspace{-3mm \\end{table* \\minisection{Object detection and instance segmentation . The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images.", "We employ the weighted cross-entropy loss with foreground weight 10 for the lane marking heads, and the gradient-based non-maximum suppression for post-processing. We construct three train sets with 10K, 20K and the full 70K images and report the evaluation results of models trained on individual tasks and both tasks in Table~. Full evaluation results for lane marking are shown in the Appendix. We observe that when training with only 10K images, the mean ODS-F score of lane marking prediction improves from 45.41 to 50.40 when jointly training with the drivable area task.", "The distribution of classes in the semantic instance segmentation dataset is shown in the Appendix. Multiple Object Tracking To understand the temporal association of objects within the videos, we provide a multiple object tracking (MOT) dataset including 2,000 videos with about 400K frames. Each video is approximately 40 seconds and annotated at 5 fps, resulting in approximately 200 frames per video.", "Such diversity allows us to study domain transfer and generalize our object detection model well on new test sets. Detailed distributions of images with weather, scene, and day hours tags are shown in the Appendix. We provide image tagging classification results using DLA-34~ in Figure~. The average classification accuracy across different weather and scenes are around 50 to 60\\%."], "referred_text_chunks": ["(b) Roughly half of the instances are occluded. (c) About 7\\% of the instances are truncated. \\vspace{-4mm \\end{figure*"]}, "16": {"question": "Why are MOTS datasets like KITTI MOTS and MOTS Challenge smaller in size compared to VOS datasets like YouTube VOS, even though BDD100K MOTS has a comparable number of annotations?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 0, 'Rationale': 'Image 4 provides a direct comparison of MOTS and VOS datasets, showing sizes and annotations which is relevant to understanding dataset differences. Text [0] explains why MOTS datasets require more detailed annotations per frame, making them generally smaller.', 'Answer': 'MOTS datasets like KITTI MOTS and MOTS Challenge require denser annotations per frame compared to VOS datasets, making them smaller in size despite the fact that larger MOTS datasets like BDD100K have a comparable number of annotations overall.'}", "all_figures_names": ["1805.04687v2-Figure12-1.png", "1805.04687v2-Table7-1.png", "1805.04687v2-Table6-1.png", "1805.04687v2-Figure3-1.png", "1805.04687v2-Table3-1.png", "1805.04687v2-Table1-1.png", "1805.04687v2-Table8-1.png", "1805.04687v2-Figure4-1.png"], "referred_figures_names": ["1805.04687v2-Table3-1.png"], "answer": "MOTS datasets require denser annotations per frame because they involve both segmentation and tracking of multiple objects in crowded scenes. This results in smaller dataset sizes compared to VOS datasets, which typically focus on segmenting a single or fewer objects. ", "figure_type": "N/A", "content_type": "table", "text_chunks": ["MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~ require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS dataset.", "Multiple Object Tracking and Segmentation We further provide a multiple object tracking and segmentation (MOTS) dataset with 90 videos. We split the dataset into 60 training videos, 10 validation videos, and 20 testing videos. Table~ shows the details of the BDD MOTS dataset and the comparison with existing multiple object tracking and segmentation (MOTS) and video object segmentation (VOS) datasets.", "To understand the potential and difficulty, we study joint training for multiple object tracking and segmentation, a downstream task to object detection, instance segmentation, and multiple object tracking. Since the MOTS dataset requires time-consuming instance segmentation annotations at each frame, the dataset is relatively limited in video diversity, with 12K frames from 60 videos in the training set. We aim to improve the performance on the task of MOTS by leveraging the diversity from the detection set with 70K images from 70K videos, the MOT set with 278K frames from 1,400 videos, and the instance segmentation set with 7K images from 7K videos.", "We observe a total number of 130.6K track identities and 3.3M bounding boxes in the training and validation set. The dataset splits are 1400 videos for training, 200 videos for validation and 400 videos for testing. Table~ shows the comparison of BDD100K with previous MOT datasets.", "The distribution of classes in the semantic instance segmentation dataset is shown in the Appendix. Multiple Object Tracking To understand the temporal association of objects within the videos, we provide a multiple object tracking (MOT) dataset including 2,000 videos with about 400K frames. Each video is approximately 40 seconds and annotated at 5 fps, resulting in approximately 200 frames per video.", "We build BDD100K, a new, diverse, and large-scale dataset of visual driving scenes, together with various tasks, to overcome the limitations. We have been able to collect and annotate the largest available dataset of annotated driving scenes, consisting of over 100K diverse video clips. BDD100K covers more realistic driving scenarios and captures more of the ``long-tail'' of appearance variation and pose configuration of categories of interest in diverse environmental domains.", "Like Vistas, our data is crowdsourced, however, our dataset is collected solely from drivers, with each annotated image corresponding to a video sequence, which enables interesting applications for modeling temporal dynamics. \\vspace{-2mm \\caption{\\small Instance statistics of our object categories. (a) Number of instances of each category, which follows a long-tail distribution.", "Our tracking benchmark provides one order-of-magnitude bigger than the previously popular tracking dataset, MOT17~. A recent dataset released by Waymo~ has fewer tracking sequences (1150 vs 2000) and fewer frames (230K vs 398K) in total, compared to ours. But Waymo data has more 2D boxes (9.9M vs 4.2M), while ours has better diversity including different weather conditions and more locations. Distributions of tracks and bounding boxes by category are shown in the Appendix."], "referred_text_chunks": ["MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~ require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS dataset."]}, "17": {"question": "What are the three main geographical regions where the data for this study was collected?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 7, 'Rationale': 'Image 4 visually identifies the specific geographical regions where the data was collected, and Text 7 provides detailed information on the regions covered by the dataset.', 'Answer': 'New York, San Francisco, Bay Area.'}", "all_figures_names": ["1805.04687v2-Table1-1.png", "1805.04687v2-Table10-1.png", "1805.04687v2-Table5-1.png", "1805.04687v2-Table11-1.png", "1805.04687v2-Figure2-1.png", "1805.04687v2-Table2-1.png", "1805.04687v2-Figure11-1.png", "1805.04687v2-Table4-1.png"], "referred_figures_names": ["1805.04687v2-Figure2-1.png"], "answer": "New York, San Francisco Bay Area, and Berkeley.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Cityscapes data is collected in German cities, while our data is mainly from the US. We observe that there is a dramatic domain shift between the two datasets for semantic segmentation models. The models perform much worse when tested on a different dataset.", "Because it is very difficult to collect data that covers a broad range of time and location, the data diversity of these datasets is limited. For a vehicle perception system to be robust, it needs to learn from a variety of road conditions in numerous cities. Our data was collected from the same original source as the videos in~.", "The detailed evaluation results for the lane marking benchmark are in Table~. Drivable area detection is a new task, so we show results of a baseline method on the task here. First, the drivable area detection is converted to 3-way segmentation task (background, directly, and alternatively drivable) by ignoring the region ID. Then, we train DRN-D-22 model~ on the 70,000 training images.", "Like Vistas, our data is crowdsourced, however, our dataset is collected solely from drivers, with each annotated image corresponding to a video sequence, which enables interesting applications for modeling temporal dynamics. \\vspace{-2mm \\caption{\\small Instance statistics of our object categories. (a) Number of instances of each category, which follows a long-tail distribution.", "We achieve an overall segmentation AP of 23.3 and MOTSA of 41.4. Conclusion In this work, we presented BDD100K, a large-scale driving video dataset with extensive annotations for heterogeneous tasks. We built a benchmark for heterogeneous multitask learning where the tasks have various prediction structures and serve different aspects of a complete driving system.", "Our experiments provided extensive analysis to different multitask learning scenarios: homogeneous multitask learning and cascaded multitask learning. The results presented interesting findings about allocating the annotation budgets in multitask learning. We hope our work can foster future studies on heterogeneous multitask learning and shed light on this important direction.", "Homogeneous Multitask Learning We first investigate the effects of jointly performing tasks with similar output structures. The BDD100K lane marking and drivable area datasets share the same set of 70K training images. Drivable area annotations consist of 2 foreground classes and lane marking annotations have 3 attributes (direction, continuity, and category).", "The dataset contains not only images with high resolution (720p) and high frame rate (30fps), but also GPS/IMU recordings to preserve the driving trajectories. In total, we have 100K driving videos (40 seconds each) collected from more than 50K rides, covering New York, San Francisco Bay Area, and other regions as shown in Figure~. The dataset contains diverse scene types such as city streets, residential areas, and highways. Furthermore, the videos were recorded in diverse weather conditions at different times of the day."], "referred_text_chunks": ["Cityscapes data is collected in German cities, while our data is mainly from the US. We observe that there is a dramatic domain shift between the two datasets for semantic segmentation models. The models perform much worse when tested on a different dataset."]}, "18": {"question": "Which category of object is the least common in the dataset?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 6, 'Rationale': 'Image 1 displays the number of instances for each object category clearly, allowing for easy identification of the least common object. Text 6 describes the dataset\u2019s diversity, but Image 1 directly provides the instance count for each object category.', 'Answer': 'Train is the least common category with 179 instances.'}", "all_figures_names": ["1805.04687v2-Table3-1.png", "1805.04687v2-Figure3-1.png", "1805.04687v2-Figure12-1.png", "1805.04687v2-Figure2-1.png", "1805.04687v2-Figure1-1.png", "1805.04687v2-Table13-1.png", "1805.04687v2-Figure10-1.png", "1805.04687v2-Figure9-1.png"], "referred_figures_names": ["1805.04687v2-Figure3-1.png"], "answer": "Train", "figure_type": "plot", "content_type": "figure", "text_chunks": ["\\vspace{-4mm \\caption{\\small Distribution of images in weather, scene, and day hours categories. \\vspace{-3mm \\end{figure* Object Detection \\vspace{-2mm Table~ compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes.", "Like Vistas, our data is crowdsourced, however, our dataset is collected solely from drivers, with each annotated image corresponding to a video sequence, which enables interesting applications for modeling temporal dynamics. \\vspace{-2mm \\caption{\\small Instance statistics of our object categories. (a) Number of instances of each category, which follows a long-tail distribution.", "Objects in our tracking data also present complicated occlusion and reappearing patterns are shown in Figure~. An object may be fully occluded or move out of the frame, and then reappear later. We observe 49,418 occurrences of occlusion in the dataset, or one occurrence of occlusion every 3.51 tracks. Our dataset shows the real challenges of object re-identification for tracking in autonomous driving.", "However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops. \\vspace{-3mm \\end{table* \\minisection{Object detection and instance segmentation . The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images.", "MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~ require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS dataset.", "To understand the potential and difficulty, we study joint training for multiple object tracking and segmentation, a downstream task to object detection, instance segmentation, and multiple object tracking. Since the MOTS dataset requires time-consuming instance segmentation annotations at each frame, the dataset is relatively limited in video diversity, with 12K frames from 60 videos in the training set. We aim to improve the performance on the task of MOTS by leveraging the diversity from the detection set with 70K images from 70K videos, the MOT set with 278K frames from 1,400 videos, and the instance segmentation set with 7K images from 7K videos.", "The entire label set consists of 40 object classes, which are chosen to capture the diversity of objects in road scenes as well as maximizing the number of labeled pixels in each image. Besides a large number of labels, our dataset exceeds previous efforts in terms of scene diversity and complexity. The whole set is split into 3 parts: 7K images for training, 1K images for validation, and 2K images for testing.", "Marking The lane marking detection is critical for vision-based vehicle localization and trajectory planning. However, available datasets are often limited in scale and diversity. For example, the Caltech Lanes Dataset~ only contains 1,224 images, and the Road Marking Dataset~ has 1,443 images labeled in 11 classes of lane markings."], "referred_text_chunks": ["\\vspace{-4mm \\caption{\\small Distribution of images in weather, scene, and day hours categories. \\vspace{-3mm \\end{figure* Object Detection \\vspace{-2mm Table~ compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes."]}, "19": {"question": "How does the BDD100K dataset compare to the KITTI and MOT17 datasets in terms of size and complexity?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 provides a direct comparison of key metrics (frames, sequences, identities, boxes) between BDD100K, KITTI, and MOT17, making it the most relevant for understanding size and complexity. Text 0 offers insights into the annotation density and dataset size comparison, providing context for the differences between BDD100K and other datasets.', 'Answer': 'The BDD100K dataset is significantly larger and more complex than the KITTI and MOT17 datasets, with 318K frames, 1,600 sequences, 131K identities, and 3.3M boxes. In comparison, KITTI has 8K frames and 47K boxes, and MOT17 has 34K frames and 337K boxes.'}", "all_figures_names": ["1805.04687v2-Table1-1.png", "1805.04687v2-Table11-1.png", "1805.04687v2-Figure11-1.png", "1805.04687v2-Figure7-1.png", "1805.04687v2-Figure10-1.png", "1805.04687v2-Figure12-1.png", "1805.04687v2-Figure13-1.png", "1805.04687v2-Table2-1.png"], "referred_figures_names": ["1805.04687v2-Table2-1.png"], "answer": "The BDD100K dataset is significantly larger and more complex than both the KITTI and MOT17 datasets. It contains roughly 40 times more frames, 16 times more sequences, and 13 times more identities than KITTI. Compared to MOT17, BDD100K has about 10 times more frames, 80 times more sequences, and 8 times more identities. This increase in size and complexity makes BDD100K a more challenging and comprehensive benchmark for multiple object tracking algorithms. ", "figure_type": "N/A", "content_type": "table", "text_chunks": ["MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~ require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS dataset.", "We observe a total number of 130.6K track identities and 3.3M bounding boxes in the training and validation set. The dataset splits are 1400 videos for training, 200 videos for validation and 400 videos for testing. Table~ shows the comparison of BDD100K with previous MOT datasets.", "We achieve an overall segmentation AP of 23.3 and MOTSA of 41.4. Conclusion In this work, we presented BDD100K, a large-scale driving video dataset with extensive annotations for heterogeneous tasks. We built a benchmark for heterogeneous multitask learning where the tasks have various prediction structures and serve different aspects of a complete driving system.", "BDD100K MOT is diverse in object scale. Figure~ (left) plots the cumulative distribution of box size, defined as \\sqrt{wh for a bounding box with width w and height h. Figure~ (middle) shows the cumulative distribution of the ratio between the maximum box size and the minimum box size along each track, and Figure~ (right) shows that of the length of each track. The distributions show that the MOT dataset is not only diverse in visual scale among and within tracks, but also in the temporal range of each track.", "We build BDD100K, a new, diverse, and large-scale dataset of visual driving scenes, together with various tasks, to overcome the limitations. We have been able to collect and annotate the largest available dataset of annotated driving scenes, consisting of over 100K diverse video clips. BDD100K covers more realistic driving scenarios and captures more of the ``long-tail'' of appearance variation and pose configuration of categories of interest in diverse environmental domains.", "BDD100K MOT has 278K training frames from 1,400 videos, whereas the detection set contains 70K images sampled from 70K videos. For the detection and MOT models, we use a modified version of Faster R-CNN~ with a shared DLA-34~ backbone. The implementation details of the tracking model are shown in the Appendix.", "Our tracking benchmark provides one order-of-magnitude bigger than the previously popular tracking dataset, MOT17~. A recent dataset released by Waymo~ has fewer tracking sequences (1150 vs 2000) and fewer frames (230K vs 398K) in total, compared to ours. But Waymo data has more 2D boxes (9.9M vs 4.2M), while ours has better diversity including different weather conditions and more locations. Distributions of tracks and bounding boxes by category are shown in the Appendix.", "However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops. \\vspace{-3mm \\end{table* \\minisection{Object detection and instance segmentation . The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images."], "referred_text_chunks": ["MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~ require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS dataset."]}, "20": {"question": "Which dataset has the most lane marking annotations?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 4, 'Rationale': 'Image 0 clearly shows the number of lane marking annotations for each dataset, with BDD100K having the most. Text 4 provides information about the scale and diversity of the BDD100K dataset, supporting the claim that it has a large number of annotations.', 'Answer': 'BDD100K has the most lane marking annotations.'}", "all_figures_names": ["1805.04687v2-Table1-1.png", "1805.04687v2-Figure10-1.png", "1805.04687v2-Table11-1.png", "1805.04687v2-Figure8-1.png", "1805.04687v2-Table7-1.png", "1805.04687v2-Figure2-1.png", "1805.04687v2-Table3-1.png", "1805.04687v2-Figure11-1.png"], "referred_figures_names": ["1805.04687v2-Table1-1.png"], "answer": "BDD100K", "figure_type": "table", "content_type": "table", "text_chunks": ["Marking The lane marking detection is critical for vision-based vehicle localization and trajectory planning. However, available datasets are often limited in scale and diversity. For example, the Caltech Lanes Dataset~ only contains 1,224 images, and the Road Marking Dataset~ has 1,443 images labeled in 11 classes of lane markings.", "The most recent work, VPGNet~, consists of about 20,000 images taken during three weeks of driving in Seoul. Our lane markings (Figure~) are labeled with 8 main categories: road curb, crosswalk, double white, double yellow, double other color, single white, single yellow, single other color. The other categories are ignored during evaluation.", "Homogeneous Multitask Learning We first investigate the effects of jointly performing tasks with similar output structures. The BDD100K lane marking and drivable area datasets share the same set of 70K training images. Drivable area annotations consist of 2 foreground classes and lane marking annotations have 3 attributes (direction, continuity, and category).", "Similarly, the direction of a lane marking is also significant for autonomous driving. For example, if a lane marking is parallel to the passing car, it may serve to guide cars and separate lanes; if it is perpendicular, it can be treated as a sign of deceleration or stop. The distribution of the number of annotations in varied driving scenes are shown in Figure~, Figure~, and Figure~.", "We build BDD100K, a new, diverse, and large-scale dataset of visual driving scenes, together with various tasks, to overcome the limitations. We have been able to collect and annotate the largest available dataset of annotated driving scenes, consisting of over 100K diverse video clips. BDD100K covers more realistic driving scenarios and captures more of the ``long-tail'' of appearance variation and pose configuration of categories of interest in diverse environmental domains.", "In addition, we provide these annotations in the context of videos, which provides an additional dimension of visual information. Although large video datasets exist~, they usually are restricted to image-level labels. \\minisection{Driving datasets have received increasing attention in the recent years, due to the popularity of autonomous vehicle technology.", "However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops. \\vspace{-3mm \\end{table* \\minisection{Object detection and instance segmentation . The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images.", "We label the attributes of continuity (full or dashed) and direction (parallel or perpendicular). Shown in Table~, our lane marking annotations cover a diverse set of classes. Detailed distributions of types of lane markings and drivable areas are shown in the Appendix."], "referred_text_chunks": ["Marking The lane marking detection is critical for vision-based vehicle localization and trajectory planning. However, available datasets are often limited in scale and diversity. For example, the Caltech Lanes Dataset~ only contains 1,224 images, and the Road Marking Dataset~ has 1,443 images labeled in 11 classes of lane markings."]}}