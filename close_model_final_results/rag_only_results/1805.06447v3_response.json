{"0": {"question": "How does the performance of ITN-V2 compare to other methods when both DDT and ST transformations are applied to the CIFAR-10 dataset?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 3, 'Rationale': 'Image 6 directly shows the performance of ITN-V2 on the CIFAR-10 dataset with DDT and ST transformations, highlighting its effectiveness. Text 3 discusses ITN\u2019s ability to handle multiple transformation functions and resist corresponding variations.', 'Answer': 'ITN-V2 shows superior performance with a testing error of 56.95% on CIFAR-10 with DDT and ST transformations, outperforming other methods.'}", "all_figures_names": ["1805.06447v3-Figure4-1.png", "1805.06447v3-Table4-1.png", "1805.06447v3-Table8-1.png", "1805.06447v3-Table1-1.png", "1805.06447v3-Figure3-1.png", "1805.06447v3-Table7-1.png", "1805.06447v3-Table3-1.png", "1805.06447v3-Figure5-1.png"], "referred_figures_names": ["1805.06447v3-Table3-1.png"], "answer": "ITN-V2 achieves the lowest testing error (56.95%) among all methods listed when both DDT and ST transformations are applied to the CIFAR-10 dataset.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Comparing against ITN-V1 (B-CNN), ITN-V2 (B-CNN) reduces the testing errors by 6.23\\% on MNIST dataset with random DDT + ST type of variations. Additionally, it reduces the testing errors by 6.61\\% on CIFAR-10 dataset. More importantly, the performance of ITN does not degrade when the model has transformation functions that do not match the type of variations in the testing data, e.g. ITN-V2 (B-CNN) on testing data with DDT only.", "Then the transformed images are randomly placed in a 42 \\times 42 images. Lastly, we replicate the same experiment on the CIFAR-10 dataset. \\vspace{-10pt Agnostic to different transformation functions We observe from Table~ that ITN-V1 (B-CNN) improves the discriminator performance by 4.35\\% from WINN (B-CNN) on MNIST with random DDT transformations and by 21.81\\% on CIFAR-10 dataset.", "\\vspace{-10pt SVHN and CIFAR-10 Next, we evaluate the performance of ITN on SVHN and CIFAR-10 datasets. SVHN~ is a dataset that contains house number images from Google Street View.", "In other words, ITN successfully resists DDT type of variations by integrating with DDT transformation function. Together with results from Table , we see that ITN can work with different types of transformation function and resists the corresponding type of variations. Integrating multiple transformation functions Another important observation from Table is that ITN-V2 (B-CNN) can utilize multiple transformations at the same time to resist a mixture of corresponding variations.", "In this section, we also use ResNet-32~ as a baseline backbone to validate the performance of our framework with deeper network architectures, following the setting in~. ITN outperforms other methods on SVHN and CIFAR-10 datasets as shown in Table . Some samples generated by ITN are shown in Figure . \\vspace{-10pt A more challenging dataset -- miniImageNet We further verify the scalability of ITN by evaluating our proposed method on a new dataset named miniImageNet~. MiniImageNet dataset is a modified version of the ILSVRC-12 dataset~, in which 600 images for each of 100 classes were randomly chosen to be part of the dataset.", "In this case, the training objective will be correctly classifying original and transformed positive samples. We evaluate the performance of this baseline on MNIST dataset, affNIST dataset, and TMTA task. We observe from Table~ that ITN-NG has the same performance as ITN on simple dataset while it has lower performance than ITN when the task becomes harder.", "Then, we include both STs and DDTs in our model and verify the performance again. Let MNIST dataset be the training set of the experiment while the testing set is the MNIST dataset with different types of transformation applied. We introduce two types of testing sets in this section.", "In the following experiments, we demonstrate the ability of ITN to resist large variations with spatial transformers (STs) as our transformation function unless specified. STs includes affine transformations, which are the most common unseen variations in most cases. More importantly, STs are fully differentiable, which allows learning by standard backpropagation."], "referred_text_chunks": ["Comparing against ITN-V1 (B-CNN), ITN-V2 (B-CNN) reduces the testing errors by 6.23\\% on MNIST dataset with random DDT + ST type of variations. Additionally, it reduces the testing errors by 6.61\\% on CIFAR-10 dataset. More importantly, the performance of ITN does not degrade when the model has transformation functions that do not match the type of variations in the testing data, e.g. ITN-V2 (B-CNN) on testing data with DDT only."]}, "1": {"question": "Which method performs the best when trained with only 1% of the MNIST training data, and how much does data augmentation improve its performance in this scenario?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 6, 'Rationale': 'Image 1 directly compares error rates of different methods with 1% MNIST training data. Text 6 discusses performance under limited training data.', 'Answer': 'ITN (B-CNN) w/ DA performs the best with 2.78% error, and data augmentation improves its performance by 0.40% (from 3.18% to 2.78%).'}", "all_figures_names": ["1805.06447v3-Table7-1.png", "1805.06447v3-Table2-1.png", "1805.06447v3-Table8-1.png", "1805.06447v3-Figure1-1.png", "1805.06447v3-Figure2-1.png", "1805.06447v3-Table5-1.png", "1805.06447v3-Figure4-1.png", "1805.06447v3-Table1-1.png"], "referred_figures_names": ["1805.06447v3-Table2-1.png"], "answer": "When trained with only 1% of the MNIST training data, ITN (B-CNN) (w/ DA) performs the best with a testing error of 2.78%. Data augmentation further improves its performance by 0.4%, bringing the testing error down to 2.78% from 3.18% achieved by ITN (B-CNN) without data augmentation.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["The reduced training set is built by randomly sampling data from the MNIST training data while keeping the number of data per class identical. As shown in Table~, our method has better results on all tasks, which are consistent with our previous results. The constantly superior performance of ITN over data augmentation indicates its effectiveness.", "It is easy to observe that the performance improvements on MNIST and affNIST datasets are marginal compared to experiments in Section 4.1. The most likely explanation for this observation is that the training samples in MNIST and affNIST represent the data distribution very well. In this case, providing more samples will not significantly boost the performance of the classifier.", "Thus, we implicitly increase the variations between the training and testing data by reducing the number of samples in the training data. The purpose of this experiment is to demonstrate the potential of ITN to resist unseen variations from a different perspective. \\vspace{-10pt We design a new experiment where the training set is the MNIST dataset with only 0.1\\%, 1\\%, 10\\% and 25\\% of the whole training set while the testing set is the entire MNIST testing set.", "In this case, the training objective will be correctly classifying original and transformed positive samples. We evaluate the performance of this baseline on MNIST dataset, affNIST dataset, and TMTA task. We observe from Table~ that ITN-NG has the same performance as ITN on simple dataset while it has lower performance than ITN when the task becomes harder.", "MNIST is a benchmark dataset that includes 55000, 5000 and 10000 handwritten digits in the training, validation and testing set, respectively. The affNIST dataset is a variant from the MNIST dataset and it is built by applying various affine transformations to the samples in MNIST dataset. To be consistent with the MNIST dataset and for the following experiments, we reduce the size of training, validation, and testing set of the affNIST dataset to 55000, 5000 and 10000, respectively.", "The most widely adopted approach is data augmentation which applies pre-defined transformations to the training data. Nevertheless, this method is lacking since the user has to specify by-hand the types and extent of transformations applied to the training set. Better methods have been proposed by building connections between generative models and discriminative classifiers~.", "As shown in Table , ITN outperforms other methods with standard data augmentation. Limited training data Another way to evaluate the ability of resisting variations is to reduce the number of training samples. Intuitively, data variations between training and testing sets become larger when the number of testing data remains the same while the number of samples in the training set shrinks.", "Then, we include both STs and DDTs in our model and verify the performance again. Let MNIST dataset be the training set of the experiment while the testing set is the MNIST dataset with different types of transformation applied. We introduce two types of testing sets in this section."], "referred_text_chunks": ["The reduced training set is built by randomly sampling data from the MNIST training data while keeping the number of data per class identical. As shown in Table~, our method has better results on all tasks, which are consistent with our previous results. The constantly superior performance of ITN over data augmentation indicates its effectiveness."]}, "2": {"question": "How does the ITN framework generate pseudo-negative samples?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 3, 'Rationale': 'Image 5 visually explains the ITN framework, showing how pseudo-negative samples refine decision boundaries, which is directly related to the generation process. Text chunk 3 describes the process where positive samples are transformed to maximize variation, and pseudo-negative samples are synthesized through a learned joint transformation and CNN classifier approach.', 'Answer': 'The ITN framework generates pseudo-negative samples by transforming positive samples to maximize their variation from the originals, then updating the CNN classifier by minimizing the classification loss on these transformed positives and synthesized pseudo-negatives. This process refines decision boundaries effectively.'}", "all_figures_names": ["1805.06447v3-Table7-1.png", "1805.06447v3-Figure5-1.png", "1805.06447v3-Table1-1.png", "1805.06447v3-Figure4-1.png", "1805.06447v3-Figure2-1.png", "1805.06447v3-Figure1-1.png", "1805.06447v3-Table6-1.png", "1805.06447v3-Figure3-1.png"], "referred_figures_names": ["1805.06447v3-Figure1-1.png"], "answer": "The ITN framework generates pseudo-negative samples by applying learned transformations to positive samples.", "figure_type": "", "content_type": "figure", "text_chunks": ["Generating pseudo-negative samples \\ ITN generates pseudo-negative samples to further enrich the coverage of unseen variations. We follow the same generation procedure as WINN. Starting from randomly initialized pseudo-negative samples, the update formula is where x_{n is the pseudo-negative samples at n-th iteration, \\lambda_g is the step size, and \\eta \\sim N(0, \\lambda_g) is the random Gaussian noise.", "Pseudo-negative samples are considered as samples generated from the transformed positive distribution. Therefore, having a pool of pseudo-negative samples that approximate transformed positive samples will cover a wider range of unseen variations. A simple evaluation is to check the performance of ITN without the generation module.", "ITN enhances the discriminator by generating additional pseudo-negative samples in the training step. \\vspace{-10pt \\end{figure* Our proposed framework is general and can theoretically work with generative adversarial networks (GANs)~. We choose INs in our approach rather than more well known GANs because existing GANs are designed to produce high quality generators while INs have been shown to produce improved discriminators.", "The intuition of our approach is illustrated in Figure~. We use a min-max formulation to learn our ITN, where the transformation module transforms positive samples to maximize their variation from the original training samples and the CNN classifier is updated by minimizing the classification loss of the transformed positive and synthesized pseudo-negative samples. The transformation modules are learned jointly with the CNN classifier, which augments training data in an intelligent manner by narrowing down the search space for the variations. \\caption{Illustration of the intuition of our ITN framework.", "Generative step WINN follows the same generative methods introduced in~. Given a set of positive samples S^{+, WINN iteratively updates self-generated pseudo-negative samples S^{- to to move them closer to the positive distribution. To directly obtain fair samples from pseudo-negative distribution efficiently, ~ interpret the discriminative model as an estimate of the likelihood ratio At iteration t, the distribution of pseudo-negatives is written as: where p_{\\phi_t^{-(x) represents the negative distribution at t-th iteration, Z_t = \\int\\exp(f_{\\phi_t(x)) p_{0^{-(x) is the normalizing factor, and p_{0^{-(x) is some initial negative distribution (e.g. a Gaussian).", "WINN then updates the set of initial pseudo-negative samples to increase f_{\\phi_t(x) via backpropagation. The update of pseudo-negative samples is given by where \\lambda_g is a time-varying step size and \\eta \\sim N(0, \\lambda_g) is the random Gaussian noise. Resisting Variations via Introspective Learning WINN shows promising classification performance by adopting the reclassification-by-synthesis algorithm proposed by ICN~. However, both of them fail to capture large data variations between training and testing data since most of the generated pseudo-negatives are very similar to original samples.", "We term this alternate model {\\em auxiliary classifier generative adversarial transformation networks (AC-GATNs) and qualitatively compare ITN and AC-GATN in Section 4.3, demonstrating the benefit of choosing ITNs for building robust image classifiers. The main contribution of the paper is a principled approach that endows classifiers with the ability to resist larger variations between training and testing data in an intelligent and efficient manner. ITN enhances the classifier by generating additional training samples, including self-generated pseudo-negative samples and transformed input samples.", "An update threshold T_u is introduced to guarantee that the generated pseudo-negative samples are above certain criteria, which prevents bad samples from degrading the classifier performance. We modify the update threshold T_u proposed in~ and keep track of the f_{\\phi_t(x) in every iteration. In particular, we build a set D by recording \\mathbb{E"], "referred_text_chunks": ["Generating pseudo-negative samples \\ ITN generates pseudo-negative samples to further enrich the coverage of unseen variations. We follow the same generation procedure as WINN. Starting from randomly initialized pseudo-negative samples, the update formula is where x_{n is the pseudo-negative samples at n-th iteration, \\lambda_g is the step size, and \\eta \\sim N(0, \\lambda_g) is the random Gaussian noise."]}, "3": {"question": "Which method performs best on the CIFAR-10 dataset?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 4, 'Rationale': 'Image 2 directly shows testing errors on the CIFAR-10 dataset for various methods, highlighting ITN as the best performer. Text 4 discusses the performance improvements by ITN-V2 on the CIFAR-10 dataset, reinforcing the image findings.', 'Answer': 'ITN (ResNet-32) with data augmentation performs best on the CIFAR-10 dataset with a testing error of 5.82%.'}", "all_figures_names": ["1805.06447v3-Table8-1.png", "1805.06447v3-Figure2-1.png", "1805.06447v3-Table5-1.png", "1805.06447v3-Figure3-1.png", "1805.06447v3-Figure5-1.png", "1805.06447v3-Table1-1.png", "1805.06447v3-Table4-1.png", "1805.06447v3-Table7-1.png"], "referred_figures_names": ["1805.06447v3-Table5-1.png"], "answer": "ITN (ResNet-32) with data augmentation performs best on the CIFAR-10 dataset with a testing error of 5.82%.", "figure_type": "table", "content_type": "table", "text_chunks": ["Then the transformed images are randomly placed in a 42 \\times 42 images. Lastly, we replicate the same experiment on the CIFAR-10 dataset. \\vspace{-10pt Agnostic to different transformation functions We observe from Table~ that ITN-V1 (B-CNN) improves the discriminator performance by 4.35\\% from WINN (B-CNN) on MNIST with random DDT transformations and by 21.81\\% on CIFAR-10 dataset.", "There are 73257 digits for training, 26032 digits for testing in SVHN dataset. The CIFAR-10 dataset~ consists of 60000 color images of size 32 \\times 32. This set of 60000 images is split into two sets, 50000 images for training and 10000 images for testing.", "In this section, we also use ResNet-32~ as a baseline backbone to validate the performance of our framework with deeper network architectures, following the setting in~. ITN outperforms other methods on SVHN and CIFAR-10 datasets as shown in Table . Some samples generated by ITN are shown in Figure . \\vspace{-10pt A more challenging dataset -- miniImageNet We further verify the scalability of ITN by evaluating our proposed method on a new dataset named miniImageNet~. MiniImageNet dataset is a modified version of the ILSVRC-12 dataset~, in which 600 images for each of 100 classes were randomly chosen to be part of the dataset.", "One possible reason is that the generative ability of WINN is still limited on this challenging dataset. Additionally, the generation speed of pseudo-negatives are about 6 times slower compared to the CIFAR-10 dataset due to the complexity of samples. \\vspace{-10pt \\vspace{-10pt Discussion Against data augmentation \\", "Comparing against ITN-V1 (B-CNN), ITN-V2 (B-CNN) reduces the testing errors by 6.23\\% on MNIST dataset with random DDT + ST type of variations. Additionally, it reduces the testing errors by 6.61\\% on CIFAR-10 dataset. More importantly, the performance of ITN does not degrade when the model has transformation functions that do not match the type of variations in the testing data, e.g. ITN-V2 (B-CNN) on testing data with DDT only.", "\\vspace{-10pt SVHN and CIFAR-10 Next, we evaluate the performance of ITN on SVHN and CIFAR-10 datasets. SVHN~ is a dataset that contains house number images from Google Street View.", "The results shown in Figure illustrate that under our framework, using INs as the generative model achieve better performance than using GANs. By visualizing the generated samples from AC-GATN and ITN (shown in Figure ), both AC-GATN and ITN generate clear and sharp images. However, samples generated by AC-GATN have lower quality on average in terms of human standards as some of them are misleading and inaccurate, i.e. the number 3 is close to number 6 in epoch 100.", "Experiments In this section, we demonstrate the ability of our algorithm to resist large variations between training and testing data through a series of experiments. First, we will present a series of analyses of resisting ability and resisting flexibility of ITN. Following that we demonstrate the strong classification performance of ITN on a series of datasets, including MNIST, affNIST, SVHN, CIFAR-10 and a more challenging dataset, miniImageNet."], "referred_text_chunks": ["Then the transformed images are randomly placed in a 42 \\times 42 images. Lastly, we replicate the same experiment on the CIFAR-10 dataset. \\vspace{-10pt Agnostic to different transformation functions We observe from Table~ that ITN-V1 (B-CNN) improves the discriminator performance by 4.35\\% from WINN (B-CNN) on MNIST with random DDT transformations and by 21.81\\% on CIFAR-10 dataset."]}, "4": {"question": "Which method achieved the lowest testing error on the miniImageNet dataset?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 3, 'Rationale': 'Image 1 specifically lists the testing errors on the miniImageNet dataset, showing ITN (ResNet-32) with data augmentation having the lowest error. Text chunk 3 discusses the performance of ITN on miniImageNet, noting consistent better performance than comparisons, aligning with Image 1 data.', 'Answer': 'ITN (ResNet-32) with data augmentation achieved the lowest testing error on the miniImageNet dataset.'}", "all_figures_names": ["1805.06447v3-Table3-1.png", "1805.06447v3-Table6-1.png", "1805.06447v3-Figure2-1.png", "1805.06447v3-Table4-1.png", "1805.06447v3-Figure4-1.png", "1805.06447v3-Table8-1.png", "1805.06447v3-Table1-1.png", "1805.06447v3-Table5-1.png"], "referred_figures_names": ["1805.06447v3-Table6-1.png"], "answer": "ITTN (ResNet-32) (w/ DA) achieved the lowest testing error on the miniImageNet dataset with an error rate of 29.65%.", "figure_type": "table", "content_type": "table", "text_chunks": ["In this section, we also use ResNet-32~ as a baseline backbone to validate the performance of our framework with deeper network architectures, following the setting in~. ITN outperforms other methods on SVHN and CIFAR-10 datasets as shown in Table . Some samples generated by ITN are shown in Figure . \\vspace{-10pt A more challenging dataset -- miniImageNet We further verify the scalability of ITN by evaluating our proposed method on a new dataset named miniImageNet~. MiniImageNet dataset is a modified version of the ILSVRC-12 dataset~, in which 600 images for each of 100 classes were randomly chosen to be part of the dataset.", "All images in this dataset are of size 84 \\times 84 pixels. Compared to previously tested datasets in this section, miniImageNet is significantly harder choice both in terms of generation and classification. Each sample contains complicated natural scenes that pose a hard challenge in generating pseudo-negative samples.", "Comparing against ITN-V1 (B-CNN), ITN-V2 (B-CNN) reduces the testing errors by 6.23\\% on MNIST dataset with random DDT + ST type of variations. Additionally, it reduces the testing errors by 6.61\\% on CIFAR-10 dataset. More importantly, the performance of ITN does not degrade when the model has transformation functions that do not match the type of variations in the testing data, e.g. ITN-V2 (B-CNN) on testing data with DDT only.", "Additionally, the number of classes are also larger than previously seen datasets. The results are shown in Table and ITN shows consistent better performance than all other comparisons. Note that the performance improvement of ITN on miniImagenet is slightly lower than expected.", "MNIST is a benchmark dataset that includes 55000, 5000 and 10000 handwritten digits in the training, validation and testing set, respectively. The affNIST dataset is a variant from the MNIST dataset and it is built by applying various affine transformations to the samples in MNIST dataset. To be consistent with the MNIST dataset and for the following experiments, we reduce the size of training, validation, and testing set of the affNIST dataset to 55000, 5000 and 10000, respectively.", "Then the transformed images are randomly placed in a 42 \\times 42 images. Lastly, we replicate the same experiment on the CIFAR-10 dataset. \\vspace{-10pt Agnostic to different transformation functions We observe from Table~ that ITN-V1 (B-CNN) improves the discriminator performance by 4.35\\% from WINN (B-CNN) on MNIST with random DDT transformations and by 21.81\\% on CIFAR-10 dataset.", "Experiments In this section, we demonstrate the ability of our algorithm to resist large variations between training and testing data through a series of experiments. First, we will present a series of analyses of resisting ability and resisting flexibility of ITN. Following that we demonstrate the strong classification performance of ITN on a series of datasets, including MNIST, affNIST, SVHN, CIFAR-10 and a more challenging dataset, miniImageNet.", "There are 73257 digits for training, 26032 digits for testing in SVHN dataset. The CIFAR-10 dataset~ consists of 60000 color images of size 32 \\times 32. This set of 60000 images is split into two sets, 50000 images for training and 10000 images for testing."], "referred_text_chunks": ["In this section, we also use ResNet-32~ as a baseline backbone to validate the performance of our framework with deeper network architectures, following the setting in~. ITN outperforms other methods on SVHN and CIFAR-10 datasets as shown in Table . Some samples generated by ITN are shown in Figure . \\vspace{-10pt A more challenging dataset -- miniImageNet We further verify the scalability of ITN by evaluating our proposed method on a new dataset named miniImageNet~. MiniImageNet dataset is a modified version of the ILSVRC-12 dataset~, in which 600 images for each of 100 classes were randomly chosen to be part of the dataset."]}, "5": {"question": "Which generative model generates more accurate and realistic samples on the MNIST dataset, AC-GATN or ITN?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 0, 'Rationale': 'Image 2 visually compares the samples generated by AC-GATN and ITN, showing clearer and sharper results from ITN. Text chunk 0 directly discusses the comparison, noting ITN provides higher quality images than AC-GATN, which creates misleading samples.', 'Answer': 'ITN generates more accurate and realistic samples on the MNIST dataset than AC-GATN.'}", "all_figures_names": ["1805.06447v3-Table1-1.png", "1805.06447v3-Table4-1.png", "1805.06447v3-Figure4-1.png", "1805.06447v3-Figure3-1.png", "1805.06447v3-Table5-1.png", "1805.06447v3-Figure2-1.png", "1805.06447v3-Table6-1.png", "1805.06447v3-Figure1-1.png"], "referred_figures_names": ["1805.06447v3-Figure4-1.png"], "answer": "ITN generates more accurate and realistic samples on the MNIST dataset compared to AC-GATN.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["The results shown in Figure illustrate that under our framework, using INs as the generative model achieve better performance than using GANs. By visualizing the generated samples from AC-GATN and ITN (shown in Figure ), both AC-GATN and ITN generate clear and sharp images. However, samples generated by AC-GATN have lower quality on average in terms of human standards as some of them are misleading and inaccurate, i.e. the number 3 is close to number 6 in epoch 100.", "We term this alternate model {\\em auxiliary classifier generative adversarial transformation networks (AC-GATNs) and qualitatively compare ITN and AC-GATN in Section 4.3, demonstrating the benefit of choosing ITNs for building robust image classifiers. The main contribution of the paper is a principled approach that endows classifiers with the ability to resist larger variations between training and testing data in an intelligent and efficient manner. ITN enhances the classifier by generating additional training samples, including self-generated pseudo-negative samples and transformed input samples.", "These lower quality samples will mislead the classifier and lead to a performance decrease. Consequently, we choose INs rather than GANs in our approach. We will provide more comparisons of AC-GATN and ITN on other datasets in the supplementary material.", "AC-GAN can generate class-dependent samples, which is required in our framework. The loss function of AC-GAN is replaced with the loss function from WGAN-GP to directly compare it with ITN. All experimental settings are the same for a fair comparison.", "Comparing against ITN-V1 (B-CNN), ITN-V2 (B-CNN) reduces the testing errors by 6.23\\% on MNIST dataset with random DDT + ST type of variations. Additionally, it reduces the testing errors by 6.61\\% on CIFAR-10 dataset. More importantly, the performance of ITN does not degrade when the model has transformation functions that do not match the type of variations in the testing data, e.g. ITN-V2 (B-CNN) on testing data with DDT only.", "In this case, the training objective will be correctly classifying original and transformed positive samples. We evaluate the performance of this baseline on MNIST dataset, affNIST dataset, and TMTA task. We observe from Table~ that ITN-NG has the same performance as ITN on simple dataset while it has lower performance than ITN when the task becomes harder.", "Choice w.r.t generative models \\ It is essential to validate our choice of generative models. We implement our framework by using AC-GAN as the generative model and name it AC-GATN.", "Then the transformed images are randomly placed in a 42 \\times 42 images. Lastly, we replicate the same experiment on the CIFAR-10 dataset. \\vspace{-10pt Agnostic to different transformation functions We observe from Table~ that ITN-V1 (B-CNN) improves the discriminator performance by 4.35\\% from WINN (B-CNN) on MNIST with random DDT transformations and by 21.81\\% on CIFAR-10 dataset."], "referred_text_chunks": ["The results shown in Figure illustrate that under our framework, using INs as the generative model achieve better performance than using GANs. By visualizing the generated samples from AC-GATN and ITN (shown in Figure ), both AC-GATN and ITN generate clear and sharp images. However, samples generated by AC-GATN have lower quality on average in terms of human standards as some of them are misleading and inaccurate, i.e. the number 3 is close to number 6 in epoch 100."]}, "6": {"question": "How does the quality of the generated samples change as the update threshold increases?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 2, 'Rationale': 'Image 6 directly shows how the generated samples vary with different thresholds, and Text 2 discusses the impact of thresholds on sample quality and performance.', 'Answer': 'The quality of the generated samples decreases as the update threshold increases.'}", "all_figures_names": ["1805.06447v3-Table6-1.png", "1805.06447v3-Figure3-1.png", "1805.06447v3-Table3-1.png", "1805.06447v3-Table2-1.png", "1805.06447v3-Table8-1.png", "1805.06447v3-Figure4-1.png", "1805.06447v3-Figure5-1.png", "1805.06447v3-Figure2-1.png"], "referred_figures_names": ["1805.06447v3-Figure5-1.png"], "answer": "The quality of the generated samples decreases as the update threshold increases.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["\\vspace{-15pt Effects of the update threshold \\mathbf{T_u \\ The update threshold T_u introduced in INs quantitatively controls the quality of samples in the generation process.", "An update threshold T_u is introduced to guarantee that the generated pseudo-negative samples are above certain criteria, which prevents bad samples from degrading the classifier performance. We modify the update threshold T_u proposed in~ and keep track of the f_{\\phi_t(x) in every iteration. In particular, we build a set D by recording \\mathbb{E", "In Table~, we present the results of ITN on the MNIST dataset with different thresholds to explore the relationship between the samples quality and thresholds. Not surprisingly, we observe that the performance of ITN drops when increasing the threshold. By visualizing the samples generated by different thresholds, it is clear that the performance drops due to the decrease in the quality of generated samples (Fig.~).", "Although our performance drops with the increase of the threshold, in a certain range (<5e-3), our result is still better than others shown in Table~, which shows that our approach tolerates samples of low qualities in some extend. \\vspace{-10pt Effects of pseudo-negative samples \\ It is worth exploring how pseudo-negative samples affect the performance of the classifier.", "Furthermore, the range and type of such augmentations are chosen heuristically with no strong theoretical justification. An alternative to augmentation is to synthesize additional training examples using a generative model. {\\em How can we automatically generate synthetic samples that are useful as training data to improve the robustness of CNNs to large variations in testing data?", "Generating pseudo-negative samples \\ ITN generates pseudo-negative samples to further enrich the coverage of unseen variations. We follow the same generation procedure as WINN. Starting from randomly initialized pseudo-negative samples, the update formula is where x_{n is the pseudo-negative samples at n-th iteration, \\lambda_g is the step size, and \\eta \\sim N(0, \\lambda_g) is the random Gaussian noise.", "WINN then updates the set of initial pseudo-negative samples to increase f_{\\phi_t(x) via backpropagation. The update of pseudo-negative samples is given by where \\lambda_g is a time-varying step size and \\eta \\sim N(0, \\lambda_g) is the random Gaussian noise. Resisting Variations via Introspective Learning WINN shows promising classification performance by adopting the reclassification-by-synthesis algorithm proposed by ICN~. However, both of them fail to capture large data variations between training and testing data since most of the generated pseudo-negatives are very similar to original samples.", "As shown in Table , ITN outperforms other methods with standard data augmentation. Limited training data Another way to evaluate the ability of resisting variations is to reduce the number of training samples. Intuitively, data variations between training and testing sets become larger when the number of testing data remains the same while the number of samples in the training set shrinks."], "referred_text_chunks": ["\\vspace{-15pt Effects of the update threshold \\mathbf{T_u \\ The update threshold T_u introduced in INs quantitatively controls the quality of samples in the generation process."]}, "7": {"question": "Describe the relationship between the update threshold (Tu) and the performance of ITN (B-CNN) on the MNIST dataset.", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 2, 'Rationale': 'Image 0 provides a direct comparison of ITN errors with various thresholds on MNIST, demonstrating the relationship between threshold Tu and performance. Text chunk 2 explains the role of the update threshold Tu in controlling the quality of generated samples, connecting directly to the performance impacts reflected in the image.', 'Answer': 'Increasing the update threshold Tu leads to higher testing errors for ITN on MNIST, indicating decreased performance. Lower thresholds maintain higher sample quality and better performance.'}", "all_figures_names": ["1805.06447v3-Table7-1.png", "1805.06447v3-Figure5-1.png", "1805.06447v3-Table2-1.png", "1805.06447v3-Table6-1.png", "1805.06447v3-Table4-1.png", "1805.06447v3-Table3-1.png", "1805.06447v3-Figure2-1.png", "1805.06447v3-Figure3-1.png"], "referred_figures_names": ["1805.06447v3-Table7-1.png"], "answer": "The performance of ITN (B-CNN) on the MNIST dataset decreases as the update threshold (Tu) increases. This is evident from the increasing ITN error percentages as Tu goes from 1e-3 to 1e-1.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["In Table~, we present the results of ITN on the MNIST dataset with different thresholds to explore the relationship between the samples quality and thresholds. Not surprisingly, we observe that the performance of ITN drops when increasing the threshold. By visualizing the samples generated by different thresholds, it is clear that the performance drops due to the decrease in the quality of generated samples (Fig.~).", "Comparing against ITN-V1 (B-CNN), ITN-V2 (B-CNN) reduces the testing errors by 6.23\\% on MNIST dataset with random DDT + ST type of variations. Additionally, it reduces the testing errors by 6.61\\% on CIFAR-10 dataset. More importantly, the performance of ITN does not degrade when the model has transformation functions that do not match the type of variations in the testing data, e.g. ITN-V2 (B-CNN) on testing data with DDT only.", "\\vspace{-15pt Effects of the update threshold \\mathbf{T_u \\ The update threshold T_u introduced in INs quantitatively controls the quality of samples in the generation process.", "Then the transformed images are randomly placed in a 42 \\times 42 images. Lastly, we replicate the same experiment on the CIFAR-10 dataset. \\vspace{-10pt Agnostic to different transformation functions We observe from Table~ that ITN-V1 (B-CNN) improves the discriminator performance by 4.35\\% from WINN (B-CNN) on MNIST with random DDT transformations and by 21.81\\% on CIFAR-10 dataset.", "In this case, the training objective will be correctly classifying original and transformed positive samples. We evaluate the performance of this baseline on MNIST dataset, affNIST dataset, and TMTA task. We observe from Table~ that ITN-NG has the same performance as ITN on simple dataset while it has lower performance than ITN when the task becomes harder.", "Classification MNIST and affNIST Now we want to back to the most common task, classification to ensure ITN not only work well on large data variations but also boost performance on well-known benchmark datatsets. We first compare the performance of ITN against other baselines on MNIST and affNIST datasets and then gradually switch to harder datasets. As shown in Table~, ITN outperforms other baselines.", "Thus, we implicitly increase the variations between the training and testing data by reducing the number of samples in the training data. The purpose of this experiment is to demonstrate the potential of ITN to resist unseen variations from a different perspective. \\vspace{-10pt We design a new experiment where the training set is the MNIST dataset with only 0.1\\%, 1\\%, 10\\% and 25\\% of the whole training set while the testing set is the entire MNIST testing set.", "Experiments In this section, we demonstrate the ability of our algorithm to resist large variations between training and testing data through a series of experiments. First, we will present a series of analyses of resisting ability and resisting flexibility of ITN. Following that we demonstrate the strong classification performance of ITN on a series of datasets, including MNIST, affNIST, SVHN, CIFAR-10 and a more challenging dataset, miniImageNet."], "referred_text_chunks": ["In Table~, we present the results of ITN on the MNIST dataset with different thresholds to explore the relationship between the samples quality and thresholds. Not surprisingly, we observe that the performance of ITN drops when increasing the threshold. By visualizing the samples generated by different thresholds, it is clear that the performance drops due to the decrease in the quality of generated samples (Fig.~)."]}, "8": {"question": "Which method has the lower testing error on the MNIST task?", "referred_figures_indices": [5], "response": "{'Image': 1, 'Text': 0, 'Rationale': \"Image 1 provides specific testing errors on the MNIST dataset for ITN compared to other methods, highlighting the lower error rate for ITN with data augmentation. Text 0 discusses ITN's performance improvement, noting its testing error reduction on MNIST, which aligns with the question's focus on performance comparison.\", 'Answer': 'ITN (B-CNN) with data augmentation has the lower testing error on the MNIST task, at 0.42%.'}", "all_figures_names": ["1805.06447v3-Table1-1.png", "1805.06447v3-Table4-1.png", "1805.06447v3-Figure1-1.png", "1805.06447v3-Table2-1.png", "1805.06447v3-Figure4-1.png", "1805.06447v3-Table8-1.png", "1805.06447v3-Figure2-1.png", "1805.06447v3-Table5-1.png"], "referred_figures_names": ["1805.06447v3-Table8-1.png"], "answer": "ITN", "figure_type": "table", "content_type": "table", "text_chunks": ["Comparing against ITN-V1 (B-CNN), ITN-V2 (B-CNN) reduces the testing errors by 6.23\\% on MNIST dataset with random DDT + ST type of variations. Additionally, it reduces the testing errors by 6.61\\% on CIFAR-10 dataset. More importantly, the performance of ITN does not degrade when the model has transformation functions that do not match the type of variations in the testing data, e.g. ITN-V2 (B-CNN) on testing data with DDT only.", "MNIST is a benchmark dataset that includes 55000, 5000 and 10000 handwritten digits in the training, validation and testing set, respectively. The affNIST dataset is a variant from the MNIST dataset and it is built by applying various affine transformations to the samples in MNIST dataset. To be consistent with the MNIST dataset and for the following experiments, we reduce the size of training, validation, and testing set of the affNIST dataset to 55000, 5000 and 10000, respectively.", "The reduced training set is built by randomly sampling data from the MNIST training data while keeping the number of data per class identical. As shown in Table~, our method has better results on all tasks, which are consistent with our previous results. The constantly superior performance of ITN over data augmentation indicates its effectiveness.", "In this case, the training objective will be correctly classifying original and transformed positive samples. We evaluate the performance of this baseline on MNIST dataset, affNIST dataset, and TMTA task. We observe from Table~ that ITN-NG has the same performance as ITN on simple dataset while it has lower performance than ITN when the task becomes harder.", "Thus, we implicitly increase the variations between the training and testing data by reducing the number of samples in the training data. The purpose of this experiment is to demonstrate the potential of ITN to resist unseen variations from a different perspective. \\vspace{-10pt We design a new experiment where the training set is the MNIST dataset with only 0.1\\%, 1\\%, 10\\% and 25\\% of the whole training set while the testing set is the entire MNIST testing set.", "It is easy to observe that the performance improvements on MNIST and affNIST datasets are marginal compared to experiments in Section 4.1. The most likely explanation for this observation is that the training samples in MNIST and affNIST represent the data distribution very well. In this case, providing more samples will not significantly boost the performance of the classifier.", "Then, we include both STs and DDTs in our model and verify the performance again. Let MNIST dataset be the training set of the experiment while the testing set is the MNIST dataset with different types of transformation applied. We introduce two types of testing sets in this section.", "Resisting Large Data Variations Training-MNIST-Testing-affNIST (TMTA) We first design a challenging classification task TMTA to verify the ability to resist large data variations of ITN. The training set in this experiment is the MNIST training data while the testing set is the affNIST testing data."], "referred_text_chunks": ["Comparing against ITN-V1 (B-CNN), ITN-V2 (B-CNN) reduces the testing errors by 6.23\\% on MNIST dataset with random DDT + ST type of variations. Additionally, it reduces the testing errors by 6.61\\% on CIFAR-10 dataset. More importantly, the performance of ITN does not degrade when the model has transformation functions that do not match the type of variations in the testing data, e.g. ITN-V2 (B-CNN) on testing data with DDT only."]}, "9": {"question": "Which generative model generates more accurate and realistic samples on the MNIST dataset, AC-GATN or ITN?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 0, 'Rationale': 'Image 4 directly compares the testing errors of AC-GATN and ITN over different epochs on the MNIST dataset, which is central to evaluating their performance. Text chunk 0 discusses the relative performance and quality of samples generated by AC-GATN and ITN.', 'Answer': 'ITN generates more accurate and realistic samples on the MNIST dataset compared to AC-GATN.'}", "all_figures_names": ["1805.06447v3-Table4-1.png", "1805.06447v3-Figure1-1.png", "1805.06447v3-Table5-1.png", "1805.06447v3-Table7-1.png", "1805.06447v3-Figure3-1.png", "1805.06447v3-Table3-1.png", "1805.06447v3-Figure5-1.png", "1805.06447v3-Table6-1.png"], "referred_figures_names": ["1805.06447v3-Figure3-1.png"], "answer": "ITN generates more accurate and realistic samples on the MNIST dataset compared to AC-GATN.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["The results shown in Figure illustrate that under our framework, using INs as the generative model achieve better performance than using GANs. By visualizing the generated samples from AC-GATN and ITN (shown in Figure ), both AC-GATN and ITN generate clear and sharp images. However, samples generated by AC-GATN have lower quality on average in terms of human standards as some of them are misleading and inaccurate, i.e. the number 3 is close to number 6 in epoch 100.", "We term this alternate model {\\em auxiliary classifier generative adversarial transformation networks (AC-GATNs) and qualitatively compare ITN and AC-GATN in Section 4.3, demonstrating the benefit of choosing ITNs for building robust image classifiers. The main contribution of the paper is a principled approach that endows classifiers with the ability to resist larger variations between training and testing data in an intelligent and efficient manner. ITN enhances the classifier by generating additional training samples, including self-generated pseudo-negative samples and transformed input samples.", "These lower quality samples will mislead the classifier and lead to a performance decrease. Consequently, we choose INs rather than GANs in our approach. We will provide more comparisons of AC-GATN and ITN on other datasets in the supplementary material.", "AC-GAN can generate class-dependent samples, which is required in our framework. The loss function of AC-GAN is replaced with the loss function from WGAN-GP to directly compare it with ITN. All experimental settings are the same for a fair comparison.", "Comparing against ITN-V1 (B-CNN), ITN-V2 (B-CNN) reduces the testing errors by 6.23\\% on MNIST dataset with random DDT + ST type of variations. Additionally, it reduces the testing errors by 6.61\\% on CIFAR-10 dataset. More importantly, the performance of ITN does not degrade when the model has transformation functions that do not match the type of variations in the testing data, e.g. ITN-V2 (B-CNN) on testing data with DDT only.", "In this case, the training objective will be correctly classifying original and transformed positive samples. We evaluate the performance of this baseline on MNIST dataset, affNIST dataset, and TMTA task. We observe from Table~ that ITN-NG has the same performance as ITN on simple dataset while it has lower performance than ITN when the task becomes harder.", "Choice w.r.t generative models \\ It is essential to validate our choice of generative models. We implement our framework by using AC-GAN as the generative model and name it AC-GATN.", "Then the transformed images are randomly placed in a 42 \\times 42 images. Lastly, we replicate the same experiment on the CIFAR-10 dataset. \\vspace{-10pt Agnostic to different transformation functions We observe from Table~ that ITN-V1 (B-CNN) improves the discriminator performance by 4.35\\% from WINN (B-CNN) on MNIST with random DDT transformations and by 21.81\\% on CIFAR-10 dataset."], "referred_text_chunks": ["The results shown in Figure illustrate that under our framework, using INs as the generative model achieve better performance than using GANs. By visualizing the generated samples from AC-GATN and ITN (shown in Figure ), both AC-GATN and ITN generate clear and sharp images. However, samples generated by AC-GATN have lower quality on average in terms of human standards as some of them are misleading and inaccurate, i.e. the number 3 is close to number 6 in epoch 100."]}, "10": {"question": "Which method performs best on the TMTA task and how much does data augmentation contribute to its performance?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 3, 'Rationale': \"Image 6 provides direct testing errors of various methods on the TMTA task, showing ITN with and without data augmentation, thus directly relating to the question. Text 3 describes the TMTA task and mentions ITN's effectiveness in handling large data variations, related to the method's performance on TMTA.\", 'Answer': \"ITN with data augmentation performs best on the TMTA task. Data augmentation significantly contributes by reducing the error from 31.67% to 21.31%.\"}", "all_figures_names": ["1805.06447v3-Figure4-1.png", "1805.06447v3-Figure2-1.png", "1805.06447v3-Table3-1.png", "1805.06447v3-Table7-1.png", "1805.06447v3-Table2-1.png", "1805.06447v3-Table8-1.png", "1805.06447v3-Table1-1.png", "1805.06447v3-Figure1-1.png"], "referred_figures_names": ["1805.06447v3-Table1-1.png"], "answer": "The ITN (B-CNN) method with data augmentation (DA) performs best on the TMTA task, achieving a testing error of 21.31%. Data augmentation contributes significantly to its performance, as the ITN (B-CNN) method without DA has a higher testing error of 31.67%.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["However, data augmentation is faster than ITN, especially when the dataset becomes large and complicated. Note that ITN works well with data augmentation as ITN (w/DA) produces the best performance. This observation adds more practical value to ITN because they can be jointly applied without contradictions.", "Comparing ITN against data augmentation, they have their unique advantages and disadvantages. ITN outperforms data augmentation given all results reported in previous sections. It is also a well-formulated model compared to an exhaustively searching method like data augmentation.", "The reduced training set is built by randomly sampling data from the MNIST training data while keeping the number of data per class identical. As shown in Table~, our method has better results on all tasks, which are consistent with our previous results. The constantly superior performance of ITN over data augmentation indicates its effectiveness.", "Resisting Large Data Variations Training-MNIST-Testing-affNIST (TMTA) We first design a challenging classification task TMTA to verify the ability to resist large data variations of ITN. The training set in this experiment is the MNIST training data while the testing set is the affNIST testing data.", "In this case, the training objective will be correctly classifying original and transformed positive samples. We evaluate the performance of this baseline on MNIST dataset, affNIST dataset, and TMTA task. We observe from Table~ that ITN-NG has the same performance as ITN on simple dataset while it has lower performance than ITN when the task becomes harder.", "\\vspace{-10pt Conclusion \\vspace{-2mm We proposed a principled and efficient approach that endows the classifiers with the ability to resist larger variations between training and testing data. Our method, ITN, strengthens the classifiers by generating unseen variations with various learned transformations. Experimental results show consistent performance improvements not only on the classification tasks but also on the other challenging classification tasks, such as TMTA.", "As shown in Table , ITN outperforms other methods with standard data augmentation. Limited training data Another way to evaluate the ability of resisting variations is to reduce the number of training samples. Intuitively, data variations between training and testing sets become larger when the number of testing data remains the same while the number of samples in the training set shrinks.", "The most widely adopted approach is data augmentation which applies pre-defined transformations to the training data. Nevertheless, this method is lacking since the user has to specify by-hand the types and extent of transformations applied to the training set. Better methods have been proposed by building connections between generative models and discriminative classifiers~."], "referred_text_chunks": ["However, data augmentation is faster than ITN, especially when the dataset becomes large and complicated. Note that ITN works well with data augmentation as ITN (w/DA) produces the best performance. This observation adds more practical value to ITN because they can be jointly applied without contradictions."]}}