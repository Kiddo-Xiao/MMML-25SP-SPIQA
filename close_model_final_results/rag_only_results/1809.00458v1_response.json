{"0": {"question": "Which dataset shows a higher variance in F1 score with increasing buffer size?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 directly shows the F1 score variance with changing buffer size for the datasets, and Text 0 discusses variance and accuracy related to buffer size.', 'Answer': 'ENRON shows a higher variance in F1 score with increasing buffer size.'}", "all_figures_names": ["1809.00458v1-TableIII-1.png", "1809.00458v1-Figure14-1.png", "1809.00458v1-Figure13-1.png", "1809.00458v1-Figure10-1.png", "1809.00458v1-Figure3-1.png", "1809.00458v1-Figure12-1.png", "1809.00458v1-Figure15-1.png", "1809.00458v1-Figure5-1.png"], "referred_figures_names": ["1809.00458v1-Figure5-1.png"], "answer": "ENRON", "figure_type": "plot", "content_type": "figure", "text_chunks": ["In Fig.~, we use NETFLIX and ENRON to evaluate the goodness of the function by comparing the trend of the variance and the estimation accuracy. By varying the buffer size r, Fig.~ reports the estimated variance (right side y axis) based on the variance function in Section~ as well as the F_1 score (left side y axis) of the corresponding \\gbkmv sketch with buffer size r. Fig.~(a) shows that the best buffer size for variance estimation (prefer small value) is around 400, while the \\gbkmv method achieves the best F_1 score (prefer large value) with buffer size around 380. They respectively become 220 and 230 in Fig.~(b).", "This suggests that our variance estimation function is quite reliable to identify a good buffer size. In the following experiments, \\gbkmv method will use buffer size suggested by this system, instead of manually tuning. We also compare the performance of \\kmv, \\gkmv, and \\gbkmv methods in Fig.~ to evaluate the effectiveness of using global threshold and the buffer on 7 datasets.", "In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score.", "Next, we compare the variance of \\hat{C and \\hat{C'. \\subsubsection{Optimal Buffer Size r In this part, we show how to find optimal buffer size r by analysing the variance for \\gbkmv method.", "\\gbkmv outperforms two exact algorithm by a big margin, especially when the record size is large, with a decent accuracy (i.e., with F_1 score and recall always larger than 0.8 and 0.9 under all settings). Discussion Summary In the accuracy comparison between \\gbkmv and \\lshe, it is remarkable to see that the accuracy (i.e., F_1 score) is very low on some datasets.", "We also plot the distribution of accuracy (i.e., min, max and avgerage value) to compare our \\gbkmv method with \\lshe in Fig.~. Meanwhile, by changing the similarity threshold, F_1 score is reported in Fig.~ on dataset NETFLIX and COD. We can see that with various similarity thresholds, our \\gbkmv always outperforms \\lshe. We also evaluate the space-accuracy trade-offs on synthetic datasets with 100K records in Fig.~ where the record size and the element frequency follow the zipf distribution.", "Then we assign 8, 16, 24,... to r and calculate the f(r, b, \\alpha_1, \\alpha_2). In this way, we can give a good guide to the choice of r. \\subsubsection{GB-KMV Sketch provides Better Accuracy than LSH-E Method In Section~, we have shown that the variance of \\lshe estimator(Equation~) is larger than that of MinHash \\lsh estimator(Equation~). Note that \\gkmv sketch is a special case of \\gbkmv sketch when the buffer size r = 0.", "Since the variance is concerned with the union size D_{\\cup, the intersection size D_{\\cap and the signature size k, we first calculate these three formulas, then compute the variance. Consider the two sets X_j, X_l from dataset \\mathcal{S with \\gbkmv sketch \\mathcal{H_{X_j\\cup \\mathcal{L_{X_j and \\mathcal{H_{X_j\\cup \\mathcal{L_{X_j respectively. The element e_i is associated with frequency f_i, and the probability of element e_i appearing in record X_j is Pr[h(e_i)\\in \\mathcal{L_{X_j] = \\frac{f_i{Nx_j."], "referred_text_chunks": ["In Fig.~, we use NETFLIX and ENRON to evaluate the goodness of the function by comparing the trend of the variance and the estimation accuracy. By varying the buffer size r, Fig.~ reports the estimated variance (right side y axis) based on the variance function in Section~ as well as the F_1 score (left side y axis) of the corresponding \\gbkmv sketch with buffer size r. Fig.~(a) shows that the best buffer size for variance estimation (prefer small value) is around 400, while the \\gbkmv method achieves the best F_1 score (prefer large value) with buffer size around 380. They respectively become 220 and 230 in Fig.~(b)."]}, "1": {"question": "Which algorithm performs best on the ENRON dataset?", "referred_figures_indices": [2], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 shows F-1 score versus time for the ENRON dataset, directly comparing GB-KMV and LSH-E. Text chunk [0] discusses the time-accuracy trade-offs for these methods, mentioning GB-KMV is significantly faster while maintaining similar F1 scores.', 'Answer': 'GB-KMV performs best on the ENRON dataset as it is much faster than LSH-E with similar accuracy.'}", "all_figures_names": ["1809.00458v1-Figure14-1.png", "1809.00458v1-Figure16-1.png", "1809.00458v1-Figure15-1.png", "1809.00458v1-Figure4-1.png", "1809.00458v1-Figure18-1.png", "1809.00458v1-Figure17-1.png", "1809.00458v1-Figure11-1.png", "1809.00458v1-Figure13-1.png"], "referred_figures_names": ["1809.00458v1-Figure15-1.png"], "answer": "GB-KMV", "figure_type": "plot", "content_type": "figure", "text_chunks": ["In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score.", "In Fig.~, we use NETFLIX and ENRON to evaluate the goodness of the function by comparing the trend of the variance and the estimation accuracy. By varying the buffer size r, Fig.~ reports the estimated variance (right side y axis) based on the variance function in Section~ as well as the F_1 score (left side y axis) of the corresponding \\gbkmv sketch with buffer size r. Fig.~(a) shows that the best buffer size for variance estimation (prefer small value) is around 400, while the \\gbkmv method achieves the best F_1 score (prefer large value) with buffer size around 380. They respectively become 220 and 230 in Fig.~(b).", "By choosing an optimal buffer size r in~, it can guarantee that the performance of \\gbkmv is not worse than \\gkmv. Below, we show that \\gkmv outperforms MinHash \\lsh in terms of estimate accuracy. where F_1 = f_{n^2, F_2 = -(f_{n^2)^2, F_3 = -f_{n^2 and f_{n^2 = \\sum_{i=1^{n \\frac{f_i^2{N^2.", "In Theorem~, we have theoretically shown that when the dataset follows uniform distribution (i.e., \\alpha_1=0 and \\alpha_2=0), our \\gbkmv method can outperform the \\lshe method. In this part, we experimentally illustrate the performance on dataset with uniform distribution. We generate 100k records where the record size is uniformly distributed between 10 and 5000, and each element is randomly chosen from 100,000 distinct elements.", "For partition-based method, in~, Arasu {\\emph et al. devise a two-level algorithm which uses partition and enumeration techniques to search for exact similar records. Deng {et al. in~ develop a partition-based method which can effectively prune the candidate size at the cost higher filtering cost.", "\\gbkmv outperforms two exact algorithm by a big margin, especially when the record size is large, with a decent accuracy (i.e., with F_1 score and recall always larger than 0.8 and 0.9 under all settings). Discussion Summary In the accuracy comparison between \\gbkmv and \\lshe, it is remarkable to see that the accuracy (i.e., F_1 score) is very low on some datasets.", "However, our initial experiments suggest that this will lead to poor accuracy because of the summation of the intersection size estimations. In Theorem~ (Section~), our theoretical analysis suggests that the combination of estimated results are very likely to make the overall accuracy worse. To avoid combining multiple estimation results, we use a bitmap buffer with size r for each record to exactly keep track of the r most frequent elements, denoted by \\mathcal{E_H.", "We give some discussions as follows. First we should point out that in ~, the accuracy of \\lshe is only evaluated on only one dataset COD, in which both our \\gbkmv method and \\lshe can achieve decent accuracy performance with F_1 score above 0.5. As mentioned in ~, the \\lshe method first transforms the containment similarity to Jaccard similarity, then in order to make use of the efficient index techniques, \\lshe partitions the dataset and uses the upper bound to approximate the record size in each partition."], "referred_text_chunks": ["In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score."]}, "2": {"question": "Which of the two algorithms (GB-KMV or LSH-E) is the fastest on average across all datasets?", "referred_figures_indices": [2], "response": "{'Image': 6, 'Text': 1, 'Rationale': 'Image 6 shows the running time of GB-KMV versus LSH-E across various datasets, directly illustrating their speeds. Text chunk 1 highlights that GB-KMV is significantly faster than LSH-E across datasets, providing more detailed context about the performance.', 'Answer': 'GB-KMV is the fastest on average across all datasets.'}", "all_figures_names": ["1809.00458v1-Figure5-1.png", "1809.00458v1-Figure8-1.png", "1809.00458v1-Figure14-1.png", "1809.00458v1-Figure17-1.png", "1809.00458v1-Figure2-1.png", "1809.00458v1-Figure1-1.png", "1809.00458v1-Figure18-1.png", "1809.00458v1-TableIII-1.png"], "referred_figures_names": ["1809.00458v1-Figure14-1.png"], "answer": "LSH-E", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Then we assign 8, 16, 24,... to r and calculate the f(r, b, \\alpha_1, \\alpha_2). In this way, we can give a good guide to the choice of r. \\subsubsection{GB-KMV Sketch provides Better Accuracy than LSH-E Method In Section~, we have shown that the variance of \\lshe estimator(Equation~) is larger than that of MinHash \\lsh estimator(Equation~). Note that \\gkmv sketch is a special case of \\gbkmv sketch when the buffer size r = 0.", "In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score.", "\\gbkmv outperforms two exact algorithm by a big margin, especially when the record size is large, with a decent accuracy (i.e., with F_1 score and recall always larger than 0.8 and 0.9 under all settings). Discussion Summary In the accuracy comparison between \\gbkmv and \\lshe, it is remarkable to see that the accuracy (i.e., F_1 score) is very low on some datasets.", "Then we apply \\gkmv technique to the remaining elements, resulting in a new augmented sketch, namely \\gbkmv. Now we can estimate |Q \\cap X| by combining the intersection of their bitmap buffers (exact solution) and \\kmv sketches (estimated solution). As shown in Fig.~, suppose we have \\mathcal{E_{H = \\{e_1, e_2 \\ and the global threshold for hash value is \\tau = 0.5, then the sketch of each record consists of two parts \\mathcal{L_{H and \\mathcal{L_{GKMV; that is, for each record we use bitmap to keep the elements corresponding to high-frequency elements \\mathcal{E_H = \\{e_1, e_2 \\, then we store the left elements with hash value less than \\tau = 0.5.", "Fig.~(a) illustrates the time-accuracy trade-off of \\gbkmv and \\lshe on the synthetic dataset with 100K records. It is reported that, to achieve the same accuracy (F_1 score), \\gbkmv consumes much less time than \\lshe. \\noindent Comparison with Exact Algorithms.", "We also give the space usage of the two methods on each dataset in Table~. The space usage of \\gbkmv is 10{\\% as mentioned in Settings. For \\lshe in some dataset, the space is over 100{\\% because there are many records with size less than the number of hash functions 256. Supplementary Experiment \\noindent Evaluation on Uniform Distribution.", "\\vspace{1mm \\noindent Comparison with \\lshe. Through theoretical analysis, we show that the performance (i.e., the variance of the estimator) of \\gbkmv can always outperform that of \\lshe in Theorem~ (Section~). Implementation of GB-KMV In this section, we introduce the technique details of our proposed \\gbkmv method.", "It is shown that the use of new KMV estimator with global threshold (i.e., Equation~) can significantly improve the search accuracy. By using a buffer whose size is suggested by the system, we can further enhance the performance under the same space budget. In the following experiments, we use \\gbkmv for the performance comparison with the state-of-the-art technique \\lshe."], "referred_text_chunks": ["Then we assign 8, 16, 24,... to r and calculate the f(r, b, \\alpha_1, \\alpha_2). In this way, we can give a good guide to the choice of r. \\subsubsection{GB-KMV Sketch provides Better Accuracy than LSH-E Method In Section~, we have shown that the variance of \\lshe estimator(Equation~) is larger than that of MinHash \\lsh estimator(Equation~). Note that \\gkmv sketch is a special case of \\gbkmv sketch when the buffer size r = 0."]}, "3": {"question": "Which method performs better, GB-KMV or LSH-E, and how does the performance change with different values of eleFreq and recSize?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 5, 'Rationale': \"Image 3 directly compares the performance of GB-KMV and LSH-E with varying eleFreq and recSize, which is directly related to the question. Text 5 offers relevant information on the accuracy of GB-KMV compared to exact algorithms, emphasizing its F1 performance.\", 'Answer': \"GB-KMV performs better than LSH-E. As eleFreq increases, GB-KMV's F1 score improves, while LSH-E shows minimal changes. With increasing recSize, GB-KMV maintains better accuracy compared to LSH-E, which declines.\"}", "all_figures_names": ["1809.00458v1-Figure15-1.png", "1809.00458v1-Figure12-1.png", "1809.00458v1-Figure7-1.png", "1809.00458v1-Figure16-1.png", "1809.00458v1-Figure10-1.png", "1809.00458v1-Figure17-1.png", "1809.00458v1-Figure14-1.png", "1809.00458v1-Figure9-1.png"], "referred_figures_names": ["1809.00458v1-Figure16-1.png"], "answer": "GB-KMV generally performs better than LSH-E, as indicated by the higher F1 scores across the range of eleFreq and recSize values. For both methods, the F1 score tends to decrease as recSize increases, while the impact of eleFreq varies depending on the method.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Then we assign 8, 16, 24,... to r and calculate the f(r, b, \\alpha_1, \\alpha_2). In this way, we can give a good guide to the choice of r. \\subsubsection{GB-KMV Sketch provides Better Accuracy than LSH-E Method In Section~, we have shown that the variance of \\lshe estimator(Equation~) is larger than that of MinHash \\lsh estimator(Equation~). Note that \\gkmv sketch is a special case of \\gbkmv sketch when the buffer size r = 0.", "\\vspace{1mm \\noindent Comparison with \\lshe. Through theoretical analysis, we show that the performance (i.e., the variance of the estimator) of \\gbkmv can always outperform that of \\lshe in Theorem~ (Section~). Implementation of GB-KMV In this section, we introduce the technique details of our proposed \\gbkmv method.", "In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score.", "Fig.~(a) illustrates the time-accuracy trade-off of \\gbkmv and \\lshe on the synthetic dataset with 100K records. It is reported that, to achieve the same accuracy (F_1 score), \\gbkmv consumes much less time than \\lshe. \\noindent Comparison with Exact Algorithms.", "For \\lshe method, we use the same default values in~ where the signature size of each record is 256 and the number of partition is 32. By varying the number of hash functions, we change the space used in \\lshe. \\vspace{-2mm Performance Tuning As shown in Section~, we can use the variance estimation function to identify a good buffer size r for \\gbkmv method based on the skewness of record size and element frequency, as well as the space budget.", "\\gbkmv outperforms two exact algorithm by a big margin, especially when the record size is large, with a decent accuracy (i.e., with F_1 score and recall always larger than 0.8 and 0.9 under all settings). Discussion Summary In the accuracy comparison between \\gbkmv and \\lshe, it is remarkable to see that the accuracy (i.e., F_1 score) is very low on some datasets.", "We give some discussions as follows. First we should point out that in ~, the accuracy of \\lshe is only evaluated on only one dataset COD, in which both our \\gbkmv method and \\lshe can achieve decent accuracy performance with F_1 score above 0.5. As mentioned in ~, the \\lshe method first transforms the containment similarity to Jaccard similarity, then in order to make use of the efficient index techniques, \\lshe partitions the dataset and uses the upper bound to approximate the record size in each partition.", "We use F_1 score, F_{0.5 score, precision and recall to measure the accuracy. By changing the number of hash functions, we tune the space used in \\lshe. It is reported that our \\gbkmv can beat the \\lshe in terms of space-accuracy trade-off with a big margin under all settings."], "referred_text_chunks": ["Then we assign 8, 16, 24,... to r and calculate the f(r, b, \\alpha_1, \\alpha_2). In this way, we can give a good guide to the choice of r. \\subsubsection{GB-KMV Sketch provides Better Accuracy than LSH-E Method In Section~, we have shown that the variance of \\lshe estimator(Equation~) is larger than that of MinHash \\lsh estimator(Equation~). Note that \\gkmv sketch is a special case of \\gbkmv sketch when the buffer size r = 0."]}, "4": {"question": "What is the difference between the Jaccard similarity and the containment similarity?", "referred_figures_indices": [5], "response": "{\n    'Image': 5, \n    'Text': 0, \n    'Rationale': 'Image 5 provides definitions of Jaccard and containment similarity, directly relevant to understanding their differences. Text chunk 0 explains the difference between containment and Jaccard similarity and describes containment similarity in detail.', \n    'Answer': 'The Jaccard similarity is symmetric, considering the intersection over the union of two sets, whereas containment similarity is asymmetric, focusing on the intersection over the size of one specific set (usually the query). Containment similarity gives more weight to the size of the query, making it useful for specific applications like record matching.'\n}", "all_figures_names": ["1809.00458v1-Figure11-1.png", "1809.00458v1-TableIII-1.png", "1809.00458v1-Figure18-1.png", "1809.00458v1-Figure19-1.png", "1809.00458v1-Figure17-1.png", "1809.00458v1-TableI-1.png", "1809.00458v1-Figure15-1.png", "1809.00458v1-Figure12-1.png"], "referred_figures_names": ["1809.00458v1-TableI-1.png"], "answer": "The Jaccard similarity measures the overlap between two sets, while the containment similarity measures how much one set is contained within another set.", "figure_type": "table", "content_type": "table", "text_chunks": ["Containment similarity (a.k.a, Jaccard containment similarity) is one of the representative asymmetric set similarity functions, where the similarity between two records X and Y is defined as f(X,Y) = \\frac{|X \\cap Y|{|X| in which |X \\cap Y | and |X| are intersection size of X and Y and the size of X, respectively. Compared with symmetric similarity such as Jaccard similarity, containment similarity gives special consideration on the query size, which makes it more suitable in some applications. As shown in~, containment similarity is useful in record matching application.", "It is easy to see that Jaccard similarity is symmetric while containment similarity is asymmetric. In this paper, we focus on the problem of containment similarity search which is to look up a set of records whose containment similarity towards a given query record is not smaller than a given threshold. The formal definition is as follows.", "\\vspace{2mm \\noindent Similarity Transformation. Given a record X with size x = |X|, a query Q with size q = |Q|, containment similarity t = C(Q,X) and Jaccard similarity s = J(Q,X). The transformation back and forth are as follows.", "i \\leq m) is a set of elements from \\mathcal{E. Before giving the definition of containment similarity, we first introduce the Jaccard similarity. \\end{definition Similar to the Jaccard similarity, the containment similarity (a.k.a Jaccard containment similarity) is defined as follows. \\end{definition Note that by replacing the union size |X \\cup Y| in Equation~ with size |X|, we get the containment similarity.", "Given the containment similarity search threshold as t^{* for the query q, we may come up with its corresponding Jaccard similarity threshold s^{* by Equation~. A straightforward solution is to apply the existing approximate Jaccard similarity search technique for each individual record X \\in \\mathcal{D with the Jaccard similarity threshold s^{* (e.g., compute Jaccard similarity between the query Q and a set X based on their MinHash signatures). In order to take advantages of the efficient indexing techniques (e.g., \\lsh forest~), \\lshe will partition the dataset \\mathcal{S. \\vspace{2mm \\noindent Data Partition. By partitioning the dataset \\mathcal{S according to the record size, \\lshe can replace x in Equation~ with its upper bound u (i.e., the largest record size in the partition) as an approximation.", "For Jaccard similarity, MinHash~ is used for approximate similarity search. Asymmetric minwise hashing is a technique for approximate containment similarity search~. This method makes use of vector transformation by padding some values into sets, which makes all sets in the index have same cardinality as the largest set.", "Assuming no hash collision, Broder showed that the Jaccard similarity of X and Y is the probability of two minimum hash values being equal: Pr[h_{min(X) = h_{min(Y)] = J(X, Y). Applying such k different independent hash functions h_1, h_2,..., h_k to a record X(Y, resp.), the MinHash signature of X(Y, resp.)", "Analysis One of the \\lshe's advantages is that it converts the containment similarity problem to Jaccard similarity search problem which can be solved by the mature and efficient MinHash \\lsh method. Also, \\lshe carefully considers the record size distribution and partitions the records by record size. In this sense, we say \\lshe is a data-dependent method and it is reported that \\lshe significantly outperforms existing asymmetric \\lsh based solutions~ (i.e., data-independent methods) as \\lshe can exploit the information of data distribution by partitioning the dataset."], "referred_text_chunks": ["Containment similarity (a.k.a, Jaccard containment similarity) is one of the representative asymmetric set similarity functions, where the similarity between two records X and Y is defined as f(X,Y) = \\frac{|X \\cap Y|{|X| in which |X \\cap Y | and |X| are intersection size of X and Y and the size of X, respectively. Compared with symmetric similarity such as Jaccard similarity, containment similarity gives special consideration on the query size, which makes it more suitable in some applications. As shown in~, containment similarity is useful in record matching application."]}, "5": {"question": "Which dataset requires the most storage space when using the LSH-E method?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'The question is about storage space usage for different datasets using the LSH-E method. Image 0 directly presents space utilization percentages for various datasets, which is essential for answering the question. Text 0 discusses space usage of the LSH-E method in detail, relevant to understanding space variations across datasets using LSH-E.', 'Answer': 'The Reuters dataset requires the most storage space when using the LSH-E method (329%).'}", "all_figures_names": ["1809.00458v1-TableIII-1.png", "1809.00458v1-Figure16-1.png", "1809.00458v1-Figure17-1.png", "1809.00458v1-TableII-1.png", "1809.00458v1-Figure7-1.png", "1809.00458v1-Figure1-1.png", "1809.00458v1-Figure18-1.png", "1809.00458v1-Figure19-1.png"], "referred_figures_names": ["1809.00458v1-TableIII-1.png"], "answer": "REUTERS", "figure_type": "table", "content_type": "table", "text_chunks": ["We also give the space usage of the two methods on each dataset in Table~. The space usage of \\gbkmv is 10{\\% as mentioned in Settings. For \\lshe in some dataset, the space is over 100{\\% because there are many records with size less than the number of hash functions 256. Supplementary Experiment \\noindent Evaluation on Uniform Distribution.", "which can favour recall but result in extra false positives as analysed in section ~. However, the \\lshe method does not provide a partition scheme associated with different data distribution, and the algorithm setting (e.g., 256 hash functions and 32 partitions) can not perform well in some dataset. \\vspace{-2mm Related Work", "Analysis One of the \\lshe's advantages is that it converts the containment similarity problem to Jaccard similarity search problem which can be solved by the mature and efficient MinHash \\lsh method. Also, \\lshe carefully considers the record size distribution and partitions the records by record size. In this sense, we say \\lshe is a data-dependent method and it is reported that \\lshe significantly outperforms existing asymmetric \\lsh based solutions~ (i.e., data-independent methods) as \\lshe can exploit the information of data distribution by partitioning the dataset.", "For \\lshe method, we use the same default values in~ where the signature size of each record is 256 and the number of partition is 32. By varying the number of hash functions, we change the space used in \\lshe. \\vspace{-2mm Performance Tuning As shown in Section~, we can use the variance estimation function to identify a good buffer size r for \\gbkmv method based on the skewness of record size and element frequency, as well as the space budget.", "We give some discussions as follows. First we should point out that in ~, the accuracy of \\lshe is only evaluated on only one dataset COD, in which both our \\gbkmv method and \\lshe can achieve decent accuracy performance with F_1 score above 0.5. As mentioned in ~, the \\lshe method first transforms the containment similarity to Jaccard similarity, then in order to make use of the efficient index techniques, \\lshe partitions the dataset and uses the upper bound to approximate the record size in each partition.", "LSH Ensemble Method \\lsh Ensemble technique, \\lshe for short, is proposed by Zhu {\\emph et. al in~ to tackle the problem of approximate containment similarity search. The key idea is : (1) transform the containment similarity search to the well-studied Jaccard similarity search; and (2) partition the data by length and then apply the \\lsh forest~ technique for each individual partition.", "In this part, we compare the sketch construction time of \\gbkmv and \\lshe on different datasets under default settings. As expected, \\gbkmv uses much less sketch construction time than that of \\lshe since \\gbkmv sketch need only one hash function, while \\lshe needs multiple for a decent accuracy. Note that, for the internet scale dataset WDC, the index construction time for \\gbkmv is around 10 minutes, while for \\lshe it is above 60 minutes.", "Note that x_t(x_1, resp.) is the largest(smallest, resp.) set size and d is the distinct number of elements. Next we take into account the minHash \\lsh method."], "referred_text_chunks": ["We also give the space usage of the two methods on each dataset in Table~. The space usage of \\gbkmv is 10{\\% as mentioned in Settings. For \\lshe in some dataset, the space is over 100{\\% because there are many records with size less than the number of hash functions 256. Supplementary Experiment \\noindent Evaluation on Uniform Distribution."]}, "6": {"question": "Which algorithm performs better in terms of F1 score and precision when the space used is 5%?", "referred_figures_indices": [6], "response": "{'Image': 2, 'Text': 1, 'Rationale': 'Image 2 directly shows F1 score and precision versus space used, which allows for easy comparison at 5% space usage. Text 1 mentions performance metrics that support the content of the images by highlighting differences in F1 score and precision.', 'Answer': 'GB-KMV performs better in terms of F1 score and precision when the space used is 5%.'}", "all_figures_names": ["1809.00458v1-Figure4-1.png", "1809.00458v1-Figure3-1.png", "1809.00458v1-Figure7-1.png", "1809.00458v1-Figure17-1.png", "1809.00458v1-Figure15-1.png", "1809.00458v1-Figure16-1.png", "1809.00458v1-Figure12-1.png", "1809.00458v1-Figure11-1.png"], "referred_figures_names": ["1809.00458v1-Figure12-1.png"], "answer": "GB-KMV performs better in terms of F1 score and precision when the space used is 5%.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["We use F_1 score, F_{0.5 score, precision and recall to measure the accuracy. By changing the number of hash functions, we tune the space used in \\lshe. It is reported that our \\gbkmv can beat the \\lshe in terms of space-accuracy trade-off with a big margin under all settings.", "\\gbkmv outperforms two exact algorithm by a big margin, especially when the record size is large, with a decent accuracy (i.e., with F_1 score and recall always larger than 0.8 and 0.9 under all settings). Discussion Summary In the accuracy comparison between \\gbkmv and \\lshe, it is remarkable to see that the accuracy (i.e., F_1 score) is very low on some datasets.", "We give some discussions as follows. First we should point out that in ~, the accuracy of \\lshe is only evaluated on only one dataset COD, in which both our \\gbkmv method and \\lshe can achieve decent accuracy performance with F_1 score above 0.5. As mentioned in ~, the \\lshe method first transforms the containment similarity to Jaccard similarity, then in order to make use of the efficient index techniques, \\lshe partitions the dataset and uses the upper bound to approximate the record size in each partition.", "It is observed that the accuracy (F_1 score) improvement of \\lshe algorithm is very slow compared with \\gbkmv method. This is because the \\lshe method favours recall and the precision performance is quite poor even for a large number of hash functions, resulting in a poor F_1 score which considers both precision and recall. Sketch Construction Time", "The precision and recall to evaluate the experiment accuracy are Precision= \\frac{|T\\cap A|{|A| and Recall = \\frac{|T\\cap A|{|T| respectively. The F_\\alpha score is defined as follows. Note that we use F_{0.5 score because \\lshe favours recall in~. We use the datasets from Table~ to evaluate the performance of our algorithm, and we randomly choose 200 queries from the dataset.", "Fig.~(a) illustrates the time-accuracy trade-off of \\gbkmv and \\lshe on the synthetic dataset with 100K records. It is reported that, to achieve the same accuracy (F_1 score), \\gbkmv consumes much less time than \\lshe. \\noindent Comparison with Exact Algorithms.", "\\vspace{-2mm Space v.s. Accuracy An important measurement for sketch technique is the trade-off between the space and accuracy. We evaluate the space-accuracy trade-offs of \\gbkmv method and \\lshe method in Figs.~- by varying the space usage on five datasets NETFLIX, DELIC, COD, ENRON, REUTERS, WEBSPAM and WDC.", "In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score."], "referred_text_chunks": ["We use F_1 score, F_{0.5 score, precision and recall to measure the accuracy. By changing the number of hash functions, we tune the space used in \\lshe. It is reported that our \\gbkmv can beat the \\lshe in terms of space-accuracy trade-off with a big margin under all settings."]}, "7": {"question": "Which method has the highest F-1 score when space used is 10%?", "referred_figures_indices": [5], "response": "{'Image': 1, 'Text': 0, 'Rationale': 'Image 1 shows the F-1 score at different space usages, clearly indicating that GB-KMV achieves a higher F-1 score compared to LSH-E at 10% space usage. Text 0 discusses the space-accuracy trade-off, supporting how GB-KMV outperforms LSH-E.', 'Answer': 'GB-KMV has the highest F-1 score when space used is 10%.'}", "all_figures_names": ["1809.00458v1-Figure2-1.png", "1809.00458v1-Figure9-1.png", "1809.00458v1-Figure8-1.png", "1809.00458v1-TableIII-1.png", "1809.00458v1-Figure16-1.png", "1809.00458v1-Figure13-1.png", "1809.00458v1-TableII-1.png", "1809.00458v1-Figure4-1.png"], "referred_figures_names": ["1809.00458v1-Figure13-1.png"], "answer": "GB-KMV", "figure_type": "plot", "content_type": "figure", "text_chunks": ["We use F_1 score, F_{0.5 score, precision and recall to measure the accuracy. By changing the number of hash functions, we tune the space used in \\lshe. It is reported that our \\gbkmv can beat the \\lshe in terms of space-accuracy trade-off with a big margin under all settings.", "We also give the space usage of the two methods on each dataset in Table~. The space usage of \\gbkmv is 10{\\% as mentioned in Settings. For \\lshe in some dataset, the space is over 100{\\% because there are many records with size less than the number of hash functions 256. Supplementary Experiment \\noindent Evaluation on Uniform Distribution.", "We give some discussions as follows. First we should point out that in ~, the accuracy of \\lshe is only evaluated on only one dataset COD, in which both our \\gbkmv method and \\lshe can achieve decent accuracy performance with F_1 score above 0.5. As mentioned in ~, the \\lshe method first transforms the containment similarity to Jaccard similarity, then in order to make use of the efficient index techniques, \\lshe partitions the dataset and uses the upper bound to approximate the record size in each partition.", "It is observed that the accuracy (F_1 score) improvement of \\lshe algorithm is very slow compared with \\gbkmv method. This is because the \\lshe method favours recall and the precision performance is quite poor even for a large number of hash functions, resulting in a poor F_1 score which considers both precision and recall. Sketch Construction Time", "\\vspace{-2mm Space v.s. Accuracy An important measurement for sketch technique is the trade-off between the space and accuracy. We evaluate the space-accuracy trade-offs of \\gbkmv method and \\lshe method in Figs.~- by varying the space usage on five datasets NETFLIX, DELIC, COD, ENRON, REUTERS, WEBSPAM and WDC.", "Our empirical study shows that \\lshe cannot achieve a good trade-off between accuracy and space, compared with our proposed method. Our Approach In this section, we introduce an augmented \\kmv sketch technique to achieve better space-accuracy trade-off for approximate containment similarity search. Section~ briefly introduces the motivation and main technique of our method, namely \\gbkmv.", "In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score.", "We also plot the distribution of accuracy (i.e., min, max and avgerage value) to compare our \\gbkmv method with \\lshe in Fig.~. Meanwhile, by changing the similarity threshold, F_1 score is reported in Fig.~ on dataset NETFLIX and COD. We can see that with various similarity thresholds, our \\gbkmv always outperforms \\lshe. We also evaluate the space-accuracy trade-offs on synthetic datasets with 100K records in Fig.~ where the record size and the element frequency follow the zipf distribution."], "referred_text_chunks": ["We use F_1 score, F_{0.5 score, precision and recall to measure the accuracy. By changing the number of hash functions, we tune the space used in \\lshe. It is reported that our \\gbkmv can beat the \\lshe in terms of space-accuracy trade-off with a big margin under all settings."]}, "8": {"question": "Which dataset has the highest average record length?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 0, 'Rationale': 'Image 4 provides a table containing the average record length for each dataset, directly answering the question. Text chunk 0 mentions the characteristics of datasets, which is relevant to finding details like average record length.', 'Answer': 'The Netflix dataset has the highest average record length at 209.25.'}", "all_figures_names": ["1809.00458v1-Figure3-1.png", "1809.00458v1-Figure16-1.png", "1809.00458v1-Figure8-1.png", "1809.00458v1-Figure17-1.png", "1809.00458v1-TableII-1.png", "1809.00458v1-Figure13-1.png", "1809.00458v1-Figure12-1.png", "1809.00458v1-Figure11-1.png"], "referred_figures_names": ["1809.00458v1-TableII-1.png"], "answer": "CaOpenData", "figure_type": "table", "content_type": "table", "text_chunks": ["We also remove the stop words (e.g., \"the\") from the dataset. Table~ shows the detailed characteristics of the 7 datasets. Each dataset is illustrated with the dataset type, the representations of record, the number of records in the dataset, the average record length, and the number of distinct elements in the dataset.", "We also compare the running time of our proposed method \\gbkmv with two exact containment similarity search methods PPjoin*~ and FreqSet~. Experiments are conducted on the dataset WebSpam, which consists of 350,000 records and has the average length around 3,700. We partition the data into 5 groups based on their record size with boundaries increasing from 1,000 to 5,000. As expected, Fig.~(b) shows that the running time of our approximate algorithm is not sensitive to the growth of the record size because a fixed number of samples are used for a given budget.", "In Theorem~, we have theoretically shown that when the dataset follows uniform distribution (i.e., \\alpha_1=0 and \\alpha_2=0), our \\gbkmv method can outperform the \\lshe method. In this part, we experimentally illustrate the performance on dataset with uniform distribution. We generate 100k records where the record size is uniformly distributed between 10 and 5000, and each element is randomly chosen from 100,000 distinct elements.", "\\noindent Datasets. We deployed 7 real-life datasets with different data properties. Note that the records with size less than 10 are discarded from dataset.", "Let the element universe be \\mathcal{E = \\{e_1, e_2,..., e_n\\ and each element is associated with its frequency in dataset \\ds. Line~ calculates a buffer size r for all records based on the skewness of record size and elements as well as the space budget b in terms of elements.", "In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score.", "In this part, we compare the sketch construction time of \\gbkmv and \\lshe on different datasets under default settings. As expected, \\gbkmv uses much less sketch construction time than that of \\lshe since \\gbkmv sketch need only one hash function, while \\lshe needs multiple for a decent accuracy. Note that, for the internet scale dataset WDC, the index construction time for \\gbkmv is around 10 minutes, while for \\lshe it is above 60 minutes.", "We also report the power-law exponent \\alpha_1 and \\alpha_2 (skewness) of the record size and element frequency of the dataset respectively. Note that we make use of the framework in~ to quantify the power-law exponent. The dataset Canadian Open Data appears in the state-of-the-art algorithm \\lshe~ ."], "referred_text_chunks": ["We also remove the stop words (e.g., \"the\") from the dataset. Table~ shows the detailed characteristics of the 7 datasets. Each dataset is illustrated with the dataset type, the representations of record, the number of records in the dataset, the average record length, and the number of distinct elements in the dataset."]}, "9": {"question": "What is the relationship between the element-hash value pairs and the signature size?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 1, 'Rationale': 'Image 7 directly shows element-hash value pairs and their relationship with the signature size for each set, as indicated by \"ki\". Text chunk 1 explains how the hash values contribute to the signature size by probability, relating directly to the question.', 'Answer': 'The element-hash value pairs determine the signature size based on the probability that a hash value is included, which is proportional to the hash threshold \u03c4 and element frequency.'}", "all_figures_names": ["1809.00458v1-TableIII-1.png", "1809.00458v1-Figure3-1.png", "1809.00458v1-Figure13-1.png", "1809.00458v1-Figure19-1.png", "1809.00458v1-Figure16-1.png", "1809.00458v1-Figure8-1.png", "1809.00458v1-Figure5-1.png", "1809.00458v1-Figure2-1.png"], "referred_figures_names": ["1809.00458v1-Figure2-1.png"], "answer": "The element-hash value pairs are the elements of the signature, and the signature size is the number of element-hash value pairs in the signature.", "figure_type": "table", "content_type": "figure", "text_chunks": ["Given a hash value threshold \\tau, the \\gkmv signature size of set X_j is computed as k_j = \\tau(x_j - |\\mathcal{H_{X_j|). The total index space in \\gkmv sketch is \\sum_{j=1^{m k_j", "The probability that hash value h(e_i) is included in signature \\mathcal{L^{GKMV_{X_j is Pr[h(e_i)\\in \\mathcal{L^{GKMV_{X_j] = \\tau \\frac{f_i{Nx_j where f_i is the frequency of element e_i, and N=\\sum_{i=1^{nf_i is the total number of elements. The size of \\mathcal{L^{GKMV_{X_j can be computed by l_j = \\sum_{i=1^{n Pr[h(e_i)\\in \\mathcal{L^{GKMV_{X_j] = \\tau x_j", "Note that x_t(x_1, resp.) is the largest(smallest, resp.) set size and d is the distinct number of elements. Next we take into account the minHash \\lsh method.", "We remark that, as a by-product, the global threshold favours the record with large size because all elements with hash value smaller than \\tau are kept for each record. Below is an example on how to compute the containment similarity based on \\gkmv sketch. \\vspace{1mm \\noindent Correctness of \\gkmv sketch.", "\\vspace{1mm \\noindent (2) Impose a Global Threshold to KMV Sketch (\\gkmv) The above analysis on optimal \\kmv sketch suggests an equal size allocation strategy, that is, each record is associated with the same size signature. Intuitively we should assign more resources (i.e., signature size) to the records with large size because they are more likely to appear in the results.", "However, this benefit is offset by the fact that the the upper bound will bring extra false positives, in addition to the error from the MinHash technique. Below we theoretically analyse the performance of \\lshe by studying the expectation and variance of its estimator. Using the notations same as above, let s = J(Q, X) be the Jaccard similarity between query Q and set X and t = C(Q, X) be the containment similarity of Q in X. By Equation~, given the MinHash signature of query Q and X respectively, an unbiased estimator \\hat{s of Jaccard similarity s = J(Q, X) is the ratio of collisions in the signature, and the variance of \\hat{s is Var[\\hat{s] = \\frac{s(1-s){k where k is signature size of each record.", "However, due to the nature of the hash function used by \\kmv sketch, the hash value of an element is independent to its frequency; that is, all elements have the same opportunity contributing to the \\kmv sketch. One possible solution is to divide the elements into multiple disjoint groups according to their frequency (e.g., low-frequency and high-frequency ones), and then apply \\kmv sketch for each individual group. The intersection size between two records Q and X can be computed within each group and then sum up together.", "For \\lshe method, we use the same default values in~ where the signature size of each record is 256 and the number of partition is 32. By varying the number of hash functions, we change the space used in \\lshe. \\vspace{-2mm Performance Tuning As shown in Section~, we can use the variance estimation function to identify a good buffer size r for \\gbkmv method based on the skewness of record size and element frequency, as well as the space budget."], "referred_text_chunks": ["Given a hash value threshold \\tau, the \\gkmv signature size of set X_j is computed as k_j = \\tau(x_j - |\\mathcal{H_{X_j|). The total index space in \\gkmv sketch is \\sum_{j=1^{m k_j"]}, "10": {"question": "How does the running time of GB-KM vary with the F-1 score?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 1, 'Rationale': 'Image 6 directly shows the relationship between time and F-1 score for GB-KMV, while Text 1 discusses the time-accuracy trade-offs for GB-KMV and provides context on its performance across different datasets.', 'Answer': 'The running time of GB-KMV increases slightly with the F-1 score, but it remains significantly lower than LSH-E at similar accuracy levels.'}", "all_figures_names": ["1809.00458v1-Figure16-1.png", "1809.00458v1-Figure8-1.png", "1809.00458v1-Figure3-1.png", "1809.00458v1-Figure4-1.png", "1809.00458v1-Figure1-1.png", "1809.00458v1-Figure11-1.png", "1809.00458v1-Figure19-1.png", "1809.00458v1-Figure5-1.png"], "referred_figures_names": ["1809.00458v1-Figure19-1.png"], "answer": "The running time of GB-KM increases as the F-1 score increases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Fig.~(a) illustrates the time-accuracy trade-off of \\gbkmv and \\lshe on the synthetic dataset with 100K records. It is reported that, to achieve the same accuracy (F_1 score), \\gbkmv consumes much less time than \\lshe. \\noindent Comparison with Exact Algorithms.", "In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score.", "\\gbkmv outperforms two exact algorithm by a big margin, especially when the record size is large, with a decent accuracy (i.e., with F_1 score and recall always larger than 0.8 and 0.9 under all settings). Discussion Summary In the accuracy comparison between \\gbkmv and \\lshe, it is remarkable to see that the accuracy (i.e., F_1 score) is very low on some datasets.", "It is observed that the accuracy (F_1 score) improvement of \\lshe algorithm is very slow compared with \\gbkmv method. This is because the \\lshe method favours recall and the precision performance is quite poor even for a large number of hash functions, resulting in a poor F_1 score which considers both precision and recall. Sketch Construction Time", "In this part, we compare the sketch construction time of \\gbkmv and \\lshe on different datasets under default settings. As expected, \\gbkmv uses much less sketch construction time than that of \\lshe since \\gbkmv sketch need only one hash function, while \\lshe needs multiple for a decent accuracy. Note that, for the internet scale dataset WDC, the index construction time for \\gbkmv is around 10 minutes, while for \\lshe it is above 60 minutes.", "\\vspace{1mm \\noindent Comparison with \\lshe. Through theoretical analysis, we show that the performance (i.e., the variance of the estimator) of \\gbkmv can always outperform that of \\lshe in Theorem~ (Section~). Implementation of GB-KMV In this section, we introduce the technique details of our proposed \\gbkmv method.", "We use F_1 score, F_{0.5 score, precision and recall to measure the accuracy. By changing the number of hash functions, we tune the space used in \\lshe. It is reported that our \\gbkmv can beat the \\lshe in terms of space-accuracy trade-off with a big margin under all settings.", "\\vspace{1mm \\noindent Comparison with \\gkmv. As the \\gkmv is a special case of \\gbkmv with buffer size 0 and we carefully choose the buffer size with our cost model, the accuracy of \\gbkmv is not worse than \\gkmv."], "referred_text_chunks": ["Fig.~(a) illustrates the time-accuracy trade-off of \\gbkmv and \\lshe on the synthetic dataset with 100K records. It is reported that, to achieve the same accuracy (F_1 score), \\gbkmv consumes much less time than \\lshe. \\noindent Comparison with Exact Algorithms."]}, "11": {"question": "Which method has a lower running time for all datasets - GB-KMV or LSH-E?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 1, 'Rationale': 'Image 2 directly compares the running times for GB-KMV and LSH-E across multiple datasets, showing GB-KMV has lower running times. Text 1 explains why GB-KMV is faster, mentioning the fewer hash functions needed.', 'Answer': 'GB-KMV has a lower running time for all datasets compared to LSH-E.'}", "all_figures_names": ["1809.00458v1-Figure11-1.png", "1809.00458v1-TableII-1.png", "1809.00458v1-Figure18-1.png", "1809.00458v1-Figure12-1.png", "1809.00458v1-Figure4-1.png", "1809.00458v1-Figure5-1.png", "1809.00458v1-Figure8-1.png", "1809.00458v1-Figure13-1.png"], "referred_figures_names": ["1809.00458v1-Figure18-1.png"], "answer": "GB-KMV", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Then we assign 8, 16, 24,... to r and calculate the f(r, b, \\alpha_1, \\alpha_2). In this way, we can give a good guide to the choice of r. \\subsubsection{GB-KMV Sketch provides Better Accuracy than LSH-E Method In Section~, we have shown that the variance of \\lshe estimator(Equation~) is larger than that of MinHash \\lsh estimator(Equation~). Note that \\gkmv sketch is a special case of \\gbkmv sketch when the buffer size r = 0.", "In this part, we compare the sketch construction time of \\gbkmv and \\lshe on different datasets under default settings. As expected, \\gbkmv uses much less sketch construction time than that of \\lshe since \\gbkmv sketch need only one hash function, while \\lshe needs multiple for a decent accuracy. Note that, for the internet scale dataset WDC, the index construction time for \\gbkmv is around 10 minutes, while for \\lshe it is above 60 minutes.", "We also give the space usage of the two methods on each dataset in Table~. The space usage of \\gbkmv is 10{\\% as mentioned in Settings. For \\lshe in some dataset, the space is over 100{\\% because there are many records with size less than the number of hash functions 256. Supplementary Experiment \\noindent Evaluation on Uniform Distribution.", "Fig.~(a) illustrates the time-accuracy trade-off of \\gbkmv and \\lshe on the synthetic dataset with 100K records. It is reported that, to achieve the same accuracy (F_1 score), \\gbkmv consumes much less time than \\lshe. \\noindent Comparison with Exact Algorithms.", "In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score.", "\\vspace{1mm \\noindent Comparison with \\lshe. Through theoretical analysis, we show that the performance (i.e., the variance of the estimator) of \\gbkmv can always outperform that of \\lshe in Theorem~ (Section~). Implementation of GB-KMV In this section, we introduce the technique details of our proposed \\gbkmv method.", "For \\lshe method, we use the same default values in~ where the signature size of each record is 256 and the number of partition is 32. By varying the number of hash functions, we change the space used in \\lshe. \\vspace{-2mm Performance Tuning As shown in Section~, we can use the variance estimation function to identify a good buffer size r for \\gbkmv method based on the skewness of record size and element frequency, as well as the space budget.", "Then we apply \\gkmv technique to the remaining elements, resulting in a new augmented sketch, namely \\gbkmv. Now we can estimate |Q \\cap X| by combining the intersection of their bitmap buffers (exact solution) and \\kmv sketches (estimated solution). As shown in Fig.~, suppose we have \\mathcal{E_{H = \\{e_1, e_2 \\ and the global threshold for hash value is \\tau = 0.5, then the sketch of each record consists of two parts \\mathcal{L_{H and \\mathcal{L_{GKMV; that is, for each record we use bitmap to keep the elements corresponding to high-frequency elements \\mathcal{E_H = \\{e_1, e_2 \\, then we store the left elements with hash value less than \\tau = 0.5."], "referred_text_chunks": ["Then we assign 8, 16, 24,... to r and calculate the f(r, b, \\alpha_1, \\alpha_2). In this way, we can give a good guide to the choice of r. \\subsubsection{GB-KMV Sketch provides Better Accuracy than LSH-E Method In Section~, we have shown that the variance of \\lshe estimator(Equation~) is larger than that of MinHash \\lsh estimator(Equation~). Note that \\gkmv sketch is a special case of \\gbkmv sketch when the buffer size r = 0."]}, "12": {"question": "What is the containment similarity of Q in X1?", "referred_figures_indices": [7], "response": "{'Image': 0, 'Text': 6, 'Rationale': 'Image 0 provides the GB-KMV sketch of Q and X1, which is necessary for calculating the containment similarity. Text 6 explains how to compute the containment similarity using the GB-KMV sketch method.', 'Answer': 'The containment similarity of Q in X1 can be calculated using the formula provided, but based on the image above, it seems to be around 0.67 as per typical estimation methods of these sketches.'}", "all_figures_names": ["1809.00458v1-Figure4-1.png", "1809.00458v1-TableII-1.png", "1809.00458v1-Figure19-1.png", "1809.00458v1-Figure18-1.png", "1809.00458v1-Figure11-1.png", "1809.00458v1-Figure5-1.png", "1809.00458v1-Figure15-1.png", "1809.00458v1-Figure1-1.png"], "referred_figures_names": ["1809.00458v1-Figure1-1.png"], "answer": "0.67", "figure_type": "table", "content_type": "figure", "text_chunks": ["\\vspace{2mm \\noindent Similarity Transformation. Given a record X with size x = |X|, a query Q with size q = |Q|, containment similarity t = C(Q,X) and Jaccard similarity s = J(Q,X). The transformation back and forth are as follows.", "Given the \\gbkmv sketch of the query record Q and the dataset \\mathcal{S, we can conduct approximate similarity search as illustrated in Algorithm~. Given a query Q with size q and the similarity threshold t^*, let \\theta = t^* * q(Lines 1-2). With \\gbkmv sketch \\{ \\mathcal{H_Q, \\mathcal{L_Q \\, we can calculate the containment similarity based on where \\hat{D_{\\cap^{GKMV is the estimation of overlap size of Q and X which is calculated by Equation~ in Section~. Note that |\\mathcal{H_Q \\cap \\mathcal{H_X| is the number of common elements of Q and X in \\mathcal{E_H. \\vspace{1mm \\noindent Implementation of Containment Similarity Search.", "Let k_{X = |\\mathcal{L_X| and k_{Y = |\\mathcal{L_Y|. The estimator of containment similarity is \\hat{C = \\frac{\\hat{D_{\\cap{q, where \\hat{D_{\\cap is the estimator of intersection size D_{\\cap and q is the query size(x or y). Corresponding to \\mathcal{E_{H_1 and \\mathcal{E_{H_2, we divide X(Y, resp.)", "Then we set k value of the \\kmv estimation as follows. Meanwhile, we have K_{\\cap = |\\mathcal{L_Q \\cap \\mathcal{L_X|. Let U_{(k) be the k-th minimal hash value in \\mathcal{L_Q \\cup \\mathcal{L_X, then the overlap size of Q and X can be estimated as Then the containment similarity of Q in X is where q is the query size.", "\\theta . Therefore, we can directly apply the \\kmv method introduced in Section~. Given \\kmv signatures of a record X and a query Q, we can estimate their intersection size (|Q \\cap X|) according to Equation~. Then the containment similarity of Q in X is immediately available given the query size |Q|.", "On the other hand, the containment similarity will lead to the desired order with f(Q,X) = \\frac{2{2 = 1.0 and f(Q,Y) = \\frac{1{2 = 0.5. Containment similarity search can also support online error-tolerant search for matching user queries against addresses (map service) and products (product search).", "Given the space budget b, we first show that the variance for \\gbkmv sketch is a function of f(r, \\alpha_1, \\alpha_2, b) and then we give a method to appropriately choose r. Below are some notations first. Given two sets X and Y with \\gkmv sketch \\mathcal{L_X and \\mathcal{L_Y respectively, the containment similarity of Q in X is computed by Equation~ as \\hat{C^{GKMV = \\frac{\\hat{D_{\\cap^{GKMV{q, where \\hat{D_\\cap^{GKMV = \\frac{K_\\cap{k\\times\\frac{k-1{U_{(k) is the overlap size. As for the \\gbkmv method of set X an Y with sketch \\mathcal{H_X \\cup \\mathcal{L_X and \\mathcal{H_Y \\cup \\mathcal{L_Y respectively, the containment similarity of Q in X is computed by Equation~ as \\hat{C^{GBKMV = \\frac{|\\mathcal{H_Q \\cap \\mathcal{H_X| + \\hat{D_{\\cap^{GKMV{q, where |\\mathcal{H_Q \\cap \\mathcal{H_X| is the number of common elements in \\mathcal{E_H part.", "Containment similarity (a.k.a, Jaccard containment similarity) is one of the representative asymmetric set similarity functions, where the similarity between two records X and Y is defined as f(X,Y) = \\frac{|X \\cap Y|{|X| in which |X \\cap Y | and |X| are intersection size of X and Y and the size of X, respectively. Compared with symmetric similarity such as Jaccard similarity, containment similarity gives special consideration on the query size, which makes it more suitable in some applications. As shown in~, containment similarity is useful in record matching application."], "referred_text_chunks": ["\\vspace{2mm \\noindent Similarity Transformation. Given a record X with size x = |X|, a query Q with size q = |Q|, containment similarity t = C(Q,X) and Jaccard similarity s = J(Q,X). The transformation back and forth are as follows."]}, "13": {"question": "Which method is more efficient at utilizing space while maintaining high accuracy?", "referred_figures_indices": [7], "response": "{'Image': 1, 'Text': 1, 'Rationale': 'Image 1 shows the space-accuracy trade-off for GB-KMV and LSH-E on the REUTERS dataset, directly addressing space efficiency. Text 1 describes GB-KMV as having a better space-accuracy trade-off compared to LSH-E.', 'Answer': 'GB-KMV is more efficient at utilizing space while maintaining high accuracy.'}", "all_figures_names": ["1809.00458v1-Figure15-1.png", "1809.00458v1-Figure11-1.png", "1809.00458v1-Figure17-1.png", "1809.00458v1-TableII-1.png", "1809.00458v1-Figure5-1.png", "1809.00458v1-Figure13-1.png", "1809.00458v1-Figure4-1.png", "1809.00458v1-Figure7-1.png"], "referred_figures_names": ["1809.00458v1-Figure7-1.png"], "answer": "GB-KMV is more efficient at utilizing space while maintaining high accuracy.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["\\vspace{-2mm Space v.s. Accuracy An important measurement for sketch technique is the trade-off between the space and accuracy. We evaluate the space-accuracy trade-offs of \\gbkmv method and \\lshe method in Figs.~- by varying the space usage on five datasets NETFLIX, DELIC, COD, ENRON, REUTERS, WEBSPAM and WDC.", "Our empirical study shows that \\lshe cannot achieve a good trade-off between accuracy and space, compared with our proposed method. Our Approach In this section, we introduce an augmented \\kmv sketch technique to achieve better space-accuracy trade-off for approximate containment similarity search. Section~ briefly introduces the motivation and main technique of our method, namely \\gbkmv.", "It is shown that the use of new KMV estimator with global threshold (i.e., Equation~) can significantly improve the search accuracy. By using a buffer whose size is suggested by the system, we can further enhance the performance under the same space budget. In the following experiments, we use \\gbkmv for the performance comparison with the state-of-the-art technique \\lshe.", "We can see that on datasets with different record size and element frequency skewness, \\gbkmv consistently outperforms \\lshe in terms of space-accuracy trade-off. Time v.s. Accuracy Another important measurement for the sketch technique is the trade-off between time and accuracy. Hopefully, the sketch should be able to quickly complete the search with a good accuracy.", "We give some discussions as follows. First we should point out that in ~, the accuracy of \\lshe is only evaluated on only one dataset COD, in which both our \\gbkmv method and \\lshe can achieve decent accuracy performance with F_1 score above 0.5. As mentioned in ~, the \\lshe method first transforms the containment similarity to Jaccard similarity, then in order to make use of the efficient index techniques, \\lshe partitions the dataset and uses the upper bound to approximate the record size in each partition.", "We use F_1 score, F_{0.5 score, precision and recall to measure the accuracy. By changing the number of hash functions, we tune the space used in \\lshe. It is reported that our \\gbkmv can beat the \\lshe in terms of space-accuracy trade-off with a big margin under all settings.", "We develop an augmented \\kmv sketch technique, namely \\gbkmv, which is data-dependent and can effectively exploit the distributions of record size and element frequency. We provide thorough theoretical analysis to justify the design of \\gbkmv, and show that the proposed method can outperform the state-of-the-art technique in terms of space-accuracy trade-off. Extensive experiments on real-life set-valued datasets from a variety of applications demonstrate the superior performance of \\gbkmv method compared with the state-of-the-art technique.", "al propose a partition-based method based on Minhash \\lsh function. By using optimal partition strategy based on the size distribution of the records, the new approach can achieve much better time-accuracy trade-off. We notice that all existing approximate solutions rely on the \\lsh functions by transforming the containment similarity to well-studied Jaccard similarity."], "referred_text_chunks": ["\\vspace{-2mm Space v.s. Accuracy An important measurement for sketch technique is the trade-off between the space and accuracy. We evaluate the space-accuracy trade-offs of \\gbkmv method and \\lshe method in Figs.~- by varying the space usage on five datasets NETFLIX, DELIC, COD, ENRON, REUTERS, WEBSPAM and WDC."]}, "14": {"question": "Which algorithm performs better in terms of F1 Score and Precision on ENRON?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 2, 'Rationale': 'Image 7 specifically shows F1 Score and Precision on the ENRON dataset, which is relevant to the question. Text 2 explains the performance of \\lshe and \\gbkmv in terms of F1 Score and Precision, highlighting \\lshe\\'s poor precision.', 'Answer': '\\\\gbkmv performs better in terms of F1 Score and Precision on ENRON.'}", "all_figures_names": ["1809.00458v1-Figure18-1.png", "1809.00458v1-TableI-1.png", "1809.00458v1-Figure10-1.png", "1809.00458v1-Figure19-1.png", "1809.00458v1-Figure13-1.png", "1809.00458v1-Figure16-1.png", "1809.00458v1-Figure17-1.png", "1809.00458v1-Figure9-1.png"], "referred_figures_names": ["1809.00458v1-Figure9-1.png"], "answer": "GB-KMV performs better than LSH-E in terms of F1 Score and Precision.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score.", "\\gbkmv outperforms two exact algorithm by a big margin, especially when the record size is large, with a decent accuracy (i.e., with F_1 score and recall always larger than 0.8 and 0.9 under all settings). Discussion Summary In the accuracy comparison between \\gbkmv and \\lshe, it is remarkable to see that the accuracy (i.e., F_1 score) is very low on some datasets.", "It is observed that the accuracy (F_1 score) improvement of \\lshe algorithm is very slow compared with \\gbkmv method. This is because the \\lshe method favours recall and the precision performance is quite poor even for a large number of hash functions, resulting in a poor F_1 score which considers both precision and recall. Sketch Construction Time", "The precision and recall to evaluate the experiment accuracy are Precision= \\frac{|T\\cap A|{|A| and Recall = \\frac{|T\\cap A|{|T| respectively. The F_\\alpha score is defined as follows. Note that we use F_{0.5 score because \\lshe favours recall in~. We use the datasets from Table~ to evaluate the performance of our algorithm, and we randomly choose 200 queries from the dataset.", "We use F_1 score, F_{0.5 score, precision and recall to measure the accuracy. By changing the number of hash functions, we tune the space used in \\lshe. It is reported that our \\gbkmv can beat the \\lshe in terms of space-accuracy trade-off with a big margin under all settings.", "In Fig.~, we use NETFLIX and ENRON to evaluate the goodness of the function by comparing the trend of the variance and the estimation accuracy. By varying the buffer size r, Fig.~ reports the estimated variance (right side y axis) based on the variance function in Section~ as well as the F_1 score (left side y axis) of the corresponding \\gbkmv sketch with buffer size r. Fig.~(a) shows that the best buffer size for variance estimation (prefer small value) is around 400, while the \\gbkmv method achieves the best F_1 score (prefer large value) with buffer size around 380. They respectively become 220 and 230 in Fig.~(b).", "Fig.~(a) illustrates the time-accuracy trade-off of \\gbkmv and \\lshe on the synthetic dataset with 100K records. It is reported that, to achieve the same accuracy (F_1 score), \\gbkmv consumes much less time than \\lshe. \\noindent Comparison with Exact Algorithms.", "We give some discussions as follows. First we should point out that in ~, the accuracy of \\lshe is only evaluated on only one dataset COD, in which both our \\gbkmv method and \\lshe can achieve decent accuracy performance with F_1 score above 0.5. As mentioned in ~, the \\lshe method first transforms the containment similarity to Jaccard similarity, then in order to make use of the efficient index techniques, \\lshe partitions the dataset and uses the upper bound to approximate the record size in each partition."], "referred_text_chunks": ["In Fig.~, we evaluate the time-accuracy trade-offs for \\gbkmv and \\lshe on four datasets COD, NETFLIX, DELIC and ENRON where the accuracy is measured by F_1 score. It is shown that with the similar accuracy (F_1 score), \\gbkmv is significantly faster than \\lshe. For datasets COD, DELIC and ENRON, \\gbkmv can be 100 times faster than \\lshe with the same F_1 score."]}}