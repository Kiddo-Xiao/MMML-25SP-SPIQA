{"0": {"question": "Which model performed best on average across all tasks?", "referred_figures_indices": [1], "response": "{'Image': 0, 'Text': 3, 'Rationale': 'Image 0 displays a comprehensive comparison of different model performances on unsupervised evaluation tasks, clearly indicating the best performing models. Text chunk 3 discusses the performance distinction between encoder and decoder with and without invertible constraints, highlighting the differences across tasks.', 'Answer': 'The Bijective model with UMBC News (U) performed best on average across all tasks.'}", "all_figures_names": ["1809.02731v3-Table3-1.png", "1809.02731v3-Table4-1.png", "1809.02731v3-Table5-1.png", "1809.02731v3-Table1-1.png", "1809.02731v3-Table2-1.png"], "referred_figures_names": ["1809.02731v3-Table4-1.png"], "answer": "The Linear model performed best on average with a score of 70.0.", "figure_type": "table", "content_type": "table", "text_chunks": ["As presented in Table , on unsupervised evaluation tasks (STS12-16 and SICK14), the ensemble of two encoding functions is averaging, which benefits from aligning representations from f_\\text{en and f_\\text{de^{-1 by enforcing the invertible constraint. While in the learning system without the invertible constraint, the ensemble of two encoding functions provides worse performance than f_\\text{de^{-1. On supervised evaluation tasks, as the ensemble method is concatenation and a linear model is applied on top of the concatenated representations, as long as the two encoding functions process sentences distinctively, the linear classifier is capable of picking relevant feature dimensions from both encoding functions to make good predictions, thus there is no significant difference between our model with and without invertible constraint.", "Table also provides evidence for the effectiveness of learning on the unsupervised evaluation tasks. \\vskip -0.1in \\end{center \\caption{Results on supervised evaluation tasks. Bold numbers are the best results among unsupervised transfer models with ordered sentences, and \\underline{underlined numbers are the best ones among all models.", "Therefore, it is important to check the necessity of the invertible constraint on the decoders. A model with the same hyperparameter settings but without the invertible constraint is trained as the baseline model, and macro-averaged results that summarise the same type of tasks are presented in Table . As noted in the prior work , there exists significant inconsistency between the group of unsupervised tasks and the group of supervised ones, it is possible for a model to excel on one group of tasks but fail on the other one.", "As presented in our table, the inverse of the decoder tends to perform better than the encoder on unsupervised tasks, and the situation reverses when it comes to the supervised ones. In our model, the invertible constraint helps the RNN encoder f_\\text{en to perform better on the unsupervised evaluation tasks, and helps the inverse of the decoder f_\\text{de^{-1 to provide better results on single sentence classification tasks. An interesting observation is that, by enforcing the invertible constraint, the model learns to sacrifice the performance of f_\\text{de^{-1 and improve the performance of f_\\text{en on unsupervised tasks to mitigate the gap between the two encoding functions, which leads to more aligned vector representations between f_\\text{en and f_\\text{de^{-1.", "\\end{table* On supervised evaluation tasks, we agree that higher dimensional vector representations give better results on the downstream tasks. Compared to random projections with 4096\\times 6 output dimensions, learning from unlabelled corpora leverages the distributional similarity at the sentence-level into the learnt representations and potentially helps capture the meaning of a sentence.", "As mentioned as one of the take-away messages , to demonstrate the effectiveness of the invertible constraint, the comparison of our model with the constraint and its own variants use the same word embeddings from FastText and have the same dimensionaility of sentence representations during learning, and use the same classifier on top of the produced representations with the same hyperparameter settings. Overall, given the performance of the inverse of each decoder presented in Table and , it is reasonable to state that the inverse of the decoder provides high-quality sentence representations as well as the encoder does. However, there is no significant difference between the two decoders in terms of the performance on the downstream tasks.", "As shown in the Table , the performance of our learnt system is better than FastText at the same dimensionality. It is worth mentioning that, in our system, the final representation is an average of postprocessed word vectors and the learnt representations \\vx, and the invertible constraint guarantees that the ensemble of both gives better performance. Otherwise, as discussed in the previous section, an ensemble of postprocessed word vectors and some random encoders won't necessarily lead to stronger results.", "Effect of Learning Recent research showed that the improvement on the supervised evaluation tasks led by learning from labelled or unlabelled corpora is rather insignificant compared to random initialised projections on top of pretrained word vectors. Another interesting direction of research that utilises probabilistic random walk models on the unit sphere derived several simple yet effective post-processing methods that operate on pretrained word vectors and are able to boost the performance of the averaged word vectors as the sentence representation on unsupervised tasks. While these papers reveal interesting aspects of the downstream tasks and question the need for optimising a learning objective, our results show that learning on unlabelled corpora helps."], "referred_text_chunks": ["As presented in Table , on unsupervised evaluation tasks (STS12-16 and SICK14), the ensemble of two encoding functions is averaging, which benefits from aligning representations from f_\\text{en and f_\\text{de^{-1 by enforcing the invertible constraint. While in the learning system without the invertible constraint, the ensemble of two encoding functions provides worse performance than f_\\text{de^{-1. On supervised evaluation tasks, as the ensemble method is concatenation and a linear model is applied on top of the concatenated representations, as long as the two encoding functions process sentences distinctively, the linear classifier is capable of picking relevant feature dimensions from both encoding functions to make good predictions, thus there is no significant difference between our model with and without invertible constraint."]}, "1": {"question": "Which corpus has more sentences, and by how much?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 directly shows the number of sentences in each corpus, which is what the question is asking for. Text chunk 0 mentions both corpora, making it the most relevant to identifying their sizes.', 'Answer': 'UMBC News has 134.5 million sentences, which is 60.5 million more than the BookCorpus, which has 74 million sentences.'}", "all_figures_names": ["1809.02731v3-Table3-1.png", "1809.02731v3-Table4-1.png", "1809.02731v3-Table5-1.png", "1809.02731v3-Table1-1.png", "1809.02731v3-Table2-1.png"], "referred_figures_names": ["1809.02731v3-Table1-1.png"], "answer": "The UMBC News corpus has more sentences, by approximately 60.5 million.", "figure_type": "table", "content_type": "table", "text_chunks": ["These corpora are referred as B, and U in Table and . The UMBC News Corpus is roughly twice as large as the BookCorpus, and the details are shown in Table . Evaluation Tasks Unsupervised Evaluation", "\\end{table* On supervised evaluation tasks, we agree that higher dimensional vector representations give better results on the downstream tasks. Compared to random projections with 4096\\times 6 output dimensions, learning from unlabelled corpora leverages the distributional similarity at the sentence-level into the learnt representations and potentially helps capture the meaning of a sentence.", "In our system, the raw representations are in 2400-dimensional space, and the use of various pooling functions expands it to 2048\\times 6 dimensions, which is half as large as the random projection dimension and still yields better performance. Both our models and random projections with no training are presented in Table . The evidence from both sets of downstream tasks support our argument that learning from unlabelled corpora helps the representations capture meaning of sentences.", "In this case, we enforce the output of the decoder \\vx_i to have the same dimensionality as the pretrained word vectors \\vv_{w_j. The loss function is summed over all contiguous sentence pairs in the training corpus. For simplicity, we omit the subscription for indexing the sentences in the following sections.", "On unsupervised evaluation tasks, in order to show that learning from an unlabelled corpus helps, the performance of our learnt representations should be directly compared with the pretrained word vectors, FastText in our system, at the same dimensionality with the same post-processing . The word vectors are scattered in the 300-dimensional space, and our model has a decoder that is learnt to project a sentence representation \\vz\\in\\sR^{d_\\vz to \\vx=f_\\text{de(\\vz;\\boldsymbol{\\phi)\\in\\sR^{300. The results of our learnt representations and averaged word vectors with the same postprocessing are presented in Table .", "In one of the two classes of models of this type, an encoder-decoder model is learnt using a corpus of contiguous sentences to make predictions of the words in the next sentence given the words in the current one. After training, the decoder is usually discarded as it is only needed during training and is not designed to produce sentence representations. In the other class of models of this type, a large language model is learnt on unlabelled corpora, which could be an autoregressive model or a masked language model, which gives extremely powerful language encoders but requires massive computing resources and training time.", "Analyses show that the invertible constraint enforced on the decoder encourages each one to learn from the other one during learning, and provides improved encoding functions after learning. Ensemble of the encoder and the inverse of the decoder gives even better performance when the invertible constraint is applied on the decoder side. Furthermore, by comparing with prior work, we argue that learning from unlabelled corpora indeed helps to improve the sentence representations, although the current way of utilising corpora might not be optimal.", "The reason for applying different representation pooling strategies for two categories of tasks is: (1) cosine similarity of two vector representations is directly calculated in unsupervised evaluation tasks to determine the textual similarity of two sentences, and it suffers from the curse-of-dimensionality , which leads to more equidistantly distributed representations for higher dimensional vector representations decreasing the difference among similarity scores. (2) given Cover's theorem and the blessings-of-dimensionality property, it is more likely for the data points to be linearly separable when they are presented in high dimensional space, and in the supervised evaluation tasks, high dimensional vector representations are preferred as a linear classifier will be learnt to evaluate how likely the produced sentence representations are linearly separable; (3) in our case, both the encoder and the inverse of the decoder are capable of producing a vector representation per time step in a given sentence, although during training, only the last one is regarded as the sentence representation for the fast training speed, it is more reasonable to make use of all representations at all time steps with various pooling functions to compute a vector representations to produce high-quality sentence representations that excel the downstream tasks. Discussion It is worth discussing the motivation of the model design and the observations in our experiments."], "referred_text_chunks": ["These corpora are referred as B, and U in Table and . The UMBC News Corpus is roughly twice as large as the BookCorpus, and the details are shown in Table . Evaluation Tasks Unsupervised Evaluation"]}, "2": {"question": "Which model performs the best on the STS16 task with unsupervised training?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 2, 'Rationale': 'Image 0 directly lists the results for the STS16 task under various conditions, highlighting the best performance in bold. Text chunk 2 explains the effectiveness of the invertible constraint, impacting the results in the table by providing improved unsupervised performance.', 'Answer': 'The model with Bijective + UMBC (U) performs the best on the STS16 task with unsupervised training, scoring 76.9.'}", "all_figures_names": ["1809.02731v3-Table3-1.png", "1809.02731v3-Table4-1.png", "1809.02731v3-Table5-1.png", "1809.02731v3-Table1-1.png", "1809.02731v3-Table2-1.png"], "referred_figures_names": ["1809.02731v3-Table3-1.png"], "answer": "The Bijective model performs the best on the STS16 task with unsupervised training.", "figure_type": "table", "content_type": "table", "text_chunks": ["As presented in Table , on unsupervised evaluation tasks (STS12-16 and SICK14), the ensemble of two encoding functions is averaging, which benefits from aligning representations from f_\\text{en and f_\\text{de^{-1 by enforcing the invertible constraint. While in the learning system without the invertible constraint, the ensemble of two encoding functions provides worse performance than f_\\text{de^{-1. On supervised evaluation tasks, as the ensemble method is concatenation and a linear model is applied on top of the concatenated representations, as long as the two encoding functions process sentences distinctively, the linear classifier is capable of picking relevant feature dimensions from both encoding functions to make good predictions, thus there is no significant difference between our model with and without invertible constraint.", "Table also provides evidence for the effectiveness of learning on the unsupervised evaluation tasks. \\vskip -0.1in \\end{center \\caption{Results on supervised evaluation tasks. Bold numbers are the best results among unsupervised transfer models with ordered sentences, and \\underline{underlined numbers are the best ones among all models.", "As presented in our table, the inverse of the decoder tends to perform better than the encoder on unsupervised tasks, and the situation reverses when it comes to the supervised ones. In our model, the invertible constraint helps the RNN encoder f_\\text{en to perform better on the unsupervised evaluation tasks, and helps the inverse of the decoder f_\\text{de^{-1 to provide better results on single sentence classification tasks. An interesting observation is that, by enforcing the invertible constraint, the model learns to sacrifice the performance of f_\\text{de^{-1 and improve the performance of f_\\text{en on unsupervised tasks to mitigate the gap between the two encoding functions, which leads to more aligned vector representations between f_\\text{en and f_\\text{de^{-1.", "\\end{table* On supervised evaluation tasks, we agree that higher dimensional vector representations give better results on the downstream tasks. Compared to random projections with 4096\\times 6 output dimensions, learning from unlabelled corpora leverages the distributional similarity at the sentence-level into the learnt representations and potentially helps capture the meaning of a sentence.", "Effect of Learning Recent research showed that the improvement on the supervised evaluation tasks led by learning from labelled or unlabelled corpora is rather insignificant compared to random initialised projections on top of pretrained word vectors. Another interesting direction of research that utilises probabilistic random walk models on the unit sphere derived several simple yet effective post-processing methods that operate on pretrained word vectors and are able to boost the performance of the averaged word vectors as the sentence representation on unsupervised tasks. While these papers reveal interesting aspects of the downstream tasks and question the need for optimising a learning objective, our results show that learning on unlabelled corpora helps.", "In our system, the raw representations are in 2400-dimensional space, and the use of various pooling functions expands it to 2048\\times 6 dimensions, which is half as large as the random projection dimension and still yields better performance. Both our models and random projections with no training are presented in Table . The evidence from both sets of downstream tasks support our argument that learning from unlabelled corpora helps the representations capture meaning of sentences.", "Therefore, it is important to check the necessity of the invertible constraint on the decoders. A model with the same hyperparameter settings but without the invertible constraint is trained as the baseline model, and macro-averaged results that summarise the same type of tasks are presented in Table . As noted in the prior work , there exists significant inconsistency between the group of unsupervised tasks and the group of supervised ones, it is possible for a model to excel on one group of tasks but fail on the other one.", "\\end{table* Hyperparameter Tuning The hyperparameters are tuned on the averaged scores on STS14 of the model trained on BookCorpus, thus it is marked with a ^\\star in tables to indicate potential overfitting. The hyperparameter setting for our model is summarised as follows: the batch size N=512, the dimension of sentence vectors d_\\vz=2048, the dimension of word vectors d_{\\vv_{w_j=300, the number of negative samples K=5, and the initial learning rate is 5\\times 10^{-4 which is kept fixed during learning."], "referred_text_chunks": ["As presented in Table , on unsupervised evaluation tasks (STS12-16 and SICK14), the ensemble of two encoding functions is averaging, which benefits from aligning representations from f_\\text{en and f_\\text{de^{-1 by enforcing the invertible constraint. While in the learning system without the invertible constraint, the ensemble of two encoding functions provides worse performance than f_\\text{de^{-1. On supervised evaluation tasks, as the ensemble method is concatenation and a linear model is applied on top of the concatenated representations, as long as the two encoding functions process sentences distinctively, the linear classifier is capable of picking relevant feature dimensions from both encoding functions to make good predictions, thus there is no significant difference between our model with and without invertible constraint."]}}