{"0": {"question": "How does batching affect the radio duty cycle and CPU duty cycle?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 directly shows the effect of batching on radio and CPU duty cycles, with a comparison between batching and no batching. Text 0 explains the efficiencies achieved through batching in terms of reduced duty cycles.', 'Answer': 'Batching significantly reduces both radio and CPU duty cycles compared to sending data without batching.'}", "all_figures_names": ["1811.02721v3-Table5-1.png", "1811.02721v3-Table6-1.png", "1811.02721v3-Figure5-1.png", "1811.02721v3-Figure9-1.png", "1811.02721v3-Table8-1.png", "1811.02721v3-Table7-1.png", "1811.02721v3-Figure11-1.png", "1811.02721v3-Figure1-1.png"], "referred_figures_names": ["1811.02721v3-Figure9-1.png"], "answer": "Batching reduces both the radio duty cycle and CPU duty cycle.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Figures and also show that all the three protocols consume similar power; TCP is comparable to LLN-specific solutions. Both the radio and CPU duty cycle are significantly smaller with batching than without batching. By sending data in batches, nodes can amortize the cost of sending data and waiting for a response.", "Figure depicts the radio duty cycle of TCP and CoAP for a trial representative of our overall results. CoAP maintains a lower duty cycle than \\sys{ outside of working hours, when there is less interference; \\sys{ has a slightly lower duty cycle than CoAP during working hours, when there is more wireless interference. \\sys{'s better performance at a higher loss rate is consistent with our results from \\secref{sec:injected.", "We also observed this problem for batch transfer over TCP; TCP's self-clocking behavior causes it to consistently experience the worst-case round-trip time. To solve this problem, we propose a technique called Adaptive Duty Cycling. After the web server receives a SYN, it reduces the sleep interval in anticipation of receiving an HTTP request.", "Compared to unreliable UDP, reliable approaches increase the radio/CPU duty cycle by 3x, in exchange for nearly 100\\% reliability. That said, the corresponding decrease in battery life will be less than 3x, because other sources of power consumption (reading from sensors, idle current) are also significant. For other sense-and-send applications that sample at a lower rate, TCP and CoAP would see higher reliability (less application queue loss), but UDP would not similarly benefit (no application queue).", "This results in an inflated RTT value that causes CoCoA to delay longer before retransmitting, causing the application-layer queue to overflow. Full-scale TCP is immune to this problem despite measuring the RTT, because the TCP timestamp option allows TCP to unambiguously determine the RTT even for retransmitted segments. Figures~ and show that, overall, TCP and CoAP perform comparably in terms of radio and CPU duty cycle.", "At 0\\% injected loss, \\sys{ has a slightly higher duty cycle, consistent with Figure . At moderate packet loss, \\sys{ appears to have a slightly lower duty cycle. This may be due to TCP's sliding window, which allows it to tolerate some ACK losses without retries.", "We compare three setups: (1) CoAP, (2) CoCoA, and (3) \\sys{. We also compare two sending scenarios: (1) sending each sensor reading right away (``No Batching''), and (2) sending sensor readings in batches of 64 (``Batching'')~. We ensure that packets in a CoAP batch are the same size as segments in TCP (five frames). All setups achieved 100\\% reliability due to end-to-end acknowledgments (figures are omitted for brevity).", "Thus, batching is the more realistic workload, so we use it to continue our evaluation. \\subsubsection{Resilience to Packet Loss In this section, we inject uniformly random packet loss at the border router and measure each solution. The result is shown in Figure ."], "referred_text_chunks": ["Figures and also show that all the three protocols consume similar power; TCP is comparable to LLN-specific solutions. Both the radio and CPU duty cycle are significantly smaller with batching than without batching. By sending data in batches, nodes can amortize the cost of sending data and waiting for a response."]}, "1": {"question": "Which technique was most effective at reducing memory consumption in both send and receive buffers?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 summarizes the improvements in resource usage, showing the effectiveness of the zero-copy send buffer and in-place reassembly. Text 0 discusses the zero-copy send buffer and its role in reducing memory usage.', 'Answer': 'The most effective technique was the zero-copy send buffer, which reduced send buffer memory usage by 50%.'}", "all_figures_names": ["1811.02721v3-Table6-1.png", "1811.02721v3-Figure12-1.png", "1811.02721v3-Figure7-1.png", "1811.02721v3-Table8-1.png", "1811.02721v3-Table2-1.png", "1811.02721v3-Figure9-1.png", "1811.02721v3-Table9-1.png", "1811.02721v3-Table1-1.png"], "referred_figures_names": ["1811.02721v3-Table1-1.png"], "answer": "The \"Resource Constraints\" challenge was addressed with two techniques: \"Zero-Copy Send\" and \"In-Place Reass.\" The first led to a 50% reduction in send buffer memory usage, while the second achieved a 38% reduction in receive buffer memory. Therefore, Zero-Copy Send was slightly more effective in reducing overall memory consumption.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["\\subsubsection{Send Buffer: Zero-Copy Zero-copy techniques~ were devised for situations where the time for the CPU to copy memory is a significant bottleneck. Our situation is very different; the radio, not the CPU, is the bottleneck, owing to the low bandwidth of IEEE 802.15.4. By using a zero-copy send buffer, however, we can avoid allocating memory to intermediate buffers that would otherwise be needed to copy data, thereby reducing the network stack's total memory usage.", "These buffers, however, are key to supporting TCP's sliding window functionality. We observe in \\secref{sec:window that \\sys{ performs well with only 2-3 KiB send and receive buffers, which comfortably fit in memory even when na\\\"ively pre-allocated at compile time. Given that buffers dominate \\sys{'s memory usage, however, we discuss techniques to optimize their memory usage.", "In contrast, choosing a larger send buffer will allow fast recovery to more effectively mask this loss~. As discussed in \\secref{ssec:congestion, these two types of losses may be caused by different factors. Therefore, we do not attempt to distinguish them on basis of probability.", "The memory required for the protocol and application state of an active TCP socket fits in a few hundred bytes, less than 1\\% of the available RAM on the Cortex-M4 (Firestorm) and 2\\% of that on the Cortex-M0+ (Hamilton). Although \\sys{ includes heavyweight features not traditionally included in embedded TCP stacks, it fits well within available memory. Memory-Efficient Data Buffering Existing embedded TCP stacks, such as uIP and BLIP, allow only one TCP packet in the air, eschewing careful implementation of send and receive buffers~.", "In our memory-constrained setting, such a design is dangerous because its memory usage is nondeterministic; there is additional memory overhead, due to headers, if the data are delivered in many small packets instead of a few large ones. We opted for a flat array-based circular buffer for the receive buffer in \\sys{, primarily owing to its determinism in a limited-memory environment: buffer space is reserved at compile-time. Head/tail pointers delimit which part of the array stores in-sequence data.", "Crucially, this means that the 2 KiB buffer size we determined in \\secref{sec:window, which fits comfortably in memory, remains applicable for up to three wireless hops. TCP Congestion Control in LLNs Recall that small send/receive buffers of only 1848 bytes (4 TCP segments) each are enough to achieve good TCP performance. This profoundly impacts TCP's congestion control mechanism.", "This is specific to small buffers. In traditional environments, where links have higher throughput and buffers are large, it takes longer for \\cwnd{ to recover after packet loss, greatly limiting the sending rate with frequent packet losses. In contrast, in LLNs, where send/receive buffers are small, \\cwnd{ recovers to the maximum size quickly after packet loss, making TCP performance robust to packet loss.", "Thus, we modified OpenThread to allow indirect messages to preempt the current frame in between link-layer retries, to minimize the time that duty-cycled leaf nodes must wait for a reply with their radios on. Both TCP and CoAP benefitted from this; TCP benefitted more because it suffered more from the problem to begin with. \\smallskip \\parhead{Power Consumption To improve power consumption for both TCP and CoAP, we adjusted parameters according to the lossy environment: (1) we enabled link-layer retries for indirect messages, (2) we decreased the data request timeout and performed link-layer retries more rapidly for indirect messages, to deliver them to leaves more quickly, and (3) given the high level of daytime interference, we decreased the MSS from five frames to three frames (as in \\secref{sec:application)."], "referred_text_chunks": ["\\subsubsection{Send Buffer: Zero-Copy Zero-copy techniques~ were devised for situations where the time for the CPU to copy memory is a significant bottleneck. Our situation is very different; the radio, not the CPU, is the bottleneck, owing to the low bandwidth of IEEE 802.15.4. By using a zero-copy send buffer, however, we can avoid allocating memory to intermediate buffers that would otherwise be needed to copy data, thereby reducing the network stack's total memory usage."]}, "2": {"question": "Based on Table 1 and the passage, why does TCP perform poorly on IEEE 802.15.4 networks compared to other network types listed? ", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 1, 'Rationale': 'Image 4 provides a direct comparison of network types, showing the capacity and MTU differences that impact TCP performance on 802.15.4 networks. Text 1 discusses specific reasons for poor TCP performance on these networks, like small link-layer frames and hidden terminal effects.', 'Answer': 'TCP performs poorly on IEEE 802.15.4 networks due to small link-layer frames increasing TCP header overhead, hidden terminal effects that complicate communication over multiple hops, and poor interaction with duty-cycled links, all of which are exacerbated by the lower capacity and smaller MTU of 802.15.4 networks.'}", "all_figures_names": ["1811.02721v3-Figure1-1.png", "1811.02721v3-Figure12-1.png", "1811.02721v3-Table3-1.png", "1811.02721v3-Figure10-1.png", "1811.02721v3-Table4-1.png", "1811.02721v3-Table2-1.png", "1811.02721v3-Table7-1.png", "1811.02721v3-Figure9-1.png"], "referred_figures_names": ["1811.02721v3-Table4-1.png"], "answer": "TCP performs poorly on IEEE 802.15.4 networks because the Maximum Transmission Unit (MTU) for these networks is significantly smaller than other network types. This small MTU size results in a high percentage of overhead due to the TCP/IP headers, exceeding 50%. ", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Although na\\\"ive TCP indeed performs poorly in LLNs, this is not due to fundamental problems with TCP as were observed in WLANs. Rather, it is caused by incompatibilities with a low-power link layer, which likely arose because canonical LLN protocols were developed in the absence of TCP considerations. We show how to fix these incompatibilities while preserving seamless interoperability with other TCP/IP networks.", "Na\\\"ively running TCP in an LLN indeed results in poor performance. However, upon close examination, we discover that this is not caused by the expected reasons, such as those listed above. The actual reasons for poor TCP performance include (1) small link-layer frames that increase TCP header overhead, (2) hidden terminal effects over multiple wireless hops, and (3) poor interaction between TCP and a duty-cycled link.", "We also propose careful link-layer queue management to make TCP more robust to interference. We demonstrate that TCP can operate at low power, comparable to alternatives tailored specifically for WSNs, and that TCP brings value for real IoT sensor applications (\\secref{sec:application). We conclude that TCP is entirely capable of running on IEEE 802.15.4 networks and low-cost embedded devices in LLN application scenarios (\\secref{sec:conclusion).", "At a lower packet loss rate, TCP performs slightly worse than CoAP. This could be due to hidden terminal losses; more retries, on average, are required for indirect messages for TCP, causing leaf nodes to stay awake longer. Overall, CoAP and \\sys{ perform similarly (Table ).", "In this context, this paper seeks to determine: Do the ``common wisdom'' concerns about TCP hold in a modern IEEE 802.15.4-based LLN? Is TCP (still) unsuitable for use in LLNs? To answer this question, we leverage the fully-featured TCP implementation in the FreeBSD Operating System (rather than a limited locally-developed implementation) and refactor it to work with the Berkeley Low-Power IP Stack (BLIP), Generic Network Stack (GNRC), and OpenThread network stack, on two modern LLN platforms (\\secref{sec:implementation).", "TCP in a Low-Power Network In this section, we characterize how full-scale TCP interacts with a low-power network stack, resource-constrained hardware, and a low-bandwidth link. Reducing Header Overhead using MSS In traditional networks, it is customary to set the Maximum Segment Size (MSS) to the link MTU (or path MTU) minus the size of the TCP/IP headers. IEEE 802.15.4 frames, however, are an order of magnitude smaller than frames in traditional networks (Table ).", "This suggests that the implementation of the underlying network stack, particularly with regard to concurrency, could affect TCP performance in LLNs. Comparison of Features in Embedded TCP Implementations Table compares the featureset of \\sys{ to features in embedded TCP stacks. The TCP implementations in uIP and BLIP lack features core to TCP.", "It naturally became taken for granted that TCP is too heavy for LLNs. Furthermore, contemporary research on TCP in WLANs~ suggested that TCP would perform poorly in LLNs even if the resource constraints were surmounted. In revisiting the TCP question, after the resource constraints relaxed, we find that the expected pitfalls of wireless TCP actually do not carry over to LLNs."], "referred_text_chunks": ["Although na\\\"ive TCP indeed performs poorly in LLNs, this is not due to fundamental problems with TCP as were observed in WLANs. Rather, it is caused by incompatibilities with a low-power link layer, which likely arose because canonical LLN protocols were developed in the absence of TCP considerations. We show how to fix these incompatibilities while preserving seamless interoperability with other TCP/IP networks."]}, "3": {"question": "Why is relying on fragmentation effective for reducing header overhead?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 2, 'Rationale': 'Image 5 and Text 2 directly relate to the effect of 6LoWPAN fragmentation on header overhead, showing how headers are managed in fragments. Text 2 explains how fragmentation reduces header overhead, which is the question focus.', 'Answer': 'Relying on fragmentation is effective for reducing header overhead because it allows for smaller headers in subsequent fragments, as only the first fragment carries complete TCP/IP headers. This is managed by 6LoWPAN, reducing the overhead within the network.'}", "all_figures_names": ["1811.02721v3-Figure5-1.png", "1811.02721v3-Figure10-1.png", "1811.02721v3-Table3-1.png", "1811.02721v3-Figure7-1.png", "1811.02721v3-Table1-1.png", "1811.02721v3-Table5-1.png", "1811.02721v3-Figure8-1.png", "1811.02721v3-Table9-1.png"], "referred_figures_names": ["1811.02721v3-Table5-1.png"], "answer": "Relying on fragmentation is effective because the TCP/IP headers are only included in the first fragment, not in subsequent fragments. This significantly reduces the overhead in later fragments.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["Relying on fragmentation is effective because, as shown in Table , TCP/IP headers consume space in the first fragment, but not in subsequent fragments. Using an excessively large MSS, however, decreases reliability because the loss of one fragment results in the loss of an entire packet. Existing work~ has identified this trade-off and investigated it in simulation in the context of power consumption.", "The TCP/IP headers consume more than half of the frame's available MTU. As a result, TCP performs poorly, incurring more than 50\\% header overhead. Earlier approaches to running TCP over low-MTU links (e.g., low-speed serial links) have used TCP/IP header compression based on per-flow state~ to reduce header overhead.", "We instead choose an MSS larger than the link MTU admits, relying on fragmentation at the lower layers to decrease header overhead. Fragmentation is handled by 6LoWPAN, which acts at Layer 2.5, between the link and network layers. Unlike end-to-end IP fragmentation, the 6LoWPAN fragments exist only within the LLN, and are reassembled into IPv6 packets when leaving the network.", "In contrast, the 6LoWPAN adaptation layer~, designed for LLNs, supports only flow-independent compression of the IPv6 header using shared link-layer state, a clear departure from per-flow techniques. A key reason for this is that the compressor and decompressor in an LLN (host and border router) are separated by several IP hops,, using IP-layer routing to determine the path packets take through the mesh~. making it desirable for intermediate nodes to be able to determine a packet's IP header without per-flow context (see \\S{10 of ). That said, compressing TCP headers separately from IP addresses using per-flow state is a promising approach to further amortize header overhead.", "Furthermore, we propose using an atypical Maximum Segment Size (MSS) to manage header overhead and packet fragmentation. As a result, full-scale TCP operates well in LLNs, with 5--40 times higher throughput than existing (relatively simplistic) embedded TCP stacks (\\secref{sec:interactions). Hidden terminals are a serious problem when running TCP over multiple wireless hops.", "Aside from congestion control, \\sys{ benefits from header prediction~, which introduces a ``fast code path'' to process common-case TCP segments (in-sequence data and ACKs) more efficiently, and Challenge ACKs~, which make it more difficult for an attacker to inject an RST into a TCP connection. Enhancements such as these make us more confident that our observed results are fundamental to TCP, as opposed to artifacts of poor implementation. Furthermore, they allow us to focus on performance problems arising from the challenges of LLNs, as opposed to general TCP-related challenges that the research community has already solved in the context of traditional networks and operating systems.", "There is preliminary work in this direction~, but it is based on uIP, which has one in-flight segment, and does not fully specify how to resynchronize compression state after packet loss with a multi-segment window. It is also not officially standardized by the IETF. Therefore, this paper takes an approach orthogonal to header compression.", "In our memory-constrained setting, such a design is dangerous because its memory usage is nondeterministic; there is additional memory overhead, due to headers, if the data are delivered in many small packets instead of a few large ones. We opted for a flat array-based circular buffer for the receive buffer in \\sys{, primarily owing to its determinism in a limited-memory environment: buffer space is reserved at compile-time. Head/tail pointers delimit which part of the array stores in-sequence data."], "referred_text_chunks": ["Relying on fragmentation is effective because, as shown in Table , TCP/IP headers consume space in the first fragment, but not in subsequent fragments. Using an excessively large MSS, however, decreases reliability because the loss of one fragment results in the loss of an entire packet. Existing work~ has identified this trade-off and investigated it in simulation in the context of power consumption."]}, "4": {"question": "How does varying the buffer size affect TCP goodput?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 1, 'Rationale': 'Image 3 directly shows the effect of varying buffer size on TCP goodput, making it the most relevant. Text chunk 1 discusses buffer size variation and its impact on TCP bandwidth, aligning with the question.', 'Answer': 'Varying the buffer size directly affects TCP goodput by altering the flow window size. Larger buffers allow more segments to be in-flight, leading to higher goodput.'}", "all_figures_names": ["1811.02721v3-Figure6-1.png", "1811.02721v3-Table4-1.png", "1811.02721v3-Figure7-1.png", "1811.02721v3-Figure3-1.png", "1811.02721v3-Figure1-1.png", "1811.02721v3-Table1-1.png", "1811.02721v3-Figure5-1.png", "1811.02721v3-Figure13-1.png"], "referred_figures_names": ["1811.02721v3-Figure3-1.png"], "answer": "Increasing the buffer size generally leads to increased TCP goodput, but only up to a certain point.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Indeed, goodput is very sensitive to RTT when the buffer size is small, because TCP exhibits ``stop-and-wait'' behavior due to the small flow window. Upper Bound on Single-Hop Goodput We consider TCP goodput between two nodes over the IEEE 802.15.4 link, over a single hop without any border router. Using the Hamilton/OpenThread platform, we are able to achieve 75 kb/s. provides the corresponding goodput figures for Hamilton/GNRC and Firestorm/BLIP platforms, for comparison.", "Whereas simple TCP stacks, like uIP, allow only one in-flight segment, full-scale TCP requires complex buffering (\\secref{sec:buffering). In this section, we vary the size of the buffers (send buffer for uplink experiments and receive buffer for downlink experiments) to study how it affects the bandwidth. In varying the buffer size, we are directly affecting the size of TCP's flow window.", "This is specific to small buffers. In traditional environments, where links have higher throughput and buffers are large, it takes longer for \\cwnd{ to recover after packet loss, greatly limiting the sending rate with frequent packet losses. In contrast, in LLNs, where send/receive buffers are small, \\cwnd{ recovers to the maximum size quickly after packet loss, making TCP performance robust to packet loss.", "Overall, we estimate a 95 kb/s upper bound on goodput (100 kb/s without TCP headers). Our 75 kb/s measurement is within 25\\% of this upper bound, substantially higher than prior work (Table ). The difference from the upper bound is likely due to network stack processing and other real-world inefficiencies.", "For small offered load, the per-flow goodput increases linearly. Once the aggregate load saturates the network, goodput declines slightly and the interquartile range increases, due to inefficiences in independent flows competing for bandwidth. Overall, TCP performs similarly to CoAP and CoCoA, indicating that TCP's congestion control remains effective despite our observations in \\secref{ssec:congestion that it behaves differently in LLNs.", "Crucially, this means that the 2 KiB buffer size we determined in \\secref{sec:window, which fits comfortably in memory, remains applicable for up to three wireless hops. TCP Congestion Control in LLNs Recall that small send/receive buffers of only 1848 bytes (4 TCP segments) each are enough to achieve good TCP performance. This profoundly impacts TCP's congestion control mechanism.", "Let w be the size of TCP's flow window, measured in segments (for our experiments in \\secref{ssec:congestion, we would have w=4). Define b as the average number of windows sent in a burst. The goodput of TCP is the number of bytes sent in each burst, which is w \\cdot b \\cdot \\mss, divided by the duration of each burst.", "Modeling TCP Goodput in an LLN Our findings in \\secref{ssec:congestion suggest that, in LLNs, \\cwnd{ is limited by the buffer size, not packet loss. To validate this, we analytically model TCP performance according to our observations in \\secref{ssec:congestion, and then check if the resulting model is consistent with the data. Comprehensive models of TCP, which take window size limitations into account, already exist~; in contrast, our model is intentionally simple to provide intuition."], "referred_text_chunks": ["Indeed, goodput is very sensitive to RTT when the buffer size is small, because TCP exhibits ``stop-and-wait'' behavior due to the small flow window. Upper Bound on Single-Hop Goodput We consider TCP goodput between two nodes over the IEEE 802.15.4 link, over a single hop without any border router. Using the Hamilton/OpenThread platform, we are able to achieve 75 kb/s. provides the corresponding goodput figures for Hamilton/GNRC and Firestorm/BLIP platforms, for comparison."]}, "5": {"question": "What is the function of the Hamilton-based PCB in the ultrasonic anemometer?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 0, 'Rationale': 'Image 2 explicitly shows the Hamilton-based PCB used in the ultrasonic anemometer. Text 0 details the function of the PCB in measuring air velocity using ultrasonic transceivers.', 'Answer': 'The Hamilton-based PCB in the ultrasonic anemometer functions by utilizing four ultrasonic transceivers, arranged as vertices of a tetrahedron, to emit and measure ultrasound bursts to determine air velocity.'}", "all_figures_names": ["1811.02721v3-Table6-1.png", "1811.02721v3-Table2-1.png", "1811.02721v3-Figure13-1.png", "1811.02721v3-Figure7-1.png", "1811.02721v3-Table1-1.png", "1811.02721v3-Figure11-1.png", "1811.02721v3-Table3-1.png", "1811.02721v3-Figure5-1.png"], "referred_figures_names": ["1811.02721v3-Figure13-1.png"], "answer": "The Hamilton-based PCB is the electronic control board of the anemometer. It houses the microcontroller, sensors, and other electronic components that are necessary for the anemometer to function.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Therefore, anemometers must be battery-powered and must transmit readings wirelessly, making LLNs attractive. We used anemometers based on the Hamilton platform~, each consisting of four ultrasonic transceivers arranged as vertices of a tetrahedron (Figure ). To measure the air velocity, each transceiver, in turn, emits a burst of ultrasound, and the impulse is measured by the other three transceivers.", "An LLN Application An anemometer is a sensor that measures air velocity. Ane-mometers may be deployed in a building to diagnose problems with the Heating, Ventilation, and Cooling system (HVAC), and also to collect air flow measurements for improved HVAC control. This requires anemometers in difficult-to-reach locations, such as in air flow ducts, where it is infeasible to run wires.", "This process results in a total of 12 measurements. Calculating the air velocity from these measurements is computationally infeasible on the anemometer itself, because Hamilton does not have hardware floating point support and the computations require complex trigonometry. Measurements must be transmitted over the network to a server that processes the data.", "We note that the 1 Hz sample rate for this application is much higher than the sample rate of most sensors deployed in buildings. For example, a sensor measuring temperature, humidity, or occupancy in a building typically only generates a single reading every few tens of seconds or every few minutes. Furthermore, each individual reading from the anemometer is quite large (82 bytes), given that it encodes all 12 measurements (plus a small header).", "Given the higher data rate requirements of the anemometer application, we plan to use a higher-capacity battery than the standard AA batteries used in most motes. The higher cost of such a battery is justified by the higher cost of the anemometer transducers.", "Leaf nodes may send upstream traffic at any time. Each node uses CSMA-CA for medium access. Embedded Hardware We use two embedded hardware platforms: Hamilton~ and Firestorm~. Hamilton uses a SAMR21 SoC with a 48 MHz Cortex-M0+, 256 KiB of ROM, and 32 KiB of RAM.", "For concreteness, we model our experiments on the deployment of anemometers in a building, a real-world LLN use case described in \\appref{app:anemometer. Anemometers collect measurements frequently (once per second), making heavy use of the transport protocol; given that our focus is on transport performance, this makes anemometers a good fit for our study. Other sensor deployments (e.g., temperature, humidity, building occupancy, etc.)", "For example, whereas conventional computers have historically had roughly 1 MiB of RAM for every MIPS of CPU, as captured by the 3M rule, Hamilton has \\approx 50 DMIPS of CPU but only 32 KiB of RAM. \\smallskip \\parhead{Link-Layer Properties IEEE 802.15.4 is a low-bandwidth, wireless link with an MTU of only 104 bytes. The research community has explored using TCP with links that are separately low-bandwidth, wireless~, or low-MTU~, but addressing these issues together raises new challenges."], "referred_text_chunks": ["Therefore, anemometers must be battery-powered and must transmit readings wirelessly, making LLNs attractive. We used anemometers based on the Hamilton platform~, each consisting of four ultrasonic transceivers arranged as vertices of a tetrahedron (Figure ). To measure the air velocity, each transceiver, in turn, emits a burst of ultrasound, and the impulse is measured by the other three transceivers."]}, "6": {"question": "What is the difference in response time between CoAP and HTTP for a response size of 50 KiB?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 0, 'Rationale': 'Image 1 directly compares the response time for CoAP and HTTP at different response sizes, including 50 KiB. Text 0 explains the behavior of CoAP and HTTP/TCP concerning response time as response size changes.', 'Answer': 'For a response size of 50 KiB, CoAP has a longer response time compared to HTTP/TCP.'}", "all_figures_names": ["1811.02721v3-Table9-1.png", "1811.02721v3-Figure8-1.png", "1811.02721v3-Figure6-1.png", "1811.02721v3-Figure12-1.png", "1811.02721v3-Table4-1.png", "1811.02721v3-Figure9-1.png", "1811.02721v3-Figure3-1.png", "1811.02721v3-Figure1-1.png"], "referred_figures_names": ["1811.02721v3-Figure8-1.png"], "answer": "The difference in response time between CoAP and HTTP for a response size of 50 KiB is approximately 20 seconds.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["It plots the median time, with quartiles shown in error bars. HTTP takes longer than CoAP when the response size is small (consistent with Figure ), but CoAP takes longer when the response size is larger. This indicates that while HTTP/TCP has a greater fixed-size overhead than CoAP (higher y-intercept), it transfers data at a higher throughput (lower slope).", "TCP achieves a higher throughput than CoAP because CoAP sends response segments one-at-a-time (``stop and wait''), whereas TCP allows multiple segments to be in flight simultaneously (``sliding window''). To quantify the difference in throughput, we compare TCP and CoAP when transferring 50 KiB of data in Figure . TCP achieves 40\\% higher throughput compared to CoAP, over multiple hops and a duty-cycled link.", "\\subsubsection{Latency Analysis An HTTP request requires two round-trips: one to establish a TCP connection, and another for request/response. CoAP requires only one round trip (no connection establishment) and has smaller headers.", "Therefore, CoAP has a lower latency than HTTP/TCP when using an always-on link (Figure ). Even so, the latency of HTTP/TCP in this case is well below 1 second, not so large as to degrade user experience. We now explore how a duty-cycled link affects the latency.", "Additionally, Figure shows that, although most of TCP's retransmissions are explained by timeouts, a significant portion were triggered in other ways (e.g., duplicate ACKs). In contrast, CoAP and CoCoA rely exclusively on timeouts, which has intrinsic limitations~. With exceptionally high packet loss rates (>15\\%), CoAP achieves higher reliability than TCP, because it ``gives up'' after just 4 retries; it exponentially increases the wait time between those retries, but then resets its RTO to 3 seconds when giving up and moving to the next packet.", "In contrast, TCP performs up to 12 retries with exponential backoff. Thus, TCP backs off further than CoAP upon consecutive packet losses, witnessed by the smaller retransmission count in Figure , causing the application-layer queue to overflow more. This performance gap could be filled by parameter tuning.", "At a lower packet loss rate, TCP performs slightly worse than CoAP. This could be due to hidden terminal losses; more retries, on average, are required for indirect messages for TCP, causing leaf nodes to stay awake longer. Overall, CoAP and \\sys{ perform similarly (Table ).", "We use most of the remaining RAM as an application-layer queue to prevent data from being lost if CoAP or TCP is in backoff after packet loss and cannot send out new data immediately. We make use of adaptive duty cycling for both TCP and CoAP, with a base sleep interval of four minutes (OpenThread's default) and decreasing it to 100 ms when a TCP ACK or CoAP response is expected. We measure a solution's reliability as the proportion of generated readings delivered to the server."], "referred_text_chunks": ["It plots the median time, with quartiles shown in error bars. HTTP takes longer than CoAP when the response size is small (consistent with Figure ), but CoAP takes longer when the response size is larger. This indicates that while HTTP/TCP has a greater fixed-size overhead than CoAP (higher y-intercept), it transfers data at a higher throughput (lower slope)."]}, "7": {"question": "How does the maximum link delay affect the segment loss rate and goodput in a TCP connection with one hop?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 5, 'Rationale': 'Image 0 examines the effect of maximum link delay on segment loss rate and goodput for a one-hop TCP connection, directly addressing the question. Text 5 provides further insight into segment loss related to link-layer retransmissions.', 'Answer': 'Increasing maximum link delay generally increases segment loss rate and reduces goodput in a TCP connection with one hop.'}", "all_figures_names": ["1811.02721v3-Figure5-1.png", "1811.02721v3-Table2-1.png", "1811.02721v3-Table5-1.png", "1811.02721v3-Table6-1.png", "1811.02721v3-Figure8-1.png", "1811.02721v3-Table3-1.png", "1811.02721v3-Table4-1.png", "1811.02721v3-Figure12-1.png"], "referred_figures_names": ["1811.02721v3-Figure5-1.png"], "answer": "As the maximum link delay increases, the segment loss rate increases and the goodput decreases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Indeed, goodput is very sensitive to RTT when the buffer size is small, because TCP exhibits ``stop-and-wait'' behavior due to the small flow window. Upper Bound on Single-Hop Goodput We consider TCP goodput between two nodes over the IEEE 802.15.4 link, over a single hop without any border router. Using the Hamilton/OpenThread platform, we are able to achieve 75 kb/s. provides the corresponding goodput figures for Hamilton/GNRC and Firestorm/BLIP platforms, for comparison.", "Goodput over one hop was 64.1 kb/s; over two hops, 28.3 kb/s; over three hops, 19.5 kb/s; and over four hops, 17.5 kb/s. This roughly fits the model. This analysis justifies why the same window size works well for both the one-hop experiments and the three-hop experiments in \\secref{ssec:lldelay. Although the RTT is three times higher, the bandwidth-delay product is approximately the same.", "As expected, single-hop performance (Figure ) decreases as the delay between link retries increases; hidden terminals are not an issue in that setting. Packet loss is high for the multihop experiment (Figure ) when the link retry delay is 0, as is expected from hidden terminals. Adding a small delay between link retries, however, effectively reduces packet loss.", "TCP achieves a higher throughput than CoAP because CoAP sends response segments one-at-a-time (``stop and wait''), whereas TCP allows multiple segments to be in flight simultaneously (``sliding window''). To quantify the difference in throughput, we compare TCP and CoAP when transferring 50 KiB of data in Figure . TCP achieves 40\\% higher throughput compared to CoAP, over multiple hops and a duty-cycled link.", "Burst lengths depend on the packet loss rate p and rest lengths depend on RTT. This approach leads to the following model (full derivation is in Appendix ): \\noindent where B, the TCP goodput, is written in terms of the maximum segment size \\text{MSS, round-trip time \\text{RTT, packet loss rate p (0<p<1), and window size w (sized to BDP, in packets). Figures and include the predicted goodput as dotted lines, calculated according to Equation using the empirical RTT and segment loss rate for each experiment.", "\\hspace{-1.5ex \\hspace{-1.5ex \\hspace{-1.5ex \\hspace{-3ex \\caption{Effect of varying time between link-layer retransmissions. Reported ``segment loss'' is the loss rate of TCP segments, not individual IEEE 802.15.4 frames. It includes only losses not masked by link-layer retries.", "Overall, we estimate a 95 kb/s upper bound on goodput (100 kb/s without TCP headers). Our 75 kb/s measurement is within 25\\% of this upper bound, substantially higher than prior work (Table ). The difference from the upper bound is likely due to network stack processing and other real-world inefficiencies.", "Indeed, the BDP in this case is about 125 \\text{kb/s \\cdot 0.1 \\text{s \\approx 1.6 \\text{KiB.. Downlink goodput at a buffer size of one segment is unusually high. This is because FreeBSD does not delay ACKs if the receive buffer is full, reducing the effective RTT from \\approx 130 ms to \\approx 70 ms."], "referred_text_chunks": ["Indeed, goodput is very sensitive to RTT when the buffer size is small, because TCP exhibits ``stop-and-wait'' behavior due to the small flow window. Upper Bound on Single-Hop Goodput We consider TCP goodput between two nodes over the IEEE 802.15.4 link, over a single hop without any border router. Using the Hamilton/OpenThread platform, we are able to achieve 75 kb/s. provides the corresponding goodput figures for Hamilton/GNRC and Firestorm/BLIP platforms, for comparison."]}, "8": {"question": "What protocol has a higher radio duty cycle in the first 7 hours of the trial?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 shows the radio duty cycle of TCP and CoAP over a 24-hour period, which directly relates to the question about protocol performance in the first 7 hours. Text chunk 0 explains the dynamics of radio duty cycle during different hours, specifically addressing the comparison between TCP and CoAP.', 'Answer': 'CoAP has a higher radio duty cycle in the first 7 hours according to Image 7.'}", "all_figures_names": ["1811.02721v3-Table4-1.png", "1811.02721v3-Figure1-1.png", "1811.02721v3-Figure12-1.png", "1811.02721v3-Figure7-1.png", "1811.02721v3-Table7-1.png", "1811.02721v3-Table9-1.png", "1811.02721v3-Figure10-1.png", "1811.02721v3-Figure11-1.png"], "referred_figures_names": ["1811.02721v3-Figure11-1.png"], "answer": "TCP", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Figure depicts the radio duty cycle of TCP and CoAP for a trial representative of our overall results. CoAP maintains a lower duty cycle than \\sys{ outside of working hours, when there is less interference; \\sys{ has a slightly lower duty cycle than CoAP during working hours, when there is more wireless interference. \\sys{'s better performance at a higher loss rate is consistent with our results from \\secref{sec:injected.", "The real protocols have much higher duty cycles than the ideal protocol would have (Figure ), suggesting that a significant amount of their overhead stems from determining which packets were lost---polling the parent node for downstream TCP ACKs/CoAP responses. This gap could be reduced by improving OpenThread's MMC protocol. For example, rather than using a fixed sleep interval of 100 ms when an ACK is expected, one could use exponential backoff to increase the sleep interval if an ACK is not quickly received.", "We evaluate the protocols over multiple hops with duty-cycled radios and wireless interference, present in our testbed in the day (\\secref{s:hardware). In our experiments, nodes 12--15 (Figure ) send data to a server running on Amazon EC2. The RTT from the border router to the server was \\approx 12 ms, much smaller than within the low-power mesh (\\approx 100-300 ms).", "Compared to unreliable UDP, reliable approaches increase the radio/CPU duty cycle by 3x, in exchange for nearly 100\\% reliability. That said, the corresponding decrease in battery life will be less than 3x, because other sources of power consumption (reading from sensors, idle current) are also significant. For other sense-and-send applications that sample at a lower rate, TCP and CoAP would see higher reliability (less application queue loss), but UDP would not similarly benefit (no application queue).", "After serving the request, it restores the sleep interval to its old value. Unlike early LLN link-layer protocols like S-MAC~ that use an adaptive duty cycle, we use transport-layer state to inform the duty cycle. Figure shows the latency with adaptive duty cycling, where the sleep interval is temporarily reduced to 100 ms after connection establishment.", "Figures and also show that all the three protocols consume similar power; TCP is comparable to LLN-specific solutions. Both the radio and CPU duty cycle are significantly smaller with batching than without batching. By sending data in batches, nodes can amortize the cost of sending data and waiting for a response.", "We also consider an ideal ``roofline'' protocol to calculate a fairly loose lower bound on the duty cycle. This ideal protocol has the same header overhead as TCP, but learns which packets were lost for ``free,'' without using ACKs or running MMC. Thus, it turns on its radio only to send out data and retransmit lost packets.", "This results in an inflated RTT value that causes CoCoA to delay longer before retransmitting, causing the application-layer queue to overflow. Full-scale TCP is immune to this problem despite measuring the RTT, because the TCP timestamp option allows TCP to unambiguously determine the RTT even for retransmitted segments. Figures~ and show that, overall, TCP and CoAP perform comparably in terms of radio and CPU duty cycle."], "referred_text_chunks": ["Figure depicts the radio duty cycle of TCP and CoAP for a trial representative of our overall results. CoAP maintains a lower duty cycle than \\sys{ outside of working hours, when there is less interference; \\sys{ has a slightly lower duty cycle than CoAP during working hours, when there is more wireless interference. \\sys{'s better performance at a higher loss rate is consistent with our results from \\secref{sec:injected."]}, "9": {"question": "How does the maximum link delay affect the number of TCP timeouts and fast retransmissions?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 4, 'Rationale': 'Image 2 directly shows TCP timeouts and fast retransmissions as a function of maximum link delay. Text 4 explains TCP behavior with packet loss and buffer sizes, relating to performance and retransmissions.', 'Answer': 'As maximum link delay increases, both TCP timeouts and fast retransmissions decrease sharply, indicating that higher delays lead to fewer of both events occurring.'}", "all_figures_names": ["1811.02721v3-Figure7-1.png", "1811.02721v3-Figure13-1.png", "1811.02721v3-Figure6-1.png", "1811.02721v3-Figure3-1.png", "1811.02721v3-Figure12-1.png", "1811.02721v3-Table4-1.png", "1811.02721v3-Table7-1.png", "1811.02721v3-Table5-1.png"], "referred_figures_names": ["1811.02721v3-Figure6-1.png"], "answer": "The number of TCP timeouts and fast retransmissions decreases as the maximum link delay increases.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Additionally, Figure shows that, although most of TCP's retransmissions are explained by timeouts, a significant portion were triggered in other ways (e.g., duplicate ACKs). In contrast, CoAP and CoCoA rely exclusively on timeouts, which has intrinsic limitations~. With exceptionally high packet loss rates (>15\\%), CoAP achieves higher reliability than TCP, because it ``gives up'' after just 4 retries; it exponentially increases the wait time between those retries, but then resets its RTO to 3 seconds when giving up and moving to the next packet.", "With a sleep interval of 100 ms, we qualitatively observed that, although \\cwnd{ tends to recover more slowly after loss, due to the smaller MSS and larger RTT, it is still ``maxed out'' past the BDP most of the time. Therefore, we expect our conclusion, that TCP is more resilient to packet loss, to also apply in this setting. One may consider adapting our model for this setting by choosing a larger value of \\ell to reflect the fact that \\cwnd{ recovers from loss less quickly due to the smaller MSS.", "We propose adding a delay d between link-layer retransmissions, and demonstrate that it effectively reduces hidden-terminal-induced packet loss for TCP. We find that, because a small window size is sufficient for good performance in LLNs, TCP is quite resilient to spurious packet losses, as the congestion window can recover to a full window quickly after loss (\\secref{sec:multihop). To run TCP in a low-power context, we adaptively duty-cycle the radio to avoid poor interactions with TCP's self-clocking behavior.", "This results in an inflated RTT value that causes CoCoA to delay longer before retransmitting, causing the application-layer queue to overflow. Full-scale TCP is immune to this problem despite measuring the RTT, because the TCP timestamp option allows TCP to unambiguously determine the RTT even for retransmitted segments. Figures~ and show that, overall, TCP and CoAP perform comparably in terms of radio and CPU duty cycle.", "This is specific to small buffers. In traditional environments, where links have higher throughput and buffers are large, it takes longer for \\cwnd{ to recover after packet loss, greatly limiting the sending rate with frequent packet losses. In contrast, in LLNs, where send/receive buffers are small, \\cwnd{ recovers to the maximum size quickly after packet loss, making TCP performance robust to packet loss.", "Therefore, CoAP has a lower latency than HTTP/TCP when using an always-on link (Figure ). Even so, the latency of HTTP/TCP in this case is well below 1 second, not so large as to degrade user experience. We now explore how a duty-cycled link affects the latency.", "After a fast retransmission, TCP enters a ``fast recovery'' state~. Fast recovery requires buffer space to be effective, however. In particular, if the buffer contains only four TCP segments, then the lost packet, and three packets afterward which resulted in duplicate ACKs, account for the entire send buffer; therefore, TCP cannot send new data during fast recovery, and instead stalls for one RTT, until the ACK for the fast retransmission is received.", "In contrast, TCP performs up to 12 retries with exponential backoff. Thus, TCP backs off further than CoAP upon consecutive packet losses, witnessed by the smaller retransmission count in Figure , causing the application-layer queue to overflow more. This performance gap could be filled by parameter tuning."], "referred_text_chunks": ["Additionally, Figure shows that, although most of TCP's retransmissions are explained by timeouts, a significant portion were triggered in other ways (e.g., duplicate ACKs). In contrast, CoAP and CoCoA rely exclusively on timeouts, which has intrinsic limitations~. With exceptionally high packet loss rates (>15\\%), CoAP achieves higher reliability than TCP, because it ``gives up'' after just 4 retries; it exponentially increases the wait time between those retries, but then resets its RTO to 3 seconds when giving up and moving to the next packet."]}, "10": {"question": "Which module of TCPlp consumes the most memory in the active RAM on TinyOS, and how much memory does it utilize?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 4, 'Rationale': 'Image 7 directly shows the memory usage by the protocol in active RAM, and Text 4 discusses the memory footprint on TinyOS.', 'Answer': 'The protocol module of TCPlp consumes the most memory in active RAM on TinyOS, utilizing 488 bytes.'}", "all_figures_names": ["1811.02721v3-Table1-1.png", "1811.02721v3-Table5-1.png", "1811.02721v3-Table7-1.png", "1811.02721v3-Figure10-1.png", "1811.02721v3-Figure8-1.png", "1811.02721v3-Table9-1.png", "1811.02721v3-Figure12-1.png", "1811.02721v3-Table8-1.png"], "referred_figures_names": ["1811.02721v3-Table8-1.png"], "answer": "The protocol implementation module consumes the most memory in the active RAM on TinyOS, utilizing 488 bytes.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["The memory required for the protocol and application state of an active TCP socket fits in a few hundred bytes, less than 1\\% of the available RAM on the Cortex-M4 (Firestorm) and 2\\% of that on the Cortex-M0+ (Hamilton). Although \\sys{ includes heavyweight features not traditionally included in embedded TCP stacks, it fits well within available memory. Memory-Efficient Data Buffering Existing embedded TCP stacks, such as uIP and BLIP, allow only one TCP packet in the air, eschewing careful implementation of send and receive buffers~.", "Given that low-power embedded sensors typically have only one CPU, however, we opted for simplicity, instead using a single global TCP lock for \\sys{. \\smallskip \\parhead{BLIP (TinyOS) TinyOS uses an event-driven concurrency model based on split-phase operations, consisting of an event loop that executes on a single stack. For concurrency, TinyOS provides three types of unique operations: commands and events, which are executed immediately, and tasks, which are scheduled for execution after all preceding tasks are completed.", "Representative operating systems, like TinyOS and Contiki, implement UDP/RPL/IPv6/6LoWPAN network stacks with IEEE 802.15.4-compatible MMC protocols for 16-bit platforms like TelosB~. TCP, however, is not widely adopted in LLNs. The few LLN studies that use TCP~ generally use a simplified TCP stack (\\appref{sec:features), such as uIP. In summary, despite the acceptance of IPv6, LLNs remain highly tailored at the transport layer to the application at hand.", "Below, we explain how LoWPANs are different from other networks where TCP has been successfully adapted. \\smallskip \\parhead{Resource Constraints When TCP was initially adopted by ARPANET in the early 1980s, contemporary Internet citizens---typically minicomputers and high-end workstations, but not yet personal computers---usually had at least 1 MiB of RAM. 1 MiB is tiny by today's standards, yet the LLN-class devices we consider in this work have 1-2 orders of magnitude less RAM than even the earliest computers connected with TCP/IP.", "Memory Usage: Connection State To complement Table , which shows \\sys{'s memory footprint on RIOT OS, we include Table , which shows \\sys{'s memory footprint on TinyOS. Performance Comparison We consider TCP goodput between two embedded nodes over the IEEE 802.15.4 link, over a single hop without any border router, as we did in \\secref{sec:upper_bound. We are able to produce a 63 kb/s goodput over a TCP connection between two Hamilton motes using RIOT's GNRC network stack.", "In TinyOS, for example, the BLIP network stack supports vectored I/O; an outgoing packet passed to the IPv6 layer is specified as an \\texttt{iovec. Instead of allocating memory in the packet heap for each outgoing packet, \\sys{ simply creates \\texttt{iovecs that point to existing data in the send buffer. This decreases the required size of the packet heap.", "\\subsubsection{Power Consumption TCP consumes more energy than CoAP due to the extra round-trip at the beginning. In practice, however, a web server is interactive, and therefore will be idle most of the time. Thus, the idle power consumption dominates.", "At night, when there are few/no people in the space, there is much less interference. Implementation of \\sys{ We seek to answer the following two questions: (1) Does full-scale TCP fit within the limited memory of modern LLN platforms? (2) How can we integrate a TCP implementation from a traditional OS into an embedded OS?"], "referred_text_chunks": ["The memory required for the protocol and application state of an active TCP socket fits in a few hundred bytes, less than 1\\% of the available RAM on the Cortex-M4 (Firestorm) and 2\\% of that on the Cortex-M0+ (Hamilton). Although \\sys{ includes heavyweight features not traditionally included in embedded TCP stacks, it fits well within available memory. Memory-Efficient Data Buffering Existing embedded TCP stacks, such as uIP and BLIP, allow only one TCP packet in the air, eschewing careful implementation of send and receive buffers~."]}, "11": {"question": "How does the reliability of CoAP compare to TCPlp and what potential factors contribute to this difference?", "referred_figures_indices": [1], "response": "{'Image': 6, 'Text': 5, 'Rationale': 'Image 6 directly compares the reliability of CoAP to TCPlp under varying loss rates, which is essential for the question. Text 5 discusses the conditions under which CoAP performs better than TCP, mentioning reliability at different loss rates, which is relevant to understanding reliability differences.', 'Answer': 'At packet loss rates below 15%, both CoAP and TCPlp achieve nearly 100% reliability. However, CoAP can perform better at higher loss rates as it gives up on a packet more quickly after retries. This difference is attributed to CoAP\u2019s strategy of limiting retries and resetting the RTO, whereas TCPlp may continue with more retries, causing potential application-layer queue overflow.'}", "all_figures_names": ["1811.02721v3-Figure13-1.png", "1811.02721v3-Table7-1.png", "1811.02721v3-Table1-1.png", "1811.02721v3-Table8-1.png", "1811.02721v3-Figure8-1.png", "1811.02721v3-Table6-1.png", "1811.02721v3-Figure10-1.png", "1811.02721v3-Figure7-1.png"], "referred_figures_names": ["1811.02721v3-Table7-1.png"], "answer": "Table 1 shows that CoAP has slightly higher reliability (99.5%) compared to TCPlp (99.3%). While both protocols perform well, this difference could be attributed to several factors, including:\n\nRetransmission mechanisms: CoAP employs a built-in retransmission mechanism for lost packets, while TCPlp relies on the underlying network layer for retransmissions. This could give CoAP an edge in recovering lost packets and achieving higher reliability.\nCongestion control: CoAP includes mechanisms to adapt to network congestion, potentially reducing packet loss and improving reliability.\nPacket size: CoAP typically uses smaller packets compared to TCPlp. Smaller packets are less prone to loss in wireless networks, potentially contributing to CoAP's slightly higher reliability.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["To verify the effectiveness of full-scale TCP in LLNs, we compare with CoAP~, CoCoA~, and unreliable UDP. CoAP is a standard LLN protocol that provides reliability on top of UDP. It is the most promising LLN alternative to TCP, gaining momentum in both academia~ and industry~, with adoption by Cisco~, Nest/Google~, and Arm~. CoCoA~ is a recent proposal that augments CoAP with RTT estimation.", "Additionally, Figure shows that, although most of TCP's retransmissions are explained by timeouts, a significant portion were triggered in other ways (e.g., duplicate ACKs). In contrast, CoAP and CoCoA rely exclusively on timeouts, which has intrinsic limitations~. With exceptionally high packet loss rates (>15\\%), CoAP achieves higher reliability than TCP, because it ``gives up'' after just 4 retries; it exponentially increases the wait time between those retries, but then resets its RTO to 3 seconds when giving up and moving to the next packet.", "TCP achieves a higher throughput than CoAP because CoAP sends response segments one-at-a-time (``stop and wait''), whereas TCP allows multiple segments to be in flight simultaneously (``sliding window''). To quantify the difference in throughput, we compare TCP and CoAP when transferring 50 KiB of data in Figure . TCP achieves 40\\% higher throughput compared to CoAP, over multiple hops and a duty-cycled link.", "In contrast, TCP performs up to 12 retries with exponential backoff. Thus, TCP backs off further than CoAP upon consecutive packet losses, witnessed by the smaller retransmission count in Figure , causing the application-layer queue to overflow more. This performance gap could be filled by parameter tuning.", "At a lower packet loss rate, TCP performs slightly worse than CoAP. This could be due to hidden terminal losses; more retries, on average, are required for indirect messages for TCP, causing leaf nodes to stay awake longer. Overall, CoAP and \\sys{ perform similarly (Table ).", "Both CoAP and TCP achieve nearly 100\\% reliability at packet loss rates less than 15\\%, as shown in Figure . At loss rates greater than 9\\%, CoCoA performs poorly. The reason is that CoCoA attempts to measure RTT for retransmitted packets, and conservatively calculates the RTT relative to the first transmission.", "It is attractive to compare TCP to a variety of commercial systems, as has been done by a number of studies in LTE/WLANs~. Unfortunately, multihop LLNs have not yet reached the level of maturity to support a variety of commercial offerings; only CoAP has an appreciable level of commercial adoption. Other protocols are research proposals that often (1) are implemented for now-outdated operating systems and hardware or exist only in simulation~, (2) target a very specific application paradigm~, and/or (3) do not use IP~. We choose CoAP and CoCoA because they are not subject to these constraints.", "Given that TCP and CoAP both guarantee reliability, a reliability measurement of less than 100\\% is caused by overflow of the application-layer queue due to poor network conditions preventing data from being reliably communicated as fast as they are generated. Generating data more slowly would result in higher reliability. \\subsubsection{Performance in Favorable Conditions We begin with experiments in our testbed at night, when there is less wireless interference."], "referred_text_chunks": ["To verify the effectiveness of full-scale TCP in LLNs, we compare with CoAP~, CoCoA~, and unreliable UDP. CoAP is a standard LLN protocol that provides reliability on top of UDP. It is the most promising LLN alternative to TCP, gaining momentum in both academia~ and industry~, with adoption by Cisco~, Nest/Google~, and Arm~. CoCoA~ is a recent proposal that augments CoAP with RTT estimation."]}, "12": {"question": "How does the memory usage of the RIOT OS posix_sockets module compare to the memory used by the protocol and socket layer combined, for both active and passive connections?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 2, 'Rationale': 'Image 6 provides a clear comparison of memory usage between the protocol, socket layer, and posix_sockets module on RIOT OS. Text chunk 2 discusses memory requirements for active TCP sockets, fitting within available RAM on specific platforms, which is relevant to memory usage discussions.', 'Answer': 'The posix_sockets module uses 48 B for both active and passive connections. In contrast, the protocol uses 364 B for active and 12 B for passive, and the socket layer uses 88 B for both, indicating that posix_sockets has lower memory usage compared to the protocol and socket layer combined.'}", "all_figures_names": ["1811.02721v3-Table9-1.png", "1811.02721v3-Figure6-1.png", "1811.02721v3-Table2-1.png", "1811.02721v3-Table8-1.png", "1811.02721v3-Figure9-1.png", "1811.02721v3-Figure3-1.png", "1811.02721v3-Table3-1.png", "1811.02721v3-Figure8-1.png"], "referred_figures_names": ["1811.02721v3-Table3-1.png"], "answer": "The posix_sockets module consistently uses less memory than the combined usage of the protocol and socket layer. For an active connection, it requires about 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers. Similarly, for a passive connection, it uses 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["We do, however, retain challenge ACKs~. We use separate structures for active sockets used to send and receive bytes, and passive sockets used to listen for incoming connections, as passive sockets require less memory. Table depicts the memory footprint of \\sys{ on RIOT OS.", "Memory Usage: Connection State To complement Table , which shows \\sys{'s memory footprint on RIOT OS, we include Table , which shows \\sys{'s memory footprint on TinyOS. Performance Comparison We consider TCP goodput between two embedded nodes over the IEEE 802.15.4 link, over a single hop without any border router, as we did in \\secref{sec:upper_bound. We are able to produce a 63 kb/s goodput over a TCP connection between two Hamilton motes using RIOT's GNRC network stack.", "The memory required for the protocol and application state of an active TCP socket fits in a few hundred bytes, less than 1\\% of the available RAM on the Cortex-M4 (Firestorm) and 2\\% of that on the Cortex-M0+ (Hamilton). Although \\sys{ includes heavyweight features not traditionally included in embedded TCP stacks, it fits well within available memory. Memory-Efficient Data Buffering Existing embedded TCP stacks, such as uIP and BLIP, allow only one TCP packet in the air, eschewing careful implementation of send and receive buffers~.", "Each network protocol supported by RIOT OS has a single thread. Because a thread's message queue in RIOT OS is used to hold both received packets and timer events, there is no guarantee when a timer expires that there is enough space in the thread message queue to accept a timer event; if there is not enough space, RIOT OS drops the timer event. Furthermore, if a timer expires multiple times before its event is handled by the thread, multiple events for the same timer can exist simultaneously in the queue; we cannot find an upper bound on the number of slots in the message queue used by a single timer.", "Each thread has a priority and can be preempted by a thread with higher priority or by an interrupt. The thread for a lower network layer has higher priority than the thread for a higher layer. The port of OpenThread for RIOT OS handles received packets in one thread and sends packets from another thread, where the thread for received packets has higher priority~.", "Asynchronous interaction with hardware is done by interrupt handlers that preempt the current thread, perform a short operation in the interrupt context, and signal a related thread to perform any remaining operation outside of interrupt context. Then the thread is placed on the RIOT OS scheduler queue and is scheduled for execution depending on its priority. The GNRC network stack for RIOT OS runs each network layer (or module) in a separate thread.", "In TinyOS, for example, the BLIP network stack supports vectored I/O; an outgoing packet passed to the IPv6 layer is specified as an \\texttt{iovec. Instead of allocating memory in the packet heap for each outgoing packet, \\sys{ simply creates \\texttt{iovecs that point to existing data in the send buffer. This decreases the required size of the packet heap.", "As explained in \\appref{s:timers, we also use an additional thread for timer callbacks in RIOT OS. Given that TCP state can be accessed concurrently from multiple threads---the TCP thread (GNRC) or receive thread (OpenThread), the application thread(s), and timer callbacks---we needed to synchronize access to it. The FreeBSD implementation allows fine-grained locking of connection state to allow different connections to be serviced in parallel on different CPUs."], "referred_text_chunks": ["We do, however, retain challenge ACKs~. We use separate structures for active sockets used to send and receive bytes, and passive sockets used to listen for incoming connections, as passive sockets require less memory. Table depicts the memory footprint of \\sys{ on RIOT OS."]}, "13": {"question": "Which TCP stack provides the most complete implementation of core TCP features, and which stack lacks the most features?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 3, 'Rationale': 'Image 7 provides a direct comparison of core TCP features across different stacks, clearly showing which stack has the most and fewest features. Text 3 discusses the benefits of a full-scale TCP implementation and highlights the completeness of certain stacks.', 'Answer': 'The TCP stack TCPlp provides the most complete implementation of core TCP features, while GNRC lacks the most features.'}", "all_figures_names": ["1811.02721v3-Figure11-1.png", "1811.02721v3-Table1-1.png", "1811.02721v3-Table5-1.png", "1811.02721v3-Figure12-1.png", "1811.02721v3-Figure3-1.png", "1811.02721v3-Figure10-1.png", "1811.02721v3-Figure5-1.png", "1811.02721v3-Table9-1.png"], "referred_figures_names": ["1811.02721v3-Table9-1.png"], "answer": "The TCP stack presented in this paper (TCPlp) provides the most complete implementation of core TCP features, including flow control, congestion control, RTT estimation, MSS option, OOO reassembly, and various advanced features like timestamps and selective ACKs. In contrast, BLIP lacks the most features, as it does not implement congestion control, RTT estimation, or several other functionalities present in other stacks.", "figure_type": "N/A", "content_type": "table", "text_chunks": ["This suggests that the implementation of the underlying network stack, particularly with regard to concurrency, could affect TCP performance in LLNs. Comparison of Features in Embedded TCP Implementations Table compares the featureset of \\sys{ to features in embedded TCP stacks. The TCP implementations in uIP and BLIP lack features core to TCP.", "Network Stack \\parhead{Transport layer That only a few full-scale TCP stacks exist, with a body of literature covering decades of refining, demonstrates that developing a feature-complete implementation of TCP is complex and error-prone~. Using a well-tested TCP implementation would ensure that results from our measurement study are due to the TCP protocol, not an artifact of the TCP implementation we used. Thus, we leverage the TCP implementation in FreeBSD 10.3~ to ground our study. We ported it to run in embedded operating systems and resource-constrained embedded devices (\\secref{s:hardware).", "The memory required for the protocol and application state of an active TCP socket fits in a few hundred bytes, less than 1\\% of the available RAM on the Cortex-M4 (Firestorm) and 2\\% of that on the Cortex-M0+ (Hamilton). Although \\sys{ includes heavyweight features not traditionally included in embedded TCP stacks, it fits well within available memory. Memory-Efficient Data Buffering Existing embedded TCP stacks, such as uIP and BLIP, allow only one TCP packet in the air, eschewing careful implementation of send and receive buffers~.", "\\smallskip \\parhead{Benefits of full-scale TCP In addition to supporting the protocol-level features summarized in Table , \\sys{ is likely more robust than other embedded TCP stacks because it is based on a well-tested TCP implementation. While seemingly minor, some details, implemented incorrectly by TCP stacks, have had important consequences for TCP's behavior~. \\sys{ benefits from a thorough implementation of each aspect of TCP. For example, \\sys{, by virtue of using the FreeBSD TCP implementation, benefits from a robust implementation of congestion control.", "uIP allows only one unACKed in-flight segment, eschewing TCP's sliding window. BLIP does not implement RTT estimation or congestion control. The TCP implementation in GNRC lacks features such as TCP timestamps, selective ACKs, and delayed ACKs, which are present in most full-scale TCP implementations.", "Despite these developments, transport in LLNs has remained ad-hoc and TCP has received little serious consideration. Many embedded IP stacks (e.g., OpenThread~) do not even support TCP, and those that do implement only a subset of its features (Appendix ). The conventional wisdom is that IP holds merit, but TCP is ill-suited to LLNs.", "Representative operating systems, like TinyOS and Contiki, implement UDP/RPL/IPv6/6LoWPAN network stacks with IEEE 802.15.4-compatible MMC protocols for 16-bit platforms like TelosB~. TCP, however, is not widely adopted in LLNs. The few LLN studies that use TCP~ generally use a simplified TCP stack (\\appref{sec:features), such as uIP. In summary, despite the acceptance of IPv6, LLNs remain highly tailored at the transport layer to the application at hand.", "At night, when there are few/no people in the space, there is much less interference. Implementation of \\sys{ We seek to answer the following two questions: (1) Does full-scale TCP fit within the limited memory of modern LLN platforms? (2) How can we integrate a TCP implementation from a traditional OS into an embedded OS?"], "referred_text_chunks": ["This suggests that the implementation of the underlying network stack, particularly with regard to concurrency, could affect TCP performance in LLNs. Comparison of Features in Embedded TCP Implementations Table compares the featureset of \\sys{ to features in embedded TCP stacks. The TCP implementations in uIP and BLIP lack features core to TCP."]}, "14": {"question": "How many hops are there between the Hamilton and the Internet?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 1, 'Rationale': 'Image 5 clearly illustrates the network topology showing the 5 hops required from Hamilton to the Internet through the border router. Text chunk 1 describes this setup, including the role of the border router and interference effects.', 'Answer': 'There are 5 hops between the Hamilton and the Internet.'}", "all_figures_names": ["1811.02721v3-Table8-1.png", "1811.02721v3-Figure13-1.png", "1811.02721v3-Table3-1.png", "1811.02721v3-Figure8-1.png", "1811.02721v3-Table4-1.png", "1811.02721v3-Figure1-1.png", "1811.02721v3-Figure7-1.png", "1811.02721v3-Table1-1.png"], "referred_figures_names": ["1811.02721v3-Figure1-1.png"], "answer": "5 hops", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Leaf nodes may send upstream traffic at any time. Each node uses CSMA-CA for medium access. Embedded Hardware We use two embedded hardware platforms: Hamilton~ and Firestorm~. Hamilton uses a SAMR21 SoC with a 48 MHz Cortex-M0+, 256 KiB of ROM, and 32 KiB of RAM.", "OpenThread forms a 3-to-5-hop topology at transmission power of -8 dBm. Embedded TCP endpoints (Hamiltons) communicate with a Linux TCP endpoint (server on Amazon EC2) via the border router. During working hours, interference is present in the channel, due to people in the space using Wi-Fi and Bluetooth devices in the 2.4 GHz frequency band.", "\\vspace{-1ex \\end{figure* TCP Over Multiple Wireless Hops We instrument TCP connections between Hamilton nodes in our multi-hop testbed, without using the EC2 server. Mitigating Hidden Terminals in LLNs Prior work over traditional WLANs has shown that hidden terminals degrade TCP performance over multiple wireless hops~. Using RTS/CTS for hidden terminal avoidance has been shown to be effective in WLANs.", "Indeed, goodput is very sensitive to RTT when the buffer size is small, because TCP exhibits ``stop-and-wait'' behavior due to the small flow window. Upper Bound on Single-Hop Goodput We consider TCP goodput between two nodes over the IEEE 802.15.4 link, over a single hop without any border router. Using the Hamilton/OpenThread platform, we are able to achieve 75 kb/s. provides the corresponding goodput figures for Hamilton/GNRC and Firestorm/BLIP platforms, for comparison.", "For example, whereas conventional computers have historically had roughly 1 MiB of RAM for every MIPS of CPU, as captured by the 3M rule, Hamilton has \\approx 50 DMIPS of CPU but only 32 KiB of RAM. \\smallskip \\parhead{Link-Layer Properties IEEE 802.15.4 is a low-bandwidth, wireless link with an MTU of only 104 bytes. The research community has explored using TCP with links that are separately low-bandwidth, wireless~, or low-MTU~, but addressing these issues together raises new challenges.", "This process results in a total of 12 measurements. Calculating the air velocity from these measurements is computationally infeasible on the anemometer itself, because Hamilton does not have hardware floating point support and the computations require complex trigonometry. Measurements must be transmitted over the network to a server that processes the data.", "Let B be the bandwidth over a single hop. Consider a two-hop setup: S \\rightarrow R_1 \\rightarrow D. R_1 cannot receive a frame from S while sending a frame to D, because its radio cannot transmit and receive simultaneously. Thus, the maximum achievable bandwidth over two hops is \\frac{B{2.", "We evaluate the protocols over multiple hops with duty-cycled radios and wireless interference, present in our testbed in the day (\\secref{s:hardware). In our experiments, nodes 12--15 (Figure ) send data to a server running on Amazon EC2. The RTT from the border router to the server was \\approx 12 ms, much smaller than within the low-power mesh (\\approx 100-300 ms)."], "referred_text_chunks": ["Leaf nodes may send upstream traffic at any time. Each node uses CSMA-CA for medium access. Embedded Hardware We use two embedded hardware platforms: Hamilton~ and Firestorm~. Hamilton uses a SAMR21 SoC with a 48 MHz Cortex-M0+, 256 KiB of ROM, and 32 KiB of RAM."]}}