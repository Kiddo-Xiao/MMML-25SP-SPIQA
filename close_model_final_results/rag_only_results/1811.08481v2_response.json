{"0": {"question": "Which method has the highest overall accuracy for answering questions about images on the 100 questions sampled from VQA v2 dataset?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 3, 'Rationale': 'Image 2 provides the overall accuracy comparison for answering 100 questions sampled from VQA v2 using the UnCoRd-VG-E method and Pythia. Text chunk 3 discusses the superiority of UnCoRd over Pythia in this context.', 'Answer': 'UnCoRd-VG-E has the highest overall accuracy for answering questions on the 100 questions sampled from the VQA v2 dataset.'}", "all_figures_names": ["1811.08481v2-Table3-1.png", "1811.08481v2-Table5-1.png", "1811.08481v2-Table8-1.png", "1811.08481v2-Figure1-1.png", "1811.08481v2-Figure9-1.png", "1811.08481v2-Table1-1.png", "1811.08481v2-Table7-1.png", "1811.08481v2-Figure2-1.png"], "referred_figures_names": ["1811.08481v2-Table8-1.png"], "answer": "UnCoRd-VG-E", "figure_type": "table", "content_type": "table", "text_chunks": ["We next describe an experiment for real-world images, where we use real world visual estimators (see Section ) and our most general trained mapper (VG-Enhanced). We compare our model to Pythia , which has top performance on the VQA v2 dataset . The experiment includes two parts: \\vspace{0.5ex \\item 'VQA\\_v2' questions: 100 questions sampled from VQA v2 dataset with terms that have visual estimators in UnCoRd and unambiguous answers (annotated by us).", "Current approaches to handle VQA by a machine take a different path, where most answering systems are trained directly to select an answer from common answers of a training set, based on fused image features (mostly using a pre-trained CNN ) and question features (mostly using an RNN).% The answering approach we take below is the first, as far as we know, that does not rely on any explicit question-answering training. It uses a process composed according to the question's structure, and applies a sequence of `visual estimators' for object detection and identifying a set of visual properties and relations. Answering by our 'Understand, Compose and Respond' (UnCoRd) approach is divided into two stages (illustrated in Figure ).", "This result is interesting as using this model provides high accuracy for CLEVR as well (see Table ). The fact that substantial performance gain is achieved for a data domain that was not used in training (the VQA dataset domain), while preserving good results on the original data (CLEVR), demonstrates the potential of the approach to provide a general answering system for visual questions. Further investigation is required for means to enrich question description examples and produce further significant improvements.", "As can be seen, UnCoRd's results are better by a large margin, compared to Pythia end-to-end model, even though questions were sampled from VQA v2, a dataset used for Pythia's training. As in the previous part, all UnCoRd's failures are only due to wrong results of the invoked visual estimators. Examples of UnCoRd's answers for the VQA\\_v2 questions are given in Figure , including the corresponding answers of Pythia.", "Some examples are shown in Figure (elaborated later in text).% \\vspace{-1ex Related Work \\vspace{-1ex Current answering schemes are dominated by end-to-end methods, trained as multi-class classifiers. Many recent works focused on improving % the image-question fused features , attention mechanisms for selecting important features , including self and guided attention , applying pre-trained networks , and incorporating outputs of other visual tasks . Some provide reasoning using \"facts\" extraction (\\eg scene type) , image caption or by linking visual \"facts\" with question's logical form .", "\\vspace{-2ex \\subsubsection{VQA Representation \\vspace{-1ex In this experiment, representation capabilities are tested for a different dataset. Since normally, annotations corresponding to our graph representation are not provided, we sampled 100 questions of the VQA validation set and manually examined the results for the eight question-to-graph models (see Section ). The results in Table express the large gaps in the abilities of models to represent new domains.", "Examples of successful UnCoRd answers to the non VQA\\_v2 questions are provided in Figure , while failure examples, including failure sources, are shown in Figure . Pythia's answers are given as well. \\definecolor{green2{rgb{0.0, 0.5, 0.0 Results for the 100 VQA\\_v2 questions are given in Table .", "The main immediate bottleneck is obtaining question-to-graph mapping with general representation capabilities for a broad range of questions. % Question graph representation may also be enhanced to support questions with more complex logic, as well as extending the scope of the supported visual categories (\\eg global scene types). Any general VQA requires vast estimation capabilities, as any visual category can be queried."], "referred_text_chunks": ["We next describe an experiment for real-world images, where we use real world visual estimators (see Section ) and our most general trained mapper (VG-Enhanced). We compare our model to Pythia , which has top performance on the VQA v2 dataset . The experiment includes two parts: \\vspace{0.5ex \\item 'VQA\\_v2' questions: 100 questions sampled from VQA v2 dataset with terms that have visual estimators in UnCoRd and unambiguous answers (annotated by us)."]}, "1": {"question": "Explain how the answering procedure works.", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 2, 'Rationale': 'Image 1 provides a schematic illustration of the answering process, including stages and components involved. Text 2 describes how a graph representation is generated and used in the answering procedure, which aligns with the explanation needed.', 'Answer': 'The answering procedure works in two stages. First, a graph is generated from the question, outlining classes, properties, and relations. Then, a recursive procedure follows this graph to find valid assignments in the image, using visual estimators to evaluate object properties and relations.'}", "all_figures_names": ["1811.08481v2-Figure4-1.png", "1811.08481v2-Figure2-1.png", "1811.08481v2-Figure1-1.png", "1811.08481v2-Table8-1.png", "1811.08481v2-Table1-1.png", "1811.08481v2-Figure5-1.png", "1811.08481v2-Table7-1.png", "1811.08481v2-Table5-1.png"], "referred_figures_names": ["1811.08481v2-Figure2-1.png"], "answer": "The answering procedure follows the question graph, searching for a valid assignment in the image. At each step, the handled node is set and objects (extracted using Mask R-CNN) are examined according to the node\u2019s requirements (utilizing corresponding visual estimators). If successful, a new node is set (according to a DFS traversal) and the function is called again to handle the unassigned subgraph.", "figure_type": "** Schematic", "content_type": "figure", "text_chunks": ["The question is answered by finding an assignment to the image that will make the statement true, and retrieving the free variables. The quantifiers derived from the question require multiple true assignments (such as '5', 'all', etc.). The procedure we use seeks the required assignments and returns the desired answer.", "% The answering process consists of two stages (see Figure for a scheme): \\pgfdeclarelayer{background \\pgfdeclarelayer{foreground \\pgfsetlayers{background,main,foreground \\tikzstyle{bp = [draw , fill= green!20, text width= 5em, text centered, minimum height=2.5em, drop shadow] \\tikzstyle{vis = [bp , fill= blue!20, text width=3.6em, minimum height=1.4em] \\tikzstyle{ann = [above, text width= 5em, text centered] \\tikzstyle{op = [bp , fill= red!20, text width= 8em, minimum height=11em, rounded corners] \\tikzstyle{ex = [op , fill= orange!20, text width= 5em, minimum height=5em] \\tikzstyle{im =", "First, a graph representation is generated for the question, in terms of classes, properties and relations, supplemented with quantifiers and logical connectives. An answering procedure then follows the question graph, and seeks either a single or multiple assignments of the classes, properties and relations in the graph to the image (Section ). The method is modular, extensible and uses intermediate results to provide elaborated answers, including alternatives to answers not grounded in the image, and notifying about unsupported categories.", "Introduction \\vspace{-1ex Visual question answering is inspired by the remarkable human ability to answer specific questions on images, which may require analysis of subtle cues, along with the integration of prior knowledge and experience. The learning of new visual classes, properties and relations, can be easily integrated into the question-answering process. Humans can elaborate on the answers they give, explain how they were derived, and why they failed to produce an adequate answer.", "Current approaches to handle VQA by a machine take a different path, where most answering systems are trained directly to select an answer from common answers of a training set, based on fused image features (mostly using a pre-trained CNN ) and question features (mostly using an RNN).% The answering approach we take below is the first, as far as we know, that does not rely on any explicit question-answering training. It uses a process composed according to the question's structure, and applies a sequence of `visual estimators' for object detection and identifying a set of visual properties and relations. Answering by our 'Understand, Compose and Respond' (UnCoRd) approach is divided into two stages (illustrated in Figure ).", "In this stage, a recursive procedure finds valid assignments of the graph in the image. The number of required assignments for each node is determined by its quantifiers. The procedure follows the graph, invoking relevant sub-procedures and integrates the information to provide the answer.", "The question graph, the image and the mask R-CNN produced for the image provide the input to the procedure that recursively processes each node (see Figure ). For each node, basic procedures (see Section ) are invoked sequentially, according to the node's requirements and activate visual estimators according to the particular visual elements. The number of required valid assignments is set by the node's quantifier (a single assignment, a specific number, or all) or by the need of all objects for evaluating the entire object set (\\eg counting, number comparisons).", "Importantly, it depends only on the abstract structure of the question graph, where the particular object classes, properties and relations are parameters, used to apply the corresponding visual estimators (\\eg which property to extract). The invoked sub-procedures are selected from a pool of the following basic procedures, which are simple visual procedures used to compose the full answering procedure: % \\end{enumerate \\vspace{0.4ex Our construction of a question graph and using its abstract structure to guide the answering procedure leads to our ability to handle novel domains by adding visual estimators but using the same answering procedure. In our method we only train the question-to-graph mappers and the required visual estimators."], "referred_text_chunks": ["The question is answered by finding an assignment to the image that will make the statement true, and retrieving the free variables. The quantifiers derived from the question require multiple true assignments (such as '5', 'all', etc.). The procedure we use seeks the required assignments and returns the desired answer."]}, "2": {"question": "Which method has the highest overall accuracy?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 5, 'Rationale': 'Image 7 shows a clear comparison of overall accuracy between methods. Text 5 discusses the general UnCoRd approach and its effectiveness across domains, supporting the accuracy data.', 'Answer': 'UnCoRd-VG-E has the highest overall accuracy with 76%.'}", "all_figures_names": ["1811.08481v2-Figure4-1.png", "1811.08481v2-Figure3-1.png", "1811.08481v2-Figure8-1.png", "1811.08481v2-Figure10-1.png", "1811.08481v2-Figure6-1.png", "1811.08481v2-Table4-1.png", "1811.08481v2-Figure2-1.png", "1811.08481v2-Table7-1.png"], "referred_figures_names": ["1811.08481v2-Table7-1.png"], "answer": "UnCoRd-VG-E", "figure_type": "table", "content_type": "table", "text_chunks": ["One method performs full scene analysis in order to carry out the program. This method uses questions-answers training to learn the programs, hence cannot be extended by a simple addition of visual estimators. Moreover, performing full scene analysis (detecting all objects, properties and relations in the scene) may become infeasible for data less restricted than CLEVR (especially for relations).", "% The above experiments on real-world images show that when corresponding visual estimators are available, our method performs better than a leading end-to-end model, both for questions outside the training domain of the end-to-end model (where the advantage is substantial) and for questions from this domain. This is achieved without any question answering training.", "All UnCoRd's failures are due to wrong results of the invoked visual estimators. Note the substantial performance difference in Pythia between yes/no and WH questions, unlike the moderate difference in UnCoRd. We found that Pythia recognizes the yes/no group (\\ie answers 'yes'/'no'), but its accuracy (56\\%) is close to chance level (50\\%).", "Some examples are shown in Figure (elaborated later in text).% \\vspace{-1ex Related Work \\vspace{-1ex Current answering schemes are dominated by end-to-end methods, trained as multi-class classifiers. Many recent works focused on improving % the image-question fused features , attention mechanisms for selecting important features , including self and guided attention , applying pre-trained networks , and incorporating outputs of other visual tasks . Some provide reasoning using \"facts\" extraction (\\eg scene type) , image caption or by linking visual \"facts\" with question's logical form .", "On a sample of 10,000 validation set examples, all mistakes were due to wrong visual estimators' predictions, mainly miss detection of a highly occluded object. Hence, accurate annotation of object coordinates (as performed in NS-VQA ) may even further reduce the small number of errors. Note that NS-VQA requires full scene analysis, which is not scalable for domain extension with a large number of objects and relations.", "With an ability to handle extended domains, the UnCoRd approach demonstrates the potential to build a general answering scheme, not coupled to a specific dataset. % Our work includes several novel contributions. First, a method that produces state-of-the-art results on the CLEVR dataset without any questions-answers training.", "The question is answered by finding an assignment to the image that will make the statement true, and retrieving the free variables. The quantifiers derived from the question require multiple true assignments (such as '5', 'all', etc.). The procedure we use seeks the required assignments and returns the desired answer.", "We next describe an experiment for real-world images, where we use real world visual estimators (see Section ) and our most general trained mapper (VG-Enhanced). We compare our model to Pythia , which has top performance on the VQA v2 dataset . The experiment includes two parts: \\vspace{0.5ex \\item 'VQA\\_v2' questions: 100 questions sampled from VQA v2 dataset with terms that have visual estimators in UnCoRd and unambiguous answers (annotated by us)."], "referred_text_chunks": ["One method performs full scene analysis in order to carry out the program. This method uses questions-answers training to learn the programs, hence cannot be extended by a simple addition of visual estimators. Moreover, performing full scene analysis (detecting all objects, properties and relations in the scene) may become infeasible for data less restricted than CLEVR (especially for relations)."]}, "3": {"question": "What is the effect of training on more diverse data on the accuracy of graph representation for VQA?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 5, 'Rationale': 'Image 3 directly shows the effect of different training data on accuracy across domains, making it the most relevant. Text 5 discusses improvement in performance as models are trained on diverse data, matching the question.', 'Answer': 'Training on more diverse data improves the accuracy of graph representation for VQA by enabling better generalization across different domains.'}", "all_figures_names": ["1811.08481v2-Figure1-1.png", "1811.08481v2-Figure3-1.png", "1811.08481v2-Figure9-1.png", "1811.08481v2-Table4-1.png", "1811.08481v2-Table1-1.png", "1811.08481v2-Figure4-1.png", "1811.08481v2-Table5-1.png", "1811.08481v2-Table2-1.png"], "referred_figures_names": ["1811.08481v2-Table4-1.png"], "answer": "Training on more diverse data improves the accuracy of graph representation for VQA.", "figure_type": "table", "content_type": "table", "text_chunks": ["\\vspace{-2ex \\subsubsection{VQA Representation \\vspace{-1ex In this experiment, representation capabilities are tested for a different dataset. Since normally, annotations corresponding to our graph representation are not provided, we sampled 100 questions of the VQA validation set and manually examined the results for the eight question-to-graph models (see Section ). The results in Table express the large gaps in the abilities of models to represent new domains.", "We next describe an experiment for real-world images, where we use real world visual estimators (see Section ) and our most general trained mapper (VG-Enhanced). We compare our model to Pythia , which has top performance on the VQA v2 dataset . The experiment includes two parts: \\vspace{0.5ex \\item 'VQA\\_v2' questions: 100 questions sampled from VQA v2 dataset with terms that have visual estimators in UnCoRd and unambiguous answers (annotated by us).", "On a sample of 10,000 validation set examples, all mistakes were due to wrong visual estimators' predictions, mainly miss detection of a highly occluded object. Hence, accurate annotation of object coordinates (as performed in NS-VQA ) may even further reduce the small number of errors. Note that NS-VQA requires full scene analysis, which is not scalable for domain extension with a large number of objects and relations.", "The main immediate bottleneck is obtaining question-to-graph mapping with general representation capabilities for a broad range of questions. % Question graph representation may also be enhanced to support questions with more complex logic, as well as extending the scope of the supported visual categories (\\eg global scene types). Any general VQA requires vast estimation capabilities, as any visual category can be queried.", "This result is interesting as using this model provides high accuracy for CLEVR as well (see Table ). The fact that substantial performance gain is achieved for a data domain that was not used in training (the VQA dataset domain), while preserving good results on the original data (CLEVR), demonstrates the potential of the approach to provide a general answering system for visual questions. Further investigation is required for means to enrich question description examples and produce further significant improvements.", "Models trained specifically on CLEVR do not generalize at all to the untrained domain. As the models are trained on more diverse data, results improve substantially, peaking clearly for VG-Enhanced model by a large margin from other models. This is also evident in the example given in Figure where adequacy of the graph increases in a similar manner.", "As can be seen, UnCoRd's results are better by a large margin, compared to Pythia end-to-end model, even though questions were sampled from VQA v2, a dataset used for Pythia's training. As in the previous part, all UnCoRd's failures are only due to wrong results of the invoked visual estimators. Examples of UnCoRd's answers for the VQA\\_v2 questions are given in Figure , including the corresponding answers of Pythia.", "It is evident that even models that were trained on a much larger vocabulary and question types than the original CLEVR data still perform well, mostly with only minor accuracy reduction. This demonstrates that with more variable training we can handle more complex questions, while maintaining good results on the simpler domains. % Examples on CLEVR images for both CLEVR questions and others are shown in Figure (using 'None-Enhanced' mapper)."], "referred_text_chunks": ["\\vspace{-2ex \\subsubsection{VQA Representation \\vspace{-1ex In this experiment, representation capabilities are tested for a different dataset. Since normally, annotations corresponding to our graph representation are not provided, we sampled 100 questions of the VQA validation set and manually examined the results for the eight question-to-graph models (see Section ). The results in Table express the large gaps in the abilities of models to represent new domains."]}, "4": {"question": "Which estimator achieves the highest accuracy on the CLEVR validation set?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 0, 'Rationale': 'Image 6 provides accuracy data for CLEVR estimators, which directly answers the question. Text chunk 0 mentions the training success on the CLEVR validation set, relevant to estimator performance.', 'Answer': 'The estimator for Size achieves the highest accuracy with 100% on the CLEVR validation set.'}", "all_figures_names": ["1811.08481v2-Table7-1.png", "1811.08481v2-Table2-1.png", "1811.08481v2-Table4-1.png", "1811.08481v2-Figure3-1.png", "1811.08481v2-Table3-1.png", "1811.08481v2-Figure4-1.png", "1811.08481v2-Table1-1.png", "1811.08481v2-Figure5-1.png"], "referred_figures_names": ["1811.08481v2-Table1-1.png"], "answer": "Size estimator.", "figure_type": "table", "content_type": "table", "text_chunks": ["\\vspace{-0.7ex We trained a question-to-graph model for \\scalebox{.98[1.0]{CLEVR ('None'-'Basic', as denoted in Section ), which generated 100\\% perfect graphs on its validation set. The visual estimators, described in Section were also trained and provided the results given in Table .", "% \\vspace{-2ex \\subsubsection{Real World Visual Estimators \\vspace{-1ex Handling questions in the real-world domain beyond CLEVR objects was performed by utilizing existing visual estimators. For instance segmentation we use a pre-trained mask R-CNN for the 80 classes of COCO dataset . Any other visual estimator may be incorporated to enhance answering capability.", "% \\vspace{-2ex \\subsubsection{CLEVR Visual Estimators \\vspace{-1ex In order to find a valid assignment of a question graph in the image, and provide the answer, corresponding visual estimators need to be trained. % Object locations are not explicitly provided for CLEVR data, however they can be automatically recovered using the provided scene annotations. This process provided approximated contour annotations for CLEVR objects (see Figure ), which were used for training.", "\\scalebox{.98[1.0]{CLEVR relations were estimated by simple rules using the objects' coordinates. % \\vspace{-2ex We tested the answering performance of the UnCoRd system on the CLEVR test set.", "In our experiments (Section and Figure ) we use color map estimation , age and gender classification (utilizing face detection ) and depth estimation (utilized for estimating spatial relations). Experiments \\vspace{-1ex The experiments tested the abilities of the UnCoRd system, to first, provide accurate results for the CLEVR dataset and second, to handle extended questions and real-world domains. Our analysis included the two answering stages: creating a correct graph representation of the question, and answering the questions.", "For each model, the same version was used in all experiments. Unless stated, system was configured to provide short answers (concise and without elaborations); markings on images in the figures correspond to intermediate results. % Code will be available at \\scalebox{0.74[1.0]{\\url{https://github.com/benyv/uncord \\vspace{-0.1ex CLEVR Experiments", "\\tikzstyle{cir1 = [draw, circle,fill=yellow!20, text centered, text width=5.2em, inner sep=0.1mm, scale=0.8] \\vspace{-2ex \\vspace{-2ex \\subsubsection{Maintaining Performance on CLEVR Questions \\vspace{-1ex We evaluated the performance change for the CLEVR test set, as the training data variability of the question-to-graph models increases. The results are given in Table .", "Models trained specifically on CLEVR do not generalize at all to the untrained domain. As the models are trained on more diverse data, results improve substantially, peaking clearly for VG-Enhanced model by a large margin from other models. This is also evident in the example given in Figure where adequacy of the graph increases in a similar manner."], "referred_text_chunks": ["\\vspace{-0.7ex We trained a question-to-graph model for \\scalebox{.98[1.0]{CLEVR ('None'-'Basic', as denoted in Section ), which generated 100\\% perfect graphs on its validation set. The visual estimators, described in Section were also trained and provided the results given in Table ."]}, "5": {"question": "Which method achieves the highest overall accuracy on the validation set?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 7, 'Rationale': 'Image 4 shows a comparison of CLEVR QA accuracy for several methods on validation sets, which directly answers the question. Text 7 discusses the accuracy of different models, highlighting the superior performance of UnCoRd across various datasets.', 'Answer': 'NS-VQA and UnCoRd both achieve the highest overall accuracy of 99.8 on the validation set.'}", "all_figures_names": ["1811.08481v2-Table7-1.png", "1811.08481v2-Figure6-1.png", "1811.08481v2-Figure4-1.png", "1811.08481v2-Table5-1.png", "1811.08481v2-Table2-1.png", "1811.08481v2-Figure1-1.png", "1811.08481v2-Figure3-1.png", "1811.08481v2-Table3-1.png"], "referred_figures_names": ["1811.08481v2-Table2-1.png"], "answer": "UnCoRd-None-B.", "figure_type": "Table.", "content_type": "table", "text_chunks": ["On a sample of 10,000 validation set examples, all mistakes were due to wrong visual estimators' predictions, mainly miss detection of a highly occluded object. Hence, accurate annotation of object coordinates (as performed in NS-VQA ) may even further reduce the small number of errors. Note that NS-VQA requires full scene analysis, which is not scalable for domain extension with a large number of objects and relations.", "All UnCoRd's failures are due to wrong results of the invoked visual estimators. Note the substantial performance difference in Pythia between yes/no and WH questions, unlike the moderate difference in UnCoRd. We found that Pythia recognizes the yes/no group (\\ie answers 'yes'/'no'), but its accuracy (56\\%) is close to chance level (50\\%).", "We next describe an experiment for real-world images, where we use real world visual estimators (see Section ) and our most general trained mapper (VG-Enhanced). We compare our model to Pythia , which has top performance on the VQA v2 dataset . The experiment includes two parts: \\vspace{0.5ex \\item 'VQA\\_v2' questions: 100 questions sampled from VQA v2 dataset with terms that have visual estimators in UnCoRd and unambiguous answers (annotated by us).", "% The above experiments on real-world images show that when corresponding visual estimators are available, our method performs better than a leading end-to-end model, both for questions outside the training domain of the end-to-end model (where the advantage is substantial) and for questions from this domain. This is achieved without any question answering training.", "\\vspace{-0.7ex We trained a question-to-graph model for \\scalebox{.98[1.0]{CLEVR ('None'-'Basic', as denoted in Section ), which generated 100\\% perfect graphs on its validation set. The visual estimators, described in Section were also trained and provided the results given in Table .", "Some examples are shown in Figure (elaborated later in text).% \\vspace{-1ex Related Work \\vspace{-1ex Current answering schemes are dominated by end-to-end methods, trained as multi-class classifiers. Many recent works focused on improving % the image-question fused features , attention mechanisms for selecting important features , including self and guided attention , applying pre-trained networks , and incorporating outputs of other visual tasks . Some provide reasoning using \"facts\" extraction (\\eg scene type) , image caption or by linking visual \"facts\" with question's logical form .", "Adam optimizer was used for question-to-graph and visual estimators training with a learning rate of 10^{-4 (10^{-3 for the 'Extended-Enhanced' model), selected according to the corresponding validation set results. Each model training was using one NVIDIA Tesla V100 GPU. All reported results are for a single evaluation.", "As can be seen, UnCoRd's results are better by a large margin, compared to Pythia end-to-end model, even though questions were sampled from VQA v2, a dataset used for Pythia's training. As in the previous part, all UnCoRd's failures are only due to wrong results of the invoked visual estimators. Examples of UnCoRd's answers for the VQA\\_v2 questions are given in Figure , including the corresponding answers of Pythia."], "referred_text_chunks": ["On a sample of 10,000 validation set examples, all mistakes were due to wrong visual estimators' predictions, mainly miss detection of a highly occluded object. Hence, accurate annotation of object coordinates (as performed in NS-VQA ) may even further reduce the small number of errors. Note that NS-VQA requires full scene analysis, which is not scalable for domain extension with a large number of objects and relations."]}}