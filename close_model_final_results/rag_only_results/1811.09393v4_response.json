{"0": {"question": "Which method has the highest T-Diff on average for the Vid4 dataset?", "referred_figures_indices": [1], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 contains a detailed table of different metrics, including T-Diff, for Vid4. Text 0 explains that T-Diff struggles with blurriness and notes metric evaluations. Together, they provide a clear comparison of T-Diff values across methods.', 'Answer': 'ENet has the highest T-Diff on average for the Vid4 dataset.'}", "all_figures_names": ["1811.09393v4-Table4-1.png", "1811.09393v4-Figure15-1.png", "1811.09393v4-Figure3-1.png", "1811.09393v4-Figure22-1.png", "1811.09393v4-Figure16-1.png", "1811.09393v4-Figure10-1.png", "1811.09393v4-Figure5-1.png", "1811.09393v4-Figure18-1.png"], "referred_figures_names": ["1811.09393v4-Figure15-1.png"], "answer": "TecoGAN.", "figure_type": "plot", "content_type": "figure", "text_chunks": ["As shown at the bottom of \\myreftab{tab:vid4metrics, T-diff, due to its local nature, is easily deceived by blurry method such as the bi-cubic interrelation and can not correlate well with visual assessments of coherence. {By using the proposed metrics, i.e. measuring the pixel-wise motion differences using \\text{tOF together with the perceptual changes over time using \\selflp{, a more nuanced evaluation can be achieved, as shown for the VSR task in the middle of \\myreftab{tab:vid4metrics. Not surprisingly, the results of ENet show larger errors for all metrics due to their strongly flickering content.", "Mean values of the Vid4 scenes~ are shown on the top of \\myreftab{tab:vid4metrics. Trained with direct vector norms losses, {FRVSR, DUF, EDVR, and RBPN achieve high PSNR scores. However, the undesirable smoothing induced by these losses manifests themselves in larger LPIPS distances.", "Based on good perceptual quality and reasonable pixel-wise accuracy, TecoGAN outperforms all other methods by more than 30\\mypercent{ for LPIPS. {\\setlength{\\tabcolsep{0.3pt { \\setlength{\\tabcolsep{2.5pt \\setlength\\extrarowheight{-0.8pt \\end{figure* Temporal Metrics For both VSR and UVT, evaluating temporal coherence without ground-truth motion is very challenging. L^2-based temporal metrics such as {\\em T-diff = \\| g_t - W( g_{t-1, v_t )\\|_1 was used~ as a rough assessment of temporal differences, and we give corresponding numbers for comparison.", "We measure the PSNR and LPIPS using the Vid4 scenes. {With a PSNR decrease of less than 2dB over DUF (which has twice the model size), the LPIPS score of TecoGAN shows an improvement of more than {40\\mypercent{. The other baselines are outperformed by similar margins.", "After collecting 1000 votes from 50 users for every scene, i.e. twice for all possible pairs (5 \\times 4 / 2 = 10 pairs), we follow common procedure and compute scores for all models with the Bradley-Terry model~(\\citeyear{bradley1952rank). The outcomes for the Vid4 scenes can be seen in \\myreffig{fig:userstudy{(overall scores are listed in \\myreftab{tab:metrics of the main document). From the Bradley-Terry scores for the Vid4 scenes we can see that the TecoGAN model performs very well, and achieves the first place in three cases, as well as a second place in the walk scene.", "Besides the previously evaluated the Vid4 dataset, with graphs shown in \\myreffig{fig:suppgraphs2, , we also get similar evaluation results on the {\\em Tears of Steel data-sets (room, bridge, and face, in the following referred to as {\\em ToS scenes) and corresponding results are shown in \\myreftab{tab: ToSmetrics and \\myreffig{fig:suppgraphs3. In all tests, we follow the procedures of previous work~\\citep{jo2018deep,sajjadi2018FRVSR to make the outputs of all methods comparable, i.e., for all result images, we first exclude spatial borders with a distance of 8 pixels to the image sides, then further shrink borders such that the LR input image is divisible by 8 and for spatial metrics, we ignore the first two and the last two frames, while for temporal metrics, we ignore first three and last two frames, as an additional previous frame is required for inference.", "Even compared to the large EDVR model using down-sampled inputs without Gaussian blur, TecoGAN still yields a 30\\mypercent{ improvement in terms of LPIPS. While traditional temporal metrics based on vector norm differences of warped frames, e.g. T-diff {=\\left\\| g_t - W( g_{t-1, v_t )\\right\\|_1 ~, can be easily deceived by very blurry results, {e.g. bi-cubic interpolated ones, we propose to use a tandem of {two new metrics, tOF and tLP, to measure the consistence over time. tOF measures the pixel-wise difference of motions estimated from sequences, and tLP measures perceptual changes over time using deep feature map: {\\small \\vspace{-3pt \\resizeEq{\\text{tOF &= \\left\\| OF(b_{t-1, b_t) - OF(g_{t-1, g_t) \\right\\|_1 & \\text{ and \\\\ \\text{\\selflp{ &= \\left\\| LP(b_{t-1, b_t) - LP(g_{t-1, g_t) \\right\\|_1 \\text{. & {eq:tmetrics{!", "\\quad c) The network sizes. \\end{minipage \\end{minipage \\end{figure* Results and Metric Evaluation For the VSR task, we test our model on a wide range of video data, including the widely used Vid4 dataset shown in \\myreffig{fig:folCMP, and , detailed scenes from the movie Tears of Steel~(ToS)~\\shortcite{tearsofsteel shown in \\myreffig{fig:suppresults1, and others shown in \\myreffig{fig:vid2."], "referred_text_chunks": ["As shown at the bottom of \\myreftab{tab:vid4metrics, T-diff, due to its local nature, is easily deceived by blurry method such as the bi-cubic interrelation and can not correlate well with visual assessments of coherence. {By using the proposed metrics, i.e. measuring the pixel-wise motion differences using \\text{tOF together with the perceptual changes over time using \\selflp{, a more nuanced evaluation can be achieved, as shown for the VSR task in the middle of \\myreftab{tab:vid4metrics. Not surprisingly, the results of ENet show larger errors for all metrics due to their strongly flickering content."]}, "1": {"question": "Which GAN model is able to generate the most realistic blinking motions?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 0, 'Rationale': 'Image 6 visually demonstrates several GAN models generating blinking motions, while Text 0 discusses cycle-consistency essential for realistic motion generation, pointing out TecoGAN\u2019s superiority.', 'Answer': 'TecoGAN is able to generate the most realistic blinking motions.'}", "all_figures_names": ["1811.09393v4-Figure5-1.png", "1811.09393v4-Table6-1.png", "1811.09393v4-Figure11-1.png", "1811.09393v4-Table4-1.png", "1811.09393v4-Figure2-1.png", "1811.09393v4-Figure20-1.png", "1811.09393v4-Figure8-1.png", "1811.09393v4-Table3-1.png"], "referred_figures_names": ["1811.09393v4-Figure8-1.png"], "answer": "TecoGAN", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Being able to establish the correct temporal cycle-consistency between domains, ours, RecycleGAN and {STC-V2V can generate correct blinking motions, shown in Sec. 4.7 of the supplemental web-page. Our model outperforms the latter two in terms of coherent detail that is generated.", "Specifically, ENet and CycleGAN represent state-of-the-art single-image adversarial models without temporal information, FRVSR and DUF are state-of-the-art VSR methods without adversarial losses, and RecycleGAN is a spatial adversarial model with a prediction network learning the temporal evolution. \\vspace{-9pt \\caption{{\\footnotesize When learning a mapping between Trump and Obama, the CycleGAN model gives good spatial features but collapses to essentially static outputs of Obama. It manages to transfer facial expressions back to Trump using tiny differences encoded in its Obama outputs, without understanding the cycle-consistency between the two domains.", "the ones from the input data { , i.e., {\\footnotesize \\text{tOF =\\left\\| OF(a_{t-1, a_t) - OF(g^{a\\rightarrow b_{t-1, g^{a\\rightarrow b_t) \\right\\|_1 and {\\footnotesize \\selflp{ = \\left\\| LP(a_{t-1, a_t) - LP(g^{a\\rightarrow b_{t-1, g^{a\\rightarrow b_t) \\right\\|_1. With sharp spatial features and coherent motion, TecoGAN outperforms {CycleGAN and RecycleGAN on the Obama\\&Trump dataset, as shown in \\myreftab{tab:UVTmetrics, although it is worth pointing out that tOF is less informative in this case, as the motion in the target domain is not necessarily pixel-wise aligned with the input. {", "Therefore, video translation from domain A to B is a tough task, as the latter contains significantly more turbulent and small-scale motions. With no temporal information available, the CycleGAN variant generates HR smoke that strongly flickers. The DsOnly model offers better temporal coherence by relying on its frame-recurrent input, but it learns a solution that largely ignores the current input and fails to keep reasonable spatio-temporal cycle-consistency links between the two domains.", "While regular losses such as L^2~ offer good performance for image super-resolution (SR) tasks in terms of PSNR metrics, previous work found adversarial training~ to significantly improve the perceptual quality in multi-modal settings such as { image generation~, colorization~, super-resolution~, and translation~ tasks. {Besides representing natural images, GAN-based frameworks are also successful at static graphic representations including {geometry synthesis~ and city modeling~. {Sequential generation tasks, on the other hand, require {the generation of realistic content that changes naturally over time~. It is especially so for conditional video generation tasks~, {where specific correlations between the input and the generated spatio-temporal evolution are required when ground-truth motions are not provided.", "Previous work, e.g. tempoGAN for fluid flow~ and vid2vid for video translation~, has proposed adversarial temporal losses to achieve time consistency. While tempoGAN employs a second temporal discriminator with multiple aligned frames to assess the realism of temporal changes, it is {not suitable for videos, as it relies on ground truth motions and employs single-frame processing that is sub-optimal for natural images. On the other hand, vid2vid focuses on paired video translations and proposes a video discriminator based on a conditional motion input that is estimated from the paired ground-truth sequences.", "While RecycleGAN uses an {L2-based cycle loss that leads to undesirable smoothing, {Park et al.~\\shortcite{park2019preserving propose to use temporal-cycle losses in together with a VGG-based content preserving loss (we will refer to this method as {\\em STC-V2V below). While the evaluation of temporal metrics for TecoGAN and STC-V2V is very close, \\myreffig{fig:UVTCMP shows that our results contain sharper spatial details, such as the eyes and eyebrows of Obama as well as the wrinkles of Trump. This is illustrated in Sec.", "{The latter is most likely caused by the overall slightly smoother images of the walk scene, in conjunction with the presence of several human faces, where our model can lead to the generation of unexpected details. However, overall the user study shows that users preferred the TecoGAN output over the other two deep-learning methods with a 63.5\\mypercent{ probability. This result also matches with our metric evaluations."], "referred_text_chunks": ["Being able to establish the correct temporal cycle-consistency between domains, ours, RecycleGAN and {STC-V2V can generate correct blinking motions, shown in Sec. 4.7 of the supplemental web-page. Our model outperforms the latter two in terms of coherent detail that is generated."]}, "2": {"question": "Which of the VSR models in the figure achieves the best balance of spatial detail and temporal coherence?", "referred_figures_indices": [4], "response": "{'Image': 0, 'Text': 5, 'Rationale': 'Image 0 provides a visual comparison of VSR models, showing both spatial detail and temporal coherence across different scenes. Text 5 discusses the performance of TecoGAN regarding spatial detail and temporal coherence, comparing it to non-adversarial methods like DUF and FRVSR, indicating its strong performance overall.', 'Answer': 'TecoGAN achieves the best balance of spatial detail and temporal coherence.'}", "all_figures_names": ["1811.09393v4-Figure12-1.png", "1811.09393v4-Figure18-1.png", "1811.09393v4-Table5-1.png", "1811.09393v4-Table3-1.png", "1811.09393v4-Figure14-1.png", "1811.09393v4-Figure5-1.png", "1811.09393v4-Figure16-1.png", "1811.09393v4-Figure9-1.png"], "referred_figures_names": ["1811.09393v4-Figure14-1.png"], "answer": "TecoGAN", "figure_type": "plot", "content_type": "figure", "text_chunks": ["Based on good perceptual quality and reasonable pixel-wise accuracy, TecoGAN outperforms all other methods by more than 30\\mypercent{ for LPIPS. {\\setlength{\\tabcolsep{0.3pt { \\setlength{\\tabcolsep{2.5pt \\setlength\\extrarowheight{-0.8pt \\end{figure* Temporal Metrics For both VSR and UVT, evaluating temporal coherence without ground-truth motion is very challenging. L^2-based temporal metrics such as {\\em T-diff = \\| g_t - W( g_{t-1, v_t )\\|_1 was used~ as a rough assessment of temporal differences, and we give corresponding numbers for comparison.", "{ \\caption{\\footnotesize{In VSR of the foliage scene, adversarial models (ENet, DsOnly, DsDt, DsDtPP, \\TecoGANsmall and TecoGAN) yield better perceptual quality than methods using L^2 loss (FRVSR and DUF). In temporal profiles on the right, DsDt, DsDtPP and TecoGAN show significantly less temporal discontinuities compared to ENet and DsOnly. The temporal information of our discriminators successfully suppresses these artifacts.", "Obama and Trump video courtesy of the White House (public domain). \\end{figure* \\paragraph{{Video Super-Resolution For VSR, we first train a {\\em DsOnly model that uses a frame-recurrent G and \\fnet with a VGG loss and only the regular spatial discriminator. Compared to ENet, which exhibits strong incoherence due to the lack of temporal information, DsOnly improves temporal coherence thanks to the frame-recurrent connection, but there are noticeable high-frequency changes between frames.", "In \\myreftab{tab:vid4metrics, while TecoGAN achieves spatial (LPIPS) improvements in all scenes, DUF and FRVSR are not far behind in the walk scene. In terms of temporal metrics \\ofmae{ and \\selflp{, TecoGAN achieves similar or lower scores compared to FRVSR and DUF for calendar, foliage and city scenes. The lower performance of our model for the walk scene is likewise captured by higher \\ofmae{ and \\selflp{ scores.", "On the contrary, our D_{s,t enables the Dst model to learn the correlation between the spatial and temporal aspects, thus improving the cycle-consistency. However, without \\mathcal{L_{pp, the Dst model (like the DsDt model of VSR) reinforces detail over time in an undesirable way. This manifests itself as inappropriate smoke density in empty regions.", "While they could be combined into a single score, we list both measurements separately, as their relative importance could vary in different application settings. Our evaluation with these temporal metrics in \\myreftab{tab:metrics shows that all temporal adversarial models outperform spatial adversarial ones and the full TecoGAN model performs very well: With a large amount of spatial detail, it still achieves good temporal coherence, on par with non-adversarial methods such as DUF, FRVSR, {RBPN and EDVR. These results are also visualized in \\myreffig{fig:metrics.", "We also motivate and propose two novel temporal metrics to quantify temporal coherence. For evaluating image SR, \\citet{blau2018perception demonstrated that there is an inherent trade-off between the perceptual quality of the result and the distortion measured with vector norms or low-level structures such as PSNR and SSIM. On the other hand, metrics based on deep feature maps such as LPIPS~ can capture more semantic similarities.", "Specifically, ENet and CycleGAN represent state-of-the-art single-image adversarial models without temporal information, FRVSR and DUF are state-of-the-art VSR methods without adversarial losses, and RecycleGAN is a spatial adversarial model with a prediction network learning the temporal evolution. \\vspace{-9pt \\caption{{\\footnotesize When learning a mapping between Trump and Obama, the CycleGAN model gives good spatial features but collapses to essentially static outputs of Obama. It manages to transfer facial expressions back to Trump using tiny differences encoded in its Obama outputs, without understanding the cycle-consistency between the two domains."], "referred_text_chunks": ["Based on good perceptual quality and reasonable pixel-wise accuracy, TecoGAN outperforms all other methods by more than 30\\mypercent{ for LPIPS. {\\setlength{\\tabcolsep{0.3pt { \\setlength{\\tabcolsep{2.5pt \\setlength\\extrarowheight{-0.8pt \\end{figure* Temporal Metrics For both VSR and UVT, evaluating temporal coherence without ground-truth motion is very challenging. L^2-based temporal metrics such as {\\em T-diff = \\| g_t - W( g_{t-1, v_t )\\|_1 was used~ as a rough assessment of temporal differences, and we give corresponding numbers for comparison."]}, "3": {"question": "Which method achieves the highest PSNR on the Vid4 data set?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 3, 'Rationale': 'Image 4 and Text 3 both focus on PSNR scores. Image 4 provides a detailed table with PSNR values for various methods, and Text 3 discusses the performance metrics of algorithms on Vid4, emphasizing PSNR.', 'Answer': 'EDVR achieves the highest PSNR on the Vid4 dataset with a score of 27.34.'}", "all_figures_names": ["1811.09393v4-Figure19-1.png", "1811.09393v4-Figure15-1.png", "1811.09393v4-Figure4-1.png", "1811.09393v4-Figure20-1.png", "1811.09393v4-Table2-1.png", "1811.09393v4-Figure11-1.png", "1811.09393v4-Figure3-1.png", "1811.09393v4-Table5-1.png"], "referred_figures_names": ["1811.09393v4-Table2-1.png"], "answer": "DUF", "figure_type": "table", "content_type": "table", "text_chunks": ["Mean values of the Vid4 scenes~ are shown on the top of \\myreftab{tab:vid4metrics. Trained with direct vector norms losses, {FRVSR, DUF, EDVR, and RBPN achieve high PSNR scores. However, the undesirable smoothing induced by these losses manifests themselves in larger LPIPS distances.", "We measure the PSNR and LPIPS using the Vid4 scenes. {With a PSNR decrease of less than 2dB over DUF (which has twice the model size), the LPIPS score of TecoGAN shows an improvement of more than {40\\mypercent{. The other baselines are outperformed by similar margins.", "\\newline Metrics and Quantitative Analysis Spatial Metrics In order to be able to compare our results with single-image methods, we evaluate all VSR methods with the purely spatial metrics PSNR together with the human-calibrated LPIPS metric~\\citep{zhang2018unreasonable. While higher PSNR values indicate a better pixel-wise accuracy, lower LPIPS values represent better perceptual quality and closer semantic similarity. {Note that both metrics are agnostic to changes over time, and hence do not suffice to really evaluate video data.", "{ Due to its complexity, VSR has also led to workshop challenges like {\\em NTIRE19~, where algorithms such as EDVR~ perform best w.r.t. PSNR-based metrics. We compare to these methods and give additional details in \\myrefapp{app:metrics.", "ENet, on the other hand, with no information from neighboring frames, yields the lowest PSNR and achieves an LPIPS score that is only slightly better than DUF and FRVSR. The TecoGAN model with adversarial training achieves an excellent LPIPS score, with a PSNR decrease of less than 2dB over DUF. This is very reasonable, since PSNR and perceptual quality were shown to be anti-correlated~\\citep{blau2018perception, especially in regions where PSNR is very high.", "After collecting 1000 votes from 50 users for every scene, i.e. twice for all possible pairs (5 \\times 4 / 2 = 10 pairs), we follow common procedure and compute scores for all models with the Bradley-Terry model~(\\citeyear{bradley1952rank). The outcomes for the Vid4 scenes can be seen in \\myreffig{fig:userstudy{(overall scores are listed in \\myreftab{tab:metrics of the main document). From the Bradley-Terry scores for the Vid4 scenes we can see that the TecoGAN model performs very well, and achieves the first place in three cases, as well as a second place in the walk scene.", "\\quad c) The network sizes. \\end{minipage \\end{minipage \\end{figure* Results and Metric Evaluation For the VSR task, we test our model on a wide range of video data, including the widely used Vid4 dataset shown in \\myreffig{fig:folCMP, and , detailed scenes from the movie Tears of Steel~(ToS)~\\shortcite{tearsofsteel shown in \\myreffig{fig:suppresults1, and others shown in \\myreffig{fig:vid2.", "Besides the previously evaluated the Vid4 dataset, with graphs shown in \\myreffig{fig:suppgraphs2, , we also get similar evaluation results on the {\\em Tears of Steel data-sets (room, bridge, and face, in the following referred to as {\\em ToS scenes) and corresponding results are shown in \\myreftab{tab: ToSmetrics and \\myreffig{fig:suppgraphs3. In all tests, we follow the procedures of previous work~\\citep{jo2018deep,sajjadi2018FRVSR to make the outputs of all methods comparable, i.e., for all result images, we first exclude spatial borders with a distance of 8 pixels to the image sides, then further shrink borders such that the LR input image is divisible by 8 and for spatial metrics, we ignore the first two and the last two frames, while for temporal metrics, we ignore first three and last two frames, as an additional previous frame is required for inference."], "referred_text_chunks": ["Mean values of the Vid4 scenes~ are shown on the top of \\myreftab{tab:vid4metrics. Trained with direct vector norms losses, {FRVSR, DUF, EDVR, and RBPN achieve high PSNR scores. However, the undesirable smoothing induced by these losses manifests themselves in larger LPIPS distances."]}, "4": {"question": " What is the role of the warped triplets in the conditional VSR Ds,t?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 1, 'Rationale': 'Image 5 shows the Conditional VSR Ds,t, highlighting the role of warped triplets in decision-making. Text 1 discusses how warped triplets contribute by transitioning with linear interpolation, providing temporal information.', 'Answer': 'Warped triplets in the conditional VSR Ds,t are used to enhance temporal information by transitioning from generated to warped frames, which helps the discriminator classify temporal changes accurately.'}", "all_figures_names": ["1811.09393v4-Figure23-1.png", "1811.09393v4-Table3-1.png", "1811.09393v4-Figure16-1.png", "1811.09393v4-Figure12-1.png", "1811.09393v4-Figure2-1.png", "1811.09393v4-Figure4-1.png", "1811.09393v4-Figure14-1.png", "1811.09393v4-Figure5-1.png"], "referred_figures_names": ["1811.09393v4-Figure4-1.png"], "answer": " The warped triplets provide additional information about the motion and appearance of the scene, which helps the VSR Ds,t to generate more accurate and realistic results.", "figure_type": "** Schematic", "content_type": "figure", "text_chunks": ["The VSR discriminator D_{s,t should guide the generator to learn the correlation between the conditional LR inputs and HR targets. Therefore, three LR frames \\text{I_a =\\{a_{t-1,a_t,a_{t+1\\ from the input domain are used as a conditional input. The input of D_{s,t can be summarized as \\text{I_{s,t^b = \\{\\text{I_b, \\text{I_{wb, \\text{I_a\\ labelled as real and the generated inputs \\text{I_{s,t^g = \\{\\text{I_g, \\text{I_{wg, \\text{I_a\\ labelled as fake, as shown in \\myreffig{fig:gnet.", "During training, 25\\mypercent{ of them gradually transition to warped triplets and another 25\\mypercent{ transition to original triplets. {The transitions of the warped triplets are computed with linear interpolation: (1-\\alpha) \\text{I_{cg + \\alpha \\text{I_{wg , with \\alpha growing form 0 to 1. For the original triplets, we additionally fade the ``warping'' operation out by using (1-\\alpha) \\text{I_{cg + \\alpha \\{ W( g_{t-1, v_t", "As such, D_{s,t can guide G to improve the image content, and F learns to warp the previous frame in accordance with the detail that G can synthesize. However, F does not adjust the motion estimation only to reduce the adversarial loss. Curriculum Learning for UVT Discriminators As mentioned in the main document, we train the UVT D_{s,t with 100\\mypercent{ spatial triplets at the very beginning.", "With our setup, the discriminator profits from the warped frames to classify realistic and unnatural temporal changes, and for situations where the motion estimation is less accurate, the discriminator can fall back to the original, i.e. not warped, images. Network Architecture for UVT While one generator is enough to map data from A to B for tasks such as VSR, unpaired generation tasks require a second generator to establish cycle consistency \\citep{zhu2017cycleGAN. For the UVT task, we use two recurrent generators{, mapping from domain A to B and back.", "{Consequently, the discriminator, despite receiving only a single triplet at once, can guide the generator to produce detailed structures that move coherently. \\caption{VSR comparisons for different captured images {in order to compare to previous work . \\end{figure* {\\setlength{\\tabcolsep{10pt \\vspace{-16pt \\caption{{Visual summary of VSR models.", "Our approach actually yields improved results, as we will show below. In practice, we found it crucial to ensure that generators first learn reasonable spatial features, and only then improve {their temporal correlation. Therefore, different to the D_{s,t of VST that always receives 3 concatenated triplets as an input, the unconditional D_{s,t of UVT only takes one triplet at a time.", "We use a standard discriminator loss for the D_{s,t of VSR and a least-square discriminator loss for the D_{s,t of UVT. Correspondingly, a non-saturated \\mathcal{L_{adv is used for the G and \\fnet of VSR and a least-squares one is used for the UVT generators. As summarized in \\myreftab{tab:loss, G and \\fnet are trained with the mean squared loss \\mathcal{L_\\text{content, adversarial losses { \\mathcal{L_{adv , perceptual losses { \\mathcal{L_{\\phi , the \\ppl{ loss { \\mathcal{L_{\\text{PP , and a warping loss \\mathcal{L_{\\text{warp, where again g, b and \\Phi stand for generated samples, ground truth images and feature maps of VGG or D_{s,t. We only show losses for the mapping from A to B for UVT tasks, as the backward mapping simply mirrors the terms.", "We then train a third model, {\\em{concat, using the original triplets and the warped ones, i.e. \\{\\text{I_g+\\text{I_{wg or \\text{I_b+\\text{I_{wb\\. In this case, the model learns to generate more spatial details with a more vivid motion. I.e., the improved temporal information from the warped triplets gives the discriminator important cues. However, the motion still does not fully resemble the target domain."], "referred_text_chunks": ["The VSR discriminator D_{s,t should guide the generator to learn the correlation between the conditional LR inputs and HR targets. Therefore, three LR frames \\text{I_a =\\{a_{t-1,a_t,a_{t+1\\ from the input domain are used as a conditional input. The input of D_{s,t can be summarized as \\text{I_{s,t^b = \\{\\text{I_b, \\text{I_{wb, \\text{I_a\\ labelled as real and the generated inputs \\text{I_{s,t^g = \\{\\text{I_g, \\text{I_{wg, \\text{I_a\\ labelled as fake, as shown in \\myreffig{fig:gnet."]}, "5": {"question": "How does the PP loss improve the temporal coherence of the video sequence?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 6, 'Rationale': \"Image 2 demonstrates the effectiveness of PP loss in reducing drifting artifacts and improving temporal coherence by using the Ping-Pong sequence, which is the focus of the question. Text 6 explains how PP loss constrains the sequence to be symmetric and reduces L2 distance between corresponding frames, which directly addresses the mechanism of temporal coherence improvement.\", 'Answer': \"The PP loss improves temporal coherence by using extended PP sequences and constraining the outputs from the forward and backward passes to be consistent, thereby reducing drifting artifacts and enhancing temporal consistency in video sequences.\"}", "all_figures_names": ["1811.09393v4-Figure22-1.png", "1811.09393v4-Figure8-1.png", "1811.09393v4-Figure3-1.png", "1811.09393v4-Figure2-1.png", "1811.09393v4-Figure16-1.png", "1811.09393v4-Figure5-1.png", "1811.09393v4-Figure14-1.png", "1811.09393v4-Figure11-1.png"], "referred_figures_names": ["1811.09393v4-Figure3-1.png"], "answer": "The PP loss constrains the output sequence to be symmetric by reducing the L2 distance between corresponding frames in the forward and backward passes. This helps to reduce drifting artifacts and improve temporal coherence.", "figure_type": "", "content_type": "figure", "text_chunks": ["For video generation, we propose a PP loss that also makes use of symmetric sequences. However, we directly constrain the PP loss via the generated video content, which successfully improves the long-term temporal consistency in the video results. {{The PP loss is effective by offering valid information in forward as well as backward passes of image sequences.", "The PP loss effectively avoids the temporal accumulation of artifacts, which can potentially benefit a variety of recurrent architectures. {We also note that most existing image metrics focus on spatial content only. We fill the gap of temporal assessment with a pair of metrics that measures the perceptual similarity over time and the similarity of motions with respect to a ground truth reference.", "In the proposed PP loss, both the Ping-Pong data augmentation and the temporal consistency constraint contribute to solving these problems. In order to show their separated contributions, we trained another TecoGAN variant that only employs the data augmentation without the constraint (i.e., \\lambda_p=0 in \\myreftab{tab:loss). Denoted as ``PP-Augment'', we show its results in comparison with the DsDt and \\TecoGANsmall models in \\myreffig{fig:PPsep.", "The proposed spatio-temporal discriminator and the PP loss can be used in video generation tasks to replace simple temporal losses, e.g. the ones based on L^2 differences and warping. In the following subsections, solutions for VSR and UVT are presented as examples in paired and unpaired data domains. Network Architecture for VSR {For video super-resolution (VSR) tasks, the input domain contains LR frames while the target domain contains high-resolution (HR) videos with more complex details and motions.", "For videos, this especially occurs along directions of motion and these solutions can be seen as a special form of temporal mode collapse, {where the training converges to a mostly constant temporal signal as a sub-optimal, trivial equilibrium. We have noticed this issue in a variety of recurrent architectures, examples are shown in \\myreffig{fig:ping_pongcomparison a) and the Dst version in \\myreffig{fig:UVTCMP. While this issue could be alleviated by training with longer sequences, {it is computationally expensive and can fail for even longer sequences, as shown in \\myrefapp{app:PPaug.", "With no ground truth motion available, the spatio-temporal adversarial loss and the recurrent structure enable our model to generate realistic results while keeping the generated structures coherent over time. With the two learning tasks we demonstrate how spatio-temporal adversarial training can be employed in paired as well as unpaired data domains. In addition to the adversarial network which supervises the short-term temporal coherence, long-term consistency is self-supervised using a novel bi-directional loss formulation, which we refer to as ``Ping-Pong'' (PP) loss in the following.", "Based on this observation, we train our networks with extended PP sequences and constrain the generated outputs from both ``legs'' to be the same using the loss: \\mathcal{L_{pp =\\sum_{t=1^{n-1 \\left\\| g_{t-g_{t{' \\right \\|_{2 . Note that in contrast to the generator loss, the L^2 norm is a correct choice here: We are not faced with multi-modal data where an L^2 norm would lead to undesirable averaging, but rather aim to constrain the recurrent generator to its own, unique version over time {without favoring smoothness. The \\ppl{ terms provide constraints for short term consistency via {\\small \\left \\| g_{n-1-g_{n-1{' \\right \\|_{2, while terms such as {\\small \\left \\| g_{1-g_{1{' \\right \\|_{2 prevent long-term drifts of the results.", "In addition, it effectively extends the training data set, and as such represents a useful form of data augmentation. { A comparison is given in \\myrefapp{app:PPaug to disentangle the effects of the augmentation of PP sequences and the temporal constraints."], "referred_text_chunks": ["For video generation, we propose a PP loss that also makes use of symmetric sequences. However, we directly constrain the PP loss via the generated video content, which successfully improves the long-term temporal consistency in the video results. {{The PP loss is effective by offering valid information in forward as well as backward passes of image sequences."]}, "6": {"question": "What is the role of the Motion Compensation block in the Frame-Recurrent Generator?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 directly shows the Motion Compensation block within the Frame-Recurrent Generator. Text 0 discusses motion estimation and compensation in video generation, relevant to the role of the Motion Compensation block.', 'Answer': 'The Motion Compensation block estimates and adjusts the motion between frames, helping to maintain spatial detail and temporal coherence by aligning previous frames with the current frame in the Frame-Recurrent Generator.'}", "all_figures_names": ["1811.09393v4-Figure16-1.png", "1811.09393v4-Figure19-1.png", "1811.09393v4-Figure22-1.png", "1811.09393v4-Figure18-1.png", "1811.09393v4-Table5-1.png", "1811.09393v4-Figure11-1.png", "1811.09393v4-Figure13-1.png", "1811.09393v4-Figure2-1.png"], "referred_figures_names": ["1811.09393v4-Figure2-1.png"], "answer": "The Motion Compensation block estimates the motion between the previous frame and the current frame, and uses this information to warp the previous frame to the current frame. This helps the generator to produce more realistic images by taking into account the temporal information in the video sequence.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Hence, motion estimation~ and compensation become crucial for video generation tasks. The compensation can take various forms, e.g., explicitly using variants of optical flow networks~ and implicitly using deformable convolution layers~ or dynamic up-sampling~. In our work, a network is trained to estimate the motion and we show that it can help generators and discriminators in spatio-temporal adversarial training. For VSR, recent work improve the spatial detail and temporal coherence by either using multiple low-resolution (LR) frames as inputs or recurrently using previously estimated outputs .", "Video results are shown in the in the supplemental material (Sec. 4.5.). \\setlength{\\tabcolsep{1 pt During training, the generator of DsDt receives 10 frames, and generators of PP-Augment and \\TecoGANsmall{ see 19 frames.", "{eq:generation{0.75 Here, the network \\fnet is trained to estimate the motion v_t from frame a_{t-1 to a_t and W denotes warping. The central building block of our approach is a novel {\\em spatio-temporal discriminator D_{s,t that receives triplets of frames, shown in \\myreffig{fig:dst c).", "I.e., one batch contains 40 frames and with the \\ppl{ loss formulation, the NN receives gradients from 76 frames in total for every training iteration. In the pre-training stage of VSR, we train the F and a generator with 10 residual blocks. An ADAM optimizer with \\beta = 0.9 is used throughout.", "With our setup, the discriminator profits from the warped frames to classify realistic and unnatural temporal changes, and for situations where the motion estimation is less accurate, the discriminator can fall back to the original, i.e. not warped, images. Network Architecture for UVT While one generator is enough to map data from A to B for tasks such as VSR, unpaired generation tasks require a second generator to establish cycle consistency \\citep{zhu2017cycleGAN. For the UVT task, we use two recurrent generators{, mapping from domain A to B and back.", "Thus, we propose an algorithm for spatio-temporal adversarial learning that is easy to integrate into existing GAN-based image generation approaches. Starting from a standard GAN for images, as shown in \\myreffig{fig:dst a), we propose to use a frame-recurrent generator (b) together with a spatio-temporal discriminator (c). { As shown in \\myreffig{fig:dst b), our generator produces an output g_t", "Additionally, by extracting shared spatio-temporal features, it allows for smaller network sizes. Self-Supervision for Long-term Temporal Consistency When relying on a previous output as input, i.e., for frame-recurrent architectures, generated structures easily accumulate frame by frame. In adversarial training, generators learn to heavily rely on previously generated frames and can easily converge towards strongly reinforcing spatial features over longer periods of time.", "While regular losses such as L^2~ offer good performance for image super-resolution (SR) tasks in terms of PSNR metrics, previous work found adversarial training~ to significantly improve the perceptual quality in multi-modal settings such as { image generation~, colorization~, super-resolution~, and translation~ tasks. {Besides representing natural images, GAN-based frameworks are also successful at static graphic representations including {geometry synthesis~ and city modeling~. {Sequential generation tasks, on the other hand, require {the generation of realistic content that changes naturally over time~. It is especially so for conditional video generation tasks~, {where specific correlations between the input and the generated spatio-temporal evolution are required when ground-truth motions are not provided."], "referred_text_chunks": ["Hence, motion estimation~ and compensation become crucial for video generation tasks. The compensation can take various forms, e.g., explicitly using variants of optical flow networks~ and implicitly using deformable convolution layers~ or dynamic up-sampling~. In our work, a network is trained to estimate the motion and we show that it can help generators and discriminators in spatio-temporal adversarial training. For VSR, recent work improve the spatial detail and temporal coherence by either using multiple low-resolution (LR) frames as inputs or recurrently using previously estimated outputs ."]}, "7": {"question": "Which method produces the least amount of artifacts?", "referred_figures_indices": [5], "response": "{'Image': 4, 'Text': 3, 'Rationale': 'Image 4 provides quantitative metrics comparing methods for artifacts, including PSNR, LPIPS, T-diff, and others, relevant to evaluating artifact presence. Text 3 describes how the DsDtPP model effectively suppresses drifting artifacts with improved temporal coherence.', 'Answer': 'The TecoGAN model produces the least amount of artifacts, as it has the lowest LPIPS, T-diff, tOF, and tLP scores.'}", "all_figures_names": ["1811.09393v4-Figure16-1.png", "1811.09393v4-Table6-1.png", "1811.09393v4-Figure13-1.png", "1811.09393v4-Figure22-1.png", "1811.09393v4-Table2-1.png", "1811.09393v4-Figure23-1.png", "1811.09393v4-Figure10-1.png", "1811.09393v4-Figure19-1.png"], "referred_figures_names": ["1811.09393v4-Figure23-1.png"], "answer": "TecoGAN\u2296.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["The results show that the temporal constraint is the key to reliably suppressing the temporal accumulation of artifacts, achieving consistency, and allowing models to infer much longer sequences than seen during training. {The majority of related work for video generation focuses on network architectures. Being orthogonal to architecture improvements, our work explores temporal self-supervision.", "Hence, with the PP constraint, the model avoids recurrent accumulation of artifacts and works well for sequences that are substantially longer than the training length. Among others, we have tested our model with ToS sequences of lengths 150, 166 and 233. For all of these sequences, the TecoGAN model successfully avoids temporal accumulation or streaking artifacts.", "While DsDt shows strong recurrent accumulation artifacts early on, the PP-Augment version slightly reduces the artifacts. In \\myreffig{fig:PPsep, it works well for frame 15, but shows artifacts from frame 32 on. Only our regular model (\\TecoGANsmall) successfully avoids temporal accumulation for all 40 frames.", "While this strategy works for generating short sequences during training, the strengthening effect can lead to very undesirable artifacts for long-sequence inferences. By adding the self-supervision for long-term temporal consistency \\mathcal{L_{pp, we arrive at the {\\em DsDtPP model, which effectively suppresses these drifting artifacts with an improved temporal coherence. In \\myreffig{fig:folCMP and \\myreffig{fig:calCMP, DsDtPP results in continuous yet detailed temporal profiles without streaks from temporal drifting.", "The PP loss effectively avoids the temporal accumulation of artifacts, which can potentially benefit a variety of recurrent architectures. {We also note that most existing image metrics focus on spatial content only. We fill the gap of temporal assessment with a pair of metrics that measures the perceptual similarity over time and the similarity of motions with respect to a ground truth reference.", "{ This bi-directional loss formulation also helps to constrain ambiguities due to disocclusions that can occur in regular training scenarios. As shown in \\myreffig{fig:ping_pongcomparison b), the \\ppl{ loss successfully removes drifting artifacts while appropriate high-frequency details are preserved.", "We concatenate all triplets together. In this way, the conditional D_{s,t will penalize G if \\text{I_g contains less spatial details or unrealistic artifacts {in comparison to \\text{I_a,\\text{I_b. At the same time, temporal relationships between the generated images \\text{I_{wg and those of the ground truth \\text{I_{wb should match.", "The generators in DsOnly, DsDt, DsDtPP and \\TecoGANsmall{ have 10 residual blocks, whereas the TecoGAN model has 6 additional residual blocks in its generator. Therefore, after loading 10 residual blocks from the pre-trained model, these additional residual blocks are faded in smoothly with a factor of 2.5*10^{-5. We found this growing training methodology~\\citep{karras2017progressive, to be stable and efficient in our tests."], "referred_text_chunks": ["The results show that the temporal constraint is the key to reliably suppressing the temporal accumulation of artifacts, achieving consistency, and allowing models to infer much longer sequences than seen during training. {The majority of related work for video generation focuses on network architectures. Being orthogonal to architecture improvements, our work explores temporal self-supervision."]}, "8": {"question": "Which method produces the most realistic results for the Vid4 scenes?", "referred_figures_indices": [7], "response": "{'Image': 2, 'Text': 2, 'Rationale': 'Image 2 contains detailed metrics for the Vid4 scenes, which directly shows the performance of different methods, including TecoGAN, across various metrics such as PSNR and LPIPS. Text chunk 2 provides user study results indicating TecoGAN performs very well for Vid4 scenes.', 'Answer': 'TecoGAN produces the most realistic results for the Vid4 scenes according to user study and metrics evaluation.'}", "all_figures_names": ["1811.09393v4-Figure15-1.png", "1811.09393v4-Figure3-1.png", "1811.09393v4-Table4-1.png", "1811.09393v4-Figure19-1.png", "1811.09393v4-Figure9-1.png", "1811.09393v4-Figure10-1.png", "1811.09393v4-Figure8-1.png", "1811.09393v4-Figure12-1.png"], "referred_figures_names": ["1811.09393v4-Figure12-1.png"], "answer": "TecoGAN.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Mean values of the Vid4 scenes~ are shown on the top of \\myreftab{tab:vid4metrics. Trained with direct vector norms losses, {FRVSR, DUF, EDVR, and RBPN achieve high PSNR scores. However, the undesirable smoothing induced by these losses manifests themselves in larger LPIPS distances.", "Below, we additionally show user study results for the Vid4 scenes. By comparing the user study results and the metric breakdowns shown in \\myreftab{tab:vid4metrics, we found our metrics to reliably capture the human temporal perception. \\end{minipage \\end{minipage \\end{minipage\\\\ \\end{figure*", "After collecting 1000 votes from 50 users for every scene, i.e. twice for all possible pairs (5 \\times 4 / 2 = 10 pairs), we follow common procedure and compute scores for all models with the Bradley-Terry model~(\\citeyear{bradley1952rank). The outcomes for the Vid4 scenes can be seen in \\myreffig{fig:userstudy{(overall scores are listed in \\myreftab{tab:metrics of the main document). From the Bradley-Terry scores for the Vid4 scenes we can see that the TecoGAN model performs very well, and achieves the first place in three cases, as well as a second place in the walk scene.", "These results are reflected in the proposed metrics and confirmed by user studies. While our method generates very realistic results for a wide range of natural images, our method can lead to temporally coherent yet sub-optimal details in certain cases such as under-resolved faces and text in VSR, or UVT tasks with strongly different motion between two domains. For the latter case, it would be interesting to apply both our method and motion translation from concurrent work~.", "In line with other VSR projects, we down-sample these frames by a factor of 2 to get the ground-truth HR frames. Corresponding LR frames are achieved by applying a Gaussian blur and sampling every fourth pixel. A Gaussian blur step is important to mimic the information loss due to the camera sensibility in a real-life capturing scenario.", "For paired video translation tasks, the {\\em{vid2vid network achieves improved temporal coherence by using a video discriminator to supervise the output sequence conditioned with the ground-truth motion. With no ground-truth data available, we train a vid2vid variant by using the estimated motions and original triplets, i.e \\{\\text{I_g + F(g_{t-1,g_t) + F(g_{t+1,g_t) or \\text{I_b+ F(b_{t-1,b_t) +", "Previous work, e.g. tempoGAN for fluid flow~ and vid2vid for video translation~, has proposed adversarial temporal losses to achieve time consistency. While tempoGAN employs a second temporal discriminator with multiple aligned frames to assess the realism of temporal changes, it is {not suitable for videos, as it relies on ground truth motions and employs single-frame processing that is sub-optimal for natural images. On the other hand, vid2vid focuses on paired video translations and proposes a video discriminator based on a conditional motion input that is estimated from the paired ground-truth sequences.", "\\quad c) The network sizes. \\end{minipage \\end{minipage \\end{figure* Results and Metric Evaluation For the VSR task, we test our model on a wide range of video data, including the widely used Vid4 dataset shown in \\myreffig{fig:folCMP, and , detailed scenes from the movie Tears of Steel~(ToS)~\\shortcite{tearsofsteel shown in \\myreffig{fig:suppresults1, and others shown in \\myreffig{fig:vid2."], "referred_text_chunks": ["Mean values of the Vid4 scenes~ are shown on the top of \\myreftab{tab:vid4metrics. Trained with direct vector norms losses, {FRVSR, DUF, EDVR, and RBPN achieve high PSNR scores. However, the undesirable smoothing induced by these losses manifests themselves in larger LPIPS distances."]}, "9": {"question": "What is the learning rate for the generator in the DsOnly model?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 3, 'Rationale': 'Image 0 contains labeled training parameters directly showing values, including the learning rate for DsOnly. Text 3 discusses training the DsOnly model and provides context on its objectives which relate to training parameters.', 'Answer': 'The learning rate for the generator in the DsOnly model is 5e-5.'}", "all_figures_names": ["1811.09393v4-Table6-1.png", "1811.09393v4-Figure22-1.png", "1811.09393v4-Table2-1.png", "1811.09393v4-Figure11-1.png", "1811.09393v4-Figure18-1.png", "1811.09393v4-Figure20-1.png", "1811.09393v4-Figure14-1.png", "1811.09393v4-Figure23-1.png"], "referred_figures_names": ["1811.09393v4-Table6-1.png"], "answer": "5.00E-05", "figure_type": "table", "content_type": "table", "text_chunks": ["The generators in DsOnly, DsDt, DsDtPP and \\TecoGANsmall{ have 10 residual blocks, whereas the TecoGAN model has 6 additional residual blocks in its generator. Therefore, after loading 10 residual blocks from the pre-trained model, these additional residual blocks are faded in smoothly with a factor of 2.5*10^{-5. We found this growing training methodology~\\citep{karras2017progressive, to be stable and efficient in our tests.", "Since the \\TecoGANsmall model requires less training resources, we also trained a larger generator with 50\\mypercent{ more weights. In the following, we will focus on this larger single-discriminator architecture with \\ppl{ loss as our full {\\em TecoGAN model for VSR. Compared to the \\TecoGANsmall model, it can generate more details, and the training process is more stable, indicating that the larger generator and D_{s,t are more evenly balanced.", "When training the VSR DsDt and DsDtPP, extra parameters are used to balance the two cooperating discriminators properly. Through experiments, we found D_t to be stronger. Therefore, we reduce the learning rate of D_t to 1.5*10^{-5 in order to keep both discriminators balanced.", "Obama and Trump video courtesy of the White House (public domain). \\end{figure* \\paragraph{{Video Super-Resolution For VSR, we first train a {\\em DsOnly model that uses a frame-recurrent G and \\fnet with a VGG loss and only the regular spatial discriminator. Compared to ENet, which exhibits strong incoherence due to the lack of temporal information, DsOnly improves temporal coherence thanks to the frame-recurrent connection, but there are noticeable high-frequency changes between frames.", "Therefore, video translation from domain A to B is a tough task, as the latter contains significantly more turbulent and small-scale motions. With no temporal information available, the CycleGAN variant generates HR smoke that strongly flickers. The DsOnly model offers better temporal coherence by relying on its frame-recurrent input, but it learns a solution that largely ignores the current input and fails to keep reasonable spatio-temporal cycle-consistency links between the two domains.", "70\\mypercent{ more GPU memory, and leads to a reduced training performance by ca. 20\\mypercent{. The \\TecoGANsmall model yields similar perceptual and temporal quality to DsDtPP with a significantly faster and more stable training.", "Although DsDtPP generates good results, it is difficult in practice to balance the generator and the two discriminators. The results shown here were achieved only after numerous runs manually tuning the weights of the different loss terms. By using the proposed D_{s,t discriminator instead, we get a first complete model for our method, denoted as {\\em \\TecoGANsmall.", "Our approach actually yields improved results, as we will show below. In practice, we found it crucial to ensure that generators first learn reasonable spatial features, and only then improve {their temporal correlation. Therefore, different to the D_{s,t of VST that always receives 3 concatenated triplets as an input, the unconditional D_{s,t of UVT only takes one triplet at a time."], "referred_text_chunks": ["The generators in DsOnly, DsDt, DsDtPP and \\TecoGANsmall{ have 10 residual blocks, whereas the TecoGAN model has 6 additional residual blocks in its generator. Therefore, after loading 10 residual blocks from the pre-trained model, these additional residual blocks are faded in smoothly with a factor of 2.5*10^{-5. We found this growing training methodology~\\citep{karras2017progressive, to be stable and efficient in our tests."]}, "10": {"question": "Why does flow estimation become less accurate near image boundaries?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 0, 'Rationale': 'Image 0 shows the flow estimation issues at boundaries with examples of original and warped frames. Text 0 explains why flow estimation is less accurate at image boundaries due to unreliable neighborhood information.', 'Answer': 'Flow estimation becomes less accurate near image boundaries because there is a lack of reliable neighborhood information, making it difficult to predict motion accurately when objects move into or out of view.'}", "all_figures_names": ["1811.09393v4-Figure22-1.png", "1811.09393v4-Figure19-1.png", "1811.09393v4-Figure2-1.png", "1811.09393v4-Figure12-1.png", "1811.09393v4-Figure13-1.png", "1811.09393v4-Figure8-1.png", "1811.09393v4-Figure20-1.png", "1811.09393v4-Figure18-1.png"], "referred_figures_names": ["1811.09393v4-Figure22-1.png"], "answer": "Flow estimation becomes less accurate near image boundaries because there is less information available to estimate the flow. This is because the pixels at the boundaries are only surrounded by pixels on one side, whereas pixels in the interior of the image are surrounded by pixels on all sides.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["\\setlength{\\columnsep{6pt Technical Details of the \\newline Spatio-Temporal Discriminator Motion Compensation Used in Warped Triplet In the TecoGAN architecture, D_{s,t detects the temporal relationships between I_{wg and I_{wb with the help of the flow estimation network F. However, at the boundary of images, the output of F is usually less accurate due to the lack of reliable neighborhood information. There is a higher chance that objects move into the field of view, or leave suddenly, which significantly affects the images warped with the inferred motion. An example is shown in \\myreffig{fig:warpcrop.", "The flow estimation network F with the loss \\mathcal{L_{G,F should only be trained to support G in reaching the output quality as determined by D_{s,t, but not the other way around. The latter could lead to F networks that confuse D_{s,t with strong distortions of I_{wg and I_{wb. In order to avoid the this undesirable case, we stop the gradient back propagation from I_{wg and I_{wb to F. In this way, gradients from D_{s,t to F are only back propagated through the generated samples g_{t-1, g_t and g_{t+1 into the generator network.", "As shown at the bottom of \\myreftab{tab:vid4metrics, T-diff, due to its local nature, is easily deceived by blurry method such as the bi-cubic interrelation and can not correlate well with visual assessments of coherence. {By using the proposed metrics, i.e. measuring the pixel-wise motion differences using \\text{tOF together with the perceptual changes over time using \\selflp{, a more nuanced evaluation can be achieved, as shown for the VSR task in the middle of \\myreftab{tab:vid4metrics. Not surprisingly, the results of ENet show larger errors for all metrics due to their strongly flickering content.", "{Since temporal evolution differs from a spatial distribution, we show how a separate handling of the temporal dimension can reduce computational costs, remove training restrictions, and most importantly improve inference quality. For tracking and optical flow estimation, L^2-based time-cycle losses~\\citep{wang2019learning were proposed to constrain motions and tracked correspondences using symmetric video inputs. By optimizing indirectly via motion compensation or tracking, this loss improves the accuracy of the results.", "Hence, motion estimation~ and compensation become crucial for video generation tasks. The compensation can take various forms, e.g., explicitly using variants of optical flow networks~ and implicitly using deformable convolution layers~ or dynamic up-sampling~. In our work, a network is trained to estimate the motion and we show that it can help generators and discriminators in spatio-temporal adversarial training. For VSR, recent work improve the spatial detail and temporal coherence by either using multiple low-resolution (LR) frames as inputs or recurrently using previously estimated outputs .", "These results are reflected in the proposed metrics and confirmed by user studies. While our method generates very realistic results for a wide range of natural images, our method can lead to temporally coherent yet sub-optimal details in certain cases such as under-resolved faces and text in VSR, or UVT tasks with strongly different motion between two domains. For the latter case, it would be interesting to apply both our method and motion translation from concurrent work~.", "OF represents an optical flow estimation with the {Farneback~\\shortcite{farneback2003two algorithm and LP is the perceptual LPIPS metric. In tLP, the behavior of the reference is also considered, as natural videos exhibit a certain degree of change over time. In conjunction, both pixel-wise differences and perceptual changes are crucial for quantifying realistic temporal coherence.", "Bi-cubic up-sampling, DUF, and FRVSR achieve very low T-diff errors due to their smooth results, representing an easy, but undesirable avenue for achieving coherency. However, the overly smooth changes of the former two are identified by the \\selflp{ scores. While our DsOnly model generates sharper results at the expense of temporal coherence, it still outperforms ENet there."], "referred_text_chunks": ["\\setlength{\\columnsep{6pt Technical Details of the \\newline Spatio-Temporal Discriminator Motion Compensation Used in Warped Triplet In the TecoGAN architecture, D_{s,t detects the temporal relationships between I_{wg and I_{wb with the help of the flow estimation network F. However, at the boundary of images, the output of F is usually less accurate due to the lack of reliable neighborhood information. There is a higher chance that objects move into the field of view, or leave suddenly, which significantly affects the images warped with the inferred motion. An example is shown in \\myreffig{fig:warpcrop."]}, "11": {"question": "Which method has the best perceptual performance according to the tOF score?", "referred_figures_indices": [6], "response": "{'Image': 1, 'Text': 2, 'Rationale': 'Image 1 provides a detailed table of metrics, including tOF scores, which directly answers the question. Text 2 discusses tOF calculations relevant to the question.', 'Answer': 'TecoGAN has the best perceptual performance according to the tOF score.'}", "all_figures_names": ["1811.09393v4-Figure11-1.png", "1811.09393v4-Table4-1.png", "1811.09393v4-Figure15-1.png", "1811.09393v4-Table5-1.png", "1811.09393v4-Figure16-1.png", "1811.09393v4-Figure12-1.png", "1811.09393v4-Figure18-1.png", "1811.09393v4-Figure9-1.png"], "referred_figures_names": ["1811.09393v4-Figure18-1.png"], "answer": "TecoGAN.", "figure_type": "plot.", "content_type": "figure", "text_chunks": ["Given that perceptual metrics are already widely used for image evaluations, we believe it is the right time to consider perceptual changes in temporal evaluations, as we did with our proposed temporal coherence metrics. {Although not perfect, they are not as easily deceived as simpler metrics. Specifically, tOF is more robust than a direct pixel-wise metric as it compares motions instead of image content.", "Even compared to the large EDVR model using down-sampled inputs without Gaussian blur, TecoGAN still yields a 30\\mypercent{ improvement in terms of LPIPS. While traditional temporal metrics based on vector norm differences of warped frames, e.g. T-diff {=\\left\\| g_t - W( g_{t-1, v_t )\\right\\|_1 ~, can be easily deceived by very blurry results, {e.g. bi-cubic interpolated ones, we propose to use a tandem of {two new metrics, tOF and tLP, to measure the consistence over time. tOF measures the pixel-wise difference of motions estimated from sequences, and tLP measures perceptual changes over time using deep feature map: {\\small \\vspace{-3pt \\resizeEq{\\text{tOF &= \\left\\| OF(b_{t-1, b_t) - OF(g_{t-1, g_t) \\right\\|_1 & \\text{ and \\\\ \\text{\\selflp{ &= \\left\\| LP(b_{t-1, b_t) - LP(g_{t-1, g_t) \\right\\|_1 \\text{. & {eq:tmetrics{!", "the ones from the input data { , i.e., {\\footnotesize \\text{tOF =\\left\\| OF(a_{t-1, a_t) - OF(g^{a\\rightarrow b_{t-1, g^{a\\rightarrow b_t) \\right\\|_1 and {\\footnotesize \\selflp{ = \\left\\| LP(a_{t-1, a_t) - LP(g^{a\\rightarrow b_{t-1, g^{a\\rightarrow b_t) \\right\\|_1. With sharp spatial features and coherent motion, TecoGAN outperforms {CycleGAN and RecycleGAN on the Obama\\&Trump dataset, as shown in \\myreftab{tab:UVTmetrics, although it is worth pointing out that tOF is less informative in this case, as the motion in the target domain is not necessarily pixel-wise aligned with the input. {", "{For VSR, we have confirmed these automated evaluations with several user studies (details in \\myrefapp{app:userstudy). Across all of them, we find that the majority of the participants considered the TecoGAN results to be closest to the ground truth, when comparing to bi-cubic interpolation, ENet, FRVSR and DUF. For the UVT tasks, where no ground-truth data is available, we can still evaluate tOF and tLP metrics by comparing the motion and the perceptual changes of the output data w.r.t.", "ENet, on the other hand, with no information from neighboring frames, yields the lowest PSNR and achieves an LPIPS score that is only slightly better than DUF and FRVSR. The TecoGAN model with adversarial training achieves an excellent LPIPS score, with a PSNR decrease of less than 2dB over DUF. This is very reasonable, since PSNR and perceptual quality were shown to be anti-correlated~\\citep{blau2018perception, especially in regions where PSNR is very high.", "Based on good perceptual quality and reasonable pixel-wise accuracy, TecoGAN outperforms all other methods by more than 30\\mypercent{ for LPIPS. {\\setlength{\\tabcolsep{0.3pt { \\setlength{\\tabcolsep{2.5pt \\setlength\\extrarowheight{-0.8pt \\end{figure* Temporal Metrics For both VSR and UVT, evaluating temporal coherence without ground-truth motion is very challenging. L^2-based temporal metrics such as {\\em T-diff = \\| g_t - W( g_{t-1, v_t )\\|_1 was used~ as a rough assessment of temporal differences, and we give corresponding numbers for comparison.", "In \\myreftab{tab:vid4metrics, while TecoGAN achieves spatial (LPIPS) improvements in all scenes, DUF and FRVSR are not far behind in the walk scene. In terms of temporal metrics \\ofmae{ and \\selflp{, TecoGAN achieves similar or lower scores compared to FRVSR and DUF for calendar, foliage and city scenes. The lower performance of our model for the walk scene is likewise captured by higher \\ofmae{ and \\selflp{ scores.", "OF represents an optical flow estimation with the {Farneback~\\shortcite{farneback2003two algorithm and LP is the perceptual LPIPS metric. In tLP, the behavior of the reference is also considered, as natural videos exhibit a certain degree of change over time. In conjunction, both pixel-wise differences and perceptual changes are crucial for quantifying realistic temporal coherence."], "referred_text_chunks": ["Given that perceptual metrics are already widely used for image evaluations, we believe it is the right time to consider perceptual changes in temporal evaluations, as we did with our proposed temporal coherence metrics. {Although not perfect, they are not as easily deceived as simpler metrics. Specifically, tOF is more robust than a direct pixel-wise metric as it compares motions instead of image content."]}, "12": {"question": "Which of the methods generated the sharpest details?", "referred_figures_indices": [6], "response": "{'Image': 6, 'Text': 7, 'Rationale': 'Image 6 shows detailed VSR comparisons highlighting sharpness in TecoGAN output. Text 7 discusses frame-recurrent generation improving sharpness.', 'Answer': 'TecoGAN generated the sharpest details.'}", "all_figures_names": ["1811.09393v4-Table6-1.png", "1811.09393v4-Figure14-1.png", "1811.09393v4-Figure20-1.png", "1811.09393v4-Figure15-1.png", "1811.09393v4-Figure8-1.png", "1811.09393v4-Figure19-1.png", "1811.09393v4-Figure11-1.png", "1811.09393v4-Figure4-1.png"], "referred_figures_names": ["1811.09393v4-Figure11-1.png"], "answer": "TecoGAN", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Based on good perceptual quality and reasonable pixel-wise accuracy, TecoGAN outperforms all other methods by more than 30\\mypercent{ for LPIPS. {\\setlength{\\tabcolsep{0.3pt { \\setlength{\\tabcolsep{2.5pt \\setlength\\extrarowheight{-0.8pt \\end{figure* Temporal Metrics For both VSR and UVT, evaluating temporal coherence without ground-truth motion is very challenging. L^2-based temporal metrics such as {\\em T-diff = \\| g_t - W( g_{t-1, v_t )\\|_1 was used~ as a rough assessment of temporal differences, and we give corresponding numbers for comparison.", "Even compared to the large EDVR model using down-sampled inputs without Gaussian blur, TecoGAN still yields a 30\\mypercent{ improvement in terms of LPIPS. While traditional temporal metrics based on vector norm differences of warped frames, e.g. T-diff {=\\left\\| g_t - W( g_{t-1, v_t )\\right\\|_1 ~, can be easily deceived by very blurry results, {e.g. bi-cubic interpolated ones, we propose to use a tandem of {two new metrics, tOF and tLP, to measure the consistence over time. tOF measures the pixel-wise difference of motions estimated from sequences, and tLP measures perceptual changes over time using deep feature map: {\\small \\vspace{-3pt \\resizeEq{\\text{tOF &= \\left\\| OF(b_{t-1, b_t) - OF(g_{t-1, g_t) \\right\\|_1 & \\text{ and \\\\ \\text{\\selflp{ &= \\left\\| LP(b_{t-1, b_t) - LP(g_{t-1, g_t) \\right\\|_1 \\text{. & {eq:tmetrics{!", "These results are reflected in the proposed metrics and confirmed by user studies. While our method generates very realistic results for a wide range of natural images, our method can lead to temporally coherent yet sub-optimal details in certain cases such as under-resolved faces and text in VSR, or UVT tasks with strongly different motion between two domains. For the latter case, it would be interesting to apply both our method and motion translation from concurrent work~.", "{The latter is most likely caused by the overall slightly smoother images of the walk scene, in conjunction with the presence of several human faces, where our model can lead to the generation of unexpected details. However, overall the user study shows that users preferred the TecoGAN output over the other two deep-learning methods with a 63.5\\mypercent{ probability. This result also matches with our metric evaluations.", "The temporal profiles of DsOnly in \\myreffig{fig:folCMP and , correspondingly contain sharp and broken lines. When adding a temporal discriminator in addition to the spatial one ({\\em DsDt), this version generates more coherent results, and its temporal profiles are sharp and coherent. However, DsDt often produces the drifting artifacts discussed in \\myrefsec{sec:method, as the generator learns to reinforce existing details from previous frames to fool D_s with sharpness, and satisfying D_t with good temporal coherence in the form of persistent detail.", "In line with other VSR projects, we down-sample these frames by a factor of 2 to get the ground-truth HR frames. Corresponding LR frames are achieved by applying a Gaussian blur and sampling every fourth pixel. A Gaussian blur step is important to mimic the information loss due to the camera sensibility in a real-life capturing scenario.", "While smoke simulations usually contain strong numerical viscosity with details limited by the simulation resolution, the real smoke from Eckert et al. \\shortcite{eckert2018coupled contains vivid motions with many vortices and high-frequency details. As shown in \\myreffig{fig:UVTmore, our method can be used to narrow the gap between simulations and real-world phenomena. While visual results discussed above provide a first indicator of the quality our approach achieves, quantitative evaluations are crucial for automated evaluations across larger numbers of samples.", "Bi-cubic up-sampling, DUF, and FRVSR achieve very low T-diff errors due to their smooth results, representing an easy, but undesirable avenue for achieving coherency. However, the overly smooth changes of the former two are identified by the \\selflp{ scores. While our DsOnly model generates sharper results at the expense of temporal coherence, it still outperforms ENet there."], "referred_text_chunks": ["Based on good perceptual quality and reasonable pixel-wise accuracy, TecoGAN outperforms all other methods by more than 30\\mypercent{ for LPIPS. {\\setlength{\\tabcolsep{0.3pt { \\setlength{\\tabcolsep{2.5pt \\setlength\\extrarowheight{-0.8pt \\end{figure* Temporal Metrics For both VSR and UVT, evaluating temporal coherence without ground-truth motion is very challenging. L^2-based temporal metrics such as {\\em T-diff = \\| g_t - W( g_{t-1, v_t )\\|_1 was used~ as a rough assessment of temporal differences, and we give corresponding numbers for comparison."]}, "13": {"question": "What is the purpose of the UVT cycle link?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 directly illustrates the UVT cycle link necessary for understanding the cycle-consistency concept. Text 0 explains the purpose of establishing temporal cycle-consistency using UVT cycle links and discriminators, which directly answers the question.', 'Answer': 'The purpose of the UVT cycle link is to establish temporal cycle-consistency between different domains using supervision from unconditional spatio-temporal discriminators, ensuring that temporal changes are realistic rather than unnatural.'}", "all_figures_names": ["1811.09393v4-Figure20-1.png", "1811.09393v4-Figure8-1.png", "1811.09393v4-Figure10-1.png", "1811.09393v4-Figure5-1.png", "1811.09393v4-Figure13-1.png", "1811.09393v4-Figure19-1.png", "1811.09393v4-Figure14-1.png", "1811.09393v4-Figure16-1.png"], "referred_figures_names": ["1811.09393v4-Figure5-1.png"], "answer": "The UVT cycle link is used to transfer knowledge between two recurrent generators.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["{to enforce consistency. An encoder-decoder structure is applied to UVT generators and F. In UVT tasks, we demonstrate that the temporal cycle-consistency between different domains can be established using the supervision of unconditional spatio-temporal discriminators. This is in contrast to previous work which focuses on the generative networks to form spatio-temporal cycle links.", "With our setup, the discriminator profits from the warped frames to classify realistic and unnatural temporal changes, and for situations where the motion estimation is less accurate, the discriminator can fall back to the original, i.e. not warped, images. Network Architecture for UVT While one generator is enough to map data from A to B for tasks such as VSR, unpaired generation tasks require a second generator to establish cycle consistency \\citep{zhu2017cycleGAN. For the UVT task, we use two recurrent generators{, mapping from domain A to B and back.", "In UVT tasks without paired ground truth data, {the generators should match the distribution of features in the target domain. Similar to a style loss for traditional style transfer tasks , we thus compute the {D_{s,t feature correlations measured by the Gram matrix for UVT tasks. The D_{s,t features contain both spatial and temporal information and hence are especially well suited for the perceptual loss.", "In line with VSR, a perceptual evaluation by humans {in a user study confirms our metric evaluations for the UVT task. The participants consistently prefer TecoGAN results over CycleGAN and RecycleGAN. The corresponding scores are given in the right column of \\myreftab{tab:UVTmetrics.", "We refer to our full model for both tasks as {\\em TecoGAN below. {The UVT data-sets are obtained from previous work~ and each data domain has around 2400 to 3600 unpaired frames. For VSR, we download 250 short videos with 120 frames each from \\url{Vimeo.com.", "Overall, the metrics confirm the performance of our TecoGAN approach and match the results of the user studies, which indicate that our proposed temporal metrics successfully capture important temporal aspects of human perception. For UVT tasks which have no ground-truth data, we carried out two sets of user studies: One uses an arbitrary sample from the target domain as the reference and the other uses the actual input from the source domain as the reference. On the Obama\\&Trump data-sets, we evaluate results from CycleGAN, RecycleGAN, and TecoGAN following the same modality, i.e. a 2AFC design with 50 users for each run.", "Our approach actually yields improved results, as we will show below. In practice, we found it crucial to ensure that generators first learn reasonable spatial features, and only then improve {their temporal correlation. Therefore, different to the D_{s,t of VST that always receives 3 concatenated triplets as an input, the unconditional D_{s,t of UVT only takes one triplet at a time.", "The additional training parameters are also listed in \\myreftab{tab:training_details. For UVT, \\mathcal{L_\\text{content and \\mathcal{L_{\\phi are only used to improve the convergence of the training process. We fade out \\mathcal{L_\\text{content in the first 10k batches and \\mathcal{L_{\\phi is used for the first 80k and faded out in last 20k."], "referred_text_chunks": ["{to enforce consistency. An encoder-decoder structure is applied to UVT generators and F. In UVT tasks, we demonstrate that the temporal cycle-consistency between different domains can be established using the supervision of unconditional spatio-temporal discriminators. This is in contrast to previous work which focuses on the generative networks to form spatio-temporal cycle links."]}, "14": {"question": "What is the purpose of the user study?", "referred_figures_indices": [1], "response": "```json\n{\n    'Image': 1,\n    'Text': 0,\n    'Rationale': 'Image 1 shows a user study setup directly related to evaluating video quality by comparing methods. Text 0 describes the purpose of user studies in evaluating VSR methods through participant preferences.',\n    'Answer': 'The purpose of the user study is to evaluate the effectiveness and user preference of different VSR methods by comparing generated videos against a reference, using a pair-wise choice design.'\n}\n```", "all_figures_names": ["1811.09393v4-Figure14-1.png", "1811.09393v4-Figure19-1.png", "1811.09393v4-Table3-1.png", "1811.09393v4-Figure23-1.png", "1811.09393v4-Figure8-1.png", "1811.09393v4-Figure20-1.png", "1811.09393v4-Figure2-1.png", "1811.09393v4-Table2-1.png"], "referred_figures_names": ["1811.09393v4-Figure19-1.png"], "answer": "The user study is designed to test which of two images is closer to a reference video.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["User Studies We conducted several user studies for the VSR task comparing five different methods: bi-cubic interpolation, ENet, FRVSR, DUF and TecoGAN. The established 2AFC design~\\citep{fechner1889elemente,um2017perceptual is applied, i.e., participants have a pair-wise choice, with the ground-truth video shown as reference. One example setup can be seen in \\myreffig{fig:userstudysample.", "Below, we additionally show user study results for the Vid4 scenes. By comparing the user study results and the metric breakdowns shown in \\myreftab{tab:vid4metrics, we found our metrics to reliably capture the human temporal perception. \\end{minipage \\end{minipage \\end{minipage\\\\ \\end{figure*", "Overall, the metrics confirm the performance of our TecoGAN approach and match the results of the user studies, which indicate that our proposed temporal metrics successfully capture important temporal aspects of human perception. For UVT tasks which have no ground-truth data, we carried out two sets of user studies: One uses an arbitrary sample from the target domain as the reference and the other uses the actual input from the source domain as the reference. On the Obama\\&Trump data-sets, we evaluate results from CycleGAN, RecycleGAN, and TecoGAN following the same modality, i.e. a 2AFC design with 50 users for each run.", "Below we focus more on the VSR task as ground-truth data is available. We conduct user studies and present evaluations of the different models w.r.t. established spatial metrics.", "The x-axis indicates whether the inferred result matches the general spatio-temporal content of the target domain. Our TecoGAN model also receives the highest scores here, although the responses are slightly more spread out. On the right of \\myreffig{fig:uvtuser, we summarize both studies in a single graph highlighting that the TecoGAN model is consistently preferred by the participants of our user studies.", "User studies confirm these metrics for both tasks. The central contributions of our work are: Together, our contributions lead to models that outperform previous work in terms of temporally-coherent detail, which we {qualitatively and quantitatively demonstrate with a wide range of content. Related Work Deep learning has made great progress for image generation tasks.", "\\appendix \\newpage \\setcounter{page{1 \\noindent In the following, we first give details of the proposed temporal evaluation metrics, and present the corresponding quantitative comparison of our method versus a range of others (\\myrefapp{app:metrics). The user studies we conduected are in support of our TecoGAN network and proposed temporal metrics, and explained in \\myrefapp{app:userstudy.", "{The latter is most likely caused by the overall slightly smoother images of the walk scene, in conjunction with the presence of several human faces, where our model can lead to the generation of unexpected details. However, overall the user study shows that users preferred the TecoGAN output over the other two deep-learning methods with a 63.5\\mypercent{ probability. This result also matches with our metric evaluations."], "referred_text_chunks": ["User Studies We conducted several user studies for the VSR task comparing five different methods: bi-cubic interpolation, ENet, FRVSR, DUF and TecoGAN. The established 2AFC design~\\citep{fechner1889elemente,um2017perceptual is applied, i.e., participants have a pair-wise choice, with the ground-truth video shown as reference. One example setup can be seen in \\myreffig{fig:userstudysample."]}}