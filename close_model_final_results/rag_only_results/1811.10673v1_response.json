{"0": {"question": "What is the effect of downsampling on the quality of reconstructed frames?", "referred_figures_indices": [7], "response": "{'Image': 7, 'Text': 0, 'Rationale': 'Image 7 shows the effect of different downsampling levels on frame quality, highlighting that higher resolution results in better recognition. Text 0 explains the scaling factors used in experiments, which directly relate to changes in frame quality.', 'Answer': 'Downsampling reduces the spatial resolution of frames, leading to lower quality. As the downsampling factor increases, the frame becomes less recognizable, but higher resolutions result in better quality and more recognizable frames.'}", "all_figures_names": ["1811.10673v1-Figure7-1.png", "1811.10673v1-Figure8-1.png", "1811.10673v1-Figure5-1.png", "1811.10673v1-Figure3-1.png", "1811.10673v1-Table2-1.png", "1811.10673v1-Figure4-1.png", "1811.10673v1-Table1-1.png", "1811.10673v1-Figure6-1.png"], "referred_figures_names": ["1811.10673v1-Figure6-1.png"], "answer": "Downsampling reduces the quality of reconstructed frames.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["To simplify experiments, the downsampler only scaled the frames by factors for 4 \\times 4 and 8 \\times 8. Quality of reconstructed downsampled frame To study the relative qualities of reconstructed frames using different levels of {downsampling, we employed three scaling sizes, (1,1), (4,4) and (8,8). Hence, an original frame of size 256\\times256, became 256\\times256, 64\\times64, and 32\\times32, respectively (Figure~).", "Let S and Q_k denote the outputs of the downsampler and the soft edge detector, and x_I' and x_G be the outputs of the lossless compressor respectively. The downsampling step reduces the spatial resolution of each frame, but does not affect the number of frames. The purpose of downsampling is to produce a more compact representation that is easier to compress.", "Quality of reconstructed quantized frame We also examined the relationship between the quality of reconstructed frames and the level of quantization delivered by the soft edge detector (see Section~). As a toy example, assume E_1=E_2=I, i.e, X_I=X_I'. E_2 taken to the soft edge detector with quantization level k (Q_k) without any downsampling (Figure~).", "In addition, at the second stage of encoding we fixed soft edge detector with quantization level 2 (Q_2) without lossless compression, to isolate the effects of downsampling. As the scaling size was increased, the reconstructed frames became more recognizable (Figure~). Accordingly, the measured VQA scores also increased monotonously, as the size of x_G was increased (Table~).", "It is quite possible that in this context, conventional VQA models may not accurately predict the subjective quality of GAN-compressed videos. As shown in Figure , our scheme apparently reconstructs frames of better quality than H.264. For example, H.264 compressed at 7.00 Kbps resulted in unrecognizable reconstructed frames (many blurred blocks), while our model at 6.51 Kbps yielded more detailed frames than even H.264 at 11 Kbps.", "At higher bitrates, our model produces competitive visual results but worse VQA scores. Much of the reason for this is that the representations created by the GAN, while visually accurate, are not guided to be pixel-wise accurate, particularly on image textures. Our approach is re-establishing the importance of edges in modern DNN-based video compression architectures.", "This is likely because the GAN-based compressor produces representations that appear very similar to the original, but differ numerically, e.g., over textured regions composed of similar, but differently arranged elements. This suggests that novel quality assessment metrics, designed for video based DNNs, are a worthy research area for further investigation. Our video compression experiments are focused on very low bitrate ranges (3-30 Kbps) and encoding resolutions (256 \\times 256).", "We employed an 8 \\times 8 downsampler and the Q_8 soft edge detector. As a method of objective quantitative analysis, the scores produced by several leading perceptual video quality metrics (PSNR, mean SSIM, mean MS-SSIM, and VMAF scores) were plotted for the reconstructed videos. The red curves denotes the results of our model, while the blue curves are those for H.264."], "referred_text_chunks": ["To simplify experiments, the downsampler only scaled the frames by factors for 4 \\times 4 and 8 \\times 8. Quality of reconstructed downsampled frame To study the relative qualities of reconstructed frames using different levels of {downsampling, we employed three scaling sizes, (1,1), (4,4) and (8,8). Hence, an original frame of size 256\\times256, became 256\\times256, 64\\times64, and 32\\times32, respectively (Figure~)."]}, "1": {"question": "How does the proposed method compare to H.264 in terms of MS-SSIM score at low bitrates?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 1, 'Rationale': 'Image 4 directly compares MS-SSIM scores of the proposed method and H.264 at various bitrates, showing the performance clearly at low bitrates. Text 1 is relevant as it highlights the performance of the model with higher MS-SSIM scores than H.264 below 10 Kbps.', 'Answer': 'The proposed method achieved significantly higher MS-SSIM scores than H.264 at low bitrates (below 10 Kbps).'}", "all_figures_names": ["1811.10673v1-Figure10-1.png", "1811.10673v1-Figure7-1.png", "1811.10673v1-Table1-1.png", "1811.10673v1-Figure6-1.png", "1811.10673v1-Figure8-1.png", "1811.10673v1-Figure9-1.png", "1811.10673v1-Figure2-1.png", "1811.10673v1-Table2-1.png"], "referred_figures_names": ["1811.10673v1-Figure8-1.png"], "answer": "The proposed method achieves significantly higher MS-SSIM scores than H.264 at bitrates below 10 Kbps.", "figure_type": "** plot", "content_type": "figure", "text_chunks": ["Our codec (4th row) delivered significantly better visual quality at low bitrates than did H.264. As a method of objective quantitative analysis, VQA metrics were plotted on the right side of Figure~. For example, our model achieved MS-SSIM of 0.9188 at 8.33 Kbps, while H.264 resulted in MS-SSIM score of 0.8884 at 13 Kbps. Since most of the videos in the KTH dataset contain simple, repeated object movements, we further validated our model on a variety of videos from the YouTube dataset~, contain more diverse activities and impredictable movements.", "The red curves correspond to our model while the blue curves represent the performance of H.264. Notably, in the below 10 Kbps, our scheme achieved significantly higher MS-SSIM scores than did H.264. Similar results were observed on PSNR, SSIM and VMAF (please see Supplementary material).", "However, the VQA scores were often lower for our model than for H.264. For example, Figure (b) plots the temporal evolution of objective video quality scores on seven consecutive frames. Remarkably, although the visual appearance of the video compressed by our model was much better than the H.264 result, the PSNR and SSIM scores at 6.51 Kbps (red lines) yielded a reverse relationship against H.264 at 7.00 and 11.00 Kbps (black and blue lines respectively).", "Our model was able to achieve much higher video quality scores below 10 Kbps than did H.264. Interestingly, our codec was able to successfully compress the video at less than 7.5 Kbps, while H.264 failed (no output). To examine the performance of the proposed method, we implemented an experiment on videos from four semantic categories in the KTH ~. Figure~ plots RD curves of bitrate against VQA scores for 100 videos.", "We employed an 8 \\times 8 downsampler and the Q_8 soft edge detector. As a method of objective quantitative analysis, the scores produced by several leading perceptual video quality metrics (PSNR, mean SSIM, mean MS-SSIM, and VMAF scores) were plotted for the reconstructed videos. The red curves denotes the results of our model, while the blue curves are those for H.264.", "In our experiments, our scheme achieved fair performance using only a very small number of key frames, e.g., \\alpha=1\\%. In the experiments in the coming sections, we set \\alpha \\leq 1.5\\%. Comparison with H.264 One of our experimental results is summarized in Figure~; seven consecutive frames from an original video and three compressed videos: (1) H.264 at 7.62 Kbps, (2) H.264 at 10.36 Kbps, and (3) our model at 7.14 Kbps, are depicted.", "\\caption{Two videos from (a) the KTH and (b) the YouTube dataset. Selected frames from original video and reconstructed videos using H.264 (low bitrate), H.264 (high bitrate), and the proposed model are aligned vertically along time. Our scheme demonstrated significantly better performance than the current standard codecs at low bitrates.", "Our model delivered noticeably better quality at the lowest bitrate than H.264. To train our network using a conditional GAN, 80 key frames (N_I=80) were collected from a single video having 8000 frames (\\alpha=1\\%), and assigned 2.02 Kbps of the overall 7.14 Kbps. Seven test frames were not included in the training set."], "referred_text_chunks": ["Our codec (4th row) delivered significantly better visual quality at low bitrates than did H.264. As a method of objective quantitative analysis, VQA metrics were plotted on the right side of Figure~. For example, our model achieved MS-SSIM of 0.9188 at 8.33 Kbps, while H.264 resulted in MS-SSIM score of 0.8884 at 13 Kbps. Since most of the videos in the KTH dataset contain simple, repeated object movements, we further validated our model on a variety of videos from the YouTube dataset~, contain more diverse activities and impredictable movements."]}, "2": {"question": "What is the purpose of the second-stage decoder $D_2$?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 6, 'Rationale': \"Image 5 outlines the framework for video compression, showing the workflow of $D_2$, including its interaction with $E_2$ and the use of soft edge maps. Text 6 explains the purpose of $D_2$, highlighting its role in reconstructing frames using soft edges and the adversarial training approach, making it directly relevant to understanding $D_2$'s role in the pipeline.\", 'Answer': 'The purpose of the second-stage decoder $D_2$ is to take the soft edges $x_G$ as input and produce reconstructed frames, thereby enhancing video quality by using a generative model trained with key frames and soft edge maps.'}", "all_figures_names": ["1811.10673v1-Figure9-1.png", "1811.10673v1-Figure3-1.png", "1811.10673v1-Table1-1.png", "1811.10673v1-Figure4-1.png", "1811.10673v1-Figure7-1.png", "1811.10673v1-Figure2-1.png", "1811.10673v1-Figure5-1.png", "1811.10673v1-Figure8-1.png"], "referred_figures_names": ["1811.10673v1-Figure2-1.png"], "answer": "The second-stage decoder $D_2$ takes soft edges $x_G$ as input and produces reconstructed frames.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Implementation ~Architecture of {\\boldmath{D_2: The second stage of the decoder D_2 is trained by a discriminator D_D. In our experiments, the original frames were re-sized and cropped to size 2^8\\times2^8 over eight consequent layers. A stride size of 2\\times2 was used when training the DNNs.", "Once D_2 is trained, it can be guided by the soft edge maps x_G to reconstruct the G-frames X_G (non-key frames), which the decoder has never seen before. When the soft edge map contains more information, it can guide the decoder to reconstruct frames with better quality. Second stage encoder, E_2 The second encoding stage is composed of a downsampler and a soft edge detector which feeds a lossless compressor (see Figure ).", "As shown in Figure ), we achieve a 4.95\\times compression gain over JBIG2, across all 567 frames for a given test video. Second stage decoder, D_2 We utilize the conditional GAN framework of Isola \\etal~ to train the decompressor D_2 and discriminator D_D by parsing the compressed data x_i' into both original data X_I' and decompressed data \\hat{X_I in an adversarial manner. Note that only key frames are used to train D_2 and D_D (Figure~).", "Here, we implement E_1/D_1 using a conventional H.264 encoder (decoder), which is efficient and performs very well for key frame compression. If every frame is selected as a key frame, then our scheme degenerates to a frame-based H.264 encoder, without any motion prediction/compensation. The second encoding stage is significantly different and involves a series of steps: down-sampling, soft edge detection and a novel spatio-temporal edge map compression scheme (see Figure ).", "If n(X_I)<N, then X_I' and X_G further follow a second stage of encoding; the encoder E_2 encodes X_I' and (X_G) to generate x_I' and x_G. Then, the second stage decoder (D2), decodes x_I' and x_G, yielding the reconstructions \\hat{X_I and (\\hat{X_G). Notably, we can use any conventional codec to implement E_1/ D_1.", "Our model can be implemented using any options for resizing and cropping. The batch size and the number of epochs were fixed at 1 and 1000, respectively. ~Architecture of {\\boldmath{E_2: The second stage encoder E_2 has a predetermined structure composed of a downsampler, the soft edge detector and the lossless compressor (Figure ).", "The corresponding decoder D_2 is trained using a GAN and a discriminator D_D. Naturally, the encoder E_2 cannot learn evolving representations of non-key frames using information from key frames only. Hence, we employ a conditional GAN to train D_2 using pairs of key frames and their corresponding soft edge maps. During training, D_2 learns associations between the key frames and the soft edge maps.", "This generative model is trained at the receiver using x'_I and X'_I using a discriminator D_D. After training, D_2 takes soft edges x_G as input and produces reconstructed frames (see also Figure ). Only x_I and x_G are required to reconstruct the decompressed video. \\end{figure*"], "referred_text_chunks": ["Implementation ~Architecture of {\\boldmath{D_2: The second stage of the decoder D_2 is trained by a discriminator D_D. In our experiments, the original frames were re-sized and cropped to size 2^8\\times2^8 over eight consequent layers. A stride size of 2\\times2 was used when training the DNNs."]}, "3": {"question": "What are the steps involved in the second encoding stage ($E_2$)?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 3, 'Rationale': 'Image 5 provides a clear visual overview of the steps involved in the second encoding stage E2, including downsampling, soft edge detection, and lossless compression. Text 3 gives a detailed explanation of these processes, emphasizing the differences from the first stage.', 'Answer': 'The second encoding stage E2 involves down-sampling, soft edge detection, and spatio-temporal lossless compression.'}", "all_figures_names": ["1811.10673v1-Figure8-1.png", "1811.10673v1-Figure6-1.png", "1811.10673v1-Table1-1.png", "1811.10673v1-Table2-1.png", "1811.10673v1-Figure4-1.png", "1811.10673v1-Figure3-1.png", "1811.10673v1-Figure2-1.png", "1811.10673v1-Figure7-1.png"], "referred_figures_names": ["1811.10673v1-Figure3-1.png"], "answer": "The second encoding stage involves three steps: down-sampling, soft edge detection, and spatio-temporal edge map compression.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["Once D_2 is trained, it can be guided by the soft edge maps x_G to reconstruct the G-frames X_G (non-key frames), which the decoder has never seen before. When the soft edge map contains more information, it can guide the decoder to reconstruct frames with better quality. Second stage encoder, E_2 The second encoding stage is composed of a downsampler and a soft edge detector which feeds a lossless compressor (see Figure ).", "Our model can be implemented using any options for resizing and cropping. The batch size and the number of epochs were fixed at 1 and 1000, respectively. ~Architecture of {\\boldmath{E_2: The second stage encoder E_2 has a predetermined structure composed of a downsampler, the soft edge detector and the lossless compressor (Figure ).", "If n(X_I)<N, then X_I' and X_G further follow a second stage of encoding; the encoder E_2 encodes X_I' and (X_G) to generate x_I' and x_G. Then, the second stage decoder (D2), decodes x_I' and x_G, yielding the reconstructions \\hat{X_I and (\\hat{X_G). Notably, we can use any conventional codec to implement E_1/ D_1.", "Here, we implement E_1/D_1 using a conventional H.264 encoder (decoder), which is efficient and performs very well for key frame compression. If every frame is selected as a key frame, then our scheme degenerates to a frame-based H.264 encoder, without any motion prediction/compensation. The second encoding stage is significantly different and involves a series of steps: down-sampling, soft edge detection and a novel spatio-temporal edge map compression scheme (see Figure ).", "Implementation ~Architecture of {\\boldmath{D_2: The second stage of the decoder D_2 is trained by a discriminator D_D. In our experiments, the original frames were re-sized and cropped to size 2^8\\times2^8 over eight consequent layers. A stride size of 2\\times2 was used when training the DNNs.", "In addition, at the second stage of encoding we fixed soft edge detector with quantization level 2 (Q_2) without lossless compression, to isolate the effects of downsampling. As the scaling size was increased, the reconstructed frames became more recognizable (Figure~). Accordingly, the measured VQA scores also increased monotonously, as the size of x_G was increased (Table~).", "As shown in Figure ), we achieve a 4.95\\times compression gain over JBIG2, across all 567 frames for a given test video. Second stage decoder, D_2 We utilize the conditional GAN framework of Isola \\etal~ to train the decompressor D_2 and discriminator D_D by parsing the compressed data x_i' into both original data X_I' and decompressed data \\hat{X_I in an adversarial manner. Note that only key frames are used to train D_2 and D_D (Figure~).", "The corresponding decoder D_2 is trained using a GAN and a discriminator D_D. Naturally, the encoder E_2 cannot learn evolving representations of non-key frames using information from key frames only. Hence, we employ a conditional GAN to train D_2 using pairs of key frames and their corresponding soft edge maps. During training, D_2 learns associations between the key frames and the soft edge maps."], "referred_text_chunks": ["Once D_2 is trained, it can be guided by the soft edge maps x_G to reconstruct the G-frames X_G (non-key frames), which the decoder has never seen before. When the soft edge map contains more information, it can guide the decoder to reconstruct frames with better quality. Second stage encoder, E_2 The second encoding stage is composed of a downsampler and a soft edge detector which feeds a lossless compressor (see Figure )."]}, "4": {"question": "Which lossless compression scheme achieved the highest compression gain in the example shown in Figure 1?", "referred_figures_indices": [0], "response": "{'Image': 0, 'Text': 3, 'Rationale': 'Image 0 shows a comparison of compression efficiency (bits per pixel) for different schemes, with the proposed method achieving the highest efficiency. Text 3 mentions the compression gain achieved by the proposed method over JBIG2, aligned with the result demonstrated in Image 0.', 'Answer': 'The proposed lossless compression scheme achieved the highest compression gain in the example shown in Figure 1.'}", "all_figures_names": ["1811.10673v1-Figure5-1.png", "1811.10673v1-Figure8-1.png", "1811.10673v1-Figure7-1.png", "1811.10673v1-Figure3-1.png", "1811.10673v1-Table2-1.png", "1811.10673v1-Figure10-1.png", "1811.10673v1-Table1-1.png", "1811.10673v1-Figure6-1.png"], "referred_figures_names": ["1811.10673v1-Figure5-1.png"], "answer": "The proposed scheme achieved the highest compression gain.", "figure_type": "photograph(s) and table", "content_type": "figure", "text_chunks": ["Therefore, we devised the following lossless compression scheme. We first applied run-length encoding~ on the soft edge maps along both the spatial and the temporal directions. Then, we performed another lossless compression step using Huffmann encoding~. Thanks to the sparse nature of the soft edge maps, our scheme produces great compression results compared to JBIG2~, a state-of-the-art approach for bi-level image compression.", "Comparison to conventional schemes We compared the performance of our GAN-based compresssor with the currently prevailing codec H.264 (refer to the supplementary material for a comparison with H.265). We do not compare our scheme with recent video compression models based on DNNs~, since (1) our model is only trained on a subset of the frames from the single target video without any pre-training process and (2) our model does not rely on any interpolation. Our innovations are complementary to the other DNN-based compression schemes and can be fruitfully combined in future work.", "The maps Q_{256 and Q_8 in Figure (a) appear quite similar, yet the entropy of Q_{8 is much reduced relative to that of Q_{256 (2.0904 vs 7.0358). \\subsubsection{Lossless compression Since most of the outputs Q of the soft edge detector are zero, we found it useful to perform lossless compression on the generated soft edge maps. However, and to the best of our knowledge, there have been no lossless compression schemes developed for soft edge maps.", "As shown in Figure ), we achieve a 4.95\\times compression gain over JBIG2, across all 567 frames for a given test video. Second stage decoder, D_2 We utilize the conditional GAN framework of Isola \\etal~ to train the decompressor D_2 and discriminator D_D by parsing the compressed data x_i' into both original data X_I' and decompressed data \\hat{X_I in an adversarial manner. Note that only key frames are used to train D_2 and D_D (Figure~).", "We also repeated this experiment with k=4 and k=8. In this toy example, our network was trained on only one image, assuming no compression for X_I (X_I'=X_I). In addition, at the second stage of encoding, no downsampler or lossless compression was used.", "While these parameters are lower than traditionally used bitrate ladders, our goal in this paper was to demonstrate a proof of concept for our GAN-based model. Conclusions We proposed a video compression framework that is based on conditional GANs guided by soft edge detection. We showed that our scheme achieved better visual results and higher objective VQA scores than current standard video codecs at low bitrates.", "Our model was able to achieve much higher video quality scores below 10 Kbps than did H.264. Interestingly, our codec was able to successfully compress the video at less than 7.5 Kbps, while H.264 failed (no output). To examine the performance of the proposed method, we implemented an experiment on videos from four semantic categories in the KTH ~. Figure~ plots RD curves of bitrate against VQA scores for 100 videos.", "Similar ideas have been applied to video compression, e.g., by casting the motion estimation task as an interpolation solved by training on a large volume of videos~. These approaches have achieved performance approaching that of prevailing standardized codecs such as H.264 and HEVC~. Here, we propose a novel video compression framework that uses conditional Generative Adversarial Networks (GANs). Our proposed model automatically generates low-level semantic label maps using a newly conceived soft edge detector combined with a down-sampler (encoder)."], "referred_text_chunks": ["Therefore, we devised the following lossless compression scheme. We first applied run-length encoding~ on the soft edge maps along both the spatial and the temporal directions. Then, we performed another lossless compression step using Huffmann encoding~. Thanks to the sparse nature of the soft edge maps, our scheme produces great compression results compared to JBIG2~, a state-of-the-art approach for bi-level image compression."]}, "5": {"question": "How does the proposed model compare to H.264 in terms of visual quality at low bitrates?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 4, 'Rationale': 'Image 3 shows reconstructed video frames comparing H.264 and the proposed model at low bitrates, clearly demonstrating superior visual quality for the proposed model. Text 4 highlights scenarios where the proposed model delivers better subjective quality than H.264 at similar or lower bitrates.', 'Answer': 'The proposed model provides better visual quality at low bitrates compared to H.264.'}", "all_figures_names": ["1811.10673v1-Figure8-1.png", "1811.10673v1-Figure3-1.png", "1811.10673v1-Figure4-1.png", "1811.10673v1-Figure9-1.png", "1811.10673v1-Figure10-1.png", "1811.10673v1-Table1-1.png", "1811.10673v1-Table2-1.png", "1811.10673v1-Figure7-1.png"], "referred_figures_names": ["1811.10673v1-Figure9-1.png"], "answer": "The proposed model delivers significantly better visual quality at low bitrates than H.264.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Our model was able to achieve much higher video quality scores below 10 Kbps than did H.264. Interestingly, our codec was able to successfully compress the video at less than 7.5 Kbps, while H.264 failed (no output). To examine the performance of the proposed method, we implemented an experiment on videos from four semantic categories in the KTH ~. Figure~ plots RD curves of bitrate against VQA scores for 100 videos.", "Our codec (4th row) delivered significantly better visual quality at low bitrates than did H.264. As a method of objective quantitative analysis, VQA metrics were plotted on the right side of Figure~. For example, our model achieved MS-SSIM of 0.9188 at 8.33 Kbps, while H.264 resulted in MS-SSIM score of 0.8884 at 13 Kbps. Since most of the videos in the KTH dataset contain simple, repeated object movements, we further validated our model on a variety of videos from the YouTube dataset~, contain more diverse activities and impredictable movements.", "\\caption{Two videos from (a) the KTH and (b) the YouTube dataset. Selected frames from original video and reconstructed videos using H.264 (low bitrate), H.264 (high bitrate), and the proposed model are aligned vertically along time. Our scheme demonstrated significantly better performance than the current standard codecs at low bitrates.", "Our model delivered noticeably better quality at the lowest bitrate than H.264. To train our network using a conditional GAN, 80 key frames (N_I=80) were collected from a single video having 8000 frames (\\alpha=1\\%), and assigned 2.02 Kbps of the overall 7.14 Kbps. Seven test frames were not included in the training set.", "It is quite possible that in this context, conventional VQA models may not accurately predict the subjective quality of GAN-compressed videos. As shown in Figure , our scheme apparently reconstructs frames of better quality than H.264. For example, H.264 compressed at 7.00 Kbps resulted in unrecognizable reconstructed frames (many blurred blocks), while our model at 6.51 Kbps yielded more detailed frames than even H.264 at 11 Kbps.", "However, the VQA scores were often lower for our model than for H.264. For example, Figure (b) plots the temporal evolution of objective video quality scores on seven consecutive frames. Remarkably, although the visual appearance of the video compressed by our model was much better than the H.264 result, the PSNR and SSIM scores at 6.51 Kbps (red lines) yielded a reverse relationship against H.264 at 7.00 and 11.00 Kbps (black and blue lines respectively).", "In our experiments, our scheme achieved fair performance using only a very small number of key frames, e.g., \\alpha=1\\%. In the experiments in the coming sections, we set \\alpha \\leq 1.5\\%. Comparison with H.264 One of our experimental results is summarized in Figure~; seven consecutive frames from an original video and three compressed videos: (1) H.264 at 7.62 Kbps, (2) H.264 at 10.36 Kbps, and (3) our model at 7.14 Kbps, are depicted.", "Figure~(b) visually compares compression performance on one YouTube video. Our proposed codec outperformed H.264 regards to perceptual quality versus bitrate. Similar results were observed on the other 30 videos from the YouTube pose dataset (see Supplementary material)."], "referred_text_chunks": ["Our model was able to achieve much higher video quality scores below 10 Kbps than did H.264. Interestingly, our codec was able to successfully compress the video at less than 7.5 Kbps, while H.264 failed (no output). To examine the performance of the proposed method, we implemented an experiment on videos from four semantic categories in the KTH ~. Figure~ plots RD curves of bitrate against VQA scores for 100 videos."]}, "6": {"question": "How does the level of quantization affect the output of the soft edge detector?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 0, 'Rationale': 'Image 5 shows the outputs of the soft edge detector at different quantization levels, directly illustrating how quantization affects the visible edges and intensity distributions. Text 0 discusses the quality of reconstructed frames and the impact of quantization, relevant to the effects on output quality.', 'Answer': 'Higher quantization levels reduce detail in the output of the soft edge detector, decreasing visible entropy and clustering pixels, resulting in fewer distinguishable edges.'}", "all_figures_names": ["1811.10673v1-Figure8-1.png", "1811.10673v1-Table2-1.png", "1811.10673v1-Figure2-1.png", "1811.10673v1-Figure6-1.png", "1811.10673v1-Table1-1.png", "1811.10673v1-Figure4-1.png", "1811.10673v1-Figure5-1.png", "1811.10673v1-Figure9-1.png"], "referred_figures_names": ["1811.10673v1-Figure4-1.png"], "answer": "As the quantization level $k$ is decreased, the cardinality of colors co-located with edges decreases.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Quality of reconstructed quantized frame We also examined the relationship between the quality of reconstructed frames and the level of quantization delivered by the soft edge detector (see Section~). As a toy example, assume E_1=E_2=I, i.e, X_I=X_I'. E_2 taken to the soft edge detector with quantization level k (Q_k) without any downsampling (Figure~).", "In addition, at the second stage of encoding we fixed soft edge detector with quantization level 2 (Q_2) without lossless compression, to isolate the effects of downsampling. As the scaling size was increased, the reconstructed frames became more recognizable (Figure~). Accordingly, the measured VQA scores also increased monotonously, as the size of x_G was increased (Table~).", "\\subsubsection{Soft edge detector Following downsampling, we perform a soft edge detection step, which further produces a compressible representation of S. First, we apply the Canny edge detector~ to find an edge pixel map I = [I_{i,j], where I_{i,j= Following edge detection, we perform vector quantization to form clusters of colored pixels mapped to edge pixels, i.e., \\noindent where s_{(i,j) and q_{(i,j) are the (i,j)-th elements of the downsampled frame S and a subsequent quantized frame Q. \\mathbb{V_k (\\cdot) is a vector quantizer which uses the k-nearest mean~, to form k-1 clusters of colored pixels mapped to I_{i,j=1. The large cluster of pixels I_{i,j=0 is not included.", "Let S and Q_k denote the outputs of the downsampler and the soft edge detector, and x_I' and x_G be the outputs of the lossless compressor respectively. The downsampling step reduces the spatial resolution of each frame, but does not affect the number of frames. The purpose of downsampling is to produce a more compact representation that is easier to compress.", "The aforementioned soft edge detection approach is illustrated in Figure . In Figure (a), we show a 64\\times 64 frame downsampled from a reconstructed 256\\times 256 frame {X'_I^{(1) from one video of the {YouTube Pose dataset~. The next four frames in Figure (a) are outputs from the soft edge detector for several different levels of quantization.", "The maps Q_{256 and Q_8 in Figure (a) appear quite similar, yet the entropy of Q_{8 is much reduced relative to that of Q_{256 (2.0904 vs 7.0358). \\subsubsection{Lossless compression Since most of the outputs Q of the soft edge detector are zero, we found it useful to perform lossless compression on the generated soft edge maps. However, and to the best of our knowledge, there have been no lossless compression schemes developed for soft edge maps.", "While these parameters are lower than traditionally used bitrate ladders, our goal in this paper was to demonstrate a proof of concept for our GAN-based model. Conclusions We proposed a video compression framework that is based on conditional GANs guided by soft edge detection. We showed that our scheme achieved better visual results and higher objective VQA scores than current standard video codecs at low bitrates.", "While this approach has been effective, it is likely inappropriate for live video streaming, since long term frame predictions can produce severe and systemic artifacts (such as missing or blurred objects). Furthermore, the predictive accuracy of theses approaches can be degraded severely in the presence of large or sudden movements of objects. Our approach seeks to combat theses problems using soft edge-guided conditional GANs, as explained in Section~. Soft edge detection The detection of substantive changes in luminance, or edges, once a cornerstone of computer vision theory, is also regarded a plausible front-end feature extraction process in biological vision systems~."], "referred_text_chunks": ["Quality of reconstructed quantized frame We also examined the relationship between the quality of reconstructed frames and the level of quantization delivered by the soft edge detector (see Section~). As a toy example, assume E_1=E_2=I, i.e, X_I=X_I'. E_2 taken to the soft edge detector with quantization level k (Q_k) without any downsampling (Figure~)."]}, "7": {"question": "How does the quality of the reconstructed frames change as the resolution increases?", "referred_figures_indices": [4], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 and Text 0 both focus on how resolution impacts the recognizability and quality of reconstructed frames. Image 3 visually shows increasing resolution leading to more recognizable frames, while Text 0 discusses changes in frame size and quality with scaling.', 'Answer': 'As the resolution increases, the quality and recognizability of reconstructed frames improve.'}", "all_figures_names": ["1811.10673v1-Figure7-1.png", "1811.10673v1-Figure10-1.png", "1811.10673v1-Figure4-1.png", "1811.10673v1-Figure6-1.png", "1811.10673v1-Table1-1.png", "1811.10673v1-Figure3-1.png", "1811.10673v1-Figure5-1.png", "1811.10673v1-Figure2-1.png"], "referred_figures_names": ["1811.10673v1-Table1-1.png"], "answer": "The quality of the reconstructed frames increases monotonically as the resolution increases.", "figure_type": "table", "content_type": "table", "text_chunks": ["To simplify experiments, the downsampler only scaled the frames by factors for 4 \\times 4 and 8 \\times 8. Quality of reconstructed downsampled frame To study the relative qualities of reconstructed frames using different levels of {downsampling, we employed three scaling sizes, (1,1), (4,4) and (8,8). Hence, an original frame of size 256\\times256, became 256\\times256, 64\\times64, and 32\\times32, respectively (Figure~).", "Quality of reconstructed quantized frame We also examined the relationship between the quality of reconstructed frames and the level of quantization delivered by the soft edge detector (see Section~). As a toy example, assume E_1=E_2=I, i.e, X_I=X_I'. E_2 taken to the soft edge detector with quantization level k (Q_k) without any downsampling (Figure~).", "As the quantization level k was increased, the quality of the reconstructed frames improved qualitatively (\\hat{X_G in Figure ). This improvement was also measured by quantitative VQA scores, which also indicated a trend of improvement with higher value of k (Table ). Performance against key frame density In our framework, performance and bitrate are closely associated with \\alpha, which we define as the ratio of the number of key frames to the total number of frames in one video.", "It is quite possible that in this context, conventional VQA models may not accurately predict the subjective quality of GAN-compressed videos. As shown in Figure , our scheme apparently reconstructs frames of better quality than H.264. For example, H.264 compressed at 7.00 Kbps resulted in unrecognizable reconstructed frames (many blurred blocks), while our model at 6.51 Kbps yielded more detailed frames than even H.264 at 11 Kbps.", "We have found that the use of soft edges, rather than strictly defined binary hard constraints on the reconstruction, produces much better results (see Section~). One important implication of our work is on re-establishing the importance of edges, as inspired by biological vision, but for informing a modern deep video compression architecture. Preliminaries Let X \\in \\mathbb{R^{W \\times H \\times 3 \\times N denote the set of whole frames in a video, having a spatial resolution of W \\times H pixels, three ({RGB) channels and temporal duration of N frames.", "At higher bitrates, our model produces competitive visual results but worse VQA scores. Much of the reason for this is that the representations created by the GAN, while visually accurate, are not guided to be pixel-wise accurate, particularly on image textures. Our approach is re-establishing the importance of edges in modern DNN-based video compression architectures.", "In addition, at the second stage of encoding we fixed soft edge detector with quantization level 2 (Q_2) without lossless compression, to isolate the effects of downsampling. As the scaling size was increased, the reconstructed frames became more recognizable (Figure~). Accordingly, the measured VQA scores also increased monotonously, as the size of x_G was increased (Table~).", "Let S and Q_k denote the outputs of the downsampler and the soft edge detector, and x_I' and x_G be the outputs of the lossless compressor respectively. The downsampling step reduces the spatial resolution of each frame, but does not affect the number of frames. The purpose of downsampling is to produce a more compact representation that is easier to compress."], "referred_text_chunks": ["To simplify experiments, the downsampler only scaled the frames by factors for 4 \\times 4 and 8 \\times 8. Quality of reconstructed downsampled frame To study the relative qualities of reconstructed frames using different levels of {downsampling, we employed three scaling sizes, (1,1), (4,4) and (8,8). Hence, an original frame of size 256\\times256, became 256\\times256, 64\\times64, and 32\\times32, respectively (Figure~)."]}, "8": {"question": "How does the quality of the reconstructed frames change as the quantization level of the soft edge detector increases?", "referred_figures_indices": [5], "response": "{'Image': 1, 'Text': 5, 'Rationale': 'Image 1 shows a table of video quality metrics improving with increased quantization levels, directly relating to the quality change asked in the question. Text 5 explains how higher quantization levels improve reconstructed frame quality, supported by numerical VQA scores.', 'Answer': 'As the quantization level of the soft edge detector increases, the quality of the reconstructed frames improves.'}", "all_figures_names": ["1811.10673v1-Figure5-1.png", "1811.10673v1-Table2-1.png", "1811.10673v1-Figure2-1.png", "1811.10673v1-Figure9-1.png", "1811.10673v1-Figure6-1.png", "1811.10673v1-Figure7-1.png", "1811.10673v1-Figure4-1.png", "1811.10673v1-Figure3-1.png"], "referred_figures_names": ["1811.10673v1-Figure7-1.png"], "answer": "The quality of the reconstructed frames increases as the quantization level of the soft edge detector increases.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Quality of reconstructed quantized frame We also examined the relationship between the quality of reconstructed frames and the level of quantization delivered by the soft edge detector (see Section~). As a toy example, assume E_1=E_2=I, i.e, X_I=X_I'. E_2 taken to the soft edge detector with quantization level k (Q_k) without any downsampling (Figure~).", "In addition, at the second stage of encoding we fixed soft edge detector with quantization level 2 (Q_2) without lossless compression, to isolate the effects of downsampling. As the scaling size was increased, the reconstructed frames became more recognizable (Figure~). Accordingly, the measured VQA scores also increased monotonously, as the size of x_G was increased (Table~).", "The aforementioned soft edge detection approach is illustrated in Figure . In Figure (a), we show a 64\\times 64 frame downsampled from a reconstructed 256\\times 256 frame {X'_I^{(1) from one video of the {YouTube Pose dataset~. The next four frames in Figure (a) are outputs from the soft edge detector for several different levels of quantization.", "Let S and Q_k denote the outputs of the downsampler and the soft edge detector, and x_I' and x_G be the outputs of the lossless compressor respectively. The downsampling step reduces the spatial resolution of each frame, but does not affect the number of frames. The purpose of downsampling is to produce a more compact representation that is easier to compress.", "\\subsubsection{Soft edge detector Following downsampling, we perform a soft edge detection step, which further produces a compressible representation of S. First, we apply the Canny edge detector~ to find an edge pixel map I = [I_{i,j], where I_{i,j= Following edge detection, we perform vector quantization to form clusters of colored pixels mapped to edge pixels, i.e., \\noindent where s_{(i,j) and q_{(i,j) are the (i,j)-th elements of the downsampled frame S and a subsequent quantized frame Q. \\mathbb{V_k (\\cdot) is a vector quantizer which uses the k-nearest mean~, to form k-1 clusters of colored pixels mapped to I_{i,j=1. The large cluster of pixels I_{i,j=0 is not included.", "As the quantization level k was increased, the quality of the reconstructed frames improved qualitatively (\\hat{X_G in Figure ). This improvement was also measured by quantitative VQA scores, which also indicated a trend of improvement with higher value of k (Table ). Performance against key frame density In our framework, performance and bitrate are closely associated with \\alpha, which we define as the ratio of the number of key frames to the total number of frames in one video.", "We have found that the use of soft edges, rather than strictly defined binary hard constraints on the reconstruction, produces much better results (see Section~). One important implication of our work is on re-establishing the importance of edges, as inspired by biological vision, but for informing a modern deep video compression architecture. Preliminaries Let X \\in \\mathbb{R^{W \\times H \\times 3 \\times N denote the set of whole frames in a video, having a spatial resolution of W \\times H pixels, three ({RGB) channels and temporal duration of N frames.", "Once D_2 is trained, it can be guided by the soft edge maps x_G to reconstruct the G-frames X_G (non-key frames), which the decoder has never seen before. When the soft edge map contains more information, it can guide the decoder to reconstruct frames with better quality. Second stage encoder, E_2 The second encoding stage is composed of a downsampler and a soft edge detector which feeds a lossless compressor (see Figure )."], "referred_text_chunks": ["Quality of reconstructed quantized frame We also examined the relationship between the quality of reconstructed frames and the level of quantization delivered by the soft edge detector (see Section~). As a toy example, assume E_1=E_2=I, i.e, X_I=X_I'. E_2 taken to the soft edge detector with quantization level k (Q_k) without any downsampling (Figure~)."]}, "9": {"question": "Which quality factor improves the most as k is increased?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 0, 'Rationale': 'Image 3 and Text 0 both focus on the improvement of reconstructed frame quality as quantization level k increases, indicating that video quality assessment scores improve.', 'Answer': 'The quality of reconstructed frames improves the most as k is increased.'}", "all_figures_names": ["1811.10673v1-Figure7-1.png", "1811.10673v1-Figure10-1.png", "1811.10673v1-Figure4-1.png", "1811.10673v1-Table2-1.png", "1811.10673v1-Figure3-1.png", "1811.10673v1-Table1-1.png", "1811.10673v1-Figure2-1.png", "1811.10673v1-Figure9-1.png"], "referred_figures_names": ["1811.10673v1-Table2-1.png"], "answer": "MS-SSIM", "figure_type": "table", "content_type": "table", "text_chunks": ["As the quantization level k was increased, the quality of the reconstructed frames improved qualitatively (\\hat{X_G in Figure ). This improvement was also measured by quantitative VQA scores, which also indicated a trend of improvement with higher value of k (Table ). Performance against key frame density In our framework, performance and bitrate are closely associated with \\alpha, which we define as the ratio of the number of key frames to the total number of frames in one video.", "As the quantization level k is decreased, the cardinality of colors co-located with edges decreases. Figure (b) plots the histograms of the gray-scale brightness in each Q_k. Each video will have a different set of centroids for a given k. Figure (c) depicts three-dimensional scatter-plots (R/G/B axis) of S with different colors assigned to each of its clusters Q_k.", "Quality of reconstructed quantized frame We also examined the relationship between the quality of reconstructed frames and the level of quantization delivered by the soft edge detector (see Section~). As a toy example, assume E_1=E_2=I, i.e, X_I=X_I'. E_2 taken to the soft edge detector with quantization level k (Q_k) without any downsampling (Figure~).", "Our codec (4th row) delivered significantly better visual quality at low bitrates than did H.264. As a method of objective quantitative analysis, VQA metrics were plotted on the right side of Figure~. For example, our model achieved MS-SSIM of 0.9188 at 8.33 Kbps, while H.264 resulted in MS-SSIM score of 0.8884 at 13 Kbps. Since most of the videos in the KTH dataset contain simple, repeated object movements, we further validated our model on a variety of videos from the YouTube dataset~, contain more diverse activities and impredictable movements.", "The scores produced by several leading perceptual video quality metrics were depicted on the right side. Please refer to the supplementary for reconstructed videos and results on additional 129 videos. \\end{figure* Limitations \\caption{Example of lower VQA scores on videos compressed by our model than those compressed using H.264, despite the apparent better subjective quality produced by our model.", "Our model was able to achieve much higher video quality scores below 10 Kbps than did H.264. Interestingly, our codec was able to successfully compress the video at less than 7.5 Kbps, while H.264 failed (no output). To examine the performance of the proposed method, we implemented an experiment on videos from four semantic categories in the KTH ~. Figure~ plots RD curves of bitrate against VQA scores for 100 videos.", "However, the VQA scores were often lower for our model than for H.264. For example, Figure (b) plots the temporal evolution of objective video quality scores on seven consecutive frames. Remarkably, although the visual appearance of the video compressed by our model was much better than the H.264 result, the PSNR and SSIM scores at 6.51 Kbps (red lines) yielded a reverse relationship against H.264 at 7.00 and 11.00 Kbps (black and blue lines respectively).", "Figure~ shows the performance of our codec, as compared to H.264. Figure~(a) shows selected frames from both an original KTH video and also compressed videos using H.264 at 9 Kbps, H.264 at 13 Kbps, and our compression model. The videos are aligned vertically for visual comparison."], "referred_text_chunks": ["As the quantization level k was increased, the quality of the reconstructed frames improved qualitatively (\\hat{X_G in Figure ). This improvement was also measured by quantitative VQA scores, which also indicated a trend of improvement with higher value of k (Table ). Performance against key frame density In our framework, performance and bitrate are closely associated with \\alpha, which we define as the ratio of the number of key frames to the total number of frames in one video."]}}