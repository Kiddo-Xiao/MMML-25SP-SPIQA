{"0": {"question": "How do the different methods compare in terms of their ability to generate realistic faces?", "referred_figures_indices": [2], "response": "{'Image': 1, 'Text': 4, 'Rationale': 'Image 1 shows the results of a user study directly comparing the realism and synchronization of different methods, making it highly relevant. Text 4 provides quantitative comparisons across state-of-the-art methods, which directly answers the question regarding generating realistic faces.', 'Answer': 'The proposed method (AMIE Ours) achieves the highest realism (56.41%) and synchronization (48.91%) compared to Zhou et al. [2019] and Chen et al. [2019], according to user study results in Image 1.'}", "all_figures_names": ["1812.06589v2-Table3-1.png", "1812.06589v2-Table4-1.png", "1812.06589v2-Figure6-1.png", "1812.06589v2-Figure2-1.png", "1812.06589v2-Figure4-1.png", "1812.06589v2-Figure5-1.png", "1812.06589v2-Figure3-1.png", "1812.06589v2-Figure1-1.png"], "referred_figures_names": ["1812.06589v2-Figure6-1.png"], "answer": "The baseline method generates faces that are blurry and unrealistic, while the other methods generate faces that are more realistic.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Then, the volunteers were asked to answer the following two questions: (1) which one appears more realistic and (2) which one provides more temporal synchronism referring to the ground truth. Table clearly demonstrates that our model achieves the highest rating in both realistic and synchronization. Conclusion We have proposed a novel model of talking face generation for arbitrary identities via exploring the cross-modality coherence in this paper.", "Based on the above discussion, we propose a novel and robust method by exploring the coherence between audio and visual modalities for arbitrary talking face generation in this paper. The proposed model consists of three components: talking face generator, asymmetric mutual information estimator, and frame discriminator, as shown in Fig. . First, the talking face generator is designed to generate target frames from the given input: one audio clip, one still facial image, and the previous generated frame.", "Introduction Talking face generation, which aims to generate a realistic talking video for the given still face image and speech clip, has been an active research topic. It has wide potential applications such as movie animation, teleconferencing, talking agents, and enhancing speech comprehension while preserving privacy. Although recent efforts have achieved impressive talking face synthesis for arbitrary identities, it is still a huge challenge due to the heterogeneous between audio and video, together with the appearance diversity of arbitrary identities.", "We observe that a talking face video is mainly composited by the identity-related feature and the lip-related feature, where the former is more stable while the latter is more temporal dynamic. Therefore, it is essential to separate these two features for arbitrary identity generation. Herein, we propose to leverage the given face and previous generated frame to provide identity-related and lip-related information, respectively.", "Quantitative Results We compare our model with four recent state-of-the-art methods, including Chung et al. \\shortcite{Chung2017YouST, Chen et al. \\shortcite{Chen2018LipMG Zhou et al. \\shortcite{Zhou2018TalkingFG, and Chen et al. \\shortcite{chen2019hierarchical. Table shows the quantitative results of our method and its competitors with higher PSNR, SSIM and lower LMD, suggesting the best quality of the generated video frames of the talking faces.", "Directly leveraging the whole area of the given face tends to generate face images with a slight jitter problem. In order to capture different information among the given faces and disentangle the identity-related and lip-related information, we introduce a dynamic attention block. When generating a video sequence, we assign an initial attention rate to the first frame, then predict the fine-grained attention mask for the following video frames.", "% Frames are aligned into 256 \\times 256 faces and audios are processed into (Mel Frequency Cepstrum Coefficient) MFCC features at the sampling rate of 5000Hz. Then we match each frame with an MFCC audio input with a size of 20 \\times 13. We use common reconstruction metrics such as PSNR and SSIM to evaluate the quality of the synthesized talking faces.", "The attention masks of the following generated frame are predicted by the previous attention mask and the generated faces. In practice, we give a coarse mask area and an attention rate, the following frames generation only predict a fine-grained mask area which is close to the lip shape, while leaving the rate unchanged. Training Details In the training stage, % frame discriminator predicts the probability of whether frame and audio are paired or not, resulting in the following loss of our GAN: \\end{equation In order to synchronize the lip movements more accurately, we employ perceptual loss to capture high-level features differences between generated images and ground-truth."], "referred_text_chunks": ["Then, the volunteers were asked to answer the following two questions: (1) which one appears more realistic and (2) which one provides more temporal synchronism referring to the ground truth. Table clearly demonstrates that our model achieves the highest rating in both realistic and synchronization. Conclusion We have proposed a novel model of talking face generation for arbitrary identities via exploring the cross-modality coherence in this paper."]}, "1": {"question": "What is the effect of adding DA to the baseline method?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 0, 'Rationale': 'Image 4 shows the effects of adding DA to the baseline in terms of PSNR, SSIM, and LMD, which are directly related to the question. Text 0 mentions the significant improvement DA provides over the baseline.', 'Answer': 'Adding DA to the baseline method improves performance in terms of SSIM (from 0.89 to 0.90) while PSNR slightly decreases and LMD increases slightly, indicating an overall positive effect especially in visual similarity.'}", "all_figures_names": ["1812.06589v2-Table2-1.png", "1812.06589v2-Figure5-1.png", "1812.06589v2-Figure4-1.png", "1812.06589v2-Table1-1.png", "1812.06589v2-Table3-1.png", "1812.06589v2-Figure1-1.png", "1812.06589v2-Table4-1.png", "1812.06589v2-Figure6-1.png"], "referred_figures_names": ["1812.06589v2-Table3-1.png"], "answer": "Adding DA to the baseline method improves the PSNR and SSIM values, while slightly decreasing the LMD value.", "figure_type": "table", "content_type": "table", "text_chunks": ["% From Table , it is clear to see that, (1) Simply adopting MI into our baseline did not significantly improve the results, comparing Table (a) to baseline. (2) DA plays an important role in talking face generation, comparing Table (b) to baseline. (3) Introducing either Asy. or JS strategy shows the great improvement on almost metrics, comparing Table (e) and (f) to Table (c).", "We first overview the architecture of our model, followed by the elaboration of two novel components: Asymmetric Mutual Information Estimator (AMIE) and Dynamic Attention (DA) modules. The training details are provided at the end of this section.", "It is clear to see that, Zhou et al. suffer from a \"zoom-in-and-out\" effect, while the lip shapes of Chen et al. appear differences from the real one. In general, our model can generate more realistic and synchronous frames. Ablation Study In order to quantify the effect of each component of our method, we conduct ablation study experiments to verify the contributions of the two key components in our model: Asymmetric Mutual Information Estimator (AMIE) and Dynamic Attention (DA), and the two important strategies in AMIE: the asymmetric training strategy (Asy.) and JS represented estimator.", "Asymmetric Mutual Information Estimator We first introduce the theory of mutual information neural estimation (MINE), followed by the mutual information estimation in talking face generation task which leverages the proposed AMIE to learn the cross-modal coherence. \\subsubsection{Preliminary Theory of 'MINE' Mutual information is a measurement of mutual dependency between two probability distributions, where p(x, y) is the joint probability function of X and Y, and p(x) and p(y) are the marginal probability distribution functions of X and Y respectively. Clearly, mutual information is equivalent to the Kullback-Leibler (KL-) divergence between the joint p(x, y) and the product of the marginal distributions p(x) and p(y): where D_{KL is defined as, Furthermore, the KL divergence admits the following Donsker-Varadhan (DV) representation : where the supremum is taken over all functions T and \\omega", "% Specifically, let F represent real video frames, we update the mutual information estimator by maximizing mutual information between the real frames and the audio (I^{JS_\\Theta(F, A)), then use the updated estimator to maximize the mutual information between the generated frames and audio (I^{JS_\\Theta(\\hat{F, A)) for better GAN training. We refer to this as Asymmetric Mutual Information Estimator (AMIE) in this paper which can improve the quality of generation as verified in our experiments. As shown in Fig. , comparing our AMIE with conventional MINE, we see that our generated samples are closer to the real samples than those from MINE.", "(4) After integrating the asymmetric training strategy (Asy.) and JS represented estimator into the MINE, our AMIE can further improve the performance on all the metrics, as shown in Table (g). (5) Integrating AMIE (comparing Table 'Ours' to Table (b)) can further boost the performance, while simply integrating MINE (comparing Table (c) to (b)) the performance declined, that verifies the contribution of proposed AMIE.", "Our model leverages the asymmetric mutual information estimator to learn the correlation of audio and facial image features and utilizes dynamic attention to simulate the process of disentangling. Extensive experimental results on benchmark datasets demonstrate the promising performance of our method.% \\section*{Acknowledgments This work is partially funded by the Beijing Natural Science Foundation (JQ18017), Youth Innovation Promotion Association CAS (Grant No. Y201929), the National Nature Science Foundation of China (61976002), and the Open Project Program of the National Laboratory of Pattern Recognition (NLPR) (201900046). \\bibliographystyle{named \\bibliography{refs_mini", "\\subset \\mathbb{R^d so that the two expectations are finite. Therefore, we leverage the bound: where I^{DV_\\Theta(X,Y) denotes the neural information measure, \\end{equation and T_\\theta denotes a neural network trained by maximizing the mutual information . \\subsubsection{'AMIE' in Talking Face Generation In this cross-modal talking face generation task, we propose to explore the mutual information between the audio and the visual modality via an Asymmetric Mutual Information Estimator."], "referred_text_chunks": ["% From Table , it is clear to see that, (1) Simply adopting MI into our baseline did not significantly improve the results, comparing Table (a) to baseline. (2) DA plays an important role in talking face generation, comparing Table (b) to baseline. (3) Introducing either Asy. or JS strategy shows the great improvement on almost metrics, comparing Table (e) and (f) to Table (c)."]}, "2": {"question": "Which method performed the best on the GRID dataset?", "referred_figures_indices": [2], "response": "{'Image': 2, 'Text': 0, 'Rationale': 'Image 2 provides the evaluation results specifically for the GRID dataset, mentioning PSNR, SSIM, and LMD for various models. Text chunk 0 highlights the performance on GRID with a focus on achieving the highest SSIM and lowest LMD.', 'Answer': 'The AMIE method performed the best on the GRID dataset.'}", "all_figures_names": ["1812.06589v2-Table1-1.png", "1812.06589v2-Table4-1.png", "1812.06589v2-Table2-1.png", "1812.06589v2-Figure2-1.png", "1812.06589v2-Figure6-1.png", "1812.06589v2-Figure5-1.png", "1812.06589v2-Table3-1.png", "1812.06589v2-Figure1-1.png"], "referred_figures_names": ["1812.06589v2-Table2-1.png"], "answer": "AMIE (Ours)", "figure_type": "table", "content_type": "table", "text_chunks": ["Although Zhou et al. obtains the lowest PSNR, it obtains the second highest SSIM and its SSIM is significantly better than Chung et al. and Chen et al. . To further verify the robustness for arbitrary person generation, we evaluate our method on another benchmark dataset GRID and report the comparison results in Table . From Table , we observe that our model achieves the highest SSIM and the lowest LMD, demonstrating the effectiveness and robustness of our method.", "We implement mutual information as described before. We try to maximize it between generated frames and audios, Our full model is optimized according to the following objective function: Experiments Dataset and Metrics We evaluate our method on prevalent benchmark datasets LRW and GRID .", "Although the PSNR of our method is a little lower than that of Chen et al. , our method surpass on the metrics of both SSIM and LMD. Therefore, our method always achieves the highest PSNR and SSIM, demonstrating our method is able to generate high-quality videos. Qualitative Results To present the superiority of our method, we provide generated samples compared with Zhou et al. \\shortcite{Zhou2018TalkingFG, and Chen et al. \\shortcite{chen2019hierarchical.", "Quantitative Results We compare our model with four recent state-of-the-art methods, including Chung et al. \\shortcite{Chung2017YouST, Chen et al. \\shortcite{Chen2018LipMG Zhou et al. \\shortcite{Zhou2018TalkingFG, and Chen et al. \\shortcite{chen2019hierarchical. Table shows the quantitative results of our method and its competitors with higher PSNR, SSIM and lower LMD, suggesting the best quality of the generated video frames of the talking faces.", "% Furthermore, we use Landmark Distance (LMD) to evaluate the accuracy of the generated lip by calculating the landmark distance between the generated video and the original video. The lower LMD, the better of the generation.", "% Specifically, let F represent real video frames, we update the mutual information estimator by maximizing mutual information between the real frames and the audio (I^{JS_\\Theta(F, A)), then use the updated estimator to maximize the mutual information between the generated frames and audio (I^{JS_\\Theta(\\hat{F, A)) for better GAN training. We refer to this as Asymmetric Mutual Information Estimator (AMIE) in this paper which can improve the quality of generation as verified in our experiments. As shown in Fig. , comparing our AMIE with conventional MINE, we see that our generated samples are closer to the real samples than those from MINE.", "\\subset \\mathbb{R^d so that the two expectations are finite. Therefore, we leverage the bound: where I^{DV_\\Theta(X,Y) denotes the neural information measure, \\end{equation and T_\\theta denotes a neural network trained by maximizing the mutual information . \\subsubsection{'AMIE' in Talking Face Generation In this cross-modal talking face generation task, we propose to explore the mutual information between the audio and the visual modality via an Asymmetric Mutual Information Estimator.", "Fig. demonstrates the visualized examples. User Study We conduct a user study on the LRW dataset with 42 volunteers in both realistic and synchronization of generation. We randomly select the samples generated by our method, Zhou et al. \\shortcite{Zhou2018TalkingFG and Chen et al. \\shortcite{chen2019hierarchical."], "referred_text_chunks": ["Although Zhou et al. obtains the lowest PSNR, it obtains the second highest SSIM and its SSIM is significantly better than Chung et al. and Chen et al. . To further verify the robustness for arbitrary person generation, we evaluate our method on another benchmark dataset GRID and report the comparison results in Table . From Table , we observe that our model achieves the highest SSIM and the lowest LMD, demonstrating the effectiveness and robustness of our method."]}, "3": {"question": "What is the role of the frame discriminator in the proposed method?", "referred_figures_indices": [3], "response": "{'Image': 3, 'Text': 4, 'Rationale': 'Image 3 provides a visual schema of the entire method, clearly highlighting the components including the frame discriminator. Text chunk 4 explains the role of the frame discriminator, which is to predict whether a frame and audio are paired, contributing to the GAN training loss.', 'Answer': 'The frame discriminator predicts whether the frame and audio are paired, contributing to the GAN training loss for better synchronization of lip movements.'}", "all_figures_names": ["1812.06589v2-Figure6-1.png", "1812.06589v2-Table1-1.png", "1812.06589v2-Table2-1.png", "1812.06589v2-Figure2-1.png", "1812.06589v2-Figure5-1.png", "1812.06589v2-Table3-1.png", "1812.06589v2-Figure1-1.png", "1812.06589v2-Figure4-1.png"], "referred_figures_names": ["1812.06589v2-Figure2-1.png"], "answer": "The frame discriminator is used to detect whether the generated frame and audio are matched or not.", "figure_type": "schematic", "content_type": "figure", "text_chunks": ["We shall elaborate on the details of the proposed AMIE in the following section. \\paragraph{Frame Discriminator. Frame discriminator is fed by the pairs of the real frame and audio clip \\{f_i, a_i\\, or the pairs of the generated frame and corresponding audio clip \\{\\hat{f_{i, a_i\\.", "Based on the above discussion, we propose a novel and robust method by exploring the coherence between audio and visual modalities for arbitrary talking face generation in this paper. The proposed model consists of three components: talking face generator, asymmetric mutual information estimator, and frame discriminator, as shown in Fig. . First, the talking face generator is designed to generate target frames from the given input: one audio clip, one still facial image, and the previous generated frame.", "The output of the discriminator is the probability of whether the inputs (audio and frame) are matched. The discriminator consists of an Image CNN, an Audio FC, and a classifier. We flatten the output of Image CNN, and the Audio FC then feed concatenated features to the final classifier to produce 1-dimensional output.", "% Specifically, let F represent real video frames, we update the mutual information estimator by maximizing mutual information between the real frames and the audio (I^{JS_\\Theta(F, A)), then use the updated estimator to maximize the mutual information between the generated frames and audio (I^{JS_\\Theta(\\hat{F, A)) for better GAN training. We refer to this as Asymmetric Mutual Information Estimator (AMIE) in this paper which can improve the quality of generation as verified in our experiments. As shown in Fig. , comparing our AMIE with conventional MINE, we see that our generated samples are closer to the real samples than those from MINE.", "The attention masks of the following generated frame are predicted by the previous attention mask and the generated faces. In practice, we give a coarse mask area and an attention rate, the following frames generation only predict a fine-grained mask area which is close to the lip shape, while leaving the rate unchanged. Training Details In the training stage, % frame discriminator predicts the probability of whether frame and audio are paired or not, resulting in the following loss of our GAN: \\end{equation In order to synchronize the lip movements more accurately, we employ perceptual loss to capture high-level features differences between generated images and ground-truth.", "Recently, Belghazi et al. \\shortcite{Belghazi2018MutualIN proposed a backpropagation MI estimator that exploited a dual optimization based on dual representations of the KL-divergence to estimate divergences beyond the minimax objective as formalized in GANs. % Proposed Method In this paper, we propose a novel model for arbitrary talking face generation by attentively discovering the cross-modal coherence, as shown in Fig. .", "We propose a dynamic attention block on lip area to preserve the identity information and leverage feature of lip motion, i.e., paying different attentions on given face image (identity-related) and the previous generated frame (lip-related) during different training stages. \\caption{Pipeline of our proposed method. \\end{figure*", "Overview Our model consists of three parts: a Talking Face Generator, a Frame Discriminator, and a Mutual Information Estimator. \\paragraph{Talking Face Generator. There are three inputs of the generator: 1) the input face I_f which ensures the texture information of the output frame, 2) the speech audio clip A = \\{a_1, a_2, ..., a_n\\, working as the condition to supervise the lip changing, and 3) the previously generated frame \\hat{f_{i-1 \\in \\hat{F=\\{\\hat{f_1,\\hat{f_2,...,\\hat{f_n\\ where the i-th frame guarantees the smoothness of the image generation by feeding more temporal information. The three inputs are fed to the Identity Encoder, Audio Encoder, and Image Encoder respectively, while the target video frame \\hat{f_i is generated by the Frame Decoder."], "referred_text_chunks": ["We shall elaborate on the details of the proposed AMIE in the following section. \\paragraph{Frame Discriminator. Frame discriminator is fed by the pairs of the real frame and audio clip \\{f_i, a_i\\, or the pairs of the generated frame and corresponding audio clip \\{\\hat{f_{i, a_i\\."]}, "4": {"question": "How does the dynamic attention block improve the transition of generated video for arbitrary identities?", "referred_figures_indices": [5], "response": "{'Image': 5, 'Text': 2, 'Rationale': 'Image 5 clearly illustrates the dynamic attention process on the lip area, which is directly relevant to how it improves transition in generated videos. Text 2 discusses the separation of identity-related and lip-related features, which is central to understanding how the dynamic attention block improves generation.', 'Answer': 'The dynamic attention block improves the transition of generated video for arbitrary identities by focusing separately on identity-related and lip-related features. This separation allows for more stable identity representation and dynamic lip motion, enhancing the naturalness of transitions between frames.'}", "all_figures_names": ["1812.06589v2-Figure5-1.png", "1812.06589v2-Table1-1.png", "1812.06589v2-Table2-1.png", "1812.06589v2-Figure2-1.png", "1812.06589v2-Table4-1.png", "1812.06589v2-Figure4-1.png", "1812.06589v2-Figure3-1.png", "1812.06589v2-Figure6-1.png"], "referred_figures_names": ["1812.06589v2-Figure4-1.png"], "answer": "The dynamic attention block decouples the lip-related and identity-related information, allowing the network to focus on the most important area for generating realistic talking faces.", "figure_type": "photograph(s)", "content_type": "figure", "text_chunks": ["Directly leveraging the whole area of the given face tends to generate face images with a slight jitter problem. In order to capture different information among the given faces and disentangle the identity-related and lip-related information, we introduce a dynamic attention block. When generating a video sequence, we assign an initial attention rate to the first frame, then predict the fine-grained attention mask for the following video frames.", "We propose a dynamic attention block on lip area to preserve the identity information and leverage feature of lip motion, i.e., paying different attentions on given face image (identity-related) and the previous generated frame (lip-related) during different training stages. \\caption{Pipeline of our proposed method. \\end{figure*", "We observe that a talking face video is mainly composited by the identity-related feature and the lip-related feature, where the former is more stable while the latter is more temporal dynamic. Therefore, it is essential to separate these two features for arbitrary identity generation. Herein, we propose to leverage the given face and previous generated frame to provide identity-related and lip-related information, respectively.", "Introduction Talking face generation, which aims to generate a realistic talking video for the given still face image and speech clip, has been an active research topic. It has wide potential applications such as movie animation, teleconferencing, talking agents, and enhancing speech comprehension while preserving privacy. Although recent efforts have achieved impressive talking face synthesis for arbitrary identities, it is still a huge challenge due to the heterogeneous between audio and video, together with the appearance diversity of arbitrary identities.", "Then, the asymmetric mutual information estimator is introduced to maximize the mutual information between generated video and audio distributions via the information learned from a neural network based on MI measure. Finally, we feed the generated frame and audio into frame discriminator to detect whether they are matched or not. In the training stage, the lip area will give different attention to make the network focus on the most important area by leveraging the Dynamic Attention block, as illustrated in Fig .", "Existing state-of-the-art works either try to disentangle the content and identity features from the speech for the video generation step or leverage the landmark as a middle feature to bridge the gap between audio and video. Despite the great progress on feature representation for video generation, they usually leverage reconstruct loss between generated and real frames, while neglecting an essential problem in talking face generation: how to sufficiently express the audio feature into generated video? To reduce the uncertainty in the audio-to-video generation process and ensure the synchronized talking face transition, we argue that the giving audio and generated video should share maximum information, i.e., the minimized sharing entropy across modalities.", "Dynamic Attention on Lip Area As we discussed in Sec. , we observe that a talking face video is mainly composited by the identity-related and lip-related features.", "% Specifically, let F represent real video frames, we update the mutual information estimator by maximizing mutual information between the real frames and the audio (I^{JS_\\Theta(F, A)), then use the updated estimator to maximize the mutual information between the generated frames and audio (I^{JS_\\Theta(\\hat{F, A)) for better GAN training. We refer to this as Asymmetric Mutual Information Estimator (AMIE) in this paper which can improve the quality of generation as verified in our experiments. As shown in Fig. , comparing our AMIE with conventional MINE, we see that our generated samples are closer to the real samples than those from MINE."], "referred_text_chunks": ["Directly leveraging the whole area of the given face tends to generate face images with a slight jitter problem. In order to capture different information among the given faces and disentangle the identity-related and lip-related information, we introduce a dynamic attention block. When generating a video sequence, we assign an initial attention rate to the first frame, then predict the fine-grained attention mask for the following video frames."]}, "5": {"question": "Which method, AMIE or MINE, produces generated frames that are closer in distribution to the real frames?", "referred_figures_indices": [1], "response": "{'Image': 1, 'Text': 1, 'Rationale': \"Image 1 visually compares distributions of real and generated frames using AMIE and MINE, directly illustrating that AMIE's generated samples are closer to real samples. Text 1 supports this by explaining AMIE improves generation quality, showing AMIE's superiority over MINE.\", 'Answer': 'AMIE produces generated frames that are closer in distribution to the real frames.'}", "all_figures_names": ["1812.06589v2-Figure1-1.png", "1812.06589v2-Figure3-1.png", "1812.06589v2-Figure2-1.png", "1812.06589v2-Figure4-1.png", "1812.06589v2-Table4-1.png", "1812.06589v2-Figure6-1.png", "1812.06589v2-Table1-1.png", "1812.06589v2-Table2-1.png"], "referred_figures_names": ["1812.06589v2-Figure3-1.png"], "answer": "MINE", "figure_type": "plot", "content_type": "figure", "text_chunks": ["The generated frames \\hat{F should be similar to real frames F = \\{f_1,f_2,...,f_n\\. \\paragraph{Asymmetric Mutual Information Estimator (AMIE). AMIE aims to approximate the mutual information between the generated video frame and audio. It is trained using \\{f_i, a_i\\ and \\{f_j, a_k\\, where j and k are random sampled from 1\\sim n.", "% Specifically, let F represent real video frames, we update the mutual information estimator by maximizing mutual information between the real frames and the audio (I^{JS_\\Theta(F, A)), then use the updated estimator to maximize the mutual information between the generated frames and audio (I^{JS_\\Theta(\\hat{F, A)) for better GAN training. We refer to this as Asymmetric Mutual Information Estimator (AMIE) in this paper which can improve the quality of generation as verified in our experiments. As shown in Fig. , comparing our AMIE with conventional MINE, we see that our generated samples are closer to the real samples than those from MINE.", "We shall elaborate on the details of the proposed AMIE in the following section. \\paragraph{Frame Discriminator. Frame discriminator is fed by the pairs of the real frame and audio clip \\{f_i, a_i\\, or the pairs of the generated frame and corresponding audio clip \\{\\hat{f_{i, a_i\\.", "Asymmetric Mutual Information Estimator We first introduce the theory of mutual information neural estimation (MINE), followed by the mutual information estimation in talking face generation task which leverages the proposed AMIE to learn the cross-modal coherence. \\subsubsection{Preliminary Theory of 'MINE' Mutual information is a measurement of mutual dependency between two probability distributions, where p(x, y) is the joint probability function of X and Y, and p(x) and p(y) are the marginal probability distribution functions of X and Y respectively. Clearly, mutual information is equivalent to the Kullback-Leibler (KL-) divergence between the joint p(x, y) and the product of the marginal distributions p(x) and p(y): where D_{KL is defined as, Furthermore, the KL divergence admits the following Donsker-Varadhan (DV) representation : where the supremum is taken over all functions T and \\omega", "Unfortunately, it cannot be directly applied to talking face generation. On the one hand, if MINE for GANs uses generated sample and input pairs for both training and estimating, the mutual information estimation may be misled due to the low quality of generation in the early stages. Thinking that GANs push the generated sample distribution to the real frame distribution, we propose to use the real sample and its corresponding input to update the mutual information estimator, followed by asymmetrically maximizing the mutual information between the generated sample and input for better GAN training.", "It is clear to see that, Zhou et al. suffer from a \"zoom-in-and-out\" effect, while the lip shapes of Chen et al. appear differences from the real one. In general, our model can generate more realistic and synchronous frames. Ablation Study In order to quantify the effect of each component of our method, we conduct ablation study experiments to verify the contributions of the two key components in our model: Asymmetric Mutual Information Estimator (AMIE) and Dynamic Attention (DA), and the two important strategies in AMIE: the asymmetric training strategy (Asy.) and JS represented estimator.", "On the other hand, our objective is to maximize the MI between heterogeneous modalities while not concerning the accurate MI value , we therefore replace the original formulation of MI by Jensen-Shannon represented MI estimator, which estimates the relative amplitude of MI rather than the exact value, and has been well studied in neural network optimization with empirically more stable learning . We refer to our strategy as Asymmetric Mutual Information Estimator (AMIE). In addition, conventional approaches directly employ the whole global area on given face image during synthesis, which is difficult for neural networks to discover the relation between audio and local lips.", "However, this may disturb the MI estimation since the generated frames are blurry and not accurate at earlier training epochs. Furthermore, GANs are usually used to learn the probability distribution consistent with the real data, and mutual information is used to estimate the amount of shared information between the two distributions. Therefore, our solution is to use mutual information in three distributions, we use the real frame and audio distribution during the training while the generated frame and audio distribution."], "referred_text_chunks": ["The generated frames \\hat{F should be similar to real frames F = \\{f_1,f_2,...,f_n\\. \\paragraph{Asymmetric Mutual Information Estimator (AMIE). AMIE aims to approximate the mutual information between the generated video frame and audio. It is trained using \\{f_i, a_i\\ and \\{f_j, a_k\\, where j and k are random sampled from 1\\sim n."]}, "6": {"question": " What are the limitations of the Zhou \\textit{et al.} and Chen \\textit{et al.} methods for generating talking-face videos, as compared to the method proposed in the paper?", "referred_figures_indices": [4], "response": "{'Image': 4, 'Text': 5, 'Rationale': 'Image 4 visually contrasts the proposed method with those of Zhou et al. and Chen et al., highlighting quality differences. Text 5 provides quantitative results showing how the proposed method outperforms others, especially in SSIM and LMD metrics.', 'Answer': 'Zhou et al. and Chen et al. methods are limited in achieving high SSIM and low LMD scores compared to the proposed method, suggesting less accurate and consistent video generation.'}", "all_figures_names": ["1812.06589v2-Figure2-1.png", "1812.06589v2-Figure6-1.png", "1812.06589v2-Table2-1.png", "1812.06589v2-Table3-1.png", "1812.06589v2-Figure5-1.png", "1812.06589v2-Table1-1.png", "1812.06589v2-Figure3-1.png", "1812.06589v2-Figure4-1.png"], "referred_figures_names": ["1812.06589v2-Figure5-1.png"], "answer": " The Zhou \\textit{et al.} method suffers from a \"zoom-in-and-out\" effect, while the Chen \\textit{et al.} method produces lip shapes that differ from the real ones.", "figure_type": "** photograph(s)", "content_type": "figure", "text_chunks": ["Quantitative Results We compare our model with four recent state-of-the-art methods, including Chung et al. \\shortcite{Chung2017YouST, Chen et al. \\shortcite{Chen2018LipMG Zhou et al. \\shortcite{Zhou2018TalkingFG, and Chen et al. \\shortcite{chen2019hierarchical. Table shows the quantitative results of our method and its competitors with higher PSNR, SSIM and lower LMD, suggesting the best quality of the generated video frames of the talking faces.", "Although the PSNR of our method is a little lower than that of Chen et al. , our method surpass on the metrics of both SSIM and LMD. Therefore, our method always achieves the highest PSNR and SSIM, demonstrating our method is able to generate high-quality videos. Qualitative Results To present the superiority of our method, we provide generated samples compared with Zhou et al. \\shortcite{Zhou2018TalkingFG, and Chen et al. \\shortcite{chen2019hierarchical.", "Chen et al. \\shortcite{Zhou2018TalkingFG proposed to leverage landmark as middle information to better guide face generation. However, they mainly focused on the inner-modal coherence, while lacking of discovering the cross-modal coherence. % Mutual Information Estimation", "Based on the above discussion, we propose a novel and robust method by exploring the coherence between audio and visual modalities for arbitrary talking face generation in this paper. The proposed model consists of three components: talking face generator, asymmetric mutual information estimator, and frame discriminator, as shown in Fig. . First, the talking face generator is designed to generate target frames from the given input: one audio clip, one still facial image, and the previous generated frame.", "Next, some works tried to synthesized the talking faces for the identities from the dataset . Recently, the synthesis of the talking face for the arbitrary identities out of the dataset has drawn much attention. Zhou et al. \\shortcite{Zhou2018TalkingFG proposed an adversarial learning method to disentangle the different information for one image during generation.", "Existing state-of-the-art works either try to disentangle the content and identity features from the speech for the video generation step or leverage the landmark as a middle feature to bridge the gap between audio and video. Despite the great progress on feature representation for video generation, they usually leverage reconstruct loss between generated and real frames, while neglecting an essential problem in talking face generation: how to sufficiently express the audio feature into generated video? To reduce the uncertainty in the audio-to-video generation process and ensure the synchronized talking face transition, we argue that the giving audio and generated video should share maximum information, i.e., the minimized sharing entropy across modalities.", "We observe that a talking face video is mainly composited by the identity-related feature and the lip-related feature, where the former is more stable while the latter is more temporal dynamic. Therefore, it is essential to separate these two features for arbitrary identity generation. Herein, we propose to leverage the given face and previous generated frame to provide identity-related and lip-related information, respectively.", "Introduction Talking face generation, which aims to generate a realistic talking video for the given still face image and speech clip, has been an active research topic. It has wide potential applications such as movie animation, teleconferencing, talking agents, and enhancing speech comprehension while preserving privacy. Although recent efforts have achieved impressive talking face synthesis for arbitrary identities, it is still a huge challenge due to the heterogeneous between audio and video, together with the appearance diversity of arbitrary identities."], "referred_text_chunks": ["Quantitative Results We compare our model with four recent state-of-the-art methods, including Chung et al. \\shortcite{Chung2017YouST, Chen et al. \\shortcite{Chen2018LipMG Zhou et al. \\shortcite{Zhou2018TalkingFG, and Chen et al. \\shortcite{chen2019hierarchical. Table shows the quantitative results of our method and its competitors with higher PSNR, SSIM and lower LMD, suggesting the best quality of the generated video frames of the talking faces."]}, "7": {"question": "Which method performed the best according to the LMD metric?", "referred_figures_indices": [7], "response": "{'Image': 6, 'Text': 0, 'Rationale': 'Image 6 provides the LMD evaluation metrics across different methods, showing that AMIE (Ours) has the lowest LMD. Text chunk 0 confirms the effectiveness of the method with the lowest LMD.', 'Answer': 'AMIE (Ours) performed the best according to the LMD metric.'}", "all_figures_names": ["1812.06589v2-Figure4-1.png", "1812.06589v2-Figure3-1.png", "1812.06589v2-Figure1-1.png", "1812.06589v2-Figure5-1.png", "1812.06589v2-Table3-1.png", "1812.06589v2-Table4-1.png", "1812.06589v2-Table2-1.png", "1812.06589v2-Table1-1.png"], "referred_figures_names": ["1812.06589v2-Table1-1.png"], "answer": "AMIE (Ours)", "figure_type": "table", "content_type": "table", "text_chunks": ["Although Zhou et al. obtains the lowest PSNR, it obtains the second highest SSIM and its SSIM is significantly better than Chung et al. and Chen et al. . To further verify the robustness for arbitrary person generation, we evaluate our method on another benchmark dataset GRID and report the comparison results in Table . From Table , we observe that our model achieves the highest SSIM and the lowest LMD, demonstrating the effectiveness and robustness of our method.", "% Furthermore, we use Landmark Distance (LMD) to evaluate the accuracy of the generated lip by calculating the landmark distance between the generated video and the original video. The lower LMD, the better of the generation.", "Quantitative Results We compare our model with four recent state-of-the-art methods, including Chung et al. \\shortcite{Chung2017YouST, Chen et al. \\shortcite{Chen2018LipMG Zhou et al. \\shortcite{Zhou2018TalkingFG, and Chen et al. \\shortcite{chen2019hierarchical. Table shows the quantitative results of our method and its competitors with higher PSNR, SSIM and lower LMD, suggesting the best quality of the generated video frames of the talking faces.", "Although the PSNR of our method is a little lower than that of Chen et al. , our method surpass on the metrics of both SSIM and LMD. Therefore, our method always achieves the highest PSNR and SSIM, demonstrating our method is able to generate high-quality videos. Qualitative Results To present the superiority of our method, we provide generated samples compared with Zhou et al. \\shortcite{Zhou2018TalkingFG, and Chen et al. \\shortcite{chen2019hierarchical.", "We implement mutual information as described before. We try to maximize it between generated frames and audios, Our full model is optimized according to the following objective function: Experiments Dataset and Metrics We evaluate our method on prevalent benchmark datasets LRW and GRID .", "Rithesh et al. \\shortcite{kumar2017obamanet used a time-delayed LSTM to generate key points synced to the audio and use another network to generate the video frames conditioned on the key points. Supasorn et al. \\shortcite{suwajanakorn2017synthesizing proposed a teeth proxy to improve the quality of the teeth during generation.", "It is clear to see that, Zhou et al. suffer from a \"zoom-in-and-out\" effect, while the lip shapes of Chen et al. appear differences from the real one. In general, our model can generate more realistic and synchronous frames. Ablation Study In order to quantify the effect of each component of our method, we conduct ablation study experiments to verify the contributions of the two key components in our model: Asymmetric Mutual Information Estimator (AMIE) and Dynamic Attention (DA), and the two important strategies in AMIE: the asymmetric training strategy (Asy.) and JS represented estimator.", "Our model leverages the asymmetric mutual information estimator to learn the correlation of audio and facial image features and utilizes dynamic attention to simulate the process of disentangling. Extensive experimental results on benchmark datasets demonstrate the promising performance of our method.% \\section*{Acknowledgments This work is partially funded by the Beijing Natural Science Foundation (JQ18017), Youth Innovation Promotion Association CAS (Grant No. Y201929), the National Nature Science Foundation of China (61976002), and the Open Project Program of the National Laboratory of Pattern Recognition (NLPR) (201900046). \\bibliographystyle{named \\bibliography{refs_mini"], "referred_text_chunks": ["Although Zhou et al. obtains the lowest PSNR, it obtains the second highest SSIM and its SSIM is significantly better than Chung et al. and Chen et al. . To further verify the robustness for arbitrary person generation, we evaluate our method on another benchmark dataset GRID and report the comparison results in Table . From Table , we observe that our model achieves the highest SSIM and the lowest LMD, demonstrating the effectiveness and robustness of our method."]}}