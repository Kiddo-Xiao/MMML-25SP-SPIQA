{
  "0": {
    "question": "How does the shape of the IRLS weight function change as the shape parameter \u03b1 increases?",
    "ground_truth": "The IRLS weight function becomes more peaked and concentrated around zero as the shape parameter \u03b1 increases.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The shape of the IRLS weight function remains constant as the shape parameter \u03b1 increases. <s>"
  },
  "1": {
    "question": "How do the reconstructed faces in the \"Mean Reconstruction\" differ from those in the \"Sampled Reconstruction\"?",
    "ground_truth": "The reconstructed faces in the \"Mean Reconstruction\" are smoother and less detailed than those in the \"Sampled Reconstruction\". This is because the mean reconstruction is based on the average of all the possible reconstructions, while the sampled reconstruction is based on a single sample from the distribution.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The difference between the \"Mean Reconstruction\" and \"Sampled Reconstruction\" lies in the quality of the reconstructed faces. The \"Mean Reconstruction\" refers to the average of all the reconstructed faces generated by the model, while the \"Sampled Reconstruction\" refers to a randomly selected face from the set of reconstructed faces. <s>"
  },
  "2": {
    "question": "How does the performance of the adaptive model compare to the fixed model with different values of \u03b1?",
    "ground_truth": "The adaptive model consistently outperforms the fixed model for all values of \u03b1.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The performance of the adaptive model is generally better than the fixed model with different values of \u03b1, as indicated by the negative log-likelihoods (NLLs) and probability densities in the images. <s>"
  },
  "3": {
    "question": "On which dataset did the gRCC* algorithm achieve the largest relative improvement over the RCC algorithm, and by approximately how much did it improve?",
    "ground_truth": "The gRCC* algorithm achieved the largest relative improvement over the RCC algorithm on the YTF dataset, with a relative improvement of approximately 31.9%.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The gRCC* algorithm achieved the largest relative improvement over the RCC algorithm on the clustering task of [32] by approximately 10%. <s>"
  },
  "4": {
    "question": "What is the relationship between the shape parameter \u03b1 and the shape of the loss function?",
    "ground_truth": "The shape parameter \u03b1 controls the shape of the loss function. As \u03b1 increases, the loss function becomes more peaked, and as \u03b1 decreases, the loss function becomes more flat.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure3-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The relationship between the shape parameter \u03b1 and the shape of the loss function is that the negative log-likelihoods (left) and probability densities (right) of the distribution corresponding to the loss function are shifted by a log partition function. <s>"
  },
  "5": {
    "question": "What is the effect of replacing the loss function in the \"Baseline\" network with the \"adaptive\" loss over wavelet coefficients?",
    "ground_truth": "Replacing the loss function in the \"Baseline\" network with the \"adaptive\" loss over wavelet coefficients results in significantly improved depth estimates.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The effect of replacing the loss function in the \"Baseline\" network with the \"adaptive\" loss over wavelet coefficients is improved performance on the clustering task. <s>"
  },
  "6": {
    "question": "Which method for setting the shape parameter of the proposed loss function achieved the best performance in terms of average error? How much improvement did it offer compared to the reproduced baseline?",
    "ground_truth": "The \"adaptive $\\power \\in (0, 2)$\" strategy, where each wavelet coefficient has its own shape parameter that is optimized during training, achieved the best performance in terms of average error. It reduced the average error by approximately 17% compared to the reproduced baseline.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The best performance in terms of average error was achieved by allowing the distribution to individually adapt its shape parameter to each coefficient, rather than setting all wavelet coefficients to have the same fixed shape parameter \u03b1. This approach is referred to as \"Wavelets + YUV\" VAE model, and it outperformed any single fixed shape parameter by a significant margin. <s>"
  },
  "7": {
    "question": "Why did the authors choose to use a nonlinearity to curve \u03b1 before fitting the cubic hermite spline?",
    "ground_truth": "The authors chose to use a nonlinearity to curve \u03b1 before fitting the cubic hermite spline because it allows for increased knot density near \u03b1 = 2 and decreased knot density when \u03b1 > 4. This helps to better approximate the log partition function, which is difficult to evaluate for arbitrary inputs.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The authors chose to use a nonlinearity to curve \u03b1 before fitting the cubic hermite spline in order to make the optimization process more stable and efficient. \n\nA nonlinearity is a mathematical function used to introduce nonlinearity into a system. In this case, the authors used a sigmoidal function, which is a type of nonlinearity commonly used in neural networks. The sigmoidal function is defined as:\n\nf(x) = 1 / (1 + e^(-x))\n\nBy introducing this nonlinearity into the optimization process, the authors"
  },
  "8": {
    "question": "What is the range of values for the shape parameter \u03b1?",
    "ground_truth": "The range of values for the shape parameter \u03b1 is from 0 to 2.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure3-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The range of values for the shape parameter \u03b1 is -2 to 0. <s>"
  },
  "9": {
    "question": "How does the performance of gFGR change as the shape parameter \u03b1 increases?",
    "ground_truth": "The performance of gFGR generally improves as the shape parameter \u03b1 increases.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The performance of gFGR increases as the shape parameter \u03b1 increases, as indicated by the negative log-likelihoods (left) and probability densities (right) of the distribution corresponding to our loss function when it is defined (\u03b1 \u2265 0) in Figure 6. <s>"
  },
  "10": {
    "question": "Which image representation results in the sharpest and highest-quality samples?",
    "ground_truth": "DCT and wavelet representations.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The image representation that results in the sharpest and highest-quality samples is monocular depth estimation. <s>"
  },
  "11": {
    "question": "Which dataset shows the greatest sensitivity to the choice of $\\power$?",
    "ground_truth": "RCV1",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png"
    ],
    "answer": "The dataset that shows the greatest sensitivity to the choice of $\\power$ is not specified in the question. <s>"
  },
  "12": {
    "question": "How does the shape of the negative log-likelihood (NLL) and probability density functions change as the value of \u03b1 increases?",
    "ground_truth": " As the value of \u03b1 increases, the NLL functions become more peaked and the probability density functions become more concentrated around the mean.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The shape of the negative log-likelihood (NLL) and probability density functions changes as the value of \u03b1 increases. <s>"
  },
  "13": {
    "question": "How do the results of the baseline and the proposed method compare in terms of accuracy?",
    "ground_truth": "The proposed method appears to be more accurate than the baseline method. The depth maps generated by the proposed method are more detailed and realistic than those generated by the baseline method.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure3-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The proposed method outperforms the baseline in terms of accuracy, as demonstrated by the lower negative log-likelihoods (NLLs) and higher probability densities in the validation set. <s>"
  },
  "14": {
    "question": "How does the choice of distribution affect the quality of the reconstructions?",
    "ground_truth": "Reconstructions from models using general distributions tend to be sharper and more detailed than reconstructions from the corresponding model that uses normal distributions.",
    "selected_figures": [
      "1701.03077v10-Figure1-1.png",
      "1701.03077v10-Figure4-1.png",
      "1701.03077v10-Table2-1.png",
      "1701.03077v10-Figure13-1.png",
      "1701.03077v10-Figure5-1.png",
      "1701.03077v10-Table3-1.png",
      "1701.03077v10-Figure7-1.png",
      "1701.03077v10-Figure12-1.png",
      "1701.03077v10-Figure11-1.png",
      "1701.03077v10-Figure10-1.png",
      "1701.03077v10-Figure16-1.png",
      "1701.03077v10-Figure9-1.png",
      "1701.03077v10-Table4-1.png",
      "1701.03077v10-Figure6-1.png",
      "1701.03077v10-Figure2-1.png",
      "1701.03077v10-Figure17-1.png",
      "1701.03077v10-Table1-1.png",
      "1701.03077v10-Figure3-1.png",
      "1701.03077v10-Figure14-1.png"
    ],
    "answer": "The choice of distribution affects the quality of the reconstructions by influencing the ability of the model to capture low-frequency image content. In general, using a more flexible and adaptive distribution, such as our general distribution, results in sharper and higher-quality samples and a better job of capturing low-frequency image content than using a specialized distribution like Student's t-distribution. <s>"
  }
}