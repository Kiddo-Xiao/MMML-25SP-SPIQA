{
  "0": {
    "question": "Which task-oriented dialog system performs the best when the percentage of unseen information in the KB is high?",
    "ground_truth": "BoSsNet",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "The task-oriented dialog system that performs the best when the percentage of unseen information in the KB is high is BOSSNET. <s>"
  },
  "1": {
    "question": "Why did Seq2Seq and Mem2Seq models perform poorly when the percentage of unseen entities in the knowledge base (KB) increased?",
    "ground_truth": "Seq2Seq and Mem2Seq models performed poorly because they struggled to capture the semantic representations of unseen entities. This means they couldn't understand the meaning and relationships of new restaurants introduced in the KB. As a result, they were unable to accurately identify the correct restaurant and provide its address when faced with unseen entities.",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "The poor performance of Seq2Seq and Mem2Seq models when the percentage of unseen entities in the knowledge base (KB) increased is due to their inability to effectively generalize from the seen entities to the unseen ones. As the percentage of unseen entities in the KB increases, the models are faced with a larger number of unseen entities, making it more challenging for them to generate coherent and meaningful responses. <s>"
  },
  "2": {
    "question": "Which model achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset?",
    "ground_truth": "The BOSSNET model achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset.",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "BOSSNET achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset. <s>"
  },
  "3": {
    "question": "Can you explain why the BOSSNET with multi-hop encoder performs better on bAbI tasks 3 and 5 compared to the 1-hop encoder, and how this relates to the tasks themselves?",
    "ground_truth": "The multi-hop encoder performs better on bAbI tasks 3 and 5 because these tasks specifically require inferencing over multiple KB tuples. In other words, the model needs to \"hop\" between different pieces of information in the knowledge base to make the correct inferences and recommendations.\n\nTask 3 involves sorting restaurants by rating, and task 5 requires recommending a restaurant based on user preferences. Both tasks necessitate the model to consider various restaurant attributes and their relationships, which the multi-hop encoder facilitates by capturing longer-range dependencies within the knowledge base.",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "The BOSSNET with multi-hop encoder performs better on bAbI tasks 3 and 5 compared to the 1-hop encoder because the multi-hop encoder is able to capture longer-term dependencies in the context of the conversation, allowing it to generate more coherent and meaningful responses. This is particularly beneficial for tasks 3 and 5, where the conversations are more complex and require a deeper understanding of the context to generate appropriate responses. <s>"
  },
  "4": {
    "question": "How does the encoder understand the last user utterance?",
    "ground_truth": "The encoder understands the last user utterance by using the memory cell representations of the dialog history and KB tuples.",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "The encoder understands the last user utterance by using a bidirectional long short-term memory (BiLSTM) network. <s>"
  },
  "5": {
    "question": "Which model performs best on tasks T3 and T3-OOV in terms of per-dialog accuracy, and how does its performance differ between the two test sets?",
    "ground_truth": "The proposed system model (BOSSNET) performs best on both tasks T3 and T3-OOV in terms of per-dialog accuracy. However, its performance is significantly higher on the T3-OOV test set (95.7%) compared to the non-OOV T3 test set (95.2%).",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "BOSSNET with one-level attention performs best on tasks T3 and T3-OOV in terms of per-dialog accuracy, and its performance does not differ significantly between the two test sets. <s>"
  },
  "6": {
    "question": "Why might the authors claim that although BOSSNET achieves a lower BLEU score than Mem2Seq on the SMD dataset, it still performs better in conveying necessary entity information?",
    "ground_truth": "While BOSSNET has a lower BLEU score than Mem2Seq on SMD, it achieves the highest Entity F1 score on that dataset. This suggests that BOSSNET is better at capturing and including the relevant entities in its responses, even though it may not have as much lexical overlap with the gold responses as Mem2Seq.",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "The authors argue that BOSSNET's lower BLEU score on the SMD dataset does not necessarily mean it performs worse than Mem2Seq in conveying necessary entity information. This is because the BLEU score only evaluates the model's ability to generate coherent and grammatically correct text, but it does not take into account the relevance of the generated text to the given context. In this case, the authors claim that BOSSNET outperforms Mem2Seq in conveying relevant entity information, even though it has a lower BLEU score. <s>"
  },
  "7": {
    "question": "Which model performs the best in terms of Entity F1 score when the percentage of unseen entities in the response is low?",
    "ground_truth": "BoSsNet",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "BOSSNET with one-level attention performs the best in terms of Entity F1 score when the percentage of unseen entities in the response is low. <s>"
  },
  "8": {
    "question": " \n\nWhat is the difference between the original and pre-processed SMD Navigate data? ",
    "ground_truth": " \n\nThe pre-processed SMD Navigate data combines all the properties (such as distance, address) of a point of interest (POI) into a single subject with the object being \"poi\". The original data had separate entries for each property. ",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "The difference between the original and pre-processed SMD Navigate data is that the pre-processed data undergoes a series of steps to improve the performance of the Mem2Seq model. <s>"
  },
  "9": {
    "question": "Which model performs best when the percentage of unseen entities in the response is low?",
    "ground_truth": "BoSsNet",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "BOSSNET with one-level attention outperforms BOSSNET with two-level attention when the percentage of unseen entities in the response is low. <s>"
  },
  "10": {
    "question": "Which task required the highest learning rate and how does this compare to the learning rate used for CamRest?",
    "ground_truth": "Task T1 and T2 required the highest learning rate of 0.001. This is twice the learning rate used for CamRest, which was trained with a learning rate of 0.0005.",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "The task that required the highest learning rate was Task 1: Per-response accuracy comparison on KA sets. <s>"
  },
  "11": {
    "question": "What is the difference between the attention weights in the two-level attention model and the one-level attention model?",
    "ground_truth": " The two-level attention model has higher attention weights on the relevant information in the memory, while the one-level attention model has more uniform attention weights.",
    "selected_figures": [
      "1805.01216v3-Figure1-1.png",
      "1805.01216v3-Table4-1.png",
      "1805.01216v3-Table3-1.png",
      "1805.01216v3-Table1-1.png",
      "1805.01216v3-Table2-1.png",
      "1805.01216v3-Figure8-1.png",
      "1805.01216v3-Figure2-1.png",
      "1805.01216v3-Table5-1.png",
      "1805.01216v3-Table13-1.png",
      "1805.01216v3-Table14-1.png",
      "1805.01216v3-Table11-1.png",
      "1805.01216v3-Table10-1.png",
      "1805.01216v3-Table12-1.png",
      "1805.01216v3-Figure4-1.png",
      "1805.01216v3-Figure5-1.png",
      "1805.01216v3-Table6-1.png",
      "1805.01216v3-Figure3-1.png",
      "1805.01216v3-Figure10-1.png",
      "1805.01216v3-Table9-1.png",
      "1805.01216v3-Figure7-1.png",
      "1805.01216v3-Table7-1.png",
      "1805.01216v3-Table8-1.png",
      "1805.01216v3-Figure9-1.png"
    ],
    "answer": "The difference between the attention weights in the two-level attention model and the one-level attention model is that the two-level attention model has two levels of attention, while the one-level attention model has only one level of attention. <s>"
  }
}