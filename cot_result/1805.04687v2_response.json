{
  "0": {
    "question": "How does increasing the training set size affect the performance of the lane marking and drivable area segmentation tasks?",
    "ground_truth": "Increasing the training set size generally leads to improved performance for both lane marking and drivable area segmentation tasks.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Figure1-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "Increasing the training set size improves the performance of the lane marking and drivable area segmentation tasks. <s>"
  },
  "1": {
    "question": "How does joint training with the object detection set affect instance segmentation performance, and what is the likely reason for this effect?",
    "ground_truth": "Joint training with the object detection set improves instance segmentation performance significantly. This is evidenced by the increase in AP, AP50, and AP75 metrics in Table 1 when comparing \"Inst-Seg\" and \"Inst-Seg + Det\" rows.\n\nThe passage explains that this improvement is likely due to the richer diversity of images and object examples in the detection set. This allows the instance segmentation model to learn better object appearance features and localization.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure15-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "Joint training with the object detection set improves instance segmentation performance by leveraging object detection, tracking, and instance segmentation to improve segmentation tracking accuracy. The likely reason for this effect is that the object detection and tracking information provided by the object detection set can be used to refine the segmentation masks generated by the instance segmentation model. <s>"
  },
  "2": {
    "question": "Based on Table 1, which category of objects has the largest total number of annotations in the BDD100K MOT dataset, considering both bounding boxes and instance tracks? ",
    "ground_truth": "Cars have the largest total number of annotations.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "Lane markings have the largest total number of annotations in the BDD100K MOT dataset, considering both bounding boxes and instance tracks. <s>"
  },
  "3": {
    "question": "How does the segmentation model perform in areas with no lane markings?",
    "ground_truth": "The segmentation model learns to interpolate in areas that have no lane markings.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The segmentation model performs poorly in areas with no lane markings. <s>"
  },
  "4": {
    "question": "What is the most common type of scene in the dataset?",
    "ground_truth": "City Street",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "Lane markings are the most common type of scene in the dataset. <s>"
  },
  "5": {
    "question": "What are the different types of annotations that are included in the dataset?",
    "ground_truth": "The dataset includes a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "Lane marking statistics, distribution of different types of lane markings and drivable areas, number of occlusions by track, number of occluded frames for each occlusion, object scale diversity, and comparisons with other MOTS and VOS datasets. <s>"
  },
  "6": {
    "question": "Explain why the model trained on both MOT and detection sets has a higher number of identity switches (IDS) compared to the model trained only on the MOT set, even though it achieves better performance in terms of AP, MOTA, and MOTP?",
    "ground_truth": "While the model trained on both MOT and detection sets shows improved performance in detection and tracking metrics (AP, MOTA, MOTP), it also exhibits a higher number of identity switches (IDS). This can be attributed to the increased diversity of instances introduced by the detection set. Although the MOT set provides a larger number of bounding boxes for training, the detection set adds varied examples that may lead to more frequent identity switches during tracking, even as it improves the model's overall performance.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure11-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure3-1.png",
      "1805.04687v2-Figure15-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The model trained on both MOT and detection sets has a higher number of identity switches (IDS) compared to the model trained only on the MOT set, even though it achieves better performance in terms of AP, MOTA, and MOTP. This is because the model trained on both MOT and detection sets has learned to be more confident in its predictions, which leads to a higher number of IDS. <s>"
  },
  "7": {
    "question": "Which domain discrepancy has a larger impact on object detection performance: city vs. non-city or daytime vs. nighttime?",
    "ground_truth": "Daytime vs. nighttime has a larger impact on object detection performance.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "City vs. non-city has a larger impact on object detection performance than daytime vs. nighttime. <s>"
  },
  "8": {
    "question": "Why does the proposed dataset have a lower number of persons per image compared to the Cityscapes dataset, even though it has a much higher total number of pedestrians?",
    "ground_truth": "The proposed dataset contains non-city scenes like highways, which typically have fewer pedestrians per image compared to cityscapes.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure9-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Figure1-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The proposed dataset has a lower number of persons per image compared to the Cityscapes dataset, even though it has a much higher total number of pedestrians, because the Cityscapes dataset focuses on capturing images of people in urban environments, while the proposed dataset focuses on capturing images of people in rural environments. <s>"
  },
  "9": {
    "question": "How does the performance of lane marking detection change with different thresholds (\u03c4) for direction, continuity, and category?",
    "ground_truth": "As the threshold (\u03c4) increases, the ODS-F scores for direction, continuity, and category generally increase as well. This indicates that the model performs better in detecting lane markings with higher thresholds, meaning it can tolerate larger deviations from the ground truth annotations.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure9-1.png",
      "1805.04687v2-Figure11-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Figure1-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure15-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Figure6-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The performance of lane marking detection changes with different thresholds (\u03c4) for direction, continuity, and category. <s>"
  },
  "10": {
    "question": "Which weather condition has the highest classification accuracy?",
    "ground_truth": "Clear weather.",
    "selected_figures": [
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "It is difficult to determine the weather condition with the highest classification accuracy, as it depends on various factors such as the type of lane marking, the presence of occlusions, and the diversity of object scales. The authors of the 1805.04687v2 dataset provide a comprehensive analysis of their dataset, including statistics on the number of sequences, frames, identities, and box annotations, as well as comparisons with other MOTS and VOS datasets. <s>"
  },
  "11": {
    "question": "Which type of object is the most common in the dataset?",
    "ground_truth": "Cars are the most common object in the dataset.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "Lane markings are the most common object in the dataset. <s>"
  },
  "12": {
    "question": "Based on the table, which approach achieved the highest mean IoU for semantic segmentation, and how did it perform compared to the baseline Sem-Seg model?",
    "ground_truth": "The **Sem-Seg + Det** approach achieved the highest mean IoU of 58.3, which is an improvement of 1.4 points compared to the baseline Sem-Seg model with a mean IoU of 56.9.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure9-1.png",
      "1805.04687v2-Figure11-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The approach that achieved the highest mean IoU for semantic segmentation was a combination of object detection, tracking, and instance segmentation, which outperformed the baseline Sem-Seg model by a significant margin. <s>"
  },
  "13": {
    "question": "Which training approach achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score?",
    "ground_truth": "The training approach \"Det + T + I + S\" achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The training approach that achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score, was to use a combination of supervised learning and unsupervised learning. <s>"
  },
  "14": {
    "question": "Which category of objects has the highest total number of annotations, and is there evidence that this category might be more challenging to annotate accurately?",
    "ground_truth": "The category with the highest total number of annotations is \"Masks,\" with 129K annotations. There is evidence that this category might be more challenging to annotate accurately because it also has the highest number of annotations in the \"Occluded\" subcategory, indicating that a large portion of these objects are partially hidden in the images.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure9-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure3-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "Lane markings have the highest total number of annotations, and there is evidence that this category might be more challenging to annotate accurately. <s>"
  },
  "15": {
    "question": "What percentage of occlusions last for more than 10 frames?",
    "ground_truth": "Approximately 80%",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure9-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Figure1-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure15-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The percentage of occlusions that last for more than 10 frames is not specified in the image. <s>"
  },
  "16": {
    "question": "Why are MOTS datasets like KITTI MOTS and MOTS Challenge smaller in size compared to VOS datasets like YouTube VOS, even though BDD100K MOTS has a comparable number of annotations?",
    "ground_truth": "MOTS datasets require denser annotations per frame because they involve both segmentation and tracking of multiple objects in crowded scenes. This results in smaller dataset sizes compared to VOS datasets, which typically focus on segmenting a single or fewer objects. ",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure9-1.png",
      "1805.04687v2-Figure11-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Figure1-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure3-1.png",
      "1805.04687v2-Figure15-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Figure6-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "MOTS datasets like KITTI MOTS and MOTS Challenge are typically smaller in size compared to VOS datasets like YouTube VOS, even though BDD100K MOTS has a comparable number of annotations. This is because MOTS datasets are specifically designed for the task of Multi-Object Tracking (MOT), while VOS datasets are focused on the task of Video Object Segmentation (VOS). \n\nMOTS datasets like KITTI MOTS and MOTS Challenge are typically smaller in size compared to VOS datasets like YouTube VOS, even though"
  },
  "17": {
    "question": "What are the three main geographical regions where the data for this study was collected?",
    "ground_truth": "New York, San Francisco Bay Area, and Berkeley.",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The data for this study was collected in three main geographical regions: Europe, North America, and Asia. <s>"
  },
  "18": {
    "question": "Which category of object is the least common in the dataset?",
    "ground_truth": "Train",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "Trains are the least common object in the dataset. <s>"
  },
  "19": {
    "question": "How does the BDD100K dataset compare to the KITTI and MOT17 datasets in terms of size and complexity?",
    "ground_truth": "The BDD100K dataset is significantly larger and more complex than both the KITTI and MOT17 datasets. It contains roughly 40 times more frames, 16 times more sequences, and 13 times more identities than KITTI. Compared to MOT17, BDD100K has about 10 times more frames, 80 times more sequences, and 8 times more identities. This increase in size and complexity makes BDD100K a more challenging and comprehensive benchmark for multiple object tracking algorithms. ",
    "selected_figures": [
      "1805.04687v2-Table4-1.png",
      "1805.04687v2-Figure9-1.png",
      "1805.04687v2-Figure11-1.png",
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Figure1-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure15-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Figure6-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The BDD100K dataset is significantly larger and more diverse than the KITTI and MOT17 datasets in terms of size and complexity. <s>"
  },
  "20": {
    "question": "Which dataset has the most lane marking annotations?",
    "ground_truth": "BDD100K",
    "selected_figures": [
      "1805.04687v2-Figure10-1.png",
      "1805.04687v2-Table7-1.png",
      "1805.04687v2-Table5-1.png",
      "1805.04687v2-Table6-1.png",
      "1805.04687v2-Table11-1.png",
      "1805.04687v2-Table10-1.png",
      "1805.04687v2-Table13-1.png",
      "1805.04687v2-Table14-1.png",
      "1805.04687v2-Figure4-1.png",
      "1805.04687v2-Figure2-1.png",
      "1805.04687v2-Figure14-1.png",
      "1805.04687v2-Table8-1.png",
      "1805.04687v2-Table9-1.png",
      "1805.04687v2-Table1-1.png",
      "1805.04687v2-Figure12-1.png",
      "1805.04687v2-Figure13-1.png",
      "1805.04687v2-Table12-1.png",
      "1805.04687v2-Figure8-1.png",
      "1805.04687v2-Table3-1.png",
      "1805.04687v2-Figure7-1.png",
      "1805.04687v2-Table2-1.png"
    ],
    "answer": "The dataset with the most lane marking annotations is 1805.04687v2. <s>"
  }
}