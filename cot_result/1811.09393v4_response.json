{
  "0": {
    "question": "Which method has the highest T-Diff on average for the Vid4 dataset?",
    "ground_truth": "TecoGAN.",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The method with the highest T-Diff on average for the Vid4 dataset is the UVT cycle link formed by two recurrent generators. <s>"
  },
  "1": {
    "question": "Which GAN model is able to generate the most realistic blinking motions?",
    "ground_truth": "TecoGAN",
    "selected_figures": [
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The TecoGAN model is able to generate the most realistic blinking motions. <s>"
  },
  "2": {
    "question": "Which of the VSR models in the figure achieves the best balance of spatial detail and temporal coherence?",
    "ground_truth": "TecoGAN",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure11-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The TecoGAN model achieves the best balance of spatial detail and temporal coherence in the video translations between renderings of smoke simulations and real-world captures for smokes. <s>"
  },
  "3": {
    "question": "Which method achieves the highest PSNR on the Vid4 data set?",
    "ground_truth": "DUF",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The method that achieves the highest PSNR on the Vid4 data set is the UVT cycle link formed by two recurrent generators. <s>"
  },
  "4": {
    "question": " What is the role of the warped triplets in the conditional VSR Ds,t?",
    "ground_truth": " The warped triplets provide additional information about the motion and appearance of the scene, which helps the VSR Ds,t to generate more accurate and realistic results.",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure11-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The role of the warped triplets in the conditional VSR Ds,t is to improve the quality of the video synthesis. <s>"
  },
  "5": {
    "question": "How does the PP loss improve the temporal coherence of the video sequence?",
    "ground_truth": "The PP loss constrains the output sequence to be symmetric by reducing the L2 distance between corresponding frames in the forward and backward passes. This helps to reduce drifting artifacts and improve temporal coherence.",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure11-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The Perceptual Loss (PP loss) improves the temporal coherence of the video sequence by minimizing the discrepancy between the generated video and the real-world video, resulting in a more realistic and coherent output. <s>"
  },
  "6": {
    "question": "What is the role of the Motion Compensation block in the Frame-Recurrent Generator?",
    "ground_truth": "The Motion Compensation block estimates the motion between the previous frame and the current frame, and uses this information to warp the previous frame to the current frame. This helps the generator to produce more realistic images by taking into account the temporal information in the video sequence.",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure11-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The Motion Compensation block in the Frame-Recurrent Generator is used to improve the quality of the generated video by compensating for the motion between frames. <s>"
  },
  "7": {
    "question": "Which method produces the least amount of artifacts?",
    "ground_truth": "TecoGAN\u2296.",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The TecoGAN model successfully removes the artifacts in the video translations between renderings of smoke simulations and real-world captures for smokes. <s>"
  },
  "8": {
    "question": "Which method produces the most realistic results for the Vid4 scenes?",
    "ground_truth": "TecoGAN.",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "1811.09393v4-Figure20-1.png: Tables and bar graphs of Bradley-Terry scores and standard errors for Vid4 VSR. 1811.09393v4-Figure18-1.png: Tables and visualization of perceptual metrics computed with PieAPP [Prashnani et al. 2018] (instead of LPIPS used in Fig. 14 previously) on ENet, FRVSR, DUF and TecoG"
  },
  "9": {
    "question": "What is the learning rate for the generator in the DsOnly model?",
    "ground_truth": "5.00E-05",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure11-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The learning rate for the generator in the DsOnly model is 0.0001. <s>"
  },
  "10": {
    "question": "Why does flow estimation become less accurate near image boundaries?",
    "ground_truth": "Flow estimation becomes less accurate near image boundaries because there is less information available to estimate the flow. This is because the pixels at the boundaries are only surrounded by pixels on one side, whereas pixels in the interior of the image are surrounded by pixels on all sides.",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure11-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The accuracy of flow estimation decreases near image boundaries due to the presence of discontinuities in the image data. These discontinuities can cause the flow estimation algorithm to become confused and produce less accurate results. <s>"
  },
  "11": {
    "question": "Which method has the best perceptual performance according to the tOF score?",
    "ground_truth": "TecoGAN.",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The TecoGAN model has the best perceptual performance according to the tOF score. <s>"
  },
  "12": {
    "question": "Which of the methods generated the sharpest details?",
    "ground_truth": "TecoGAN",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure10-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The TecoGAN method generated the sharpest details in the video translations between renderings of smoke simulations and real-world captures for smokes. <s>"
  },
  "13": {
    "question": "What is the purpose of the UVT cycle link?",
    "ground_truth": "The UVT cycle link is used to transfer knowledge between two recurrent generators.",
    "selected_figures": [
      "1811.09393v4-Figure14-1.png",
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure11-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure19-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The purpose of the UVT cycle link is to demonstrate the ability of the TecoGAN model to generate sharp details in both scenes. <s>"
  },
  "14": {
    "question": "What is the purpose of the user study?",
    "ground_truth": "The user study is designed to test which of two images is closer to a reference video.",
    "selected_figures": [
      "1811.09393v4-Figure13-1.png",
      "1811.09393v4-Table2-1.png",
      "1811.09393v4-Figure15-1.png",
      "1811.09393v4-Figure16-1.png",
      "1811.09393v4-Figure12-1.png",
      "1811.09393v4-Table4-1.png",
      "1811.09393v4-Table5-1.png",
      "1811.09393v4-Figure2-1.png",
      "1811.09393v4-Figure23-1.png",
      "1811.09393v4-Figure4-1.png",
      "1811.09393v4-Table6-1.png",
      "1811.09393v4-Table3-1.png",
      "1811.09393v4-Figure11-1.png",
      "1811.09393v4-Figure5-1.png",
      "1811.09393v4-Figure18-1.png",
      "1811.09393v4-Figure20-1.png"
    ],
    "answer": "The purpose of the user study is to evaluate the quality of the video super-resolution (VSR) generated by various models. <s>"
  }
}