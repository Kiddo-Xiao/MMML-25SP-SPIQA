image_path,true_label,predicted_label,true_answer,predicted_answer,model_name
OverheatData/OverheatData\1901.00056v2\1901.00056v2-Figure2-1.png,511,61," The four steps involved in the synonym discovery process are: 

1. **Entity representation learning:** Learn entity representations from the corpus using WEMBED.
2. **NN search:** Perform a nearest neighbor search to find candidate entities for the query entity.
3. **Synonym score calculation:** Calculate the synonym score between the query entity and each candidate entity using SYNONYM NET.
4. **Synonym entity discovery:** Select the candidate entities with the highest synonym scores as the discovered synonym entities."," The frequency reduction process takes a high-frequency trajectory and samples it at a lower frequency, resulting in multiple trajectories with different starting and ending points. ",resnet18
OverheatData/OverheatData\1709.08294v3\1709.08294v3-Table3-1.png,626,56,The two-way AdaQA model significantly outperforms the one-way AdaQA model and all other CNN-based baseline models on the WikiQA dataset. This is evident from the higher MAP and MRR values achieved by the two-way model (0.7107 and 0.7304 respectively) compared to the one-way model (0.7005 and 0.7161) and the baseline models.,The complexity of the cake shape generally leads to a higher minimum number of blanks required for a complete partition.,resnet18
OverheatData/OverheatData\1803.01128v3\1803.01128v3-Table1-1.png,76,129,"Seq2Sick differs from existing attack methods in two key aspects:

1. Search Strategy: While previous methods primarily rely on greedy search, which becomes increasingly inefficient for longer sequences, Seq2Sick employs group lasso regularization and projected gradient descent with gradient regularization. This allows for simultaneous searching of all replacement positions, leading to improved efficiency.

2. Targeted Attack Type: Existing methods focus on targeting specific classes or binary classifications, while Seq2Sick introduces a novel ""keyword"" target type, allowing attacks to be directed towards specific keywords within the generated sequence.","The LSTNet model has four main types of layers:

1. Convolutional layer: This layer extracts local dependency patterns from the input data. 
2. Recurrent and recurrent-skip layer: These layers capture long-term dependencies in the data. 
3. Fully connected and element-wise sum output layer: This layer combines the outputs from the convolutional and recurrent layers to produce the final prediction.
4. Autoregressive layer: This layer provides a linear bypass to the non-linear neural network part of the model. 

The convolutional layer receives the input data and passes its output to the recurrent and recurrent-skip layers. These layers then pass their output to the fully connected and element-wise sum output layer. The autoregressive layer receives the input data directly and its output is also fed into the fully connected and element-wise sum output layer.",resnet18
OverheatData/OverheatData\1708.03797v1\1708.03797v1-Figure1-1.png,583,237,The code layer is responsible for generating a compressed representation of the input data. This compressed representation is then used by the decoder to reconstruct the original data.,The quality of gradient estimation increases as the number of state-action pairs used in estimation increases.,resnet18
OverheatData/OverheatData\1704.07854v4\1704.07854v4-Figure15-1.png,218,111,The corrected gradient method leads to a more stable and lower loss value during training.,The RR optimization helps to reduce the number of messages that need to be exchanged between replicas.,resnet18
OverheatData/OverheatData\1704.08615v2\1704.08615v2-Figure6-1.png,231,12,The CC score increases as the number of fixations increases.,DMRNet,resnet18
OverheatData/OverheatData\1706.00633v4\1706.00633v4-Table3-1.png,393,14,RCE,Yes.,resnet18
OverheatData/OverheatData\1811.09393v4\1811.09393v4-Figure19-1.png,359,454,The user study is designed to test which of two images is closer to a reference video.,Action Search,resnet18
OverheatData/OverheatData\1809.03149v2\1809.03149v2-Figure2-1.png,597,453,The Higher Level Policy sets constraints for the next sub-trajectory and provides information about the previous stage to the Lower Level Policy.,"The three main components of the Action Search model architecture are the visual encoder, the LSTM, and the spotting target.",resnet18
OverheatData/OverheatData\1809.02731v3\1809.02731v3-Table1-1.png,593,277,"The UMBC News corpus has more sentences, by approximately 60.5 million.",Explicitly modeling meaning-preserving invariances leads to the generation of better paraphrases.,resnet18
OverheatData/OverheatData\1811.08257v1\1811.08257v1-Figure1-1.png,296,570,The activation layer applies a non-linear function to the output of the convolution layer. This allows the network to learn more complex features from the data.,"LIME appears to place the most emphasis on specific, localized features.",resnet18
OverheatData/OverheatData\1704.07854v4\1704.07854v4-Figure2-1.png,222,225,The parameter network is used to infer a weighting function.,"Documents are first matched using a pre-defined match plan. Then, they are passed through additional rank-and-prune stages, which are implemented as a cascade of machine learning models.",resnet18
OverheatData/OverheatData\1703.07015v3\1703.07015v3-Figure3-1.png,124,72,The Traffic dataset.,One Step ALOQ,resnet18
OverheatData/OverheatData\1804.00863v3\1804.00863v3-Figure1-1.png,189,370,The sphere in the re-synthesis using DAMs appears to have a more even and consistent surface texture than the reference image.,The segmentation model learns to interpolate in areas that have no lane markings.,resnet18
OverheatData/OverheatData\1804.07931v2\1804.07931v2-Figure2-1.png,279,491,The two auxiliary tasks are CTR and CTCVR.,The performance of all models is generally lower on Rest15 because it has a larger number of aspect categories (13) compared to Rest14 (5). This increased complexity makes it more challenging for the models to accurately identify and classify the aspects.,resnet18
OverheatData/OverheatData\1809.00263v5\1809.00263v5-Figure4-1.png,544,601,The residual connections add the output of the previous layer to the input of the next layer. This helps to improve the flow of information through the network and can help to prevent vanishing gradients.,The test Top-1 error rate decreases as the budget in FLOPS increases for all three training strategies.,resnet18
OverheatData/OverheatData\1708.01425v4\1708.01425v4-Table2-1.png,565,169,Intra-warrant attention with context.,The performance of the model increases with increasing bounding box area and decreases with increasing similarity of the concept with ImageNet classes.,resnet18
OverheatData/OverheatData\1703.10730v2\1703.10730v2-Figure13-1.png,138,140,The input patches are used to generate the images. The generator network takes the input patches as input and generates new images that are similar to the input patches.,"The presence of noise in the input image can degrade the quality of the generated images, but the proposed algorithm is still able to generate realistic images even with a certain amount of noise.",resnet18
OverheatData/OverheatData\1803.01128v3\1803.01128v3-Table4-1.png,78,460,The difficulty of performing a successful targeted keywords attack increases as the number of targeted keywords increases.,The proposed method performs the best on average across all noise levels tested on the Kodak dataset.,resnet18
OverheatData/OverheatData\1707.08608v3\1707.08608v3-Table4-1.png,535,308,Beam search with a width of 9 consistently leads to the highest F1 score on the failure set across all three networks.,"The topic with the highest internal coherence value is ""turks armenian armenia turkish roads escape soviet muslim mountain soul"".",resnet18
OverheatData/OverheatData\1710.05654v2\1710.05654v2-Figure5-1.png,614,233,The log model.,"As the number of state-action pairs increases, the reward landscape for both the surrogate and true reward functions becomes smoother and more accurate.",resnet18
OverheatData/OverheatData\1804.05936v2\1804.05936v2-Figure1-1.png,248,235,The GRU is used to process the ranked list of documents provided by a global ranking function.,TRPO generally converges faster to the true gradient than PPO.,resnet18
OverheatData/OverheatData\1704.08615v2\1704.08615v2-Figure5-1.png,232,484,The fixation density map predicts the probability of a person fixating on a particular location in the image. The ground truth fixations are the actual locations where people fixated on the image.,FLoss,resnet18
OverheatData/OverheatData\1811.02721v3\1811.02721v3-Table9-1.png,214,313,"The TCP stack presented in this paper (TCPlp) provides the most complete implementation of core TCP features, including flow control, congestion control, RTT estimation, MSS option, OOO reassembly, and various advanced features like timestamps and selective ACKs. In contrast, BLIP lacks the most features, as it does not implement congestion control, RTT estimation, or several other functionalities present in other stacks.","Seq2Seq and Mem2Seq models performed poorly because they struggled to capture the semantic representations of unseen entities. This means they couldn't understand the meaning and relationships of new restaurants introduced in the KB. As a result, they were unable to accurately identify the correct restaurant and provide its address when faced with unseen entities.",resnet18
OverheatData/OverheatData\1809.02731v3\1809.02731v3-Table3-1.png,594,308,The Bijective model performs the best on the STS16 task with unsupervised training.,"The topic with the highest internal coherence value is ""turks armenian armenia turkish roads escape soviet muslim mountain soul"".",resnet18
OverheatData/OverheatData\1802.07459v2\1802.07459v2-Figure2-1.png,103,453,"The different stages involved in constructing the Concept Interaction Graph (CIG) from a pair of documents are: (a) Representation, (b) Encoding, (c) Transformation, and (d) Aggregation.","The three main components of the Action Search model architecture are the visual encoder, the LSTM, and the spotting target.",resnet18
OverheatData/OverheatData\1812.00281v3\1812.00281v3-Table8-1.png,439,123,Training with UP-3D + HUMBI resulted in the lowest prediction error for both UP-3D and HUMBI test sets.,"When the sample size is 2000, the two-phase framework (MSG) achieves lower discrimination in prediction compared to DI, both with and without classifier tweaking.

With classifier tweaking: MSG achieves a discrimination level of 0.016 ± 5.3E-4, while DI shows a significantly higher level of 0.095 ± 1.6E-3.
Without classifier tweaking: MSG still demonstrates lower discrimination with 0.067 ± 4.3E-3 compared to DI's 0.095 ± 1.6E-3.

This indicates that the two-phase framework is more effective in removing discrimination from predictions than DI, regardless of whether classifier tweaking is applied.",resnet18
OverheatData/OverheatData\1811.10673v1\1811.10673v1-Figure2-1.png,399,237,The second-stage decoder $D_2$ takes soft edges $x_G$ as input and produces reconstructed frames.,The quality of gradient estimation increases as the number of state-action pairs used in estimation increases.,resnet18
OverheatData/OverheatData\1809.01989v2\1809.01989v2-Table1-1.png,646,258,"The Ridge method achieved the lowest sum of absolute percentage errors (136.84), indicating the highest tracking accuracy in terms of minimizing absolute deviations from the index. However, this doesn't necessarily translate to the best overall performance.",The model with convolutional self-correction consistently outperforms the model with no self-correction as the number of images in set $\mathcal{F}$ increases.,resnet18
OverheatData/OverheatData\1809.01246v1\1809.01246v1-TableI-1.png,580,90,GSS (no sampling),Removing the normalization in the relation modules had the most significant negative impact on performance for the KITTI 2015 dataset.,resnet18
OverheatData/OverheatData\1803.06506v3\1803.06506v3-Figure2-1.png,164,262,The Joint Attention Module takes the embedded image and phrase features as input and uses them to induce a parameterization for spatial attention. This spatial attention map is then used by the decoder to predict the common concept.,The bounding box encoder network embeds bounding box information at different scales and outputs attention maps that are used to fuse with feature maps from the encoder before being passed to the decoder.,resnet18
OverheatData/OverheatData\1710.06177v2\1710.06177v2-Table1-1.png,634,492,"VAGER+Voting consistently outperforms all other VAGER variants in both 1-shot and 20-shot settings, achieving the highest AUC and F1 scores.","For binary classification on the Rest15 dataset, M-CAN-2$R_o$ achieved the highest performance with an accuracy of 82.14% and Macro-F1 of 81.58%. In comparison, the best performing model for 3-way classification on Rest15 was M-CAN-2$R_s$, achieving an accuracy of 78.22% and Macro-F1 of 55.80%. This indicates that M-CAN-2$R_o$ performed better in both accuracy and Macro-F1 for binary classification compared to the best model for 3-way classification on the same dataset.",resnet18
OverheatData/OverheatData\1812.10735v2\1812.10735v2-Table1-1.png,490,292,Rest14 has a higher proportion of sentences containing multiple aspects compared to Rest15.,FALCON,resnet18
OverheatData/OverheatData\1605.07496v3\1605.07496v3-Figure3-1.png,66,573,ALOQ.,The PE-N=5 sampler performs better than the HMC sampler.,resnet18
OverheatData/OverheatData\1708.03797v1\1708.03797v1-Table2-1.png,584,337,HDMF achieved the best overall performance.,UnCoRd-None-B.,resnet18
OverheatData/OverheatData\1805.06431v4\1805.06431v4-Figure14-1.png,422,586,The WideResNet model has higher accuracy than the ChoiceNet model on the CIFAR-10 dataset with 50% random shuffle.,The processing time of SRU is significantly faster than that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3.,resnet18
OverheatData/OverheatData\1704.05426v4\1704.05426v4-Table3-1.png,158,147,"The genre with the highest percentage of 'S' parses is **9/11**, with **99%** of its sentences receiving this parse. This is higher than the overall average for the MultiNLI corpus, which sits at **91%**.",The AUC of RippleNet first increases and then decreases with the increase of the dimension of embedding.,resnet18
OverheatData/OverheatData\1705.09966v2\1705.09966v2-Figure13-1.png,344,191,The input is a low-resolution frontal face image and a high-resolution side-face image. The output is a high-resolution frontal face image.,The reconstructions are very similar to the original samples.,resnet18
OverheatData/OverheatData\1809.00263v5\1809.00263v5-Figure6-1.png,545,49,The sampled vector is element-wise multiplied by the feature map of $\sigma$ and added to the feature map of $\mu$.,RCV1,resnet18
OverheatData/OverheatData\1710.05654v2\1710.05654v2-Figure15-1.png,610,152,The time needed for learning a graph with a subset of allowed edges $\mathcal{E}^\text{allowed}$ increases linearly as the number of edges per node increases.,SPARTan converged faster in both cases of target rank.,resnet18
OverheatData/OverheatData\1805.06431v4\1805.06431v4-Table2-1.png,424,90,"ChoiceNet generally performs well compared to other methods, achieving the highest accuracy on both symmetric noise settings (sym-50% and sym-20%). However, it falls to second place under the Pair-45% asymmetric noise setting, indicating a weakness in handling this specific type of noise.",Removing the normalization in the relation modules had the most significant negative impact on performance for the KITTI 2015 dataset.,resnet18
OverheatData/OverheatData\1703.10730v2\1703.10730v2-Figure7-1.png,137,467,"The network initially focuses on predicting a good mask. As the epoch increases, the input parts become sharper. Finally, the network concentrates on generating realistic images."," The Zhou \textit{et al.} method suffers from a ""zoom-in-and-out"" effect, while the Chen \textit{et al.} method produces lip shapes that differ from the real ones.",resnet18
OverheatData/OverheatData\1702.03584v3\1702.03584v3-Figure2-1.png,99,630,SPIRAL-DTW-kMeans performs better than k-Shape and CLDS on most datasets.,The AdaQA model generates context-aware filters through the filter generation module. This module takes the question and answer as input and outputs a set of filters that are specific to the question and answer pair.,resnet18
OverheatData/OverheatData\1805.06431v4\1805.06431v4-Table15-1.png,421,570,"When there is no label corruption (p = 0%), Mixup achieves the highest test accuracy of 79.77%. However, as the corruption level increases, Mixup's performance deteriorates more rapidly compared to other methods. ChoiceNet, on the other hand, demonstrates a more stable performance across different corruption levels, maintaining the highest accuracy when p is 10%, 20%, 30%, and 40%.","LIME appears to place the most emphasis on specific, localized features.",resnet18
OverheatData/OverheatData\1804.05936v2\1804.05936v2-Table4-1.png,252,229,"LambdaMART initial list, DLCM model, and AttRank loss function achieved the best overall performance on the Yahoo! set 1, with an nDCG@10 of 0.743 and an ERR@10 of 0.453.",The saliency map method with the highest sAUC score is **SIM**. Its sAUC score appears to be significantly higher than all other methods listed in the table.,resnet18
OverheatData/OverheatData\1809.00458v1\1809.00458v1-TableI-1.png,554,543,"The Jaccard similarity measures the overlap between two sets, while the containment similarity measures how much one set is contained within another set.",The sliding tendency of SepConv will cause motion errors and high LMS.,resnet18
OverheatData/OverheatData\1804.07849v4\1804.07849v4-Table3-1.png,256,324,Variational J^var (7),"Both TF-IDF and BM25 are features used to estimate the relevance of a document to a query. However, they differ in their underlying calculations.

TF-IDF: This feature represents the average product of term frequency (TF) and inverse document frequency (IDF) for each query term within different document sections (URL, title, content, and whole document). TF measures how often a term appears in a specific document section, while IDF measures how important that term is across the entire document collection.

BM25: This feature utilizes the BM25 ranking function, which is a probabilistic model that considers term frequency, document length, and average document length to estimate relevance. While it also considers term frequency like TF-IDF, it incorporates additional factors to improve the weighting scheme.",resnet18
OverheatData/OverheatData\1804.05936v2\1804.05936v2-Figure4-1.png,251,273,LambdaMART,Precision generally decreases and recall generally increases as k increases.,resnet18
OverheatData/OverheatData\1805.04687v2\1805.04687v2-Table1-1.png,387,383,BDD100K,"MOTS datasets require denser annotations per frame because they involve both segmentation and tracking of multiple objects in crowded scenes. This results in smaller dataset sizes compared to VOS datasets, which typically focus on segmenting a single or fewer objects. ",resnet18
OverheatData/OverheatData\1805.07567v2\1805.07567v2-Table4-1.png,482,492,"The FLoss method performs better than the balanced cross-entropy loss because it can automatically adjust to data imbalance using the F-measure criterion, while the balanced cross-entropy loss relies on pre-defined weights for positive and negative samples.","For binary classification on the Rest15 dataset, M-CAN-2$R_o$ achieved the highest performance with an accuracy of 82.14% and Macro-F1 of 81.58%. In comparison, the best performing model for 3-way classification on Rest15 was M-CAN-2$R_s$, achieving an accuracy of 78.22% and Macro-F1 of 55.80%. This indicates that M-CAN-2$R_o$ performed better in both accuracy and Macro-F1 for binary classification compared to the best model for 3-way classification on the same dataset.",resnet18
OverheatData/OverheatData\1705.02946v3\1705.02946v3-Table1-1.png,267,229,O(n^3 / ε),The saliency map method with the highest sAUC score is **SIM**. Its sAUC score appears to be significantly higher than all other methods listed in the table.,resnet18
OverheatData/OverheatData\1706.03847v3\1706.03847v3-Table2-1.png,446,429,The highest Recall@20 score was achieved by the GRU4Rec with additional samples and BPR-max loss function on the RSC15 dataset. This score was 42.37% higher than the Recall@20 score of the original GRU4Rec model on the same dataset.,The accuracy of the Mixup method decreases as the level of random shuffle increases.,resnet18
OverheatData/OverheatData\1804.07849v4\1804.07849v4-Table4-1.png,255,408,"The Variational $\wh{J}^{\mathrm{var}}$ method achieved the highest average VM score (50.4). Its average score is 39.6 points higher than the Baum-Welch method, which achieved an average VM score of 10.8.",Multi-X,resnet18
OverheatData/OverheatData\1803.04572v2\1803.04572v2-Figure8-1.png,151,235,"The temporal patterns of phenotype magnitude differ between sickle cell anemia and leukemia patients in terms of both shape and magnitude. For sickle cell anemia patients, the patterns are generally smoother and more periodic, with lower overall magnitude. For leukemia patients, the patterns are more erratic and have higher overall magnitude.",TRPO generally converges faster to the true gradient than PPO.,resnet18
OverheatData/OverheatData\1705.10667v4\1705.10667v4-Figure3-1.png,363,105,CDAN-fg,The C-Tarone method has higher precision and F-measure than the binarization method in all datasets. The C-Tarone method has better or competitive recall than the binarization method. The running time of the C-Tarone method is competitive with the binarization method.,resnet18
OverheatData/OverheatData\1705.09882v2\1705.09882v2-Figure5-1.png,331,171,The proposed RGB-to-Depth transfer performs slightly better than Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned.,"The Silver Snatch and the Gold Snatch are positively correlated. As the Gold Snatch increases, the Silver Snatch also increases.",resnet18
OverheatData/OverheatData\1805.04687v2\1805.04687v2-Table8-1.png,379,233,"The **Sem-Seg + Det** approach achieved the highest mean IoU of 58.3, which is an improvement of 1.4 points compared to the baseline Sem-Seg model with a mean IoU of 56.9.","As the number of state-action pairs increases, the reward landscape for both the surrogate and true reward functions becomes smoother and more accurate.",resnet18
OverheatData/OverheatData\1706.00633v4\1706.00633v4-Figure1-1.png,395,129," The non-ME metric measures the entropy of the normalized non-maximal elements in the final hidden vector of the classifier. Adversarial examples often have low non-ME values, indicating that they are close to the decision boundary and have high confidence in the incorrect class.","The LSTNet model has four main types of layers:

1. Convolutional layer: This layer extracts local dependency patterns from the input data. 
2. Recurrent and recurrent-skip layer: These layers capture long-term dependencies in the data. 
3. Fully connected and element-wise sum output layer: This layer combines the outputs from the convolutional and recurrent layers to produce the final prediction.
4. Autoregressive layer: This layer provides a linear bypass to the non-linear neural network part of the model. 

The convolutional layer receives the input data and passes its output to the recurrent and recurrent-skip layers. These layers then pass their output to the fully connected and element-wise sum output layer. The autoregressive layer receives the input data directly and its output is also fed into the fully connected and element-wise sum output layer.",resnet18
OverheatData/OverheatData\1709.02755v5\1709.02755v5-Table2-1.png,587,619,The SRU model outperforms the LSTM model in both accuracy and training speed on the SQuAD dataset.,"The Entropy Balancing (EB) estimator performs best across all measures (Bias, MAE, and MSE) when confounders are noisy. While the CC estimator also performs well, it exhibits slightly higher bias and MAE compared to EB.",resnet18
OverheatData/OverheatData\1809.04276v2\1809.04276v2-Table1-1.png,640,44,"The model is discouraged because it is trained using the Maximum Likelihood Estimation (MLE) objective, which prioritizes generating responses that are identical to the ground-truth (GT) response. Even though the RSP integrates relevant content from the candidates and seems appropriate in the context, it is penalized because it deviates from the exact wording of the GT.","The ""adaptive $\power \in (0, 2)$"" strategy, where each wavelet coefficient has its own shape parameter that is optimized during training, achieved the best performance in terms of average error. It reduced the average error by approximately 17% compared to the reproduced baseline.",resnet18
OverheatData/OverheatData\1803.04572v2\1803.04572v2-Table7-1.png,149,129,"According to the table, some common medications used to treat Sickle Cell Anemia include:

Beta-adrenergic agents
Analgesics (narcotics and non-narcotics)
NSAIDs (cyclooxygenase inhibitor - type)
Potassium replacement
Sodium/saline preparations
General inhalation agents
Laxatives and cathartics
IV solutions (dextrose-saline)
Antiemetic/antivertigo agents
Sedative-hypnotics (non-barbiturate)
Glucocorticoids (orally inhaled)
Folic acid preparations
Analgesic narcotic anesthetic adjunct agents","The LSTNet model has four main types of layers:

1. Convolutional layer: This layer extracts local dependency patterns from the input data. 
2. Recurrent and recurrent-skip layer: These layers capture long-term dependencies in the data. 
3. Fully connected and element-wise sum output layer: This layer combines the outputs from the convolutional and recurrent layers to produce the final prediction.
4. Autoregressive layer: This layer provides a linear bypass to the non-linear neural network part of the model. 

The convolutional layer receives the input data and passes its output to the recurrent and recurrent-skip layers. These layers then pass their output to the fully connected and element-wise sum output layer. The autoregressive layer receives the input data directly and its output is also fed into the fully connected and element-wise sum output layer.",resnet18
OverheatData/OverheatData\1701.06171v4\1701.06171v4-Figure2-1.png,120,247,The variables in the Compositional Active Basis Model are hierarchically dependent. The variables at each layer are dependent on the variables at the layer above it.,The perfect results received the largest promotions in rank.,resnet18
OverheatData/OverheatData\1706.00827v2\1706.00827v2-Table1-1.png,407,253,Multi-X achieved the most accurate results for simultaneous line and circle fitting.,"The Variational $\wh{J}^{\mathrm{var}}$ method achieved the highest accuracy of 78.1% on the 45-tag Penn WSJ dataset. This is significantly higher than all other methods listed in the table, with the next best performing method (Berg-Kirkpatrick et al., 2010) achieving an accuracy of 74.9%.",resnet18
OverheatData/OverheatData\1705.09882v2\1705.09882v2-Table1-1.png,325,259,"The proposed method with RTA attention achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset with a score of 50.0%. This is significantly higher than the best single-shot method on the same dataset, which is our method (CNN) with a score of 25.4%.","The Conv. Self-Corr. method achieved the highest performance on the PASCAL VOC 2012 test set with a score of 82.72. This is approximately 1.11 points higher than the baseline model (""No Self-Corr."") which achieved a score of 81.61.",resnet18
OverheatData/OverheatData\1811.02721v3\1811.02721v3-Table8-1.png,211,213,"The protocol implementation module consumes the most memory in the active RAM on TinyOS, utilizing 488 bytes.","The posix_sockets module consistently uses less memory than the combined usage of the protocol and socket layer. For an active connection, it requires about 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers. Similarly, for a passive connection, it uses 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers.",resnet18
OverheatData/OverheatData\1812.06589v2\1812.06589v2-Table3-1.png,462,463,"Adding DA to the baseline method improves the PSNR and SSIM values, while slightly decreasing the LMD value.",AMIE (Ours),resnet18
OverheatData/OverheatData\1612.02803v5\1612.02803v5-Figure1-1.png,73,308,"The equation that describes the motion of a mass attached to a spring is:
```
m d^2 X / dt^2 + kX = 0
```
where:
* m is the mass of the object
* X is the displacement of the object from its equilibrium position
* k is the spring constant
* t is time","The topic with the highest internal coherence value is ""turks armenian armenia turkish roads escape soviet muslim mountain soul"".",resnet18
OverheatData/OverheatData\1611.04684v1\1611.04684v1-Table1-1.png,0,310,"The Bonaparte school focuses on outdoor physical activities, maneuvers, and strategies, with a specialization in horse riding, lances, and swords. They aim to develop students into good leaders. The Voltaire school, on the other hand, encourages independent thinking and focuses on indoor activities. They aim to instill good moral values and develop students into philosophical thinkers.","""english language city spanish community""",resnet18
OverheatData/OverheatData\1812.00281v3\1812.00281v3-Figure15-1.png,436,350,The results of the monocular 3D body prediction network trained with different dataset combinations show that the Up3d+HUMBI dataset combination produces the most accurate results. This is evident in the images where the predicted 3D body poses are closer to the ground-truth poses than the other dataset combinations.,TecoGAN,resnet18
OverheatData/OverheatData\1707.06320v2\1707.06320v2-Table2-1.png,526,383,"GroundSent-Cap appears to be most beneficial for the MRPC task, achieving an accuracy of 72.9/82.2 compared to the baseline model ST-LN's 69.6/81.2.","MOTS datasets require denser annotations per frame because they involve both segmentation and tracking of multiple objects in crowded scenes. This results in smaller dataset sizes compared to VOS datasets, which typically focus on segmenting a single or fewer objects. ",resnet18
OverheatData/OverheatData\1611.03780v2\1611.03780v2-Figure1-1.png,17,55,"The Hilbert space-filling curve is constructed recursively. The curve starts with a simple square, and then at each subsequent iteration, the curve is subdivided into four smaller squares. The curve is then drawn through each of these squares in a specific order.",The blank space labeled Z'5 is used to complete the allocation of the original pieces.,resnet18
OverheatData/OverheatData\1803.01128v3\1803.01128v3-Table6-1.png,80,278,"No, adversarial examples generated with the 2-keyword constraint deviate significantly from the original syntactic structure.","The Text-to-parse model performs the best at predicting the delexicalised constituency tree, achieving an unlabelled F1 score of 87.5. This is significantly higher than the baseline Unconditional model, which achieves an unlabelled F1 score of 38.5. The Text-to-parse model therefore performs approximately 49 points better than the baseline.",resnet18
OverheatData/OverheatData\1703.04887v4\1703.04887v4-Table2-1.png,133,641,"BR-CSGAN consistently outperforms MRT on both Chinese-English and English-German translation tasks, achieving higher BLEU scores.

While both methods optimize similar objectives, BR-CSGAN uses a reinforcement learning procedure with a dynamic discriminator to maximize rewards for the generator. This dynamic feedback seems to be more effective than the static objective and random sampling approach used by MRT, leading to better translation performance.",The discriminator in the author's approach achieves higher accuracy (95.72%) compared to the conventional discriminator in AL (94.01%).,resnet18
OverheatData/OverheatData\1706.04269v2\1706.04269v2-Figure5-1.png,455,641,Action Search uses temporal context to reason about where to search next by looking at the frames before and after the current frame. This allows the model to learn the temporal patterns of actions and to predict where the action is most likely to occur in the next frame.,The discriminator in the author's approach achieves higher accuracy (95.72%) compared to the conventional discriminator in AL (94.01%).,resnet18
OverheatData/OverheatData\1901.00056v2\1901.00056v2-Table5-1.png,507,294,The range of values for the context number hyperparameter is from 1 to 20.,FALCON is faster for both setting up and running the FC layer.,resnet18
OverheatData/OverheatData\1803.04383v2\1803.04383v2-Figure5-1.png,178,112,The maximum profit criteria ($\maxprof$) results in the highest loan approval rate for the Black group when the loss/profit ratio is -4.,The average metadata required per node for the Op-based BP+RR approach increases as the number of nodes in the network increases.,resnet18
OverheatData/OverheatData\1707.00524v2\1707.00524v2-Figure3-1.png,487,183,"The predicted frame is generated by the prediction model, while the reconstructed frame is generated by the autoencoder. The predicted frame is typically more accurate than the reconstructed frame, as the prediction model is trained to predict the future state of the environment, while the autoencoder is only trained to reconstruct the input image.","The shortcuts in the Visual7W dataset can be remedied by creating alternative decoys that are more likely to be correct, based on either the image or the question alone. This forces the machine to consider all of the information together in order to select the correct answer.",resnet18
OverheatData/OverheatData\1701.03077v10\1701.03077v10-Figure8-1.png,45,235,"The authors chose to use a nonlinearity to curve α before fitting the cubic hermite spline because it allows for increased knot density near α = 2 and decreased knot density when α > 4. This helps to better approximate the log partition function, which is difficult to evaluate for arbitrary inputs.",TRPO generally converges faster to the true gradient than PPO.,resnet18
OverheatData/OverheatData\1708.01425v4\1708.01425v4-Table1-1.png,567,308,"Step 4, Reason disambiguation.","The topic with the highest internal coherence value is ""turks armenian armenia turkish roads escape soviet muslim mountain soul"".",resnet18
OverheatData/OverheatData\1901.00056v2\1901.00056v2-Table4-1.png,513,186,The largest performance gap is observed in the PubMed + UMLS dataset using the F1@K metric with K=1.,"The MLP-IQA model performs the best when considering the combined accuracy of identifying irrelevant image-question and question-answer pairs, achieving an accuracy of 75.9%. This is significantly higher than the MLP-A model, which only observes answers and achieves a combined accuracy of 26.6%, close to random performance.",resnet18
OverheatData/OverheatData\1705.07164v8\1705.07164v8-Table1-1.png,271,492,"The Euclidean Bregman cost function is simply the squared difference between two points, while the Mahalanobis Bregman cost function takes into account the covariance of the data.","For binary classification on the Rest15 dataset, M-CAN-2$R_o$ achieved the highest performance with an accuracy of 82.14% and Macro-F1 of 81.58%. In comparison, the best performing model for 3-way classification on Rest15 was M-CAN-2$R_s$, achieving an accuracy of 78.22% and Macro-F1 of 55.80%. This indicates that M-CAN-2$R_o$ performed better in both accuracy and Macro-F1 for binary classification compared to the best model for 3-way classification on the same dataset.",resnet18
OverheatData/OverheatData\1803.04572v2\1803.04572v2-Figure3-1.png,148,128,The smoothness constraint on $\M{U_k}$ has the most significant impact on the FIT values for the CMS data set when the target rank is 15.,The solar dataset has the highest temporal resolution.,resnet18
OverheatData/OverheatData\1811.09393v4\1811.09393v4-Figure2-1.png,354,269,"The Motion Compensation block estimates the motion between the previous frame and the current frame, and uses this information to warp the previous frame to the current frame. This helps the generator to produce more realistic images by taking into account the temporal information in the video sequence.","It is difficult to say definitively which model performs better based on the training curves alone. However, it appears that the WGAN(g) model may be performing better than the other models, as its generator and discriminator losses are both lower than the other models.",resnet18
OverheatData/OverheatData\1701.03077v10\1701.03077v10-Figure13-1.png,46,510,The range of values for the shape parameter α is from 0 to 2.,MedBook + MKG,resnet18
OverheatData/OverheatData\1811.09393v4\1811.09393v4-Figure4-1.png,352,390," The warped triplets provide additional information about the motion and appearance of the scene, which helps the VSR Ds,t to generate more accurate and realistic results.",The most effective attack method at reducing the accuracy of the Resnet-32 model on the MNIST dataset is BIM/CE.,resnet18
OverheatData/OverheatData\1809.01246v1\1809.01246v1-Figure11-1.png,577,579,"The ARE of node queries generally decreases as the width increases for all configurations of GSS and TCM. However, there are some fluctuations in the ARE for some configurations.",The graph for the Caida-networkflow dataset shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method.,resnet18
OverheatData/OverheatData\1708.05239v3\1708.05239v3-Figure9-1.png,572,171,PE-HMC (N=5),"The Silver Snatch and the Gold Snatch are positively correlated. As the Gold Snatch increases, the Silver Snatch also increases.",resnet18
OverheatData/OverheatData\1707.01922v5\1707.01922v5-Figure1-1.png,517,286,The task-irrelevant data is used to simulate the RGB representation using the gray scale image. This allows ZDDA to learn a joint network that can be used to classify digits in both the gray scale and RGB domains.,PC improves the localization ability of a CNN.,resnet18
OverheatData/OverheatData\1809.00458v1\1809.00458v1-Figure19-1.png,559,316,The running time of GB-KM increases as the F-1 score increases.,The encoder understands the last user utterance by using the memory cell representations of the dialog history and KB tuples.,resnet18
OverheatData/OverheatData\1603.03833v4\1603.03833v4-Table1-1.png,60,107,"The passage mentions that additional trajectories were generated for the ""Pick and Place"" task by reducing the frequency of the recorded demonstrations. This process was not applied to the ""Push to Pose"" task, therefore no ""Demonstrations after shift"" are listed for it.","The ""wdbc"" dataset would likely require the most computational resources for C-Tarone to analyze.",resnet18
OverheatData/OverheatData\1809.00458v1\1809.00458v1-Figure18-1.png,551,29,GB-KMV,The singular value decomposition step is used to find the projection matrices U and V.,resnet18
OverheatData/OverheatData\1701.03077v10\1701.03077v10-Figure17-1.png,51,240,The proposed method appears to be more accurate than the baseline method. The depth maps generated by the proposed method are more detailed and realistic than those generated by the baseline method.,"The sequential generation scheme generates each frame of the video independently, while the recurrent generation scheme uses the previous frame to generate the next frame. This can be seen in the optical flow images, which show the motion of pixels between frames. The recurrent scheme has a smoother flow of motion, while the sequential scheme has more abrupt changes.",resnet18
OverheatData/OverheatData\1901.00056v2\1901.00056v2-Figure1-1.png,512,595,The Leaky Unit helps to aggregate the context information from different sources and allows the model to learn the relationships between entities and their contexts.,"The relationship between position and CTR is complex and non-linear. In general, CTR decreases as position increases, but there are also local peaks and valleys in the CTR curve. This suggests that there are other factors besides position that affect CTR.",resnet18
OverheatData/OverheatData\1802.07351v2\1802.07351v2-Figure3-1.png,93,53,The relation module (Rt) is responsible for capturing the spatial relationships between the features extracted from the first and second images.,The most network-related reboots occurred between 18:00 and 20:00.,resnet18
OverheatData/OverheatData\1707.01917v2\1707.01917v2-Table5-1.png,499,324,TFBA,"Both TF-IDF and BM25 are features used to estimate the relevance of a document to a query. However, they differ in their underlying calculations.

TF-IDF: This feature represents the average product of term frequency (TF) and inverse document frequency (IDF) for each query term within different document sections (URL, title, content, and whole document). TF measures how often a term appears in a specific document section, while IDF measures how important that term is across the entire document collection.

BM25: This feature utilizes the BM25 ranking function, which is a probabilistic model that considers term frequency, document length, and average document length to estimate relevance. While it also considers term frequency like TF-IDF, it incorporates additional factors to improve the weighting scheme.",resnet18
OverheatData/OverheatData\1805.04687v2\1805.04687v2-Table10-1.png,375,101,"The proposed dataset contains non-city scenes like highways, which typically have fewer pedestrians per image compared to cityscapes.","There are approximately 9,719 negative samples in the training set of the CNSE dataset.",resnet18
OverheatData/OverheatData\1805.06431v4\1805.06431v4-Table7-1.png,414,420,"ChoiceNet appears to be the safest method for autonomous driving on straight lanes, regardless of the percentage of outlier vehicles present.",ChoiceNet appears to be the most robust to outliers in the training data.,resnet18
OverheatData/OverheatData\1805.00912v4\1805.00912v4-Figure1-1.png,299,453,MTSA,"The three main components of the Action Search model architecture are the visual encoder, the LSTM, and the spotting target.",resnet18
OverheatData/OverheatData\1805.06447v3\1805.06447v3-Table5-1.png,471,477,ITN (ResNet-32) with data augmentation performs best on the CIFAR-10 dataset with a testing error of 5.82%.,"The ITN (B-CNN) method with data augmentation (DA) performs best on the TMTA task, achieving a testing error of 21.31%. Data augmentation contributes significantly to its performance, as the ITN (B-CNN) method without DA has a higher testing error of 31.67%.",resnet18
OverheatData/OverheatData\1706.00633v4\1706.00633v4-Table2-1.png,389,460,RCE training combined with the K-density metric consistently performs the best across both MNIST and CIFAR-10 datasets for all attack types.,The proposed method performs the best on average across all noise levels tested on the Kodak dataset.,resnet18
OverheatData/OverheatData\1701.06171v4\1701.06171v4-Figure4-1.png,119,269,22 iterations,"It is difficult to say definitively which model performs better based on the training curves alone. However, it appears that the WGAN(g) model may be performing better than the other models, as its generator and discriminator losses are both lower than the other models.",resnet18
OverheatData/OverheatData\1706.04284v3\1706.04284v3-Figure5-1.png,458,191," The denoiser trained with the classification network and evaluated for semantic segmentation performs the best on the sheep image. This is because the segmentation label map for this denoiser is the most accurate, and it correctly identifies the sheep's body and legs. ",The reconstructions are very similar to the original samples.,resnet18
OverheatData/OverheatData\1809.03149v2\1809.03149v2-Figure3-1.png,599,249,The percentage of ads displayed for each user is higher when CHER is used.,The NegPair reduction generally increases as the number of perfect results in a query increases.,resnet18
OverheatData/OverheatData\1707.08608v3\1707.08608v3-Table11-1.png,530,308,"The PT genre within the SRL-NW network has the lowest failure rate at 10.01%. Its inference time is also the lowest across all genres in the SRL-NW network for all three inference procedures (Viterbi, GBI, and A*).","The topic with the highest internal coherence value is ""turks armenian armenia turkish roads escape soviet muslim mountain soul"".",resnet18
OverheatData/OverheatData\1811.07073v3\1811.07073v3-Figure3-1.png,260,464,The input to the convolutional self-correction model is the logits generated by the primary and ancillary models.,The frame discriminator is used to detect whether the generated frame and audio are matched or not.,resnet18
OverheatData/OverheatData\1809.01246v1\1809.01246v1-Figure13-1.png,575,225,The buffer percentage decreases as the width of the room increases.,"Documents are first matched using a pre-defined match plan. Then, they are passed through additional rank-and-prune stages, which are implemented as a cascade of machine learning models.",resnet18
OverheatData/OverheatData\1809.03449v3\1809.03449v3-Figure1-1.png,608,505,"The Knowledge Aided Similarity Matrix is used to compute the similarity between the question and passage context embeddings. This similarity score is then used to weight the passage context embeddings, giving more weight to those parts of the passage that are most relevant to the question.",Increasing the projection dimension d decreases the approximation error for both sparse PCA and NMF.,resnet18
OverheatData/OverheatData\1805.04687v2\1805.04687v2-Table9-1.png,380,552,"The training approach ""Det + T + I + S"" achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score.",LSH-E,resnet18
OverheatData/OverheatData\1803.02750v3\1803.02750v3-Figure8-1.png,115,85,Mesh,Headlines.,resnet18
OverheatData/OverheatData\1811.02721v3\1811.02721v3-Figure11-1.png,209,225,TCP,"Documents are first matched using a pre-defined match plan. Then, they are passed through additional rank-and-prune stages, which are implemented as a cascade of machine learning models.",resnet18
OverheatData/OverheatData\1603.00286v5\1603.00286v5-Figure2-1.png,58,15,Six.,The classification error of a residual network generally increases as the average path length increases.,resnet18
OverheatData/OverheatData\1901.00056v2\1901.00056v2-Table2-1.png,509,313,"The SYNONYMNET(Pairwise) model with Leaky Unit performs best on the PubMed + UMLS dataset, achieving an AUC of 0.9838 and a MAP of 0.9872. This is a statistically significant improvement over the DPE baseline, which achieved an AUC of 0.9513 and a MAP of 0.9623.","Seq2Seq and Mem2Seq models performed poorly because they struggled to capture the semantic representations of unseen entities. This means they couldn't understand the meaning and relationships of new restaurants introduced in the KB. As a result, they were unable to accurately identify the correct restaurant and provide its address when faced with unseen entities.",resnet18
OverheatData/OverheatData\1805.00912v4\1805.00912v4-Table2-1.png,301,383,The Transfer + MTSA model performed best on the SNLI test set with an accuracy of 86.9%.,"MOTS datasets require denser annotations per frame because they involve both segmentation and tracking of multiple objects in crowded scenes. This results in smaller dataset sizes compared to VOS datasets, which typically focus on segmenting a single or fewer objects. ",resnet18
OverheatData/OverheatData\1812.00108v4\1812.00108v4-Table3-1.png,431,432,"The performance of the model generally improves as the number of views increases. For example, when the model is trained and tested on two-view data, the F1-score is 29.67. However, when the model is trained and tested on three-view data, the F1-score increases to 30.2. This suggests that the model is able to learn more effectively from data with more views.",Ours-supervised achieved the highest F1 score on the Lobby dataset with a score of 93.4.,resnet18
OverheatData/OverheatData\1804.01429v3\1804.01429v3-Figure1-1.png,195,191,"An agent-in-place action is an action that is performed by an agent in a specific place, while a generic action category is a more general category of action that does not specify the place where the action is performed.",The reconstructions are very similar to the original samples.,resnet18
OverheatData/OverheatData\1812.06589v2\1812.06589v2-Figure4-1.png,465,346,"The dynamic attention block decouples the lip-related and identity-related information, allowing the network to focus on the most important area for generating realistic talking faces.",The proposed method is able to generate images with different hair colors more accurately than icGAN.,resnet18
OverheatData/OverheatData\1611.07718v2\1611.07718v2-Figure2-1.png,9,586,"Deep residual networks have skip connections that allow the gradient to flow directly from one layer to another, while networks built by stacking inception-like blocks do not.",The processing time of SRU is significantly faster than that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3.,resnet18
OverheatData/OverheatData\1707.01917v2\1707.01917v2-Table3-1.png,495,146,The NYT Sports dataset has the highest value for λa (0.9).,MovieLens-1M,resnet18
OverheatData/OverheatData\1805.06431v4\1805.06431v4-Figure8-1.png,415,484,ChoiceNet,FLoss,resnet18
OverheatData/OverheatData\1611.03780v2\1611.03780v2-Figure2-1.png,19,621," The interference graph is a folded version of the query graph. The nodes in the interference graph represent regions, and the edges represent the interference between regions. The edge weights in the interference graph are calculated from the edge weights in the query graph.",The Entropy Balancing (EB) and Covariate Control (CC) estimators.,resnet18
OverheatData/OverheatData\1608.02784v2\1608.02784v2-Figure6-1.png,27,370,"The SMT outputs are literal translations of the images, while the CCA outputs take into account the context of the images and generate more natural-sounding descriptions.",The segmentation model learns to interpolate in areas that have no lane markings.,resnet18
OverheatData/OverheatData\1701.03077v10\1701.03077v10-Figure5-1.png,47,585,The performance of gFGR generally improves as the shape parameter α increases.,The SRU model achieves comparable or slightly higher validation accuracy than the cuDNN LSTM and CNN models on all six benchmarks.,resnet18
OverheatData/OverheatData\1705.02798v6\1705.02798v6-Table3-1.png,242,337,R.M.-Reader.,UnCoRd-None-B.,resnet18
OverheatData/OverheatData\1805.04609v3\1805.04609v3-Figure3-1.png,360,473,US-HC-MQ,ITN generates more accurate and realistic samples on the MNIST dataset compared to AC-GATN.,resnet18
OverheatData/OverheatData\1611.03780v2\1611.03780v2-Table1-1.png,23,92,Both GeoCUTS and DMA perform equally well for highly active users in the US.,"The residual connection adds the output of a layer to the output of another layer, which helps to prevent the vanishing gradient problem.",resnet18
OverheatData/OverheatData\1901.00398v2\1901.00398v2-Figure8-1.png,523,61,The AMT workers are being asked to decide whether each of twenty one paragraphs extracted from product reviews is real (written by a person) or fake (written by a computer algorithm).," The frequency reduction process takes a high-frequency trajectory and samples it at a lower frequency, resulting in multiple trajectories with different starting and ending points. ",resnet18
OverheatData/OverheatData\1906.06589v3\1906.06589v3-Figure2-1.png,605,556,The generalization gap increases as the average X_ref entropy increases.,GB-KMV performs better in terms of F1 score and precision when the space used is 5%.,resnet18
OverheatData/OverheatData\1811.07073v3\1811.07073v3-Figure5-1.png,264,191,The ancillary heatmap is used to correct the labels for missing or oversegmented objects in the images.,The reconstructions are very similar to the original samples.,resnet18
OverheatData/OverheatData\1809.00458v1\1809.00458v1-Figure5-1.png,550,586,ENRON,The processing time of SRU is significantly faster than that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3.,resnet18
OverheatData/OverheatData\1803.03467v4\1803.03467v4-Table3-1.png,143,525,RippleNet*,Cap2Img performs the best for caption retrieval in terms of both R@1 (27.1) and MEDR (4.0). This suggests that the model is more successful at retrieving the most relevant caption for a given image compared to other models.,resnet18
OverheatData/OverheatData\1809.00458v1\1809.00458v1-Figure1-1.png,560,292,0.67,FALCON,resnet18
OverheatData/OverheatData\1606.07384v2\1606.07384v2-Figure2-1.png,75,129,RANSAC,"The LSTNet model has four main types of layers:

1. Convolutional layer: This layer extracts local dependency patterns from the input data. 
2. Recurrent and recurrent-skip layer: These layers capture long-term dependencies in the data. 
3. Fully connected and element-wise sum output layer: This layer combines the outputs from the convolutional and recurrent layers to produce the final prediction.
4. Autoregressive layer: This layer provides a linear bypass to the non-linear neural network part of the model. 

The convolutional layer receives the input data and passes its output to the recurrent and recurrent-skip layers. These layers then pass their output to the fully connected and element-wise sum output layer. The autoregressive layer receives the input data directly and its output is also fed into the fully connected and element-wise sum output layer.",resnet18
