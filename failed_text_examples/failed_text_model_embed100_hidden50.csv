paper_id,question,true_label,predicted_label,true_answer,predicted_answer,model_name
1804.07931v2,What are the two auxiliary tasks that are used in the ESMM architecture for CVR modeling?,279,408,The two auxiliary tasks are CTR and CTCVR.,Multi-X,text_model_embed100_hidden50
1803.03467v4,Which dataset has the highest AUC for all ripple set sizes?,146,451,MovieLens-1M,VIDXL has the highest Recall@20 and MRR@20.,text_model_embed100_hidden50
1804.05938v2,"Explain the difference between the features ""TF-IDF"" and ""BM25"".",324,215,"Both TF-IDF and BM25 are features used to estimate the relevance of a document to a query. However, they differ in their underlying calculations.

TF-IDF: This feature represents the average product of term frequency (TF) and inverse document frequency (IDF) for each query term within different document sections (URL, title, content, and whole document). TF measures how often a term appears in a specific document section, while IDF measures how important that term is across the entire document collection.

BM25: This feature utilizes the BM25 ranking function, which is a probabilistic model that considers term frequency, document length, and average document length to estimate relevance. While it also considers term frequency like TF-IDF, it incorporates additional factors to improve the weighting scheme.",5 hops,text_model_embed100_hidden50
1805.01216v3,Which model performs best when the percentage of unseen entities in the response is low?,312,350,BoSsNet,TecoGAN,text_model_embed100_hidden50
1701.06171v4,How many iterations did the greedy EM-type learning process take to learn the part models for the watch image?,119,508,22 iterations,"The AUC and MAP values initially increase with increasing margin, but then decrease after a certain point.",text_model_embed100_hidden50
1704.07121v2,"On the VQAv2-2017 validation set, which model performs best when considering all three sources of information (images, questions, and answers) and how does its performance compare to the model that only uses answers?",187,95,"The model that performs best on VQAv2-2017 val when considering all three sources of information is MLP-IQA. It achieves an accuracy of 61.1% on the \IU+\QU -decoys metric, significantly outperforming the model that only uses answers (MLP-A) which achieves only 27.7% on the same metric. This demonstrates the importance of incorporating all available information for accurate prediction.","PWC-Net (ft) achieved the best performance on the KITTI 2015 test set with an F1-all score of 9.16%. This is significantly better than Devon (ft), which achieved an F1-all score of 14.31% on the same dataset. ",text_model_embed100_hidden50
1809.03550v3,Why is the optimal threshold chosen to be at the right margin of the region around the mode of the histogram?,622,483,"The region around the mode of the histogram mostly contains noise. Therefore, the optimal threshold is chosen to be at the right margin of this region to avoid including too much noise in the thresholded image."," 

Increasing the value of β2 decreases the precision and increases the recall of the model.",text_model_embed100_hidden50
1707.00524v2,Which game has the highest code loss in phase 2?,486,79,Pacman,The 1-keyword method resulted in the highest BLEU score of 0.705.,text_model_embed100_hidden50
1809.02731v3,Which model performed best on average across all tasks?,592,451,The Linear model performed best on average with a score of 70.0.,VIDXL has the highest Recall@20 and MRR@20.,text_model_embed100_hidden50
1706.00633v4,Which objective function resulted in a higher ratio of f2(x∗) > 0 for the MNIST dataset?,393,378,RCE,Cars are the most common object in the dataset.,text_model_embed100_hidden50
1809.00458v1,Which method is more efficient at utilizing space while maintaining high accuracy?,561,378,GB-KMV is more efficient at utilizing space while maintaining high accuracy.,Cars are the most common object in the dataset.,text_model_embed100_hidden50
1708.01425v4,Which step in the methodology resulted in the largest decrease in the size of the dataset?,567,51,"Step 4, Reason disambiguation.",The proposed method appears to be more accurate than the baseline method. The depth maps generated by the proposed method are more detailed and realistic than those generated by the baseline method.,text_model_embed100_hidden50
1804.05995v2,What are the top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method?,272,73,"The top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method are HISTORY, DEMOGRAPHICS, ECONOMY, EDUCATION, and POLITICS.","The equation that describes the motion of a mass attached to a spring is:
```
m d^2 X / dt^2 + kX = 0
```
where:
* m is the mass of the object
* X is the displacement of the object from its equilibrium position
* k is the spring constant
* t is time",text_model_embed100_hidden50
1803.03467v4,What is the role of the ripple sets in the RippleNet framework?,144,354,The ripple sets are used to propagate a user's preferences from his or her click history to his or her relevant entities.,"The Motion Compensation block estimates the motion between the previous frame and the current frame, and uses this information to warp the previous frame to the current frame. This helps the generator to produce more realistic images by taking into account the temporal information in the video sequence.",text_model_embed100_hidden50
1803.04572v2,What are some common medications used to treat Sickle Cell Anemia?,149,382,"According to the table, some common medications used to treat Sickle Cell Anemia include:

Beta-adrenergic agents
Analgesics (narcotics and non-narcotics)
NSAIDs (cyclooxygenase inhibitor - type)
Potassium replacement
Sodium/saline preparations
General inhalation agents
Laxatives and cathartics
IV solutions (dextrose-saline)
Antiemetic/antivertigo agents
Sedative-hypnotics (non-barbiturate)
Glucocorticoids (orally inhaled)
Folic acid preparations
Analgesic narcotic anesthetic adjunct agents",Approximately 80%,text_model_embed100_hidden50
1809.00263v5,What is the value of the learning rate α for the BAIR dataset?,537,329,0.0002,"The Bernoulli parameter is a measure of the probability of a pixel being foreground or background. The higher the Bernoulli parameter, the more likely the pixel is to be foreground. This is reflected in the images, where the pixels with higher Bernoulli parameters are more likely to be part of the person's silhouette.",text_model_embed100_hidden50
1803.02750v3,What is the difference between the `inc_i(p)` and `inc_i'(p)` operations in the Grow-only Counter data type?,114,93,"The `inc_i(p)` operation increments the value associated with the key `i` in the counter `p`, while the `inc_i'(p)` operation increments the value associated with the key `i` in the counter `p` only if the key `i` is not already present in the counter.",The relation module (Rt) is responsible for capturing the spatial relationships between the features extracted from the first and second images.,text_model_embed100_hidden50
1706.04269v2,What are the three main components of the Action Search model architecture?,453,260,"The three main components of the Action Search model architecture are the visual encoder, the LSTM, and the spotting target.",The input to the convolutional self-correction model is the logits generated by the primary and ancillary models.,text_model_embed100_hidden50
1706.00633v4,How does the accuracy of the model change as the value of c increases?,392,431,The accuracy of the model decreases as the value of c increases.,"The performance of the model generally improves as the number of views increases. For example, when the model is trained and tested on two-view data, the F1-score is 29.67. However, when the model is trained and tested on three-view data, the F1-score increases to 30.2. This suggests that the model is able to learn more effectively from data with more views.",text_model_embed100_hidden50
1812.00281v3,What are the different stages of HUMBI body and cloth reconstruction?,437,359,"The different stages of HUMBI body and cloth reconstruction are: 
1. Input image of the person (Ibody)
2. Keypoint estimation (Kbody)
3. Occupancy map generation (Obody)
4. Body model fitting (Mbody)
5. Cloth model fitting (Mcloth)",The user study is designed to test which of two images is closer to a reference video.,text_model_embed100_hidden50
1706.00633v4,Which of the following algorithms performs the best when trained via the CE?,391,350,C&W-hc,TecoGAN,text_model_embed100_hidden50
1705.09882v2,Which part of the model is responsible for deciding which frames are most important for the re-identification task?,330,288,The Reinforced Temporal Attention (RTA) unit.,ImageNet-Dogs benefited more from the PC optimization method compared to ImageNet-Random.,text_model_embed100_hidden50
1706.03847v3,How does the training time of the different losses change as the number of additional samples increases?,450,474,The training time of all losses increases as the number of additional samples increases.,The quality of the generated samples decreases as the update threshold increases.,text_model_embed100_hidden50
1611.02654v2,Which model performed the best on the SICK dataset according to the MSE metric?,33,549,The supervised model performed the best on the SICK dataset according to the MSE metric.,"The CoNLL model performed best on the WikiCoref dataset, with an F1 score of 53.40 when using the +linguistic evaluation metric.",text_model_embed100_hidden50
1811.08257v1,Which framework has the lowest total communication cost for MNIST?,292,334,FALCON,"The answering procedure follows the question graph, searching for a valid assignment in the image. At each step, the handled node is set and objects (extracted using Mask R-CNN) are examined according to the node’s requirements (utilizing corresponding visual estimators). If successful, a new node is set (according to a DFS traversal) and the function is called again to handle the unassigned subgraph.",text_model_embed100_hidden50
1611.03780v2,How is the Hilbert space-filling curve constructed?,17,218,"The Hilbert space-filling curve is constructed recursively. The curve starts with a simple square, and then at each subsequent iteration, the curve is subdivided into four smaller squares. The curve is then drawn through each of these squares in a specific order.",The corrected gradient method leads to a more stable and lower loss value during training.,text_model_embed100_hidden50
1707.00524v2,What is the difference between the encoder and decoder networks in the action-conditional prediction model?,488,221,The encoder network takes a one-hot action and the current state as input and outputs a latent representation of the state. The decoder network takes the latent representation and outputs a prediction of the next state.," The higher the resolution of the simulation, the longer the training time. ",text_model_embed100_hidden50
1804.07849v4,"According to the ablation experiments, which factor contributes the most to the best model's performance compared to the baseline model?",254,513,Morphological modeling with LSTMs contributes the most to the best model's performance compared to the baseline model.,The largest performance gap is observed in the PubMed + UMLS dataset using the F1@K metric with K=1.,text_model_embed100_hidden50
1811.09393v4,Which method has the best perceptual performance according to the tOF score?,349,415,TecoGAN.,ChoiceNet,text_model_embed100_hidden50
1805.04687v2,"Why does the proposed dataset have a lower number of persons per image compared to the Cityscapes dataset, even though it has a much higher total number of pedestrians?",375,469,"The proposed dataset contains non-city scenes like highways, which typically have fewer pedestrians per image compared to cityscapes.","When trained with only 1% of the MNIST training data, ITN (B-CNN) (w/ DA) performs the best with a testing error of 2.78%. Data augmentation further improves its performance by 0.4%, bringing the testing error down to 2.78% from 3.18% achieved by ITN (B-CNN) without data augmentation.",text_model_embed100_hidden50
1812.00281v3,"What is the difference between the ""median appearance"" and the ""view-specific appearance""?",442,193,"The median appearance is the average of all the multiview images, while the view-specific appearance is a single image that is rendered from a specific viewpoint.","The representation task takes an appearance as input and outputs an RGB value, while the learning-to-learn task takes an image as input and outputs a DAM representation.",text_model_embed100_hidden50
1805.08465v3,Which steganography method achieves the best performance in terms of distortion for both cover and secret images when embedding 2 bits per channel?,514,51,The proposed method achieves the best performance for both cover and secret images when embedding 2 bits per channel.,The proposed method appears to be more accurate than the baseline method. The depth maps generated by the proposed method are more detailed and realistic than those generated by the baseline method.,text_model_embed100_hidden50
1811.02553v4,How does the number of state-action pairs affect the optimization landscape for the PPO algorithm?,234,132,"As the number of state-action pairs increases, the optimization landscape becomes more complex and has more local optima. This makes it more difficult for the PPO algorithm to find the global optimum.",The BLEU score decreases as the initial accuracy of the discriminator increases.,text_model_embed100_hidden50
1805.04687v2,How does the BDD100K dataset compare to the KITTI and MOT17 datasets in terms of size and complexity?,386,634,"The BDD100K dataset is significantly larger and more complex than both the KITTI and MOT17 datasets. It contains roughly 40 times more frames, 16 times more sequences, and 13 times more identities than KITTI. Compared to MOT17, BDD100K has about 10 times more frames, 80 times more sequences, and 8 times more identities. This increase in size and complexity makes BDD100K a more challenging and comprehensive benchmark for multiple object tracking algorithms. ","VAGER+Voting consistently outperforms all other VAGER variants in both 1-shot and 20-shot settings, achieving the highest AUC and F1 scores.",text_model_embed100_hidden50
1707.08608v3,Which genre in the SRL-NW network has the lowest failure rate and how does its inference time compare to other genres within the same network?,530,37,"The PT genre within the SRL-NW network has the lowest failure rate at 10.01%. Its inference time is also the lowest across all genres in the SRL-NW network for all three inference procedures (Viterbi, GBI, and A*).","The proposed model has the highest accuracy on the NIPS Abstracts dataset, with an accuracy of 51.55.",text_model_embed100_hidden50
1804.05938v2,"Which correction method resulted in the best performance in terms of nDCG@10 and ERR@10, and how does it compare to not using any correction method?",322,105,"The DNN trained with DLA achieved the best performance in terms of both nDCG@10 (0.421) and ERR@10 (0.582). Compared to not using any correction method (NoCorrect), DLA shows a significant improvement in both metrics, with nDCG@10 being higher by 0.063 and ERR@10 being higher by 0.082.",The C-Tarone method has higher precision and F-measure than the binarization method in all datasets. The C-Tarone method has better or competitive recall than the binarization method. The running time of the C-Tarone method is competitive with the binarization method.,text_model_embed100_hidden50
1704.07854v4,"Which of the two scenes, Drop or Staris, requires more computation time for rendering?",220,378,Staris,Cars are the most common object in the dataset.,text_model_embed100_hidden50
1605.07496v3,How does the predicted return change as a function of θ for a fixed value of π = 1.5?,70,284,"The predicted return decreases as θ increases, with a minimum at around θ = 0.5.",The test accuracy of all models decreases as λ increases.,text_model_embed100_hidden50
1812.06589v2,What is the role of the frame discriminator in the proposed method?,464,544,The frame discriminator is used to detect whether the generated frame and audio are matched or not.,The residual connections add the output of the previous layer to the input of the next layer. This helps to improve the flow of information through the network and can help to prevent vanishing gradients.,text_model_embed100_hidden50
1809.01989v2,Which method achieved the highest tracking accuracy in terms of minimizing the sum of absolute percentage errors? Does this necessarily mean it had the best overall performance?,646,469,"The Ridge method achieved the lowest sum of absolute percentage errors (136.84), indicating the highest tracking accuracy in terms of minimizing absolute deviations from the index. However, this doesn't necessarily translate to the best overall performance.","When trained with only 1% of the MNIST training data, ITN (B-CNN) (w/ DA) performs the best with a testing error of 2.78%. Data augmentation further improves its performance by 0.4%, bringing the testing error down to 2.78% from 3.18% achieved by ITN (B-CNN) without data augmentation.",text_model_embed100_hidden50
1704.07854v4,Which of the methods is able to reconstruct the shape of the liquid properly?,223,284,Only the full method with a deformation network is able to produce a perfect reconstruction.,The test accuracy of all models decreases as λ increases.,text_model_embed100_hidden50
1705.09966v2,What is the difference between the input and output of the frontal face generation process?,344,10,The input is a low-resolution frontal face image and a high-resolution side-face image. The output is a high-resolution frontal face image.,"The residual block assembles two residual branches sequentially, while the merge-and-run block assembles the same two residual branches in parallel.",text_model_embed100_hidden50
1805.06431v4,Which of the four methods has the best performance in terms of average error for the step function?,423,100,The proposed method.,The SPIRAL-MSM-kMeans method performs the best in terms of NMI with a score of 0.365. It outperforms the other methods on 89.4% of the datasets.,text_model_embed100_hidden50
1705.09882v2,How does the proposed split-rate RGB-to-Depth transfer scheme differ from the R3D [90] method of Yosinski et al.?,326,339,"The proposed split-rate RGB-to-Depth transfer scheme differs from the R3D [90] method in two ways. First, the proposed method uses a different learning rate for the bottom three layers of the network. Second, the proposed method uses a different initialization for the weights of the bottom three layers of the network.",The proposed method produces more realistic and natural-looking images than the method in~\cite{kim2017learning}.,text_model_embed100_hidden50
1812.06589v2,Which method performed the best according to the LMD metric?,463,415,AMIE (Ours),ChoiceNet,text_model_embed100_hidden50
1706.08146v3,What is the effect of increasing the projection dimension d on the approximation error for sparse PCA and NMF?,505,29,Increasing the projection dimension d decreases the approximation error for both sparse PCA and NMF.,The singular value decomposition step is used to find the projection matrices U and V.,text_model_embed100_hidden50
1603.00286v5,What is the minimum number of sides that a rectilinear polygon with four reflex vertices must have?,58,264,Six.,The ancillary heatmap is used to correct the labels for missing or oversegmented objects in the images.,text_model_embed100_hidden50
1704.05426v4,Which type of word has the greatest difference in frequency of occurrence between MultiNLI and SNLI?,156,190,Negation (PTB),"The ""OUR"" method performs best for the ""Representation"" task when the view is ""Novel"".",text_model_embed100_hidden50
1811.02721v3,How does varying the buffer size affect TCP goodput?,205,316,"Increasing the buffer size generally leads to increased TCP goodput, but only up to a certain point.",The encoder understands the last user utterance by using the memory cell representations of the dialog history and KB tuples.,text_model_embed100_hidden50
1805.04687v2,Which category of object is the least common in the dataset?,385,378,Train,Cars are the most common object in the dataset.,text_model_embed100_hidden50
1703.02507v3,Which dataset has the shortest average sentence length?,85,349,Headlines.,TecoGAN.,text_model_embed100_hidden50
1608.02784v2,What is the relationship between BLEU score and human ranking for CCA and SMT systems?,32,595,The correlation between BLEU scores and human ranking is not high for either CCA or SMT systems.,"The relationship between position and CTR is complex and non-linear. In general, CTR decreases as position increases, but there are also local peaks and valleys in the CTR curve. This suggests that there are other factors besides position that affect CTR.",text_model_embed100_hidden50
1803.03467v4,How does the dimension of embedding affect the AUC of RippleNet on MovieLens-1M?,147,132,The AUC of RippleNet first increases and then decreases with the increase of the dimension of embedding.,The BLEU score decreases as the initial accuracy of the discriminator increases.,text_model_embed100_hidden50
1611.07718v2,How does the classification error of a residual network change as the average path length increases?,15,284,The classification error of a residual network generally increases as the average path length increases.,The test accuracy of all models decreases as λ increases.,text_model_embed100_hidden50
1705.07164v8,Which method achieved the highest Inception Score (IS) at the end of training for both CIFAR10 and ImageNet datasets? Did this method also achieve the highest initial IS score?,270,448,"For CIFAR10, WGAN achieved the highest IS at the end of training (2.42). However, RWGAN had a higher initial IS score (1.86) compared to WGAN's 1.63. 

For ImageNet, WGAN again achieved the highest final IS (2.80), while RWGAN had a slightly higher initial IS (2.04 vs. 2.00). ","The VIDXL dataset contains the most interactions (events) in the training set with 69,312,698 events. This is roughly 7.7 times larger than the RSC15 dataset, which has the least interactions (9,011,321) in the training set. ",text_model_embed100_hidden50
1605.07496v3,Which of the algorithms performs the best on the robotic arm joint breakage task?,66,350,ALOQ.,TecoGAN,text_model_embed100_hidden50
1805.01216v3,"Why might the authors claim that although BOSSNET achieves a lower BLEU score than Mem2Seq on the SMD dataset, it still performs better in conveying necessary entity information?",318,571,"While BOSSNET has a lower BLEU score than Mem2Seq on SMD, it achieves the highest Entity F1 score on that dataset. This suggests that BOSSNET is better at capturing and including the relevant entities in its responses, even though it may not have as much lexical overlap with the gold responses as Mem2Seq.","The ""Last contact"" feature shows a strong positive influence on the SSL score because it is likely correlated with other features that are used by the algorithm. As the passage mentions, data-driven methods like MIM can assign influence to features that are not directly used by the model if they are correlated with other influential features. In this case, a recent ""Last contact"" date might be correlated with a higher number of recent offenses or other factors that contribute to a higher SSL score.",text_model_embed100_hidden50
1812.00281v3,How does the number of cameras used affect the accuracy of the garment reconstruction?,440,132,The accuracy of the garment reconstruction increases as the number of cameras used increases.,The BLEU score decreases as the initial accuracy of the discriminator increases.,text_model_embed100_hidden50
1605.07496v3,Which algorithm performs the best in the Joint Breakage experiment?,67,637,ALOQ,VAGER + Voting,text_model_embed100_hidden50
1603.00286v5,Which agent values the entire share $Z_j$?,54,360,Agent $j$.,US-HC-MQ,text_model_embed100_hidden50
1701.03077v10,Why did the authors choose to use a nonlinearity to curve α before fitting the cubic hermite spline?,45,415,"The authors chose to use a nonlinearity to curve α before fitting the cubic hermite spline because it allows for increased knot density near α = 2 and decreased knot density when α > 4. This helps to better approximate the log partition function, which is difficult to evaluate for arbitrary inputs.",ChoiceNet,text_model_embed100_hidden50
1703.10730v2,What are the inputs to the image generation network?,139,359,The inputs to the image generation network are the observed images (x) and a random noise vector (z).,The user study is designed to test which of two images is closer to a reference video.,text_model_embed100_hidden50
1809.00458v1,Which dataset has the highest average record length?,557,551,CaOpenData,GB-KMV,text_model_embed100_hidden50
1709.02418v2,What is the effect of performing a left-swap on a binary vector y at index j′?,647,415,The left-swap increases the number of misclassified pairs by one.,ChoiceNet,text_model_embed100_hidden50
1809.00458v1,What is the relationship between the element-hash value pairs and the signature size?,558,329,"The element-hash value pairs are the elements of the signature, and the signature size is the number of element-hash value pairs in the signature.","The Bernoulli parameter is a measure of the probability of a pixel being foreground or background. The higher the Bernoulli parameter, the more likely the pixel is to be foreground. This is reflected in the images, where the pixels with higher Bernoulli parameters are more likely to be part of the person's silhouette.",text_model_embed100_hidden50
1805.06431v4,What is the purpose of the Cholesky Block in this figure?,427,517,The Cholesky Block is used to distinguish abnormal patterns from normal patterns.,The task-irrelevant data is used to simulate the RGB representation using the gray scale image. This allows ZDDA to learn a joint network that can be used to classify digits in both the gray scale and RGB domains.,text_model_embed100_hidden50
1809.03149v2,What is the effect of using CHER on the percentage of ads displayed for each user?,599,426,The percentage of ads displayed for each user is higher when CHER is used.,"The Cholesky block is used to decompose the covariance matrix Σk into a lower triangular matrix and its transpose. This decomposition is used to ensure that the covariance matrix is positive definite, which is a requirement for the Gaussian distribution.",text_model_embed100_hidden50
1809.03449v3,Which model performs the best when trained on 60% of the training data?,607,350,KAR,TecoGAN,text_model_embed100_hidden50
1707.00189v3,"Which model performs best when trained on the NYT dataset and evaluated on the WT14 dataset, and how does its performance compare to the baselines?",500,527,"The Conv-KNRM model performs best when trained on the NYT dataset and evaluated on the WT14 dataset, achieving an nDCG@20 score of 0.3215. This performance is significantly better than all the baselines: BM25 (B), WT10 (W), and AOL (A).","The GroundSent-Both model performs best on the SNLI dataset, achieving an accuracy of 72.0%. Grounding contributes to an improvement of 4.7% compared to the baseline STb-1024 model, which achieves 67.3%.",text_model_embed100_hidden50
1811.02553v4,How does the number of state-action pairs affect the reward landscape for the surrogate and true reward functions?,233,291,"As the number of state-action pairs increases, the reward landscape for both the surrogate and true reward functions becomes smoother and more accurate.",The setup and online time for the Softmax increases as the number of classes increases.,text_model_embed100_hidden50
1805.08751v2,What is the role of the GDU and HFLU modules in the FAKEDETECTOR framework?,524,263,"The GDU (Gated Dilated Unit) modules are responsible for extracting features from the input data, while the HFLU (Hybrid Feature Learning Unit) modules are responsible for fusing the features extracted by the GDU modules.",The self-correction module refines the segmentations generated by the ancillary and current primary model for the weak set.,text_model_embed100_hidden50
1803.04572v2,Which dataset has the largest number of clinical visits per patient?,154,287,CMS,SVHN,text_model_embed100_hidden50
1802.07459v2,What are the different stages involved in constructing the Concept Interaction Graph (CIG) from a pair of documents?,103,546,"The different stages involved in constructing the Concept Interaction Graph (CIG) from a pair of documents are: (a) Representation, (b) Encoding, (c) Transformation, and (d) Aggregation.",1,text_model_embed100_hidden50
1707.01917v2,"What does the induced schema Win <A4, B3, C2> represent?",494,177,"The induced schema Win <A4, B3, C2> represents the fact that player A4 won tournament C2, defeating player B3.",The probability of repaying a debt increases with credit score.,text_model_embed100_hidden50
1608.02784v2,What is the relationship between the input space and the output space in CCA inference?,30,197,The input space and the output space are related by a cosine similarity measure.,The 3D ConvNet is used to extract features from the input images. These features are then used to generate place-based feature descriptions.,text_model_embed100_hidden50
1802.07351v2,Explain the rationale behind using five deformable cost volumes with different hyperparameter settings in Devon's relation module.,94,600,"The five deformable cost volumes in Devon's relation module are designed to capture multi-scale motion by combining dense correspondences near the image center with sparser correspondences in the periphery. This is achieved by using different neighborhood sizes (k) and dilation rates (r) for each cost volume, as shown in Table 1. Smaller neighborhood sizes and dilation rates result in denser correspondences, focusing on finer details and small displacements, while larger values capture broader context and larger motions.",MSDNNet38,text_model_embed100_hidden50
1804.05936v2,Which relevance label category of documents received the most significant rank promotion according to the NegPair reduction metric?,247,415,The perfect results received the largest promotions in rank.,ChoiceNet,text_model_embed100_hidden50
1811.08257v1,Which operation has the lowest online time?,298,360,ReLU,US-HC-MQ,text_model_embed100_hidden50
1704.07854v4,How do the parameter network and the deformation network differ in terms of complexity and function?,217,284,"The parameter network is a simple structure with two fully connected layers, while the deformation network is more complex and contains two fully connected layers followed by two or more four-dimensional de-convolution layers. The parameter network learns how to apply multiple long-range, non-linear deformation fields, while the deformation network learns to generate dense deformation fields to refine the final surface.",The test accuracy of all models decreases as λ increases.,text_model_embed100_hidden50
1811.10673v1,How does the quality of the reconstructed frames change as the quantization level of the soft edge detector increases?,405,112,The quality of the reconstructed frames increases as the quantization level of the soft edge detector increases.,The average metadata required per node for the Op-based BP+RR approach increases as the number of nodes in the network increases.,text_model_embed100_hidden50
1812.10735v2,Why do you think the performance of all models is generally lower on Rest15 compared to Rest14?,491,415,The performance of all models is generally lower on Rest15 because it has a larger number of aspect categories (13) compared to Rest14 (5). This increased complexity makes it more challenging for the models to accurately identify and classify the aspects.,ChoiceNet,text_model_embed100_hidden50
1809.01246v1,How does the average precision of TCM(256*memory) compare to the other two algorithms in the email-EuAll dataset?,581,526,The average precision of TCM(256*memory) is lower than the other two algorithms in the email-EuAll dataset.,"GroundSent-Cap appears to be most beneficial for the MRPC task, achieving an accuracy of 72.9/82.2 compared to the baseline model ST-LN's 69.6/81.2.",text_model_embed100_hidden50
1809.01246v1,What is the relationship between the table and the graph sketch in the figure?,578,29,"The table provides the mapping between the nodes in the original graph and their corresponding hash values, which are used to create the graph sketch.",The singular value decomposition step is used to find the projection matrices U and V.,text_model_embed100_hidden50
1805.06447v3,Describe the relationship between the update threshold (Tu) and the performance of ITN (B-CNN) on the MNIST dataset.,475,513,The performance of ITN (B-CNN) on the MNIST dataset decreases as the update threshold (Tu) increases. This is evident from the increasing ITN error percentages as Tu goes from 1e-3 to 1e-1.,The largest performance gap is observed in the PubMed + UMLS dataset using the F1@K metric with K=1.,text_model_embed100_hidden50
1803.04572v2,Which constraint has the most significant impact on the FIT values for the CMS data set when the target rank is 15?,148,221,The smoothness constraint on $\M{U_k}$ has the most significant impact on the FIT values for the CMS data set when the target rank is 15.," The higher the resolution of the simulation, the longer the training time. ",text_model_embed100_hidden50
1803.06506v3,"How does the quality of the output heatmap change when the selected concept, predicted concept, and the real entity to be grounded are all aligned?",165,123,"When the selected concept, predicted concept, and the real entity to be grounded are all aligned, the generated heatmap produces a good localization of the phrase.","When the sample size is 2000, the two-phase framework (MSG) achieves lower discrimination in prediction compared to DI, both with and without classifier tweaking.

With classifier tweaking: MSG achieves a discrimination level of 0.016 ± 5.3E-4, while DI shows a significantly higher level of 0.095 ± 1.6E-3.
Without classifier tweaking: MSG still demonstrates lower discrimination with 0.067 ± 4.3E-3 compared to DI's 0.095 ± 1.6E-3.

This indicates that the two-phase framework is more effective in removing discrimination from predictions than DI, regardless of whether classifier tweaking is applied.",text_model_embed100_hidden50
1704.05426v4,Which model performs better on the MultiNLI dataset when considering the percentage of individual labels that match the author's label?,157,312,"SNLI performs better than MultiNLI when considering the percentage of individual labels that match the author's label. SNLI has a score of 85.8%, while MultiNLI has a score of 85.2%.",BoSsNet,text_model_embed100_hidden50
1809.00458v1,Which algorithm performs better in terms of F1 score and precision when the space used is 5%?,556,562,GB-KMV performs better in terms of F1 score and precision when the space used is 5%.,GB-KMV performs better than LSH-E in terms of F1 Score and Precision.,text_model_embed100_hidden50
1809.04276v2,How does the performance of the discriminator in the proposed approach compare to the conventional discriminator in AL? What evidence suggests this difference in performance?,641,18,The discriminator in the author's approach achieves higher accuracy (95.72%) compared to the conventional discriminator in AL (94.01%).,"GeoCUTS consistently outperforms the Grid method in identifying highly mobile clusters, regardless of the number of clusters. However, the performance of both methods decreases as the number of clusters increases.",text_model_embed100_hidden50
1702.03584v3,How does the observed error compare to the underlying true error as CPU time increases?,98,129,"The observed error is initially higher than the underlying true error, but it quickly decreases and converges to the true error as CPU time increases.","The LSTNet model has four main types of layers:

1. Convolutional layer: This layer extracts local dependency patterns from the input data. 
2. Recurrent and recurrent-skip layer: These layers capture long-term dependencies in the data. 
3. Fully connected and element-wise sum output layer: This layer combines the outputs from the convolutional and recurrent layers to produce the final prediction.
4. Autoregressive layer: This layer provides a linear bypass to the non-linear neural network part of the model. 

The convolutional layer receives the input data and passes its output to the recurrent and recurrent-skip layers. These layers then pass their output to the fully connected and element-wise sum output layer. The autoregressive layer receives the input data directly and its output is also fed into the fully connected and element-wise sum output layer.",text_model_embed100_hidden50
1705.02946v3,"What is the distance from equitability for the allocation that can be obtained by cutting at $x$ with the player order $(1,2)$?",266,93,The distance from equitability is $b-a$.,The relation module (Rt) is responsible for capturing the spatial relationships between the features extracted from the first and second images.,text_model_embed100_hidden50
1701.03077v10,"On which dataset did the gRCC* algorithm achieve the largest relative improvement over the RCC algorithm, and by approximately how much did it improve?",41,607,"The gRCC* algorithm achieved the largest relative improvement over the RCC algorithm on the YTF dataset, with a relative improvement of approximately 31.9%.",KAR,text_model_embed100_hidden50
1811.02721v3,"Which TCP stack provides the most complete implementation of core TCP features, and which stack lacks the most features?",214,350,"The TCP stack presented in this paper (TCPlp) provides the most complete implementation of core TCP features, including flow control, congestion control, RTT estimation, MSS option, OOO reassembly, and various advanced features like timestamps and selective ACKs. In contrast, BLIP lacks the most features, as it does not implement congestion control, RTT estimation, or several other functionalities present in other stacks.",TecoGAN,text_model_embed100_hidden50
1707.01917v2,What is the shape of the tensor $x^1$ for the Shootings dataset?,496,493,The shape of the tensor $x^1$ for the Shootings dataset is 3365 x 1295 x 50.,Aspect-level sentiment classification (ALSC) and aspect category detection (ACD).,text_model_embed100_hidden50
1811.08481v2,Which estimator achieves the highest accuracy on the CLEVR validation set?,336,337,Size estimator.,UnCoRd-None-B.,text_model_embed100_hidden50
1804.04410v2,How does the performance of the learned policy compare to the production baseline for CAT2 queries in terms of relevance and efficiency?,227,424,"For CAT2 queries, the learned policy shows a slight improvement in relevance (NCG) for the weighted set and a significant reduction in index blocks accessed for both weighted and unweighted sets.","ChoiceNet generally performs well compared to other methods, achieving the highest accuracy on both symmetric noise settings (sym-50% and sym-20%). However, it falls to second place under the Pair-45% asymmetric noise setting, indicating a weakness in handling this specific type of noise.",text_model_embed100_hidden50
1812.00281v3,What is the relationship between the camera yaw angle and the silhouette distance?,433,329,The silhouette distance generally increases as the camera yaw angle increases.,"The Bernoulli parameter is a measure of the probability of a pixel being foreground or background. The higher the Bernoulli parameter, the more likely the pixel is to be foreground. This is reflected in the images, where the pixels with higher Bernoulli parameters are more likely to be part of the person's silhouette.",text_model_embed100_hidden50
1901.00398v2," Which type of review was more accurately identified by the human evaluators, human-written or machine-generated? ",521,417,The human evaluators were more accurate at identifying human-written reviews than machine-generated reviews.,ChoiceNet.,text_model_embed100_hidden50
1608.02784v2,What is the role of the temperature variable t in the CCA decoding algorithm?,31,197,"The temperature variable t controls the probability of accepting a new candidate solution y. As t decreases, the probability of accepting a worse solution decreases.",The 3D ConvNet is used to extract features from the input images. These features are then used to generate place-based feature descriptions.,text_model_embed100_hidden50
1707.06320v2,How do the word embeddings learned by the Cap2Img model compare to the original GloVe embeddings in terms of semantic similarity?,528,133,The word embeddings learned by the Cap2Img model outperform the original GloVe embeddings in terms of semantic similarity.,"BR-CSGAN consistently outperforms MRT on both Chinese-English and English-German translation tasks, achieving higher BLEU scores.

While both methods optimize similar objectives, BR-CSGAN uses a reinforcement learning procedure with a dynamic discriminator to maximize rewards for the generator. This dynamic feedback seems to be more effective than the static objective and random sampling approach used by MRT, leading to better translation performance.",text_model_embed100_hidden50
1707.01917v2,Which method achieves the highest accuracy on the Shootings dataset?,499,333,TFBA,UnCoRd-VG-E,text_model_embed100_hidden50
1803.04572v2,How do the temporal patterns of phenotype magnitude differ between sickle cell anemia and leukemia patients?,151,191,"The temporal patterns of phenotype magnitude differ between sickle cell anemia and leukemia patients in terms of both shape and magnitude. For sickle cell anemia patients, the patterns are generally smoother and more periodic, with lower overall magnitude. For leukemia patients, the patterns are more erratic and have higher overall magnitude.",The reconstructions are very similar to the original samples.,text_model_embed100_hidden50
1811.10673v1,How does the proposed method compare to H.264 in terms of MS-SSIM score at low bitrates?,398,402,The proposed method achieves significantly higher MS-SSIM scores than H.264 at bitrates below 10 Kbps.,The proposed model delivers significantly better visual quality at low bitrates than H.264.,text_model_embed100_hidden50
1811.10673v1,How does the quality of the reconstructed frames change as the resolution increases?,404,643,The quality of the reconstructed frames increases monotonically as the resolution increases.,"The discriminator takes as input a response and the N-best response candidates, and outputs the probability that the response is human-generated.",text_model_embed100_hidden50
1803.02750v3,Which topology has the highest transmission rate for GMap 100%?,115,387,Mesh,BDD100K,text_model_embed100_hidden50
1704.07854v4,How do the initial conditions of the simulations vary?,224,191,The initial conditions of the simulations vary in two dimensions: the position of the liquid drop along the x-axis (α1) and the size of the drop (α2).,The reconstructions are very similar to the original samples.,text_model_embed100_hidden50
1812.00281v3,How does HUMBI capture diverse appearance of human expressions?,441,574,"HUMBI includes 772 distinctive subjects across gender, ethnicity, age, clothing style, and physical condition, which generates diverse appearance of human expressions.","Square hashing is a process that uses two hash functions to map a source/destination pair to a bucket in a two-dimensional array. The first hash function, h_i(s), maps the source address to a row in the array, and the second hash function, h_i(d), maps the destination address to a column in the array. The intersection of the row and column is the bucket where the fingerprint is stored.",text_model_embed100_hidden50
1805.04687v2,Which weather condition has the highest classification accuracy?,377,360,Clear weather.,US-HC-MQ,text_model_embed100_hidden50
1703.04887v4,"Which model and configuration achieves the best performance on the Chinese-English translation task, and how much improvement does it offer compared to the baseline RNNSearch model?",134,509,The Transformer+BR-CSGAN model with λ=0.8 achieves the best performance on the Chinese-English translation task with an average BLEU score of 42.61. This represents an improvement of 0.81 BLEU points compared to the baseline RNNSearch model.,"The SYNONYMNET(Pairwise) model with Leaky Unit performs best on the PubMed + UMLS dataset, achieving an AUC of 0.9838 and a MAP of 0.9872. This is a statistically significant improvement over the DPE baseline, which achieved an AUC of 0.9513 and a MAP of 0.9623.",text_model_embed100_hidden50
1703.04887v4,What is the relationship between the number of Monte Carlo samples (N) and the translation performance of the BR-CSGAN model? Why is there a trade-off when choosing the value of N?,131,332,"The table and passage show that the translation performance of the BR-CSGAN model generally improves as the number of Monte Carlo samples (N) increases. However, this improvement plateaus after N reaches a certain point (around 20 in this case).

There is a trade-off when choosing the value of N because increasing N also increases the computational complexity and training time. While a higher N leads to more accurate reward estimations and better performance, it also requires more computational resources and longer training times. Therefore, choosing the optimal N involves balancing the desired performance with the available computational resources and time constraints.","The filter responses from the “conv1”, “conv2” and “conv3” layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB are more detailed and contain more information than the filter responses from the fCNN of a framework that utilizes depth data. This is because RGB images contain more information than depth images.",text_model_embed100_hidden50
1708.05239v3,How does the performance of the PE-N=5 sampler compare to the HMC sampler?,573,212,The PE-N=5 sampler performs better than the HMC sampler.,"Table 1 shows that CoAP has slightly higher reliability (99.5%) compared to TCPlp (99.3%). While both protocols perform well, this difference could be attributed to several factors, including:

Retransmission mechanisms: CoAP employs a built-in retransmission mechanism for lost packets, while TCPlp relies on the underlying network layer for retransmissions. This could give CoAP an edge in recovering lost packets and achieving higher reliability.
Congestion control: CoAP includes mechanisms to adapt to network congestion, potentially reducing packet loss and improving reliability.
Packet size: CoAP typically uses smaller packets compared to TCPlp. Smaller packets are less prone to loss in wireless networks, potentially contributing to CoAP's slightly higher reliability.",text_model_embed100_hidden50
1805.06447v3,How does the ITN framework generate pseudo-negative samples?,470,316,The ITN framework generates pseudo-negative samples by applying learned transformations to positive samples.,The encoder understands the last user utterance by using the memory cell representations of the dialog history and KB tuples.,text_model_embed100_hidden50
1701.03077v10,"What is the effect of replacing the loss function in the ""Baseline"" network with the ""adaptive"" loss over wavelet coefficients?",43,289,"Replacing the loss function in the ""Baseline"" network with the ""adaptive"" loss over wavelet coefficients results in significantly improved depth estimates.","The shared weights allow the two branches of the network to learn similar representations of the input images. This helps to improve the performance of the Euclidean Confusion loss, which measures the distance between the conditional probability distributions of the two branches.",text_model_embed100_hidden50
1611.05742v3,"Which method performs best on the PaSC dataset for the handheld testing scenario (PaSC2), and how does its performance compare to other methods?",7,526,"The method that performs best on the PaSC dataset for the handheld testing scenario (PaSC2) is SPDNet, with an accuracy of 72.83%. This performance is slightly higher than GrNet-2Blocks (72.76%) and significantly higher than other methods like VGGDeepFace (68.24%) and DeepO2P (60.14%).","GroundSent-Cap appears to be most beneficial for the MRPC task, achieving an accuracy of 72.9/82.2 compared to the baseline model ST-LN's 69.6/81.2.",text_model_embed100_hidden50
1707.01922v5,What is the difference between testing domain adaptation and testing sensor fusion?,515,359,"In testing domain adaptation, the source and target CNNs are trained on different domains, and the joint classifier is used to predict the class of the target data. In testing sensor fusion, the source and target CNNs are trained on the same domain, and the joint classifier is used to predict the class of the target data using both the source and target data.",The user study is designed to test which of two images is closer to a reference video.,text_model_embed100_hidden50
1611.02654v2,What can you say about the relationship between the sentences in a document based on the t-SNE embeddings?,36,231,Sentences that are closer together in the embedding space are more semantically similar than those that are farther apart.,The CC score increases as the number of fixations increases.,text_model_embed100_hidden50
1804.07849v4,"Which method achieved the highest average V-measure (VM) across all languages, and how much higher was its average compared to the Baum-Welch method?",255,579,"The Variational $\wh{J}^{\mathrm{var}}$ method achieved the highest average VM score (50.4). Its average score is 39.6 points higher than the Baum-Welch method, which achieved an average VM score of 10.8.",The graph for the Caida-networkflow dataset shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method.,text_model_embed100_hidden50
1811.08257v1,How is convolution represented in the frequency domain?,290,345,"In the frequency domain, convolution is represented by element-wise multiplication.","The proposed method utilizes Light-CNN as both the source of the identity features and face verification loss. This allows the method to transfer the appearance of eyes, eyebrows, hairs, etc., while keeping other factors intact, e.g., head pose, shape of face, and facial expression.",text_model_embed100_hidden50
1706.00633v4,Which combination of training procedure and thresholding metric consistently performs the best across both MNIST and CIFAR-10 datasets for all attack types?,389,477,RCE training combined with the K-density metric consistently performs the best across both MNIST and CIFAR-10 datasets for all attack types.,"The ITN (B-CNN) method with data augmentation (DA) performs best on the TMTA task, achieving a testing error of 21.31%. Data augmentation contributes significantly to its performance, as the ITN (B-CNN) method without DA has a higher testing error of 31.67%.",text_model_embed100_hidden50
1706.03847v3,What is the performance of GRU4Rec relative to the baseline in terms of watch time?,447,60,GRU4Rec has a slightly higher performance than the baseline in terms of watch time.,"The passage mentions that additional trajectories were generated for the ""Pick and Place"" task by reducing the frequency of the recorded demonstrations. This process was not applied to the ""Push to Pose"" task, therefore no ""Demonstrations after shift"" are listed for it.",text_model_embed100_hidden50
1710.05654v2,"How well do the approximate bounds of $\theta$ predict sparsity in the ""spherical"" dataset?",613,132,"The approximate bounds of $\theta$ are very effective at predicting sparsity in the ""spherical"" dataset.",The BLEU score decreases as the initial accuracy of the discriminator increases.,text_model_embed100_hidden50
1811.08257v1,What is the function of the DataPreprocessing function?,293,359,"The DataPreprocessing function performs Yao Sharing, which is a cryptographic technique for securely sharing data between multiple parties.",The user study is designed to test which of two images is closer to a reference video.,text_model_embed100_hidden50
1709.02755v5,"Based on the table, how does the training process handle large vocabulary sizes?",588,412,"The training process uses several techniques to handle large vocabulary sizes. These include:

1. **Token-based batching:** Instead of grouping sentences of similar lengths together, the training process batches together a fixed number of tokens (5120 tokens per batch). This approach ensures that the model sees a consistent amount of vocabulary regardless of sentence length variation.
2. **Shared embedding:** This technique maps both source and target words to the same embedding space, effectively reducing the memory footprint needed to store word representations. 
3. **Positional encoding:** This method injects information about the position of words in a sentence into the model, helping it better understand long-range dependencies within the text. ",The Mean-Shift algorithm is robust to outliers.,text_model_embed100_hidden50
1705.09882v2,What is the difference between the grayscale depth representation and the result after background subtraction?,327,193," The grayscale depth representation shows the depth of each pixel in the image, with darker pixels representing closer objects and lighter pixels representing further objects. The result after background subtraction shows only the foreground object, with the background removed.","The representation task takes an appearance as input and outputs an RGB value, while the learning-to-learn task takes an image as input and outputs a DAM representation.",text_model_embed100_hidden50
1605.07496v3,"Between WSN and ALOQ, which method is the most efficient in terms of runtime for both F-SRE1 and F-SRE2?",69,127,ALOQ is significantly more efficient than WSN.,LSTNet,text_model_embed100_hidden50
1704.05958v2,Can you estimate the percentage of entity pairs in the NYT training set that have a corresponding relational fact in the Knowledge Base (KB)?,160,425,Approximately 6.66%.,"ChoiceNet generally performed better than MDN in the HalfCheetah task. This is evident from the higher average returns of ChoiceNet across all outlier percentages (10%, 20%, and 30%).

The performance gap between ChoiceNet and MDN appears to decrease as the percentage of outliers increases. At 10% outliers, ChoiceNet has a significantly higher average return than MDN (2068.14 vs. 192.53). However, at 30% outliers, the difference in average return is smaller (2035.91 vs. 363.08).",text_model_embed100_hidden50
1705.02946v3,What is the upper bound on the query complexity for finding an ε-perfect allocation with minimum cuts for 3 or more players?,267,487,O(n^3 / ε),"The predicted frame is generated by the prediction model, while the reconstructed frame is generated by the autoencoder. The predicted frame is typically more accurate than the reconstructed frame, as the prediction model is trained to predict the future state of the environment, while the autoencoder is only trained to reconstruct the input image.",text_model_embed100_hidden50
1705.09296v2,How does the model capture different perspectives on immigration when considering tone as a covariate?,311,594,"The model captures different perspectives on immigration by highlighting contrasting words associated with the same topic, depending on whether the tone is anti-immigration or pro-immigration.",The Bijective model performs the best on the STS16 task with unsupervised training.,text_model_embed100_hidden50
1703.04887v4,What is the role of the discriminator (D) in the proposed BR-CSGAN model?,135,29," The discriminator (D) is responsible for distinguishing between real sentence pairs translated by humans and generated sentence pairs produced by the generator (G). It provides feedback to G in the form of rewards, helping G improve its ability to generate realistic sentence pairs.",The singular value decomposition step is used to find the projection matrices U and V.,text_model_embed100_hidden50
1804.07849v4,"Which method achieved the highest accuracy on the 45-tag Penn WSJ dataset, and how does its performance compare to the other methods?",253,633,"The Variational $\wh{J}^{\mathrm{var}}$ method achieved the highest accuracy of 78.1% on the 45-tag Penn WSJ dataset. This is significantly higher than all other methods listed in the table, with the next best performing method (Berg-Kirkpatrick et al., 2010) achieving an accuracy of 74.9%.","The AdaQA (two-way) + att. model achieves the best performance on the SelQA dataset with a MAP score of 0.9021 and an MRR score of 0.9103. Compared to the baseline CNN model from Jurczyk et al. (2016) which has a MAP score of 0.8320 and an MRR score of 0.8420, the AdaQA (two-way) + att. model demonstrates a significant improvement in both metrics.",text_model_embed100_hidden50
1705.09966v2,"Which of the methods among Conditional GAN, Unsupervised GAN and Consitional CycleGAN would you expect to produce images that are most visually similar to the real images in the CelebA dataset?",340,44,The Conditional CycleGAN method is expected to produce images most visually similar to the real images.,"The ""adaptive $\power \in (0, 2)$"" strategy, where each wavelet coefficient has its own shape parameter that is optimized during training, achieved the best performance in terms of average error. It reduced the average error by approximately 17% compared to the reproduced baseline.",text_model_embed100_hidden50
1805.06431v4,How does the performance of the different models change as the corruption level increases? Which model appears to be the most robust to label corruption?,419,35,"As the corruption level increases, the performance of all models decreases. However, ChoiceNet consistently outperforms both ConvNet and ConvNet+Mixup across all corruption levels, maintaining high accuracy even when almost half of the labels are incorrect. This suggests that ChoiceNet is significantly more robust to label corruption compared to the other models.","The proposed model in this paper achieves the best performance for the order discrimination task on the Accidents dataset with an accuracy of 0.944. It outperforms the other data-driven approaches, namely Window (Recurrent) with 0.840, Window (Recursive) with 0.864, and Seq2seq with 0.930.",text_model_embed100_hidden50
1811.02721v3,"Based on Table 1 and the passage, why does TCP perform poorly on IEEE 802.15.4 networks compared to other network types listed? ",203,509,"TCP performs poorly on IEEE 802.15.4 networks because the Maximum Transmission Unit (MTU) for these networks is significantly smaller than other network types. This small MTU size results in a high percentage of overhead due to the TCP/IP headers, exceeding 50%. ","The SYNONYMNET(Pairwise) model with Leaky Unit performs best on the PubMed + UMLS dataset, achieving an AUC of 0.9838 and a MAP of 0.9872. This is a statistically significant improvement over the DPE baseline, which achieved an AUC of 0.9513 and a MAP of 0.9623.",text_model_embed100_hidden50
