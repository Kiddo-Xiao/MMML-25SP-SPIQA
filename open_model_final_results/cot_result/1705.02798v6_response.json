{
  "0": {
    "question": "Which component of the model seems to have the biggest impact on the F1 score on SQuAD dataset, and how much does removing it affect the score?",
    "ground_truth": "The DCRL training method appears to have the biggest impact on the F1 score. Removing it leads to a drop of 0.9 points in F1, which is the largest decrease observed for any single component in the ablation study.",
    "selected_figures": [
      "1705.02798v6-Table4-1.png",
      "1705.02798v6-Table3-1.png",
      "1705.02798v6-Table2-1.png",
      "1705.02798v6-Figure1-1.png",
      "1705.02798v6-Table1-1.png",
      "1705.02798v6-Figure5-1.png",
      "1705.02798v6-Table5-1.png",
      "1705.02798v6-Figure2-1.png",
      "1705.02798v6-Figure3-1.png",
      "1705.02798v6-Figure4-1.png"
    ],
    "answer": "The BiDAF + Self Attention + ELMo component has the biggest impact on the F1 score on the SQuAD dataset, and removing it affects the score significantly. <s>"
  },
  "1": {
    "question": "Which model performs the best on the AddOneSent dataset in terms of F1 score?",
    "ground_truth": "R.M.-Reader.",
    "selected_figures": [
      "1705.02798v6-Table4-1.png",
      "1705.02798v6-Table3-1.png",
      "1705.02798v6-Table2-1.png",
      "1705.02798v6-Figure1-1.png",
      "1705.02798v6-Table1-1.png",
      "1705.02798v6-Figure5-1.png",
      "1705.02798v6-Table5-1.png",
      "1705.02798v6-Figure2-1.png",
      "1705.02798v6-Figure3-1.png",
      "1705.02798v6-Figure4-1.png"
    ],
    "answer": "The model that performs the best on the AddOneSent dataset in terms of F1 score is Wang et al.[2017]2. <s>"
  },
  "2": {
    "question": "How does the performance of the single R.M-Reader model compare to the best single models of other approaches on the SQuAD test set?",
    "ground_truth": "The single R.M-Reader model achieves an EM score of 79.5% and an F1 score of 86.6% on the SQuAD test set. This performance is better than all other single models listed in the table, except for SLQA and Hybrid AoA Reader, which achieve slightly higher F1 scores of 87.0% and 87.3%, respectively.",
    "selected_figures": [
      "1705.02798v6-Table4-1.png",
      "1705.02798v6-Table3-1.png",
      "1705.02798v6-Table2-1.png",
      "1705.02798v6-Figure1-1.png",
      "1705.02798v6-Table1-1.png",
      "1705.02798v6-Figure5-1.png",
      "1705.02798v6-Table5-1.png",
      "1705.02798v6-Figure2-1.png",
      "1705.02798v6-Figure3-1.png",
      "1705.02798v6-Figure4-1.png"
    ],
    "answer": "The single R.M.-Reader model outperforms the best single models of other approaches on the SQuAD test set. <s>"
  },
  "3": {
    "question": "What are the two types of attention mechanisms used in the Reinforced Mnemonic Reader architecture?",
    "ground_truth": "The two types of attention mechanisms are reattention and self-attention.",
    "selected_figures": [
      "1705.02798v6-Table4-1.png",
      "1705.02798v6-Table3-1.png",
      "1705.02798v6-Table2-1.png",
      "1705.02798v6-Figure1-1.png",
      "1705.02798v6-Table1-1.png",
      "1705.02798v6-Figure5-1.png",
      "1705.02798v6-Table5-1.png",
      "1705.02798v6-Figure2-1.png",
      "1705.02798v6-Figure3-1.png",
      "1705.02798v6-Figure4-1.png"
    ],
    "answer": "The two types of attention mechanisms used in the Reinforced Mnemonic Reader architecture are BiDAF and Self Attention. <s>"
  },
  "4": {
    "question": "What is the purpose of the fusion modules in the interactive alignment and self-alignment modules?",
    "ground_truth": "The fusion modules are used to combine the outputs of the interactive alignment and self-alignment modules.",
    "selected_figures": [
      "1705.02798v6-Table4-1.png",
      "1705.02798v6-Table3-1.png",
      "1705.02798v6-Table2-1.png",
      "1705.02798v6-Figure1-1.png",
      "1705.02798v6-Table1-1.png",
      "1705.02798v6-Figure5-1.png",
      "1705.02798v6-Table5-1.png",
      "1705.02798v6-Figure2-1.png",
      "1705.02798v6-Figure3-1.png",
      "1705.02798v6-Figure4-1.png"
    ],
    "answer": "The purpose of the fusion modules in the interactive alignment and self-alignment modules is to combine information from multiple sources to improve the overall performance of the model. <s>"
  },
  "5": {
    "question": "How does reattention affect the redundancy and deficiency of attention distributions? Can you explain the observed differences in the impact of reattention on different blocks?",
    "ground_truth": "This paper shows that reattention helps alleviate both redundancy and deficiency in attention distributions.\n\nRedundancy: Reattention increases the KL divergence between adjacent attention blocks, indicating that the attention distributions across blocks become more distinct and less redundant.\nDeficiency: Reattention reduces the KL divergence between the normalized attention distribution ($E^t$) and the ideal uniform distribution (${E^t}^*$), suggesting that the attention becomes more balanced and closer to the desired distribution.\nHowever, the improvement in redundancy is more pronounced between the first two blocks ($E^1$ to $E^2$) than the last two blocks ($B^2$ to $B^3$). This suggests that the first reattention is more effective in capturing word pair similarities using the original word representations. In contrast, the later reattention might be negatively impacted by the highly non-linear word representations generated in the previous layers.",
    "selected_figures": [
      "1705.02798v6-Table4-1.png",
      "1705.02798v6-Table3-1.png",
      "1705.02798v6-Table2-1.png",
      "1705.02798v6-Figure1-1.png",
      "1705.02798v6-Table1-1.png",
      "1705.02798v6-Figure5-1.png",
      "1705.02798v6-Table5-1.png",
      "1705.02798v6-Figure2-1.png",
      "1705.02798v6-Figure3-1.png",
      "1705.02798v6-Figure4-1.png"
    ],
    "answer": "Reattention is a mechanism used in the Reinforced Mnemonic Reader (RMR) architecture to improve the performance of the BiDAF + Self Attention + ELMo (BSE) model on the SQuAD (Stanford Question Answering Dataset) task. The reattention mechanism allows the model to refine the attention distribution by attending to both the query and the context simultaneously. By doing this, the model is able to reduce the redundancy and deficiency of the attention distributions, leading to improved performance on the SQuAD task.\n\nThe observed differences in"
  }
}