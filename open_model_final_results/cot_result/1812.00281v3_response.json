{
  "0": {
    "question": "What is the relationship between the camera yaw angle and the silhouette distance?",
    "ground_truth": "The silhouette distance generally increases as the camera yaw angle increases.",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Figure8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "The relationship between the camera yaw angle and the silhouette distance is that a larger camera yaw angle leads to a larger silhouette distance. <s>"
  },
  "1": {
    "question": "How does HUMBI compare to other datasets in terms of the number of subjects?",
    "ground_truth": "HUMBI has the highest number of subjects compared to the other datasets.",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Figure2-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Figure8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "HUMBI has 772 distinctive subjects. <s>"
  },
  "2": {
    "question": "Which dataset has the most concentrated distribution of gaze and head pose?",
    "ground_truth": "MPII-Gaze",
    "selected_figures": [
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "1812.00281v3-Table4-1.png: Table 4: The mean error of 3D gaze prediction for the cross-data evaluation (unit: degree). 1812.00281v3-Table7-1.png: Table 7: Cross-data evaluation results of 3D body keypoint prediction. Metric is AUC of PCK calculated over an error range of 0-150 mm. 1812.00281v3-Table8-1.png:"
  },
  "3": {
    "question": "What are the differences between the results of the monocular 3D body prediction network trained with different dataset combinations?",
    "ground_truth": "The results of the monocular 3D body prediction network trained with different dataset combinations show that the Up3d+HUMBI dataset combination produces the most accurate results. This is evident in the images where the predicted 3D body poses are closer to the ground-truth poses than the other dataset combinations.",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Figure2-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Figure8-1.png",
      "1812.00281v3-Table6-1.png",
      "1812.00281v3-Figure3-1.png",
      "1812.00281v3-Figure4-1.png"
    ],
    "answer": "The differences between the results of the monocular 3D body prediction network trained with different dataset combinations refer to the accuracy and robustness of the predicted 3D body shape and pose, as well as the quality of the garment reconstruction. \n\nThe training setup for 3D mesh prediction from a single image involves the use of a convolutional neural network (CNN) to predict the 3D body shape and pose from a single image. The authors of this paper evaluate the performance of this network by comparing the results generated by training it on different combinations of datasets. These datasets include HUMBI (H"
  },
  "4": {
    "question": "What are the different stages of HUMBI body and cloth reconstruction?",
    "ground_truth": "The different stages of HUMBI body and cloth reconstruction are: \n1. Input image of the person (Ibody)\n2. Keypoint estimation (Kbody)\n3. Occupancy map generation (Obody)\n4. Body model fitting (Mbody)\n5. Cloth model fitting (Mcloth)",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Figure2-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Figure8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "The stages of HUMBI body and cloth reconstruction include:\n\n1. 3D mesh prediction from a single image\n2. Human body expression datasets\n3. Body and cloth reconstruction results\n4. Camera-ablation study to evaluate the accuracy of the garment reconstruction\n5. Cross-data evaluation results of 3D body keypoint prediction\n6. The mean error of 3D body mesh prediction for cross-data evaluation\n7. The mean error of 3D gaze prediction for the cross-data evaluation\n8. The training setup for 3D mesh prediction from a single"
  },
  "5": {
    "question": "Which dataset performs best when used alone for training a 3D body keypoint prediction model, and how does its performance compare to models trained on combined datasets?",
    "ground_truth": "HUMBI performs best when used alone for training, with an average AUC of 0.399. While this is lower than the average AUC of models trained on combined datasets (0.433 for H36M+HUMBI and 0.413 for MI3D+HUMBI), HUMBI still achieves the highest score among the individual datasets.",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Figure2-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Figure8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "The HUMBI dataset performs best when used alone for training a 3D body keypoint prediction model, and its performance is significantly better than models trained on combined datasets. <s>"
  },
  "6": {
    "question": "Which training data configuration resulted in the lowest prediction error for both UP-3D and HUMBI test sets?",
    "ground_truth": "Training with UP-3D + HUMBI resulted in the lowest prediction error for both UP-3D and HUMBI test sets.",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "The training setup for 3D mesh prediction from a single image resulted in the lowest prediction error for both UP-3D and HUMBI test sets. <s>"
  },
  "7": {
    "question": "How does the number of cameras used affect the accuracy of the garment reconstruction?",
    "ground_truth": "The accuracy of the garment reconstruction increases as the number of cameras used increases.",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Figure8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "The number of cameras used affects the accuracy of the garment reconstruction in terms of the density and the accuracy. <s>"
  },
  "8": {
    "question": "How does HUMBI capture diverse appearance of human expressions?",
    "ground_truth": "HUMBI includes 772 distinctive subjects across gender, ethnicity, age, clothing style, and physical condition, which generates diverse appearance of human expressions.",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "HUMBI captures diverse appearance of human expressions by training a deep neural network on a large and diverse dataset of human body shapes, poses, and expressions. The network is trained to predict the 3D structure of the human body from a single image, allowing it to capture subtle variations in body shape, pose, and expression. The network's ability to generalize to new, unseen images is enhanced by the diversity of the training data, which includes a wide range of body shapes, poses, and expressions. \n\nIn summary, HUMBI captures diverse appearance of human"
  },
  "9": {
    "question": "What is the difference between the \"median appearance\" and the \"view-specific appearance\"?",
    "ground_truth": "The median appearance is the average of all the multiview images, while the view-specific appearance is a single image that is rendered from a specific viewpoint.",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Figure2-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Figure8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "The difference between the \"median appearance\" and the \"view-specific appearance\" is that the \"median appearance\" refers to the average appearance of all subjects, while the \"view-specific appearance\" refers to the unique appearance of each subject in a specific view. <s>"
  },
  "10": {
    "question": "What is the purpose of the decoder in the 3D mesh prediction pipeline?",
    "ground_truth": "The decoder is responsible for generating the final 3D mesh from the intermediate representations produced by the regression network.",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Figure8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "The purpose of the decoder in the 3D mesh prediction pipeline is to generate a 3D mesh from a 2D image. <s>"
  },
  "11": {
    "question": "Which dataset provides data for **both** facial expressions and full-body motion capture, including clothing, in a natural setting (not synthesized)? ",
    "ground_truth": "HUMBI is the only dataset that provides data for both facial expressions and full-body motion capture, including clothing, in a natural setting. ",
    "selected_figures": [
      "1812.00281v3-Figure5-1.png",
      "1812.00281v3-Figure6-1.png",
      "1812.00281v3-Table2-1.png",
      "1812.00281v3-Figure15-1.png",
      "1812.00281v3-Figure16-1.png",
      "1812.00281v3-Table4-1.png",
      "1812.00281v3-Table3-1.png",
      "1812.00281v3-Figure12-1.png",
      "1812.00281v3-Table1-1.png",
      "1812.00281v3-Figure11-1.png",
      "1812.00281v3-Table7-1.png",
      "1812.00281v3-Table8-1.png",
      "1812.00281v3-Table6-1.png"
    ],
    "answer": "The HUMBI dataset provides data for both facial expressions and full-body motion capture, including clothing, in a natural setting (not synthesized). <s>"
  }
}