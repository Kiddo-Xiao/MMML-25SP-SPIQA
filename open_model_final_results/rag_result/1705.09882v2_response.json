{"0": {"question": "Which method achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset, and how does it compare to the best single-shot method on the same dataset?", "response": {"1705.09882v2-Table1-1.png": ["10.6% <s>", "Multi-Shot Person Re-identification on DPI-T, BIWI and IIT PAVIS <s>"]}, "referred_figures_names": ["1705.09882v2-Table1-1.png"], "answer": "The proposed method with RTA attention achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset with a score of 50.0%. This is significantly higher than the best single-shot method on the same dataset, which is our method (CNN) with a score of 25.4%."}, "1": {"question": "How does the proposed split-rate RGB-to-Depth transfer scheme differ from the R3D [90] method of Yosinski et al.?", "response": {"1705.09882v2-Figure4-1.png": ["Yes <s>", "The proposed split-rate RGB-to-Depth transfer scheme differs from the R3D [90] method of Yosinski et al. in several ways, summarized in the table below:  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |"]}, "referred_figures_names": ["1705.09882v2-Figure4-1.png"], "answer": "The proposed split-rate RGB-to-Depth transfer scheme differs from the R3D [90] method in two ways. First, the proposed method uses a different learning rate for the bottom three layers of the network. Second, the proposed method uses a different initialization for the weights of the bottom three layers of the network."}, "2": {"question": "What is the difference between the grayscale depth representation and the result after background subtraction?", "response": {"1705.09882v2-Figure2-1.png": ["1 <s>", "Background subtraction <s>"]}, "referred_figures_names": ["1705.09882v2-Figure2-1.png"], "answer": " The grayscale depth representation shows the depth of each pixel in the image, with darker pixels representing closer objects and lighter pixels representing further objects. The result after background subtraction shows only the foreground object, with the background removed."}, "3": {"question": "Which modality achieved the highest top-1 accuracy in the multi-shot evaluation on TUM-GAID?", "response": {"1705.09882v2-Table2-1.png": ["10 <s>", "Top-1 re-identification accuracy (top-1, %) and normalized Area Under the Curve (nAUC, %) on TUM-GAID in newclothes scenario with single-shot (ss) and multi-shot (ms) evaluation <s>"]}, "referred_figures_names": ["1705.09882v2-Table2-1.png"], "answer": "Body Depth & Head RGB (ms: LSTM & RTA)"}, "4": {"question": "What is the relationship between the Bernoulli parameter and the image?", "response": {"1705.09882v2-Figure6-1.png": ["1 <s>", "Bernoulli parameter <s>"]}, "referred_figures_names": ["1705.09882v2-Figure6-1.png"], "answer": "The Bernoulli parameter is a measure of the probability of a pixel being foreground or background. The higher the Bernoulli parameter, the more likely the pixel is to be foreground. This is reflected in the images, where the pixels with higher Bernoulli parameters are more likely to be part of the person's silhouette."}, "5": {"question": "Which part of the model is responsible for deciding which frames are most important for the re-identification task?", "response": {"1705.09882v2-Figure3-1.png": ["Yes <s>", "The recurrent layer (fLSTM) <s>"]}, "referred_figures_names": ["1705.09882v2-Figure3-1.png"], "answer": "The Reinforced Temporal Attention (RTA) unit."}, "6": {"question": "How does the performance of our RGB-to-Depth transfer compare to Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned?", "response": {"1705.09882v2-Figure5-1.png": ["10. 92 <s>", "Fine-tuned <s>"]}, "referred_figures_names": ["1705.09882v2-Figure5-1.png"], "answer": "The proposed RGB-to-Depth transfer performs slightly better than Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned."}, "7": {"question": "What is the difference between the filter responses from the \u201cconv1\u201d, \u201cconv2\u201d and \u201cconv3\u201d layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB and the feature embedding fCNN of a framework that utilizes depth data?", "response": {"1705.09882v2-Figure1-1.png": ["Yes <s>", "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |"]}, "referred_figures_names": ["1705.09882v2-Figure1-1.png"], "answer": "The filter responses from the \u201cconv1\u201d, \u201cconv2\u201d and \u201cconv3\u201d layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB are more detailed and contain more information than the filter responses from the fCNN of a framework that utilizes depth data. This is because RGB images contain more information than depth images."}}