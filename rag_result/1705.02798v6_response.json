{"0": {"question": "Which component of the model seems to have the biggest impact on the F1 score on SQuAD dataset, and how much does removing it affect the score?", "response": {"1705.02798v6-Table4-1.png": ["1 <s>", "Residual LSTM (RLSTM) has the biggest impact on the F1 score on the SQuAD dataset, and removing it affects the score by 0.5 points. <s>"]}, "referred_figures_names": ["1705.02798v6-Table4-1.png"], "answer": "The DCRL training method appears to have the biggest impact on the F1 score. Removing it leads to a drop of 0.9 points in F1, which is the largest decrease observed for any single component in the ablation study."}, "1": {"question": "Which model performs the best on the AddOneSent dataset in terms of F1 score?", "response": {"1705.02798v6-Table3-1.png": ["1 <s>", "Huang et al.[2017]5 <s>"]}, "referred_figures_names": ["1705.02798v6-Table3-1.png"], "answer": "R.M.-Reader."}, "2": {"question": "How does the performance of the single R.M-Reader model compare to the best single models of other approaches on the SQuAD test set?", "response": {"1705.02798v6-Table2-1.png": ["1 <s>", "BSE refers to BiDAF + Self Attention + ELMo <s>"]}, "referred_figures_names": ["1705.02798v6-Table2-1.png"], "answer": "The single R.M-Reader model achieves an EM score of 79.5% and an F1 score of 86.6% on the SQuAD test set. This performance is better than all other single models listed in the table, except for SLQA and Hybrid AoA Reader, which achieve slightly higher F1 scores of 87.0% and 87.3%, respectively."}, "3": {"question": "What are the two types of attention mechanisms used in the Reinforced Mnemonic Reader architecture?", "response": {"1705.02798v6-Figure3-1.png": ["1) Reattention <s>", "Interactive Attention and Reattention <s>"]}, "referred_figures_names": ["1705.02798v6-Figure3-1.png"], "answer": "The two types of attention mechanisms are reattention and self-attention."}, "4": {"question": "What is the purpose of the fusion modules in the interactive alignment and self-alignment modules?", "response": {"1705.02798v6-Figure4-1.png": ["Yes <s>", "The purpose of the fusion modules in the interactive alignment and self-alignment modules is to combine information from multiple sources, such as the input image and caption, into a more comprehensive representation of the image. <s>"]}, "referred_figures_names": ["1705.02798v6-Figure4-1.png"], "answer": "The fusion modules are used to combine the outputs of the interactive alignment and self-alignment modules."}, "5": {"question": "How does reattention affect the redundancy and deficiency of attention distributions? Can you explain the observed differences in the impact of reattention on different blocks?", "response": {"1705.02798v6-Table5-1.png": ["Yes <s>", "<s>"]}, "referred_figures_names": ["1705.02798v6-Table5-1.png"], "answer": "This paper shows that reattention helps alleviate both redundancy and deficiency in attention distributions.\n\nRedundancy: Reattention increases the KL divergence between adjacent attention blocks, indicating that the attention distributions across blocks become more distinct and less redundant.\nDeficiency: Reattention reduces the KL divergence between the normalized attention distribution ($E^t$) and the ideal uniform distribution (${E^t}^*$), suggesting that the attention becomes more balanced and closer to the desired distribution.\nHowever, the improvement in redundancy is more pronounced between the first two blocks ($E^1$ to $E^2$) than the last two blocks ($B^2$ to $B^3$). This suggests that the first reattention is more effective in capturing word pair similarities using the original word representations. In contrast, the later reattention might be negatively impacted by the highly non-linear word representations generated in the previous layers."}}