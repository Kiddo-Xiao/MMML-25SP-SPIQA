{"0": {"question": "What are the different steps involved in reconstructing implicit warrants for argument reasoning comprehension?", "response": {"1708.01425v4-Figure2-1.png": ["1 2 <s>", "Reconstruction of implicit warrants for argument reasoning comprehension <s>"]}, "referred_figures_names": ["1708.01425v4-Figure2-1.png"], "answer": "The different steps involved in reconstructing implicit warrants for argument reasoning comprehension are:\n1. Sampling comments\n2. Stance annotation\n3. Reason span annotations\n4. Reason gist summarization\n5. Reason disambiguation\n6. Alternative warrant\n7. Alternative warrant validation\n8. Warrant for original claim\n9. Warrant validation"}, "1": {"question": "What is the relationship between the number of workers per \"expert\" and Cohen's kappa agreement for stance annotation?", "response": {"1708.01425v4-Figure3-1.png": ["1: 0.58 <s>", "Moderate to substantial agreement is achieved with 5 annotators and a threshold of 0.95. <s>"]}, "referred_figures_names": ["1708.01425v4-Figure3-1.png"], "answer": "The Cohen's kappa agreement for stance annotation increases as the number of workers per \"expert\" increases."}, "2": {"question": "Which approach performs best on the development set?", "response": {"1708.01425v4-Table2-1.png": ["1 <s>", "Humans <s>"]}, "referred_figures_names": ["1708.01425v4-Table2-1.png"], "answer": "Intra-warrant attention with context."}, "3": {"question": "Does formal training in reasoning, logic, or argumentation seem to have a significant effect on argument reasoning comprehension accuracy for people with graduate degrees?", "response": {"1708.01425v4-Figure4-1.png": ["Yes <s>", "No, it does not seem to have a significant effect on argument reasoning comprehension accuracy for people with graduate degrees. <s>"]}, "referred_figures_names": ["1708.01425v4-Figure4-1.png"], "answer": "No, it does not appear to have a significant effect."}, "4": {"question": "Which step in the methodology resulted in the largest decrease in the size of the dataset?", "response": {"1708.01425v4-Table1-1.png": ["1 <s>", "Step 6 <s>"]}, "referred_figures_names": ["1708.01425v4-Table1-1.png"], "answer": "Step 4, Reason disambiguation."}, "5": {"question": "How does the intra-warrant attention mechanism work?", "response": {"1708.01425v4-Figure5-1.png": ["1 <s>", "The attention vector for the warrant W1 is shown; the attention vector for W0 is constructed analogously. Grey areas represent a modification with additional context. <s>"]}, "referred_figures_names": ["1708.01425v4-Figure5-1.png"], "answer": "The intra-warrant attention mechanism uses a BiLSTM to encode the reason and claim, and then provides this encoded information as an attention vector to LSTM layers for each warrant. The attention vector allows the model to focus on specific parts of the reason and claim that are most relevant to each warrant."}}