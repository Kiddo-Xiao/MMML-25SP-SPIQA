{"0": {"question": "What is the role of the Joint Attention Module in the model?", "response": {"1803.06506v3-Figure2-1.png": ["Yes <s>", "The role of the Joint Attention Module is to predict the common concept for each image-phrase pair. <s>"]}, "referred_figures_names": ["1803.06506v3-Figure2-1.png"], "answer": "The Joint Attention Module takes the embedded image and phrase features as input and uses them to induce a parameterization for spatial attention. This spatial attention map is then used by the decoder to predict the common concept."}, "1": {"question": "How does the quality of the output heatmap change when the selected concept, predicted concept, and the real entity to be grounded are all aligned?", "response": {"1803.06506v3-Figure6-1.png": ["<s>", "<s>"]}, "referred_figures_names": ["1803.06506v3-Figure6-1.png"], "answer": "When the selected concept, predicted concept, and the real entity to be grounded are all aligned, the generated heatmap produces a good localization of the phrase."}, "2": {"question": "Why does the Semantic self-supervision model perform better on ReferIt (mask) compared to ReferIt (bbox), and how does this relate to the difference in performance between Visual Genome and Flickr30k datasets? ", "response": {"1803.06506v3-Table2-1.png": ["Yes <s>", "<s>"]}, "referred_figures_names": ["1803.06506v3-Table2-1.png"], "answer": "The Semantic self-supervision model performs better on ReferIt (mask) compared to ReferIt (bbox) because mask-based annotations are considered more precise and accurate for measuring localization performance than bounding boxes. Masks tightly encompass the specific region referred to by the phrase, whereas bounding boxes can include extraneous areas. \n\nThis relates to the difference in performance between Visual Genome and Flickr30k because both Visual Genome and ReferIt (mask) contain phrases that refer to very specific regions or non-salient objects. Precise localization is crucial for achieving high accuracy on these datasets. Flickr30k, on the other hand, annotates all bounding boxes referring to a phrase, leading to potentially less precise localization requirements and generally higher performance across methods. "}, "3": {"question": "How does the proposed method's attention map differ from the VGG16 feature map?", "response": {"1803.06506v3-Figure5-1.png": ["1 <s>", "The proposed method's attention map highlights areas of interest in the input image, while the VGG16 feature map focuses on specific features within the image. <s>"]}, "referred_figures_names": ["1803.06506v3-Figure5-1.png"], "answer": "The attention map generated by proposed method is able to localize regions that were weak or non-existent in the activations of the input maps, while the VGG16 feature map simply amplifies the activations present in VGG16 channels."}, "4": {"question": "Which loss type performs best when the concept batch size is 5k?", "response": {"1803.06506v3-Table3-1.png": ["1 <s>", "Mean Squared Error <s>"]}, "referred_figures_names": ["1803.06506v3-Table3-1.png"], "answer": "Independent and common concept"}, "5": {"question": "How does the performance of the model vary with respect to the bounding box area and the similarity of the concept with ImageNet classes?", "response": {"1803.06506v3-Figure3-1.png": ["<s>", "<s>"]}, "referred_figures_names": ["1803.06506v3-Figure3-1.png"], "answer": "The performance of the model increases with increasing bounding box area and decreases with increasing similarity of the concept with ImageNet classes."}, "6": {"question": "Which dataset would you expect to be the easiest for a model to localize phrases in, and why?", "response": {"1803.06506v3-Table1-1.png": ["1 <s>", "Table 1. Phrase-region related statistics for datasets used in evaluation. The numbers reflect the relative complexity of these datasets.. <s>"]}, "referred_figures_names": ["1803.06506v3-Table1-1.png"], "answer": "Flickr30k is likely the easiest dataset for a model to localize phrases in. \nFlickr30k has the shortest average phrase length (2.3 words) and the lowest average noun count (1.2) per phrase. This suggests that the phrases in this dataset are simpler and often directly refer to single objects present in the image. This makes the localization task easier, almost approaching a weakly supervised setting where the object to be localized is explicitly named in the phrase."}}