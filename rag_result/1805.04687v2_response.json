{"0": {"question": "How does increasing the training set size affect the performance of the lane marking and drivable area segmentation tasks?", "response": {"1805.04687v2-Table5-1.png": ["Yes <s>", "<s>"]}, "referred_figures_names": ["1805.04687v2-Table5-1.png"], "answer": "Increasing the training set size generally leads to improved performance for both lane marking and drivable area segmentation tasks."}, "1": {"question": "How does joint training with the object detection set affect instance segmentation performance, and what is the likely reason for this effect?", "response": {"1805.04687v2-Table6-1.png": ["Yes <s>", "Joint training with the object detection set improves instance segmentation significantly. <s>"]}, "referred_figures_names": ["1805.04687v2-Table6-1.png"], "answer": "Joint training with the object detection set improves instance segmentation performance significantly. This is evidenced by the increase in AP, AP50, and AP75 metrics in Table 1 when comparing \"Inst-Seg\" and \"Inst-Seg + Det\" rows.\n\nThe passage explains that this improvement is likely due to the richer diversity of images and object examples in the detection set. This allows the instance segmentation model to learn better object appearance features and localization."}, "2": {"question": "Based on Table 1, which category of objects has the largest total number of annotations in the BDD100K MOT dataset, considering both bounding boxes and instance tracks? ", "response": {"1805.04687v2-Table11-1.png": ["10 <s>", "<s>"]}, "referred_figures_names": ["1805.04687v2-Table11-1.png"], "answer": "Cars have the largest total number of annotations."}, "3": {"question": "How does the segmentation model perform in areas with no lane markings?", "response": {"1805.04687v2-Figure11-1.png": ["<s>", "The segmentation model learns to interpolate in areas that have no lane markings. <s>"]}, "referred_figures_names": ["1805.04687v2-Figure11-1.png"], "answer": "The segmentation model learns to interpolate in areas that have no lane markings."}, "4": {"question": "What is the most common type of scene in the dataset?", "response": {"1805.04687v2-Figure10-1.png": ["1 <s>", "Landscape <s>"]}, "referred_figures_names": ["1805.04687v2-Figure10-1.png"], "answer": "City Street"}, "5": {"question": "What are the different types of annotations that are included in the dataset?", "response": {"1805.04687v2-Figure1-1.png": ["Yes <s>", "Scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation. <s>"]}, "referred_figures_names": ["1805.04687v2-Figure1-1.png"], "answer": "The dataset includes a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation."}, "6": {"question": "Explain why the model trained on both MOT and detection sets has a higher number of identity switches (IDS) compared to the model trained only on the MOT set, even though it achieves better performance in terms of AP, MOTA, and MOTP?", "response": {"1805.04687v2-Table7-1.png": ["<s>", "The model trained on both MOT and detection sets has a higher number of identity switches (IDS) compared to the model trained only on the MOT set, even though it achieves better performance in terms of AP, MOTA, and MOTP. <s>"]}, "referred_figures_names": ["1805.04687v2-Table7-1.png"], "answer": "While the model trained on both MOT and detection sets shows improved performance in detection and tracking metrics (AP, MOTA, MOTP), it also exhibits a higher number of identity switches (IDS). This can be attributed to the increased diversity of instances introduced by the detection set. Although the MOT set provides a larger number of bounding boxes for training, the detection set adds varied examples that may lead to more frequent identity switches during tracking, even as it improves the model's overall performance."}, "7": {"question": "Which domain discrepancy has a larger impact on object detection performance: city vs. non-city or daytime vs. nighttime?", "response": {"1805.04687v2-Table4-1.png": ["1 <s>", "City vs. non-city <s>"]}, "referred_figures_names": ["1805.04687v2-Table4-1.png"], "answer": "Daytime vs. nighttime has a larger impact on object detection performance."}, "8": {"question": "Why does the proposed dataset have a lower number of persons per image compared to the Cityscapes dataset, even though it has a much higher total number of pedestrians?", "response": {"1805.04687v2-Table10-1.png": ["10 <s>", "The proposed dataset has a lower number of persons per image compared to the Cityscapes dataset, even though it has a much higher total number of pedestrians. <s>"]}, "referred_figures_names": ["1805.04687v2-Table10-1.png"], "answer": "The proposed dataset contains non-city scenes like highways, which typically have fewer pedestrians per image compared to cityscapes."}, "9": {"question": "How does the performance of lane marking detection change with different thresholds (\u03c4) for direction, continuity, and category?", "response": {"1805.04687v2-Table14-1.png": ["Yes <s>", "Threshold \u03c4 = 1, 2, 10 pixels of direction, continuity as well as each category. <s>"]}, "referred_figures_names": ["1805.04687v2-Table14-1.png"], "answer": "As the threshold (\u03c4) increases, the ODS-F scores for direction, continuity, and category generally increase as well. This indicates that the model performs better in detecting lane markings with higher thresholds, meaning it can tolerate larger deviations from the ground truth annotations."}, "10": {"question": "Which weather condition has the highest classification accuracy?", "response": {"1805.04687v2-Figure4-1.png": ["10 87 <s>", "100% 50% 60% 70% 80% 90% 100% 50% 60% 70% 80% 90% 100% 50% 60% 70% 80% 90% 100% 50% 60% 70% 80% 90% 100% 50% 60% 70% 80% 90% 100% 50% 60% 70% 80% 90% 100% 50% 60% 70% 80% 90% 100% 50% 60% 70% 80% 90% 100% 50% 60% 70% 80% 90% 100% 50% 60% 70% 80% 90% 100%"]}, "referred_figures_names": ["1805.04687v2-Figure4-1.png"], "answer": "Clear weather."}, "11": {"question": "Which type of object is the most common in the dataset?", "response": {"1805.04687v2-Figure14-1.png": ["10 <s>", "Cars <s>"]}, "referred_figures_names": ["1805.04687v2-Figure14-1.png"], "answer": "Cars are the most common object in the dataset."}, "12": {"question": "Based on the table, which approach achieved the highest mean IoU for semantic segmentation, and how did it perform compared to the baseline Sem-Seg model?", "response": {"1805.04687v2-Table8-1.png": ["1 <s>", "Mean Intersection Over Union (IoU) <s>"]}, "referred_figures_names": ["1805.04687v2-Table8-1.png"], "answer": "The **Sem-Seg + Det** approach achieved the highest mean IoU of 58.3, which is an improvement of 1.4 points compared to the baseline Sem-Seg model with a mean IoU of 56.9."}, "13": {"question": "Which training approach achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score?", "response": {"1805.04687v2-Table9-1.png": ["1 <s>", "MOTS <s>"]}, "referred_figures_names": ["1805.04687v2-Table9-1.png"], "answer": "The training approach \"Det + T + I + S\" achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score."}, "14": {"question": "Which category of objects has the highest total number of annotations, and is there evidence that this category might be more challenging to annotate accurately?", "response": {"1805.04687v2-Table12-1.png": ["1 <s>", "The category of objects that has the highest total number of annotations, and is there evidence that this category might be more challenging to annotate accurately? <s>"]}, "referred_figures_names": ["1805.04687v2-Table12-1.png"], "answer": "The category with the highest total number of annotations is \"Masks,\" with 129K annotations. There is evidence that this category might be more challenging to annotate accurately because it also has the highest number of annotations in the \"Occluded\" subcategory, indicating that a large portion of these objects are partially hidden in the images."}, "15": {"question": "What percentage of occlusions last for more than 10 frames?", "response": {"1805.04687v2-Figure8-1.png": ["10. 25 <s>", "10.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"]}, "referred_figures_names": ["1805.04687v2-Figure8-1.png"], "answer": "Approximately 80%"}, "16": {"question": "Why are MOTS datasets like KITTI MOTS and MOTS Challenge smaller in size compared to VOS datasets like YouTube VOS, even though BDD100K MOTS has a comparable number of annotations?", "response": {"1805.04687v2-Table3-1.png": ["1 <s>", "Because MOTS datasets are smaller in size <s>"]}, "referred_figures_names": ["1805.04687v2-Table3-1.png"], "answer": "MOTS datasets require denser annotations per frame because they involve both segmentation and tracking of multiple objects in crowded scenes. This results in smaller dataset sizes compared to VOS datasets, which typically focus on segmenting a single or fewer objects. "}, "17": {"question": "What are the three main geographical regions where the data for this study was collected?", "response": {"1805.04687v2-Figure2-1.png": ["1 <s>", "West Coast, East Coast, Midwest <s>"]}, "referred_figures_names": ["1805.04687v2-Figure2-1.png"], "answer": "New York, San Francisco Bay Area, and Berkeley."}, "18": {"question": "Which category of object is the least common in the dataset?", "response": {"1805.04687v2-Figure3-1.png": ["1 <s>", "(c) Truncated <s>"]}, "referred_figures_names": ["1805.04687v2-Figure3-1.png"], "answer": "Train"}, "19": {"question": "How does the BDD100K dataset compare to the KITTI and MOT17 datasets in terms of size and complexity?", "response": {"1805.04687v2-Table2-1.png": ["10k frames <s>", "The BDD100K dataset has more sequences, frames, identities as well as more box annotations than the KITTI and MOT17 datasets. <s>"]}, "referred_figures_names": ["1805.04687v2-Table2-1.png"], "answer": "The BDD100K dataset is significantly larger and more complex than both the KITTI and MOT17 datasets. It contains roughly 40 times more frames, 16 times more sequences, and 13 times more identities than KITTI. Compared to MOT17, BDD100K has about 10 times more frames, 80 times more sequences, and 8 times more identities. This increase in size and complexity makes BDD100K a more challenging and comprehensive benchmark for multiple object tracking algorithms. "}, "20": {"question": "Which dataset has the most lane marking annotations?", "response": {"1805.04687v2-Table1-1.png": ["10 756 <s>", "Dataset 1 <s>"]}, "referred_figures_names": ["1805.04687v2-Table1-1.png"], "answer": "BDD100K"}}